Reported Date,Resolved Date,Incident,Incident Start Time,Incident End Time,MTTR (hh:mm),Business Platform - Area of Cause,Product Portfolio -Area of cause,Impacted Business Platform,Impacted Product portfolio,Impacted business process,Inc Category,Description,Organisation,Team ,Financial Impact,Application/Service Impacted?,RC Status,Problem Record,Background,Next Steps / Solution,Initial RCA Date,Final RCA Date (Once the cause is fully known),Solution Identified Date,Planned Solution Deployment  Date,Actual Solution Deployment  Date,Does an alert already exist? Is configuration required/applicable?,Was the incident identified through the alert?,Does a  workaround exist ? Was it used?,Is Permanent Fix Deployed?,RC Category,Issue caused due to CR or WO,Root Cause,Reference CR/WO,Reason for failed change/issue,Risk ID/ Backlog id,Closed Date,Closed Month,Closed Year,Problem manager,Open/Closed,MIM,Problem actions - Added?  Y/N/NA,Year,Month #,Month Name,Current day,Ageing,Aging Bucket
4/19/2025,4/19/2025,90735114,13:20,15:10,01:50,GTS,Enterprise Technology Platform,"Customer Channels, Group Platforms, C&H and Intl","Selling Experience, Service Experience, Platform and Store Ops, HR, Finance, C&H and Intl Supply Chain",Manage Payment & Point of Sales in store,MI,"Store tills not accepting contactless Payment, Digital Cafe and C&C down",,,,,RC in progress,PBI000000072502,"Between 13:20 and 15:10, issues were reported with contactless payments and digital café operations. Contactless payments were impacted across all stores, and digital cafés were unable to serve customers on the kiosks. In-store applications such as Collect, In-Store Fulfilment, and Returns were unavailable, and colleagues were unable to log in to Honeywell devices. Additionally, the Salford Quays and Chester Contact Centers were unable to receive customer calls. Operations at the Swindon and Ollerton DCs were also affected. Investigations revealed a spike in Dynatrace sessions on the Stockley Park FortiGate firewall, leading to increased CPU utilization. Services were restored after disabling the IPS engine on the firewall. Root cause analysis is currently underway to determine the next steps and develop a plan for re-enabling the IPS engine.",,,,,,,,,,,,,,,,,,,,,Open,,,2025,4,April,9/16/2025,150,>60
4/17/2025,4/19/2025,90730450,4:00,12:00,56:00,Group Platforms,Finance,"Group Platforms, Data, C&H and Intl","Finance, Data, C&H Commercial Trading",,SI,"Delay in overnight GMOR, EDW and BEAM Reporting batches",SAP,NA,,,,PBI000000072503,"Alerting indicated a delay in the overnight GMOR batch completion at 04:00 due to performance issues within the SAP ECC system. This impacted the availability of GMOR Business Object reports, EDW day-1 finance reports, Foods daily board reports, BEAM daily C&H Goods receipt and Purchase order reports. The initial root cause is attributed to a high utilization of work processors in SAP ECC system caused by ECOM billing job. A restart of SAP ECC and SAP BW system was performed on 17/04, however, the performance improvements were observed since 18:00, 18/04 and the GMOR batches on 19/04 completed without any issues. Finance Day-1 reports were made available with a delay by 11:38 on 19/04. Performance of the SAP ECC system have remained stable since 18/04 and all overnight batches completed without any delay. Hypercare monitoring in place as we await updates from vendor SAP.",,,,,,,,,,,,,,,,,,,,,Open,,,2025,4,April,9/16/2025,152,>60
4/17/2025,4/17/2025,TBC,0:00,13:30,13:30,Customer Channels,Selling Experience,Customer Channels,"Customer engagement  &
Selling Experience",Manage Online Payment & Content ,MI,Promotion error live on .com Website,,,,,RC identified,,"Customers were placing orders with a 15% student promotion on the Customer UK website and mobile apps from 00:00. Around 11.7 K orders were placed with the incorrect promotion resulting in loss of revenue. The root cause was attributed incorrect 15% promotion setup at site level instead of products based promotion by the Online Promotion team. To mitigate the impact, the incorrect promotion was disabled across all the website pages and WCS nightly batch was executed to refresh pricing information in the site. Root cause investigations underway",,,,,,,,,,,,,,,,,,,,,Open,,,2025,4,April,9/16/2025,152,>60
4/15/2025,4/15/2025,90726388,,,00:00,Foods,Food Supply Chain,Foods,Food Supply Chain,Manage Allocation of Food Stock,KI,Delay in sending foods final orders to the suppliers,,,,,RC identified,,There was a delay in sending the foods final orders to suppliers impacting the suppliers picking operations. Root cause was attributed to a combination of a delay in Relex overnight batch completion and connectivity issues with the message processing within FOAL layer due to an underlying auto scaling configuration issue in the North Europe region. The FOAL system was failed over to the West Europe region and the supplier orders were sent at 06:30. The configuration for Auto-scaling was enabled in North Europe and FOAL system failed back to North Europe for resilience by 16:00. Overnight monitoring is in place.,,,,,,,,,,,,,,,,,,,,,Open,,,2025,4,April,9/16/2025,154,>60
4/10/2025,4/10/2025,90719020,9:26,10:07,00:41,Customer Channels,Service Experience,"Customer Channels, C&H and Intl","C&H and Intl Supply Chain, Platform and Store Ops, Service Experience",Manage Online Payment & Content ,MI,Order history unavailable for customers and colleagues impacting returns,Customer,OMS,See note,.com Orders,RC identified,72278,"Alerting indicated an API connectivity failure within the Order Management System (OMS) which prevented customers from accessing order history and initiate returns between 09:26 and 10:07. Store colleagues were unable to process returns using Honeywell and self-service kiosk tablets. Return operations at the Ollerton DC were affected, resulting in a capability loss of 460 orders and productivity loss of 10 hours. The root cause was attributed to a password change within the OMS database, which was reinstated to restore services.",,4/10/2025,,,,,,,,,Change Failure,,"
Root cause attributed to the password change activity, due to misunderstanding the activity was performed during a no change day window which will be reviewed and change management process will followed moving forward.
DIAG user account has read only access and used by OMS Support/Dev teams for the purpose of operational support (using Bastion) only. It was not known why/when it was configured in returns prod API, although the non-prod is configured to use a different account (stradmin).",,Knowledge gap,,,,,Gokul,Open,.Com SM,Y,2025,4,April,9/16/2025,159,>60
4/9/2025,4/9/2025,90716711,10:30,12:00,01:30,Customer Channels,Customer engagement,Customer Channels,Customer engagement,Manage Payment & Point of Sales in store,SI,Card payment failures across Travel Money Bureau stores,Verifone,NA,See note,Travel money exchange -Card Transactions,RC in progress,72394,"Customers were unable to exchange currency using their credit/debit cards across 47 Travel Bureau stores on the Verifone pin-pads between 10:30 and 12:00. However, as a workaround, customers were able to use cash or click and collect on the website. The initial root cause was attributed to a connectivity issue within Verifone's POS cloud. Awaiting root cause and resolution details from Verifone",,4/9/2025,,,,,,,,,External Dependencies,,The initial root cause was attributed to a connectivity issue within Verifone's POS cloud. Awaiting root cause and resolution details from Verifone,,,,,,,Naveen,Open,MIM,Y,2025,4,April,9/16/2025,160,>60
4/3/2025,4/3/2025,90704278,5:21,9:28,04:07,Business Process Services,Business Process Services,Customer Channels,"Customer engagement  &
Selling Experience",Manage Online Payment & Content ,MI,Search Services performance impacting search results & PLPs,Customer,BPS Team (Merch hub),"£115,000.00",.com Orders,RC identified,72356,Business colleagues highlighted that customers were unable to search products or view PLPs on the UK & ROI website across all channels between 05:21 and 09:28. Approx. 300 customers/min were impacted resulting in a poor customer experience & loss of sales. The root cause was attributed to a business error whilst amending a global rule configuration within Merch Hub application which was reverted to restore the services.,,4/3/2025,,,,,,,,,Operational Process Issues,,"A global “Include Only” rule was inadvertently implemented in Merch Hub by the business, which restricted the visibility of products to only those associated with the brand GOLA. This rule functioned as intended, applying a filter across all search results and product listing pages (PLPs) to exclusively display GOLA products",NA,NA,NA,,,,Gokul,Open,.Com SM,Y,2025,4,April,9/16/2025,166,>60
4/2/2025,4/7/2025,90702974,12:00,13:00,121:00,C&H and Intl ,C&H Commercial Trading, C&H and Intl,C&H Commercial Trading,Others,MI,C&H PLM (Product Lifecycle Management) application has been made unavailable due to a data issue,PTC,PTC,No Financial Impact,PLM (Product Lifecycle management),RC identified,72270,"C&H PLM application was unavailable to colleagues and suppliers, impacting product management activities between 12:00 and 22:44. This has also caused a cascading effect on cost prices, article updates, and seasonal changes sent to Domain Services, SSI, and Range Planner. The root cause was attributed to the data refresh activity across PLM non prod environments by vendor PTC resulting in flow of PLM non prod data into downstream systems. The impacted articles were retriggered by PTC and majority of the data recovery is completed based on the comparison with non-prod and further recovery actions will be performed on Monday. ",,4/2/2025,,,,,,,,,External Dependencies,,The root cause was attributed to the data refresh activity across PLM non prod environments by vendor PTC resulting in flow of PLM non prod data into downstream systems,NA,NA,NA,,,,Saloni,Open,MIM,,2025,4,April,9/16/2025,167,>60
4/2/2025,4/2/2025,90702108,8:40,14:00,05:20,GTS,Digital Workplace Services,"Customer Channels, Food, C&H and Intl, Group platforms","Data, C&H Supply Chain, Foods, Platform and Store Ops, Finance, Service Experience",Others,SI,Power BI Reports inaccessible due to Central Microsoft issue,Microsoft,Microsoft,No Financial Impact,"Compliance Reports, Foods Analytics Reporting, Stock Integrity & Finance accounting, DC planning activities",RC identified,72358,"Power BI reports were inaccessible across multiple business areas impacting Store Compliance Reports, Foods Analytics Reporting, Stock Integrity & Finance accounting, DC planning activities etc. The root cause was attributed to a central issue within Microsoft which was restored by Microsoft at 14:00. We await root cause & recovery updates from Microsoft.",,3/2/2025,,,,,,,,,External Dependencies,,"Microsoft updated that ""A recent deployment introduced a race condition which could cause system nodes to have a lower connection limit. This caused customer requests routed to the affected nodes to experience additional request latency or request timeouts.""",NA,NA,NA,,,,Balaji,Open,MIM,PIR scheduled on 8th,2025,4,April,9/16/2025,167,>60
4/1/2025,4/2/2025,90700168,16:44,15:57,23:13,Customer Channels,Service Experience,Customer Channels,"Platform and Store Ops, Service Experience",Others,KI,Missing customer information from parcels impacting C&C store operations,Customer,Click and Collect,See note,Click and collect,RC identified,72404,"Ecom DC operations reported an increase in number of orders that is missing billing information and the stand operation procedure was followed to fulfil these orders. Around 1507 orders were impacted. Stores colleagues and customer were unable to search the orders by name, however order IDs were used to find the orders by Store colleagues. The root cause has been attributed to a change made on 01/04 for click and collect customers, which resulted in the billing address being sent as blank to the order and warehouse management systems. The change was reverted to restore the services and to mitigate the impact, customer names for all impacted orders were provided to stores to aid in the collection process.",,4/1/2025,,,,,,,,,Change Failure,,"The root cause has been attributed to a change made on 01/04 for click and collect customers, which resulted in the billing address being sent as blank to the order and warehouse management systems",TBC,TBC,,,,,Kavitha,Open,.Com SM,PIR to be held ,2025,4,April,9/16/2025,168,>60
4/1/2025,4/1/2025,90698496,9:43,11:15,01:32,GTS,Enterprise Technology Platform,"Customer Channels, Group Platforms","Selling Experience, Service Experience, Data, Finance",Others,MI,Microsoft central outage impacting multiple services hosted within Northern Europe,Microsoft,Microsoft,TBC,"Drop in .com orders, Scan & Shop Mobile, Retail In-day Sales Dashboard, Stock Movement Tool, Power BI Reports, and SAP POSDTA application",RC in progress,72363,"Multiple applications/services hosted on Microsoft Azure North Europe region were impacted between 09:40 and 11:30. Customers were unable to place orders on the UK and Ireland website resulting in an order drop and loss of sales. Also, Scan & Shop Mobile, Retail In-day Sales Dashboard, Stock Movement Tool, Power BI Reports, and SAP POSDTA application were impacted. The root cause was attributed to an unplanned power outage at Microsoft’s North Europe Data Centre. Services were recovered after the power was restored. ",,4/1/2025,,,,,,,,,External Dependencies,,The root cause was attributed to an unplanned power outage at Microsoft’s North Europe Data Centre.,NA,NA,NA,,,,Rehan,Open,MIM,Y,2025,4,April,9/16/2025,168,>60
3/31/2025,TBC,90708017,06:00,TBC,TBC,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Service Experience ",Manage Online Payment & Content ,SI,Delay in payment processing  via Apple pay for large order value purchases,Customer,TBC,No Financial Impact,OMS,RC in progress,See Note,"Customers using the mobile app to pay via Apple Pay were receiving a ""payment failed"" message, even though the orders were successful. This issue affected approximately 100 orders per day. The root cause is related to latency in the order reservation processing time. A fix is scheduled for deployment on 15/04, which includes redeploying the previously reverted WCS release aimed at resolving this issue. Additionally, customer orders have not been placed successfully due to multiple scenarios, impacting around 350 orders per week. Further investigations are ongoing.",,TBC,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"The root cause is related to latency in the order reservation processing time. A fix is scheduled for deployment on 15/04, which includes redeploying the previously reverted WCS release aimed at resolving this issue. ",,,,,,,Naveen,Open,.Com SM,Incident in progress,2025,3,March,9/16/2025,169,>60
3/30/2025,4/1/2025,90694336,9:15,10:30,49:15,Customer Channels,Service Experience,Customer Channels,Platform and store ops,Others,KI,Some UK Mainchain and Outlet stores facing issues with incoming calls on their Ascom phones,Mitel,NA,No Financial Impact, ASCOM Phones,RC identified,72365,"Around 32 stores (UK Mainchain + outlet) reported issues whilst receiving incoming calls on ASCOM phones at stores. This impacted colleagues ability to address customer queries and also SOC , City FM and various helpdesks were unable to reach out to the stores causing inconvenience to colleagues. To mitigate the impact, all calls were routed to the secondary Mitel IP in Manchester at 18:18 on 31/03. The root cause was attributed to the addition of new SIP (Session Initiation Protocol) link to the Mitel Mi Voice Border Gateway (MBG) whilst testing the ROI calls from Sabio to Twilio resulting in an IP conflict. The permanent fix was applied by disabling the new SIP link on 10:30, 01/04 , followed by which the resilience has been restored.",,3/31/2025,,,,,,,,,External Dependencies,,. The root cause was attributed to the addition of new SIP (Session Initiation Protocol) link to the Mitel Mi Voice Border Gateway (MBG) whilst testing the ROI calls from Sabio to Twilio resulting in an IP conflic,NA,NA,,,,,Pavithra,Open,MIM,PIR to be held ,2025,3,March,9/16/2025,170,>60
3/29/2025,3/29/2025,90693741,12:15,16:54,04:39,Business Process Services,Business Process Services,"C&H and Intl, Customer Channels","C&H and Intl Supply Chain, Service Experience","Manage Pick & Allocation of Stock and Orders (Castle Donington), Manage Pick of C&H Stock, Manage Payment & Content Customer.Com",MI," Ecomm DC's (Donington, Ollerton and Brands) unable to view orders with delivery date on 30/03 in WMS",Customer,TBC - C&H Fulfilment Operations (Business),"£472,000.00","OMS, WMS",RC in progress,72329,"Ecomm DCs (Donington, Ollerton, and Brands) were unable to view orders scheduled for delivery on 30/03 between 06:00 and 16:54. Impact to customer miss promise is being ascertained. Order proposition was moved to +24 hours, hence customers were unable to place orders for delivery date 30/03 between 10:00 and 16:54. Approximately 10k orders had an incorrect delivery date of 31/03, a workaround was applied by the DC operations to process these orders.  The root cause was attributed to the system design in OMS and WMS unable to handle the 24 hours delivery lead time ahead of clock change. Services were restored by adjusting the ordering and carrier cut-off times in the Order Management System (OMS).",,3/31/2025,,,,,,,,,Software Defects/Limitation,,The root cause was attributed to the system design in OMS and WMS unable to handle the 24 hours delivery lead time ahead of clock change.,NA,NA,,,,,Pavithra,Open,MIM,,2025,3,March,9/16/2025,171,>60
3/26/2025,3/26/2025,TBC,07:30;11:30,08:15; 13:00,02:15,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Issues with pack benches & auto bagger impacting packing operations at Donington,Customer,Warehouse Platform,"£1,440.00",Packing operations,RC in progress,,"Castle Donington colleagues reported that pack benches were randomly logging out of WMS between 07:30 and 08:15. Additionally, the auto-bagger functionality, which depends on the pack benches to process orders, was disrupted between 11:30 and 13:00. Approx. 175 pack benches were impacted leading to a capability loss of 2.4k singles and a finance loss of £1,440. Root cause was attributed to a pack screen URL change which was reverted to restore the services.",,3/26/2025,,,,,,,,,Change Failure,, Root cause was attributed to a pack screen URL change which was reverted to restore the services.,Unauthorised change,TBC,NA,,,,Rehan,Open,MIM,,2025,3,March,9/16/2025,174,>60
3/25/2025,3/25/2025,90685299,05:00,5:29,00:29,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,MI,Customers unable to place orders across UK & IE website.,Customer,WCS,"£12,000.00",Website,RC identified,72326,"Customers were unable to place orders on UK and Ireland website across all channels between 05:00 and 05:29. There was order drop of 200 orders resulting in poor customer experience and finance loss of £12,000. The root cause is attributed to a human error resulting in configuring the website traffic to the incorrect WCS environment as part of the monthly WCS release. Services were restored by rerouting the traffic to the correct WCS origin.  ",,3/26/2025,,,,,,,,,Change Failure,,"The root cause was human error whilst making configuration changes for the GTM and while reviewing the changes that had been made.
There are seven property changes in the Akamai Configuration for Global Traffic Management (GTM) that need to be made in order to move the customer traffic between halls. 
This configuration has to be changed manually via Akamai GUI. However, instead of all 7 properties being modified correctly, only the first property was correctly changed. This resulted in only the first property routing traffic to Hall 1 (i.e. the hall which should receive all customer traffic whilst the deployment took place on hall 2).
2.Although a peer view of the configuration changes took place, instead of validating all 7 properties, only the first property was checked.  The reviewer saw that the 1st property was pointing to the correct hall and did not notice that the remaining 6 were pointing to the wrong hall. ",CR220912,Human Error,NA,,,,Balaji,Open,.Com SM,,2025,3,March,,,
3/21/2025,3/22/2025,90679421,13:30,6:38,17:08,Customer Channels,Service Experience,Various,"Service Experience , Platform & Store Ops , Data , Foods","Manage Payments & Point of Sales in store, Manage Allocation of Food Stock, other",MI,"Store tills offline to POS Beanstore impacting refunds, gift card transactions",Customer,POS Team,No Financial Impact,"POS tills, Refunds, Gift card transactions, Retail Dashboard data",RC in progress,72312,"Store tills were offline to POS Beanstore between 13:30 and 18:38 impacting refunds, gift card transactions, store stock position, critical Food reports & Retail Dashboard inday sales. However, card payments remained unaffected. Store counting was blocked to avoid stock corruption. The initial root cause is attributed to the Azure express route failover after a fiber break within Vodafone infrastructure causing a spike in CPU utilization on the POS BeanStore application servers, leading to slowness in message processing.  A clean restart of the POS Beanstore application cluster was performed followed by disabling Dynatrace agent and the piled-up messages were processed from POS by 00:30. Investigations are ongoing to determine the underlying root cause, while hypercare is in place to closely monitor application performance.",,3/21/2025,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"The initial root cause is attributed to the Azure express route failover after a fiber break within Vodafone infrastructure causing a spike in CPU utilization on the POS BeanStore application servers, leading to slowness in message processing.",NA,,,,,,Saloni,Open,MIM,PIR held,2025,3,March,9/16/2025,179,>60
3/21/2025,3/21/2025,90678495,07:00,13:35,06:35,Business Process Services,Business Process Services,Customer Channels,Selling Experience & Customer Engagement,Manage Payment & Content Customer.Com,SI,"Incorrect promo code badge appearing across high volume of PLP's, SRP's & PDP's for UK (Web & App)",Customer,Online BPS,No Financial Impact,Website,RC identified,72313,"Customers were encountering a ""test promo code badge"" across a high volume of PLPs, PDPs, SRPs on the Customer UK website. Around 125k items were impacted resulting in a sub optimal customer experience across web and apps, however, there was no order drop across web and apps. The root cause is attributed to an incorrect promotion code set up by the business teams. The promotion has been deactivated and multiple workarounds have been applied to remove the promotion from the PLPs, PDPs on the website by 13:35.",,3/21/2025,,,,,,,,,Operational Process Issues,, The root cause is attributed to an incorrect promotion code set up by the business teams.,NA,,,,,,Balaji,Open,.Com SM,"PIR doc sent, Actions yet to be added.",2025,3,March,9/16/2025,179,>60
3/17/2025,3/17/2025,90669693,05:30,6:20,00:50,Business Process Services,Business Process Services,Customer Channels,Selling Experience & Service Experience,Manage Payment & Content Customer.Com,MI,Goodwill voucher of $100 auto applied for international orders from US site,Customer,Online BPS,"£36,854.00",Website,RC identified,72375,"Vendor Global E reported an increase in international order volume due to a $100 Goodwill Voucher applied to international orders from the US side between 5:30 and 6:20. Around 25k orders were impacted resulting in poor customer experience and colleague inconvenience at Donington & Ollerton DCs. 223 orders were delivered to customers and a £10 voucher was issued to customers with cancelled orders incurring a financial loss of £36,084. The root cause was attributed to a promotion automation deployment on 14/03, where a $100 Goodwill Voucher was inadvertently applied during functionality testing by the Online Business Process Services team. Services were restored by deactivating the promotion, and impacted orders were either short shipped at the CD & Ollerton DCs and cancelled or not delivered to customers by Global E.",,3/17/2025,,,,,,,,,Change Failure,,"The root cause was attributed to a promotion automation deployment on 14/03, where a $100 Goodwill Voucher was inadvertently applied during functionality testing by the Online Business Process Services team",CR218286,Inadequate testing,,,,,Pavithra,Open,.Com SM,,2025,3,March,9/16/2025,183,>60
3/14/2025,3/14/2025,90661983,10:15,12:45,02:30,Business Process Services,Business Process Services,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,SI,"Multiple wine products scanning at incorrect prices across Scotland, Wales and Ireland stores",Customer,Foods BPS,"£5,500.00",Website ,RC identified,72002,"61 wine products across 116 stores in Scotland, Ireland and Wales were scanning at incorrect prices on store tills between 10:15 and 12:45. Approx. 500 customers were impacted resulting in poor customer experience and a finance loss of £5.5k. The root cause was attributed to a human error whilst updating the unit quantity for multiple wine products as requested by the Business. Services were restored after the prices were manually corrected in POS and sent to the store tills.",,3/14/2025,,,,,,,,,Operational Process Issues,,The root cause was attributed to a human error whilst updating the unit quantity for multiple wine products as requested by the Business,NA,,,,,,Saloni,Open,MIM,Y,2025,3,March,9/16/2025,186,>60
3/14/2025,3/14/2025,90664432,09:49,11:42,01:53,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,MI,Customers on Ireland site redirecting to UK site,Customer,Onyx,"£20,000.00",Website,RC identified,72110,"Customers trying to access the Customer Ireland website were being redirected to the UK site between 09:49 and 11:42. There was a drop in 278 orders resulting in poor customer experience and a financial loss of £20k. The root cause was attributed to a change deployed on the Website Home Page, which was reverted to restore services.",,3/14/2025,,,,,,,,,Change Failure,,"The root cause was attributed to a change deployed on the Website Home Page, which was reverted to restore services.",Unauthorised Change ,Insufficient regression testing,NA,,,,Naveen,Open,.Com SM,Y,2025,3,March,9/16/2025,186,>60
3/14/2025,3/14/2025,90664966,08:00,13:00,05:00,Foods,Food Supply Chain,Foods,Food Supply Chain,Manage Allocation of Food Stock,SI,17 Foods suppliers served by True Commerce were getting duplicated advance shipping notice,True commerce,NA,"£3,900.00","Foods RDC's, FITS",RC identified,72211,Approximately 17 Foods suppliers served by True Commerce were getting duplicated advance shipping notices (ASN's) resulting in a stock receiving issue in the RDC's. This impacted 25k trays and required 216 man hours to manually receive the stock at the sites.  True Commerce reverted a system mapping change in their environment to resolve the issue.  Further details being sought from True Commerce. ,,3/14/2025,,,,,,,,,External Dependencies,,"a new setting was added by True Commerce to send messages to supplier GXS. A test message format was created, but sender/receiver details were missing. As a result, duplicate ASNs were generated and sent in two different formats. This resulted in Food RDC's getting an original ASN and a subsequent duplicate ASN which was non-compliant.",#220178,,,,,,Rehan,Open,MIM,Y,2025,3,March,9/16/2025,186,>60
3/12/2025,3/12/2025,90659216,14:40,15:20,00:40,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,MI,Customers unable to access the basket page via desktop and mobile browsers,Customer,Price Domain,No Financial Impact,Website,RC identified,72377,"Alerting indicated that customers were experiencing errors while accessing the basket page via desktop and mobile web browsers between 14:40 and 15:20. Approx. 1000 customers were unable to view their baskets or proceed to checkout, resulting in a poor customer experience and loss of sales. The root cause was attributed to a Price Domain URL change which was reverted to restore the services.",,3/12/2025,,,,,,,,,Change Failure,,The root cause was attributed to a Price Domain URL change which was reverted to restore the services.,Unauthorised change,,,,,,Balaji,Open,.Com SM,,2025,3,March,9/16/2025,188,>60
3/12/2025,3/12/2025,9065926,09:10,9:30,00:20,Customer Channels,Customer Engagement,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,MI,Customers unable to update profiles and navigate basket pages in mobile apps (iOS & Android),Secure Auth,NA,"£13,500.00",Mobile apps,RC identified,See Note,"Alerting indicated that customers were experiencing errors while updating their profiles and navigating basket pages on mobile apps (iOS & Android) between 09:10 and 09:30.  This led to a drop in 660 orders, resulting in a poor customer experience and a financial loss of £39k. The root cause was attributed to a change deployed by vendor Secure Auth, which was reverted to restore services. A detailed service review is scheduled with Secure Auth to improve their change stability and customer communication.",,3/12/2025,,,,,,,,,External Dependencies,," The root cause was attributed to a change deployed by vendor Secure Auth, which was reverted to restore services.",NA,,,,,,Kavitha,Open,.Com SM,,2025,3,March,9/16/2025,188,>60
3/10/2025,3/10/2025,90653767,11:00,11:45,00:45,Data,Data,Various,"Food, Finance, C&H Commercial Trading, International Commercial Trading",Others,KI,Colleagues unable to access scheduled/canned reports on SAP Business Objects Universe,TCS,Network,No Financial Impact,SAP BO,RC identified,72030,"C&H, Food, Finance & International colleagues were unable to access scheduled/canned reports on SAP Business objects  (BO) universe between 11:00 and 11:45. Around 70 colleagues would have been impacted based on last Monday's stats. The root cause was attributed to the deletion of BO storage account as part of housekeeping activity. Services were restored by reinstating the DNS entry within Infoblox.",,3/10/2025,,,,,,,,,Operational Process Issues,,Root cause is attributed to the deletion of the storage account responsible for the network connectivity between SAP BO Universe and Cloud Platform due to a process gap during the housekeeping activity.,WO,Please provide the WO number,NA,,,,Gokul,Open,MIM,Y,2025,3,March,9/16/2025,190,>60
3/9/2025,3/9/2025,90652016,04:29,17:35,13:06,GTS,Enterprise Technology Platform,Data,Data,Others,KI,Data Replication delay between EDW Prod and DR databases,TCS,Database,No Financial Impact,EDW DR Data,RC in progress,72005,"Alerting indicated a delay in replication between EDW DR and the production database servers, impacting Power BI self-service reports that rely on EDW DR resulting in a delay in data availability. The root cause was attributed to slowness in the EDW DR database replication after the GPFS firmware upgrade activity on one of the EDW DR Stockley Park servers on 08/03. Services were restored after isolating the affected server from the cluster and the DR and prod databases are in synch from 17:35. RCA underway with vendors IBM and HP.",,3/9/2025,,,,,,,,,Change Failure,,The root cause was attributed to slowness in the EDW DR database replication after the GPFS firmware upgrade activity on one of the EDW DR Stockley Park servers on 08/03,#216703,Lack of the lower environment,,,,,Saloni,Open,MIM,Y,2025,3,March,9/16/2025,191,>60
3/7/2025,3/7/2025,90641788,10:00,11:30,01:30,Customer Channels,Customer Engagement,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,KI,Intermittent card payment failures across multiple Travel Money Bureau stores,Customer,Project team,No Financial Impact,Travel money exchange -Card Transactions,RC identified,71944,"After the Travel Bureau go-live to 15 stores, customers were unable to exchange currency using their credit / debit cards across 6 Travel Bureau stores.  However, as a workaround, customers were able to use cash or click & collect option on the website. The root cause was attributed to Zscaler intervention impacting the traffic flow to Verifone and a conflict within Verifone cloud due to similar pin pad names (Ignite & Eurochange). A network policy was created to include Verifone IP address to bypass the Zscaler and a API key was recreated by Verifone to restore services by 11:15.  Further actions will be picked up as part of the programme team.",,3/7/2025,,,,,,,,,Configuration Errors,, The root cause was attributed to Zscaler intervention impacting the traffic flow to Verifone and a conflict within Verifone cloud due to similar pin pad names (Ignite & Eurochange). ,NA,NA,NA,3/25/2025,March,2025,Naveen,Closed,MIM,NA - Issue as part of a program. Hence no PIR ,2025,3,March,9/16/2025,193,>60
3/6/2025,3/7/2025,90647077,22:45,1:50,03:05,GTS,Enterprise Integration, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,SI,Carrier label printing failures at Ollerton DC impacting packing & despatch operations,Customer,Integration Services,"£6,233.00",Ollerton Label printing,RC in progress,71904,"Ollerton DC colleagues were unable to print carrier labels from 22:45 impacting packing and despatch operations resulting in a customer miss promise of 317 orders and capability loss of 7.7k singles incurring a financial loss of £6,233. The initial root cause was attributed to Mule engine going into ""hung"" state resulting in Sorted label printing failures. Services were restored by restarting the Mule services by 01:50.",,3/7/2025,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"The initial root cause was attributed to Mule engine going into ""hung"" state resulting in Sorted label printing failures.",NA,NA,,,,,Balaji,Open,MIM,,2025,3,March,9/16/2025,194,>60
3/1/2025,3/3/2025,90636458,16:00,5:00,37:00,Customer Channels,Service Experience,Various,"Platform & Store Ops , Food Supply Chain , Service Experience, Data",Manage Allocation of Food Stock,MI,Delay in sending order payment messages from POS Beanstore to Sterling and CSSM,Customer,POS Team,"£4,872.00","OMS,CSSM, Food allocations, Data",RC in progress,71711,"Alerting indicated a delay in sending order payment messages from POS Beanstore to Sterling, CSSM resulting in a cancellation of 121 customer orders incurring a financial loss of £4,872 and inaccurate stock position for ~4997 store UPC combinations across 622 stores leading to stock corruption. Contingency was invoked to generate Foods ordering & allocations with 28 day order plan on hold for 02/03 resulting in Foods analytics reports not being updated with latest order plan information impacting planning activities. The root cause was attributed to the one of the POS channels going into a ""retry state"" resulting in message backlogs. Services were restored by manually resetting the channel to process the message backlog. Mandated counts were sent to the impacted stores and Foods allocations and ordering were generated using the latest CSSM stock file on 03/03.",,3/1/2025,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"The initial root cause was attributed to the one of the POS channels going into a ""retry state"" resulting in message backlogs",NA,NA,NA,,,,Saloni,Open,MIM,Y,2025,3,March,9/16/2025,199,>60
3/1/2025,3/3/2025,90643248,13:30,13:58,48:28,Foods,Foods Supply Chain,Customer Channels,Platform & Store Ops ,Others,KI,Issues with Track My vehicle application across stores,Customer,TMV Application team,No Financial Impact,Track My Vehicle,RC unknown,71717,"Multiple stores were experiencing issues with the TrackMyVehicle application over the weekend, impacting vehicle tracking and planning activities resulting in inconvenience across stores. As a workaround, stores were calling GIST IT desk for tracking information & delivery ETAs. However, no issues were reported after 02/03. Root cause investigations underway.",,NA,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"There were no sufficient logs to investigate further on the root cause by the product teams. Hence, root cause is inconclusive",NA,NA,NA,3/24/2025,March,2025,Gokul,Closed,MIM,NA,2025,3,March,9/16/2025,199,>60
2/28/2025,2/28/2025,90641785,06:00,9:30,03:30,GTS,Enterprise Integration,Data,Data,Others,KI,Delay in sales data availability on the Retail Dashboard,Customer,Cloud Integration Frameworks,No Financial Impact,Retail inday sales Dashboard,RC identified,71725, Retail Dashboard Inday sales reports were not updated with latest sales information between 06:30 and 09:00. The root cause has been attributed to an incorrect code deployment during the MQ upgrade activity which was reverted to restore services.,,2/28/2025,,,,,,,,,Change Failure,,The root cause has been attributed to an incorrect code deployment during the MQ upgrade activity which was reverted to restore services.,#217223,NA,NA,3/18/2025,March,2025,Kavitha,Closed,MIM,Y,2025,2,February,9/16/2025,200,>60
2/28/2025,2/28/2025,90634845,10:30,13:30,03:00,GTS,Enterprise Technology Platform,Group Platforms,Finance,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,SAP POSDTA application was inaccessible,Customer,Cloud Dev Sec Ops,No Financial Impact," POS sales feeds, GMOR feeds to the downstream systems",RC identified,71723,"SAP POSDTA was inaccessible between 10:30 and 13:00 impacting POS sales feeds, GMOR feeds to the downstream systems, however, with no business impact. The root cause was attributed to an inadvertent deletion of Vnet peer within Azure network as part of a security housekeeping activity. The Vnet peer was re-instated to restore the services by 13:00.",,2/28/2025,,,,,,,,,Change Failure,, The root cause was attributed to an inadvertent deletion of Vnet peer within Azure network as part of a security housekeeping activity.,Unauthorised Change ,Human Error,NA,,,,Saloni,Open,MIM,Y,2025,2,February,9/16/2025,200,>60
2/27/2025,2/27/2025,90633124,07:00,15:09,08:09,GTS,Enterprise Integration,Customer Channels,"Selling Experience, Service Experience",Manage Payment & Content Customer.Com,MI,Intermittent timeout errors in API calls between OMS Cluster & APIGEE endpoints ,Apigee,Apigee,"£50,000.00","Return label printing, collection date transactions, and eGift card fulfilment ",RC identified,71726,"Through alerting, intermittent timeout errors were identified between Apigee and the OMS cluster, impacting API calls across multiple services between 07:00 and 15:09. Customers were unable to place orders for flowers and hampers as they were appearing out of stock during checkout incurring a financial loss of £50K. Return label printing, collection date transactions, and eGift card fulfilment were also impacted. The root cause was attributed to additional IPs introduced by Apigee from 24/02 that were blocked by the Customer network firewall. Services were restored after updating the firewall rule to allow network traffic based on URL.",,3/27/2025,,,,,,,,,External Dependencies,,The root cause was attributed to additional IPs introduced by Apigee from 24/02 that were blocked by the Customer network firewall.,NA,NA,NA,,,,Balaji,Open,.Com SM,Y,2025,2,February,9/16/2025,201,>60
2/27/2025,2/27/2025,90632370,11:13,11:16,00:03,Customer Channels,Customer Engagement,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,MI,Drop in orders on the Customer website,Secure Auth,Secure Auth,"£6,360.00",Ecom orders,RC identified,71617,"Alerting indicated a drop in orders on the website between 11:13 and 11:16. Customers were unable to place orders resulting in a drop of 240 orders with a financial loss of £6,360 and poor customer experience. The initial root cause is attributed to an issue within vendor Secure Auth environment, and we await root cause and recovery details from the vendor.",,3/27/2025,,,,,,,,,External Dependencies,,"The initial root cause is attributed to an issue within vendor Secure Auth environment, and we await root cause and recovery details from the vendor.",NA,NA,NA,,,,Balaji,Open,.Com SM,Y,2025,2,February,9/16/2025,201,>60
2/26/2025,2/26/2025,90643228,15:43,21:00,05:17,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Others,SI,Drop in products on the Customer Ireland website,Customer,Navigation API Team,"£5,000.00",Ireland website,RC identified,71719,Business ops team highlighted a drop in products (~800) across 4 categories on the Ireland website between 15:43 and 21:00. Customers were unable to place orders resulting in poor customer experience and loss of sales. The root cause was attributed to a script executed on the Navigation API database resulting in filtering out the live products being set up against Off sale categories. Services were restored after the change was reverted followed by executing the product search job. ,,2/26/2025,,,,,,,,,Change Failure,,The root cause was attributed to a script executed on the Navigation API database resulting in filtering out the live products being set up against Off sale categories,Unauthorised Change ,Process gap,NA,,,,Balaji,Open,.Com SM,Y,2025,2,February,9/16/2025,202,>60
2/27/2025,2/28/2025,90633159,23:45,3:00,03:15, C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Carrier label printing failure at Castle Donington impacting packing operations ,Sorted,NA,"£12,000.00",Donnington Label printing,RC identified,71616,"Donington colleagues were unable to print carrier labels from 23:45 impacting packing operations resulting in capability loss of 43k singles and CFR of 331 orders. The initial root cause was attributed to an issue with
Royal Mail sequence service going out of range. Services were restored after the range was amended followed by restarting the service by 03:00 (28/02). Root cause investigations underway with Sorted.",,2/27/2025,,,,,,,,,External Dependencies,,"Allocation failures were caused by the range for the Royal Mail service used by Customer Castle Donnington location running out. 
The alert which should have triggered when the range reached 0% did not trigger due to a human error - an indavertant typo by Sorted when setting up the Royal Mail range.
Sorted have a monitor in place to alert them when a carrier range is close to exhaustion so that they can request and implement a new one, but this range was not picked up by this monitor. This was because the range didn't follow the convention for naming.",NA,NA,NA,,,,Rehan,Open,MIM,Y,2025,2,February,9/16/2025,201,>60
2/27/2025,2/27/2025,90633315,11:00,21:00,10:00,GTS,Digital Workplace Services,Foods,Foods Supply Chain,Others,KI,Customer Buyers unable to access Foods RDA application,Customer,Active Directory / Network,No Financial Impact,RDA (Reference Data Approval,RC identified,71713,"Foods Buyers were unable to access RDA (Reference Data Approval) application between 11:00 and 21:00 impacting their ability to approve the supplier’s updates on cost price, equipment type, unit per tray etc on products. However, no impact to the suppliers. The root cause is attributed to a change to disable a Y account used for Kerberos authentication in the Active Directory, which was reinstated to restore the service.",,2/27/2025,,,,,,,,,Change Failure,,"The root cause is attributed to a change to disable a Y account used for Kerberos authentication in the Active Directory,",CR#217552,Insufficient impact assessment,NA,,,,Pavithra,Open,MIM,Y,2025,2,February,9/16/2025,201,>60
2/24/2025,2/24/2025,90624297,00:43,11:30,10:47,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,SI,Delay in Relex nightly batch impacting overnight foods critical flows,Relex,NA,No Financial Impact,"Food Supplier orders, NDC allocations",RC identified,71510,Alerting indicated a slowness in Relex nightly batch resulting in a delay to the overnight critical flows. Contingency was applied to generate the supplier orders by 06:43 and NDC allocations by 05:14.   ,,2/24/2025,,,,,,,,,External Dependencies,,"Relex are currently in the process of decommissioning an outdated datacentre.  As a result of the decommissioning, the old hard disk drives (HDDs) were excluded from the Index Lifecycle Management (ILM) policy.This exclusion removes the HDDs from the ILM framework. This led to an excessive increase in the utilisation of solid-state drives (SSDs) as the system started removing all the disk copies that were created.  To avoid exhausting the SSDs, it became necessary to direct data ingestion towards the HDDs.  However, this approach resulted in the overloading of the HDD nodes, leading to backup transfer failure. The Relex connect is an integration layer/ tooling used for processing/ transferring the interface files to the Relex plan layer. The Relex connect job was in progress during the backup issue within the storage system hence the impact to the critical flow.",NA,NA,NA,4/15/2025,April,2025,Naveen,Closed,MIM,Y,2025,2,February,9/16/2025,204,>60
2/22/2025,2/22/2025,90622655,08:30,13:10,04:40,Group Platforms,HR,Group Platforms,HR,Manage & Maintain Payroll,KI,Issues with external recruitment page on the Customer career site impacting external candidates,Oracle/ Thirty-Three,NA,No Financial Impact,External - My Career site,RC identified,71352,"Following the Oracle 25A quarterly patching, issues were observed whilst accessing the Customer external career page within the career site between 08:00 and 13:10, preventing external applicants from applying jobs. The initial root cause was attributed to a URL structural change resulting from the patching. Services were restored after a fix was deployed by Vendor Thirty-three. Root cause investigations underway with Oracle.",,2/22/2025,,,,,,,,,External Dependencies,,The root cause is attributed to a URL structural change resulting from the Oracle quarterly patching.,NA,NA,NA,,,,Gokul,Open,MIM,Y,2025,2,February,9/16/2025,206,>60
2/20/2025,2/21/2025,90617876,04:51,9:00,29:51,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,SI,"	Delay in Foods NDC allocations impacting picking operations at the sites",Customer,Foods Business,"£9,400.00","Food Supplier orders, NDC allocations, Food reporting",RC identified,71505,Alerting indicated an overallocation of Relex products generated for Bradford and Milton Keynes DCs impacting their picking operations. Contingency allocations were generated and sent to Bradford and Milton Keynes DCs by 07:40. Supplier orders and Bedworth frozen allocations were also delayed. ,,2/20/2025,,,,,,,,,Operational Process Issues,,The root cause was attributed to incorrect updates made by a business colleague on the store forecast figures in Relex UI resulting in inflated allocations.,NA,NA,NA,,,,Kavitha,Open,MIM,Y,2025,2,February,9/16/2025,208,>60
2/20/2025,2/20/2025,90618941,05:29,6:24,00:55,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Others,SI,Customers unable to check out on the International sites,Salesforce Commerce Cloud,NA,"£3,500.00",International Website,RC identified,71428,"Through alerting it was identified that customers were unable to check out on the international website between 05:29 and 06:24. There was a drop in 35 orders resulting poor customer experience and financial loss of £3,500. ",,2/20/2025,,,,,,,,,External Dependencies,,"The Technology team and third-party vendor determined the initial root cause of the incident was a software bug that caused the load balancer to reboot. Typically, the pair of load balancers remain in sync to allow a graceful failover. In this case, there was a subset of VIP pools that were not configured the same way on the passive load balancer. The third-party vendor identified a further bug that was deemed to be the cause of the load balancers not remaining in sync.
The Technology team further addressed this issue by executing a change to sync the passive load balancer after the disruption. The Technology team also implemented alternate telemetry to identify when load balancers are out of sync.",NA,NA,NA,,,,Balaji,Open,.Com SM,Y,2025,2,February,9/16/2025,208,>60
2/18/2025,2/19/2025,90610998,08:30,8:30,24:00,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Inflated Allocations for Thatcham (Foods) RDC impacting stock deliveries to stores,GIST,NA,No Financial Impact,Food supplier planning,RC identified,71358," On 17/02, a new functionality was tested by GIST from Thatcham RDC for Hedge End Store, leading to stock corruption due to a mismatch in the ""into store delivery date"" between the product and DWS system. Multiple adjustments were made in CSSM to mitigate the impact. Subsequently, GIST reported inflated allocations at Thatcham Depot and contingency allocations were applied to mitigate impact to the stock deliveries across 150 stores serviced by Thatcham. ",,2/18/2025,,,,,,,,,External Dependencies,,The root cause was attributed to an issue within CSSM leading to the system not accounting for the adjustments on 17/02 thus sending negative stock in transit values to RELEX. ,NA,NA,NA,,,,Saloni,Open,MIM,Y,2025,2,February,9/16/2025,210,>60
2/11/2025,2/11/2025,90599097,10:50,11:30,00:40, C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Castle Donington DC colleagues were unable to scan SYW (Shop your way) orders,Customer,WMS,"£10,000.00",WMS ,RC identified,71332,Castle Donington DC colleagues were unable to scan Click & Collect orders between 10:50 and 11:30 impacting packing operations resulting in a capability loss of 28.5k singles incurring a financial loss of £10k. Order proposition was moved for C&C and NDD by 24 hours to mitigate customer miss promises. The root cause was attributed to a human error whilst deploying a configuration change on the WMS database. The database table config was restored from the backup and product team worked with site operations to mitigate the residual impact.,,2/11/2025,,,,,,,,,Change Failure,,"The root cause is attributed to a human error whilst deploying a configuration change within WMS database, where instead of updating one record within the database, all records have been updated resulting in errors in pack screen ",CR#216118,Human error,NA,4/9/2025,April,2025,Pavithra,Closed,MIM,Y,2025,2,February,9/16/2025,217,>60
2/9/2025,2/9/2025,90594365,01:00,10:30,09:30,Customer Channels,Service Experience,Customer Channels,Service Experience,Manage Payment & Content Customer.Com,KI,Some Flowers and hampers products were not available for next day delivery,Customer,Availability Services,"£15,000.00",flower & hamper products for NDD,RC identified,71403,"Some flower and hamper products were unavailable for next-day delivery on the website between 01:00 and 10:30, which adversely affected customers' ability to place orders and led to a poor customer experience. The cause has been identified as a time alignment issue with an automated process within the availability services. The start of day (SOD) stock file was received sooner than expected, preventing the job from processing the file correctly, which resulted in data override. Services were reinstated by manually updating the stock data. Further root cause is in progress.",,2/9/2025,,,,,,,,,Software Defects/Limitation,,"The cause has been identified as a time alignment issue with an automated process within the availability services (AS). The start of day (SOD) stock file was received sooner than expected, preventing the job from processing the file correctly, which resulted in data override. The availability services have an automated job which executes at 01:00 AM prior to SOD processing, however on 09th Feb SOD files arrived at 00:48 AM (1hr 12 mins) earlier than expected time range (between 02:00 – 03:00
AM) impacting the processing sequence, later AS job executed as per schedule overwriting the on-hand quantity to zero which resulted 49 flowers and 55 hamper products not able to offer next day delivery to customers.",NA,NA,NA,4/11/2025,April,2025,Kavitha,Closed,.Com SM,Y,2025,2,February,9/16/2025,219,>60
2/7/2025,2/12/2025,90591874,11:15,22:00,128:45,GTS,Enterprise Technology Platform,"Group Platforms, C&H and Intl","Finance, HR, C&H and Intl Supply Chain",Infrastructure,SI,Missing files after HNAS to Pure Storage migration,TCS,Storage ,No Financial Impact,Shared files,RC identified,71401,"Finance, HR, and Donington colleagues reported the absence of certain files and folders in the Shared Path, which affected their daily activities. The cause has been linked to the migration of file paths from HNAS to Pure Storage on 2nd Feb. The storage team is actively working to manually restore the missing files. Additionally, a solution is has been implemented to transfer the missing files from HNAS to Pure Storage and the majority of files have files have now been recovered over the weekend.",,2/6/2025,,,,,,,,,Change Failure,,"Once the data transfer commenced, any subsequent modifications made by users were not captured in the new Pure storage. This resulted in reports of absent file paths from the finance, HR, and Donington business teams",215139,,,,,,Rehan,Open,MIM,Y,2025,2,February,9/16/2025,221,>60
2/7/2025,2/7/2025,90591809,09:09,10:25,01:16,Foods,Food Supply Chain,Foods,Food Supply chain,Manage Allocation of Food Stock,KI,Increase in supplier orders volume for Foods chilled & ambient lines,Customer,FOAL,No Financial Impact,Food supplier planning,RC identified,71404,Business colleagues reported an increase in supplier orders volume for Foods Chilled & Ambient Lines impacting the Food supplier planning activities. The root cause has been attributed to combination of an ASO upgrade and an overrunning job resulting in partial SAND (stock allocated & not despatched) file being sent to Relex. Contingency plan was invoked for RDC allocations and orders were sent at 10:25.,,2/11/2025,,,,,,,,,Data Integrity & State Issues,,"The SAND file was not available for collection by FOAL at the scheduled time of 23:05, as it was delayed by one minute due to high volume. Earlier in the day, during an ASO upgrade validation, a SAND file was generated at 15:00. This file was collected by FOAL in place of the file that was supposed to be generated at 23:00, which was intended for extraction at that same hour.",NA,NA,NA,,,,Rehan,Open,MIM,Y,2025,2,February,9/16/2025,221,>60
2/6/2025,2/6/2025,90590123,13:38,14:56,01:18,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Service Experience","Manage Payment & Point of Sales in store, Manage Payment & Content on Customer.com",MI,Significant drop in orders on website and various Store impacts,Customer,Browse,"£276,000.00",.Com Website & Digital Café,RC identified,71299,"Alerting indicated a significant drop in orders and increase in generic error pages on the website between 13:38 and 14:56 impacting 4.9k orders and incurring a financial loss of £276k. Digital Cafes were also unavailable to place orders, resulting in a poor customer experience.  Instore applications such as ISF, Parcel Scanning and Assist were also impacted. The cause was related to the implementation of a new feature in Onyx Auth, which led to a surge in traffic directed towards Apigee, ultimately resulting in its blockage by Akamai. Services were reinstated by reversing the changes made in Akamai, followed by an additional modification to permit Apigee traffic through the existing DDoS protection rules. Further investigations to continue into the root cause.",,2/6/2025,,,,,,,,,Change Failure,,"The root cause was a change made the previous day by the Browse team. This change enabled Onyx Auth across all of Browse. By applying it, the traffic for a specific URL that was accessed via Apigee increased significantly. Akamai usually excludes blocking Apigee IP Addresses for API access. However, since the URL being accessed was a WCS command (rather than a WCS API), it was not part of the exclusion. (i.e. Akamai Rules did not expect Apigee to hit the website directly). Because Apigee was making many requests, the rate limit blocked the Apigee IP Addresses which reduced the API traffic and prevented customers from placing orders.",Unauthorised Change ,NA,NA,,,,Gokul,Open,.Com SM,Y,2025,2,February,9/16/2025,222,>60
2/6/2025,2/6/2025,90594352,09:00,11:30,02:30,Customer Channels,Selling Experience,Customer Channels,Customer Engagement,Manage Payment & Content Customer.Com,KI,Gifting and Entertainment Hampers & Wine range out of stock on product display pages,Customer,OMS,"£10,000.00",Gifting and Entertainment Hampers & Wine products availability,RC identified,71322,"Majority of Gifting and Entertainment Hampers & Wine products were showing as ""out of stock"" between 09:00 and 11:30 impacting customer's ability to place orders resulting in a financial loss of £10k and poor customer experience .The root cause has been attributed to a manual error whilst resolving an incorrect set up for item capping on 05/01 impacting the WCS processing resulting the product quantity to be updated as zero. Services were restored by updating and processing the product quantity in WCS. Root cause investigations underway.",,2/6/2025,,,,,,,,,Operational Process Issues,,"The root cause was attributed to manual error while applying workaround for a capping transition issue.  Whilst processing the pre item capping orders, the time sensitive flags were temporarily removed 
from the items to process the failed ASN’s successful and the flag should have been re-enabled.  Whilst reapplying the time sensitive flag after processing, the wrong rule (monitoring node) was enabled causing the issue.
",NA,NA,NA,2/26/2025,February,2025,Pavithra,Closed,.Com SM,Y,2025,2,February,9/16/2025,222,>60
2/4/2025,2/5/2025,90575084,12:22,23:00,34:38,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops,Network,KI,Convenience stores experiencing network performance issues,Vodafone,NA,No Financial Impact,Stores Network,RC identified,71302,"Since 31/01, around 29 Convenient (Franchise) stores reported network performance issues whilst accessing internet using both Wi-Fi and LAN, impacting the store operations. The root cause was attributed to a network change implemented on 30/01, to enable Swindon as the primary for the internet services for Vodafone MPLS (Multiprotocol Label Switching) stores. Services were restored by reverting the change on 05/02. Further discussions will be held with Vodafone to perform proper testing before redeploying this change, meanwhile, a risk has been raised.",,2/5/2025,,,,,,,,,External Dependencies,,"The root cause was attributed to a network change implemented on 30/01, to enable Swindon as the primary for the internet services for Vodafone MPLS (Multiprotocol Label Switching) stores.  Vodafone are continuing testing the VFR1 for issues and are planning an outage for four hours for this testing",214267,Improper Implementation Planning,NA,,,,Balaji,Open,MIM,Y,2025,2,February,9/16/2025,224,>60
1/29/2025,1/29/2025,DN ref: 128197457,13:34,13:48,00:14,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,KI,17 Digital Café stores unable to serve customers,Akamai,NA,"£1,000.00",Digital Café,RC unknown,See Note,"Customers were unable to pay for their orders on the Digital Café Kiosks between 13:34 and 13:48. Payments on the kiosks were declined impacting 70 transactions incurring a financial loss of £1k. API response timeout errors were also observed on multiple in-store applications like Collect, Returns, Intelligent Waste, Refund Validation Services, VoD resulting in intermittent connectivity issues. The initial root cause is attributed to a connectivity issue between Apigee and the Retail AKS cluster. Root cause investigations underway.",,1/29/2025,,,,,,,,,Root Cause Undetermined/Still Under Investigation,,"The root cause of the incident remains inconclusive; however, error logs were observed in the apigee layer indicating timeout errors to the destination IP (AKS retail cluster). But there was no explicit issue within the impacted application, hosted in Retail AKS Cluster.  Checks were performed at various levels like network firewall, nginx, Azure AKS platform but no errors/denial observed during the incident time frame. Therefore, we are unable to identify the exact root cause of the issue. ",NA,NA,NA,,January,1900,Pavithra,Closed,MIM,Y,2025,1,January,9/16/2025,230,>60
1/27/2025,1/29/2025,90565423,11:10,8:00,44:50,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,KI,Issues with Contact Centre Goodwill and BI/Reporting Data flows,Customer,Contact centre,No Financial Impact,Contact Centre Goodwill and BI/Reporting Data flows,RC identified,71264,Contact Centre goodwill e-gift cards & reporting data flows were impacted after the historical Genesys records were imported into the Zendesk ticketing services as part of the Contact Centre platform migration on 23/01.   Approximately 1.2k goodwill transactions have breached the 72-hour customer SLA and Contact Centre case management reports were impacted. Root cause was attributed to the data migration activity resulting in significant volume of data processing impacting the Contact Centre reporting flows. The Zendesk ticketing service and Goodwill gift card data flows were restored on 27/01 and 29/01 respectively.,,1/27/2025,,,,,,,,,Change Failure,,Root cause was attributed to the data migration activity (contact centre platform from Sabio (Genesys) to Google email channel) resulting in significant volume of data processing impacting the Contact Centre reporting flows,213974,TBC,NA,2/27/2025,February,2025,Balaji,Closed,.Com SM,No Problem actions - PIR has been clubbed with the CR#213974/MI ,2025,1,January,9/16/2025,232,>60
1/27/2025,1/27/2025,90565151,09:00,9:22,00:22,Customer Channels,Customer Engagement,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,KI,Increase in Generic Error Pages & Decline in orders on Customer.com Website,Secure Auth,NA,"£26,000.00",.Com Website - Desktop,RC identified,71263,"Alerts indicated that customers were encountering generic error pages whilst accessing the website via desktop between 09:02 and 09:22 resulting in a drop of 450 orders incurring a financial loss of £26k. However, customers with existing sessions on the website and mobile applications (IOS and Android) were not impacted. The root cause was attributed to a bug within Secure Auth causing an invalid cache value resulting in connectivity issues. A cache refresh was performed by vendor Secure Auth to restore the services.",,1/26/2025,,,,,,,,,External Dependencies,,The root cause was attributed to a bug within Secure Auth causing an invalid cache value resulting in connectivity issues.,NA,NA,NA,,,,Kavitha,Open,.Com SM,Y,2025,1,January,9/16/2025,232,>60
1/27/2025,1/27/2025,90563356,06:30,10:20,03:50,C&H and Intl,C&H Commercial Trading, C&H and Intl,C&H Commercial Trading,Others,KI,Delay in Merchandise planning  application availability,Customer,MP-MSP,No Financial Impact,MP-MSP,RC identified,71244,"Merchandise Planning (MP) application is unavailable to C&H merchandise planners impacting their planning activities and delays in availability of latest data in SSI (Sales, Stock & Intake) reporting.  Root cause is attributed to high volume of data from the C&H restructure activity along with the annual purge job execution resulting in the delay of weekly batch completion.  MP application is expected to be made available by 10:30.",,1/27/2025,,,,,,,,,Data Integrity & State Issues,,"Root cause is attributed to high volume of data from the C&H restructure activity along with the annual purge job execution resulting in the delay of weekly batch completion - For the permanent fix, there is a version upgrade planned for May 2025 which should resolve high lssues in future",NA,NA,NA,2/2/2025,February,2025,Pavithra,Closed,MIM,NA - Not MIM managed incident,2025,1,January,9/16/2025,232,>60
1/21/2025,1/23/2025,CR#213974,11:00,18:00,55:00,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,SI,Contact Center colleagues encountered intermittent performance degradation while handling emails,Google,NA,No Financial Impact,Contact center emails,RC identified,See Note,"From 11:00, contact centre colleagues were reporting intermittent performance issues whilst handling customer emails, resulting in poor customer experience. The root cause is attributed to a change deployed to migrate contact centre platform from Sabio (Genesys) to Google. A fix was deployed by Google on 23/01 to restore the services. Hypercare monitoring is in place to ensure stability.",,1/21/2025,,,,,,,,,External Dependencies,,The root cause is attributed to a change deployed to migrate contact centre platform from Sabio (Genesys) to Google.,213974,TBC,NA,,,,Rehan,Open,MIM,Y,2025,1,January,9/16/2025,238,>60
1/16/2025,1/17/2025,90548919,10:00,10:24,00:24,Customer Channels,Customer Engagement,Customer Channels,"Customer Engagement, Selling Experience",Manage Payment & Content Customer.Com,KI,Customers unable to sign up in Mobile Apps,Customer,Identity,"£5,500.00",Mobile App,RC identified,71224,"Customers were experiencing errors while trying to signup, reset password or change their username on the Customer mobile app (both iOS & Android) from 15:30 on 16/01 resulting in poor customer experience.  Desktop and Mobile Web were not impacted. 18.5k password resets, 13.7k signups, 671 password updates, and 917 username updates were impacted during the incident window. The root cause was attributed to a change implemented by the Identity team which was reverted at 10:24 on 17/01 to restore the services.",,1/17/2025,,,,,,,,,Change Failure,,There was a Micronaut version upgrade by Renovate bot. This change upgrades micronaut-platform from 4.6.3 to 4.7.4.  The new version of Micronaut introduced a bug. It appends headers on request to jwks endpoint which finally resulting in header exceeding the max allowed size. This resulted  in 401 from all identity service endpoints except customer grants. ,Unauthorised Change ,Product bug,NA,,,,Kavitha,Open,.Com SM,Y,2025,1,January,9/16/2025,243,>60
1/14/2025,1/14/2025,90538790,06:00,8:39,02:39,C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,"Outlet DC Network connectivity issue impacting network connectivity to JDA's , HHT's",GXO,NA,"Capability loss of ~100k singles incurring a financial loss of £1,134","Outlet WMS, HHT",RC identified,71143,"Outlet DC colleagues were experiencing HHT connectivity issues from 06:00 resulting in a capability loss of ~100k singles incurring a financial loss of £1,134 with no impact to operations. The root cause was attributed to a central network connectivity issue within GXO which was resolved by 08:39 to restore services.",,1/14/2025,,,,,,,,,External Dependencies,, The root cause was attributed to a central network issue within GXO which was resolved by 08:39. Awaiting detailed Root cause analysis from GXO IT,NA,NA,NA,26/03/25,March,2025,Balaji,Closed,MIM,Y,2025,1,January,9/16/2025,245,>60
1/13/2025,1/15/2025,90528545,12:27,12:59,48:32,C&H and Intl,C&H & Intl Supply Chain,Customer Channels,"Service Experience, Platform & Store Ops",Others,KI,"Stores unable to book in parcels shipped from Ollerton DC due to incorrect status of ""Awaiting dispatch"" in OMS",Customer,WMS," ~1600 customers incurring a financial loss of £38,880.","OMS,WMS, Parcel booking ",RC identified,71217,"Store colleagues were unable to book in parcels shipped from Ollerton DC before 11/01 impacting   ~1600 customers who were unable to collect their orders resulting in a financial loss of £38,880. The root cause was attributed to a combination of a product bug in WMS and operational issues at Ollerton DC resulting in the orders being stuck in ""Awaiting dispatch"" status in OMS. A decision was taken with Retail Ops and .com Ops to cancel those orders in OMS and comms were sent to customers. A temporary fix was implemented in WMS to send the ASN's to OMS for orders shipped from Ollerton DC and the permanent fix will be deployed on 28th Jan.",,1/15/2025,,,,,,,,,Software Defects/Limitation,,"The root cause has been attributed to a combination of product bug in WMS and operations issue at Ollerton resulting in the orders being stuck in ""Awaiting dispatch"" status in OMS",NA,NA,NA,,,,Gokul,Open,MIM,Y,2025,1,January,9/16/2025,246,>60
1/13/2025,1/13/2025,90537221,04:13,7:25,03:12,Foods,Food Supply Chain,Foods,Food Supply chain,Manage Allocation of Food Stock,KI,Delay in Foods NDC allocations impacting operations,Customer,Relex (Accenture),"£8,800.00","Food Supplier orders, NDC allocations, Food reporting",RC identified,71206,"Alerting indicated overallocation of Relex products generated for Bradford and Milton Keynes DCs impacting their picking operations. Supplier orders were sent with a delay by 07:25 and 28 day order plan and critical reports like  Foods Compliance, FLIC and OTS reports were also delayed. Contingency allocations were generated and sent to Bradford and Milton Keynes DCs by 07:20. The initial root cause has been attributed to a ~2M drop in Ambient stock volume after the stock take activity on 12/01. Further investigations are underway.",,1/13/2025,,,,,,,,,Software Defects/Limitation,,"A drop of 2M Ambient stock volume was observed in Relex resulting in generation of additional volume for Bradford and Milton Keynes NDCs after the stock take activity performed on 12/01. The stock loss is part of the stock take activity, and it is expected to change the stock position in the CSSM and downstream systems. Hence, both systems confirmed to be behaving as expected. ",NA,NA,NA,2/25/2025,February,2025,Kavitha,Closed,MIM,Y,2025,1,January,9/16/2025,246,>60
1/9/2025,1/9/2025,90531649,10:07,15:26,05:19,GTS,Enterprise Technology Platform,GTS,Enterprise Technology Platform,Manage Pick of C&H stock,KI,Gift Card and Scan tower operations impacted at Cross dock DCs,TCS,Network,Productivity loss of 31.5 hours incurring a financial loss of £550,"Scan Tower, Gift Card",RC identified,71121,"Cross dock DCs (Hydepark, Westfield & Giftcard) reported issues with scan tower and gift card operations between 10:07 - 15:26 resulting in a productivity loss of 31.5 hours incurring a financial loss of £550. However, manual workarounds were in place to mitigate the operational impact. The root cause was attributed to a removal of the network route on the primary switch whilst restarting the Linux clusters on 08/01. The routing path was reinstated on the switch to restore the services.
",,1/9/2025,,,,,,,,,Operational Process Issues,,"The root cause of the issue has been attributed to the removal of the cluster IP from the routing table within one of the network switches, whilst troubleshooting another issue. ",NA,NA,NA,2/2/2025,February,2025,Pavithra,Closed,MIM,Y,2025,1,January,9/16/2025,250,>60
1/7/2025,1/7/2025,90527210,07:37,13:25,05:48,C&H and Intl,C&H & Intl Supply Chain,GTS,Enterprise Technology Platform,Manage Pick of C&H stock,KI,Intermittent carrier label printing issues at Ollerton DC,RedHat,NA,"£11,558.00",Carrier label printing,RC identified,71123,"Ollerton DC colleagues were experiencing intermittent label printing issues between 07:37 and 13:25 resulting in a capability loss of 12k singles and productivity loss of 379 hours. The initial root cause was attributed to intermittent connectivity issues on the Linux cluster. Services were restored after the application, Mule and MQ clusters were restarted.",,1/7/2025,,,,,,,,,External Dependencies,,"The root cause behind the intermittent drop in Linux cluster connectivity was due to a bug in the recent linux patch within the kernel version 7.9. It has been reverted to across all the affected 4 DC's (Ollerton, Welham, Xdoc, Outlet) to ensure stability. ",CRQ214035,Product bug,,,,,Saloni,Open,MIM,Y,2025,1,January,9/16/2025,252,>60
1/7/2025,1/7/2025,90527201,04:35,9:09,04:34,Foods,Food Supply Chain,Foods,Food Supply chain,Manage Allocation of Food Stock,KI,Relex reported high volumes for NDC allocations,Customer,Foods,"£13,800.00","Food Supplier orders, NDC allocations, Food reporting",RC identified,71125,Alerting indicated that Foods NDCs have received overallocation of Relex products resulting in a delay in their picking activities with no major operational impact. Foods final orders were also delayed and sent to the suppliers by 05:46. Contingency allocations were generated and sent to Bradford and Milton Keynes DCs by 09:09. The root cause was attributed to incorrect safety stock updates in Relex by the supply chain colleagues resulting in significant increase of allocation volumes.,,1/7/2025,,,,,,,,,Operational Process Issues,,The root cause was attributed to incorrect safety stock updates in Relex by the supply chain colleagues resulting in significant increase of allocation volumes.,NA,NA,NA,4/1/2025,April,2025,Naveen,Closed,MIM,y,2025,1,January,9/16/2025,252,>60
1/5/2025,1/5/2025,90523891,14:00,17:10,03:10,C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Ollerton DC reporting issues with clustering impacting picking operations,Customer,WMS,£310.80,Ollerton WMS,RC identified,70896,Ollerton DC colleagues reported issues with clustering ecomm orders from 14:00 impacting picking capability loss of 518 singles with no CFR impact. The root cause was attributed to a Control M job that was released inadvertently after the Linux OS patching activity impacting clustering operations.  Services were restored after the Control M job was put on hold followed by a restart of the WMS clustering daemon by 17:10,,1/5/2025,,,,,,,,,Change Failure,, The root cause was attributed to a Control M job that was released inadvertently after the Linux OS patching activity impacting clustering operations,CRQ214035,Inadequte prerequisite activity,NA,,,,Saloni,Open,MIM,Y,2025,1,January,9/16/2025,254,>60
1/3/2025,1/3/2025,90519981,08:00,9:00,01:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Digital Café kiosks were unable to serve customers between 08:00 & 09:00,Counter Solutions,NA,"£2,416.00",Digital Café,RC identified,70998,"18 Digital Café stores were unable to trade or serve customers via kiosks between 08:00 and 09:00 resulting in loss of sales and poor customer experience. The root cause has been attributed to a deployment by Counter-Solutions. Services were restored after a fix was implemented by Counter-Solutions, awaiting further updates on resolution and root cause.",,1/3/2025,,,,,,,,,External Dependencies,,The root cause has been attributed to a deployment by Counter-Solutions.,NA,NA,NA,1/20/2025,January,2025,Pavithra,Closed,MIM,Y,2025,1,January,9/16/2025,256,>60
1/3/2025,1/3/2025,90518708,00:04,5:00,04:56,GTS,Enterprise Technology Platform,"Foods, C&H and Intl","Food Supply chain, C&H and Intl Supply Chain","Manage Pick of C&H Stock, Manage Allocation of C&H stock, Manage Pick of Food stock,  Network",SI,Faulty network hardware on the Swindon Nexus 7k impacting DC operations & supplier orders,TCS,Network,"Approx. £4,760.00",DC Operations and Food Supplier orders,RC identified,71000,"C&H and Foods DC colleagues (except CD) reported slowness whilst accessing the JDA dispatcher application between 00:04 and 05:00 resulting in a productivity loss of 280 hours across the C&H sites. Retail orders to C&H DCs, Foods supplier orders and critical foods reports were sent with a delay at 09:14. The root cause has been attributed to a faulty network hardware component on the Nexus 7k switch at Swindon Data Centre. Services were restored after isolating the problematic interfaces on the switch.  Hypercare in place over the weekend to ensure stability. ",,1/3/2025,,,,,,,,,Infrastructure or Platform Failures,,The root cause has been attributed to a faulty network hardware component on the Nexus 7k switch at Swindon Data Centre.,NA,NA,NA,1/28/2025,January,2025,Rehan,Closed,MIM,N - No PIR actions as per the published PIR,2025,1,January,9/16/2025,256,>60
12/24/2024,12/24/2024,90503293,01:45,10:42,08:57,Foods,Food Supply Chain,Foods,Food Supply chain,Manage Allocation of Food Stock,SI,Delay in Foods Relex Overnight Batch impacting Foods Critical Flows,Relex,NA,"£11,281.00",Food Supplier orders,RC identified,70876,"Food supplier orders, NDC/RDC allocations were delayed due to an issue with the Relex overnight batch on 24/12 impacting picking operations.  The 28-day order plan, Foods Compliance, FLIC and OTS reports were also delayed. Suppliers were asked to use the previous day's 28 day order plan to mitigate the impact.  The root cause was attributed to a faulty network switch port within the Relex datacentre resulting in performance issues on one of their production environments. The faulty port was disabled by Relex to resume the overnight batch, NDC allocations were sent to the DCs by 07:52 and Foods supplier orders were sent by 10:42.",,12/24/2024,,,,,"Yes, No",Yes,"Yes, No",No,External Dependencies,No, The root cause was attributed to a faulty network switch port within the Relex datacentre resulting in performance issues on one of their production environments.,N/A,N/A,,,,,Naveen,Open,MIM,,2024,12,December,9/16/2025,266,>60
12/22/2024,12/22/2024,90501176,08:00,12:03,04:03,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Understated frozen allocations impacting Bedworth site operations,Relex,NA,£530.00,Bedworth Frozen Allocation,RC identified,70982,"Alerting indicated that the In-Store Bakery (ISB) allocations were lower than expected impacting planning activities at the Bedworth Depot. The root cause is attributed to an issue with the constraint functionality within Relex not being considered for zero day allocations. To mitigate the impact, contingency allocations for 22/12 were invoked, and planning operations resumed at site by 12:03. Contingency allocations have also been used for today (23/12) to support the volume for into store 27th and 28th Dec. Hypercare monitoring will be in place to ensure stability.",,12/22/2024,,,,,,,,,External Dependencies,,The root cause is attributed to an issue with the constraint functionality within Relex not being considered for zero day allocations.,NA,NA,NA,2/4/2025,February,2025,Naveen,Closed,MIM,Y,2024,12,December,9/16/2025,268,>60
12/21/2024,See note,TBC,See note,See note,See note,Customer Channels,Service Experience,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,KI,Payment issues through flexecash (Love2shop) gift cards on SCO tills,Flooid,NA,£1.05 M,Payments,RC identified,TBC,"As part of monitoring, issues were observed with part-payment transactions made through flexecash (Love2shop) gift cards on the self-checkouts tills potentially causing a revenue loss. The root cause has been attributed to an issue impacting the secondary payments through other cards within Worldline after the SCOT release on 01/12. As a workaround, the affected transactions are being processed manually via Worldline. Vendor Flooid had identified a solution which was implemented for 10 pilot tills on 21/12. Monitoring is in place and further deployment will be carried out after Christmas with required validations.",,12/21/2024,,,,,,,,,External Dependencies,,The root cause has been attributed to an issue impacting the secondary payments through other cards within Worldline after the SCOT release on 01/12,See note,TBC,NA,,,,Rehan,Open,MIM,,2024,12,December,9/16/2025,269,>60
12/13/2024,12/13/2024,90483254,08:00,9:24,01:24,Data,Data,Data,Data,Others,KI,Delay in Retail Dashboard sales data availability for 12/12,Customer,EDW,No Financial Impact,Retail Dashboard,RC identified,70837,"Through monitoring, it was identified that sales data for 12/12 was not updated in the Retail Dashboard resulting in a 90 mins delay in the availability of the data impacting the retail planning activities. The root cause has been attributed to an error whilst recovering a failed job responsible for processing sales data into Retail Dashboard. Services were restored after the job was retriggered by 09:24.",,12/13/2024,,,,,"Yes, No",No,"No, No",No,Operational Process Issues,N/A," On 11/12, the DS job responsible for processing sales data into Retail Dashboard had failed due to a database deadlock error and the underlying steps - load, transfer, extract steps within the failed job were manually recovered. However, product teams had missed to reset the actual job which resulted in no sales data being processed into Retail dashboard causing the issue.",N/A,N/A,N/A,12/18/2024,December,2024,Saloni,Closed,MIM,,2024,12,December,9/16/2025,277,>60
12/10/2024,12/13/2024,90484323,09:00,16:00,79:00,C&H and Intl,C&H & Intl Supply Chain,Customer Channels,Platform & Store Ops ,Manage Payment & Content Customer.Com,SI,Stores receiving C&C parcels for future collection dates causing capacity constraint,Customer,WMS,No Financial Impact,C&C orders,RC identified,70943,"Stores reported that they are receiving C&C orders much earlier than the expected collection date. Stores were unable to book in parcels due to space constraints and the collections slots for 104 stores were disabled between 10/12 and 13/12 on the website resulting in poor customer experience. The initial root cause has been attributed to an ""order hold rule"" in WMS, which references the ship-by-date in OMS (Sterling). As an interim fix, the “order hold rule” for Donington and Ollerton DC’s was amended to mitigate the impact to stores.",,12/10/2024,,,,,"No, Yes",No,"Yes, Yes",Yes,Software Defects/Limitation,N/A,"The Click & Collect (C&C) hold rule is designed to keep small store orders on hold until the ""Ship By Date"" interfaced from the OMS system. However, OMS has confirmed that this date should not be considered for C&C orders because it represents the earliest ""ship by date,"" causing orders to be released much earlier than their delivery date. ",N/A,N/A,N/A,3/6/2025,March,2025,Balaji,Closed,MIM,Y,2024,12,December,9/16/2025,280,>60
12/5/2024,12/20/2024,90474650,13:17,9:00,355:00,Customer Channels,Customer Engagement,Customer channels,Selling Experience,Manage Payment & Content Customer.Com,MI,"Customers encountering ""Sorry, Something Went Wrong"" errors on Customer.com website",Next Auth,NA,"£200,000.00",.Com Website,RC identified,70863,"Some customers are experiencing “Sorry, Something Went Wrong” errors whilst browsing on the Customer.com website.  This is impacting ~20k unique customers daily from early October resulting in a poor customer experience and an estimated potential revenue loss of £80k over a period of a month.  The initial root cause has been attributed to an issue in a third-party library that manages customer authentication. The product teams have identified a solution to introduce a retry mechanism for failed requests to mitigate impact to customer browsing journey. The solution has been tested environment and deployment is being planned.                                                                               ",,12/5/2024,,,,,"No, Yes",No,,No,External Dependencies,N/A,"The root cause has been attributed to authentication failures due to NextAuth’s session retrieval logic, which aborts API Auth requests on page navigations resulting in empty sessions and sign-in loops, leading to error pages. The issue is more frequent in Safari and is linked to deprecated API Auth provider usage in older NextAuth versions. 
 ",N/A,N/A,N/A,,,,Saloni,Open,.Com SM,Y,2024,12,December,9/16/2025,285,>60
12/4/2024,12/4/2024,90465705,14:00,19:11,05:11,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Castle Donington - Payment Initiation Messages not processing in MULE,TCS,Linux,"£10,800",Packing operations,RC unknown,70801,"Alerting indicated that Demand Notification, Order Fulfilment, ASNs & Goods Receipts messages were not processed into WMS at Donington, affecting 4.6k next-day delivery (NDD) orders. This impacted the pack capability loss of 18k singles & productivity loss of 308 hours incurring a financial loss of £10,800.  Order proposition was moved to +24 hours to mitigate customer miss promises (CFR - 0.52%). A workaround was implemented to mitigate the impact to NDD orders by 16:30. Initial root cause is attributed to a high number of uninterruptable processes were seen and the cause of these is being investigated along with Redhat. Services were restored after the Mule servers were restarted at 19:11.",,12/4/2024,,,,,"Yes, No",Yes,"Yes, Yes",No,Root Cause Undetermined/Still Under Investigation,No,"The root cause was attributed to uninterruptable sleep processes within the mule application resulting in no messages processing from mule to WMS. However, the actual reason for the uninterruptable process remains inconclusive.",NA,NA,NA,12/25/2024,December,2024,Pavithra,Closed,MIM,,2024,12,December,9/16/2025,286,>60
12/4/2024,12/5/2024,90453291,12:02,18:21,30:19,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,High number of Open POs in Relex impacting Foods Store Stock Availability,Customer,N/A,No Financial Impact,Stock availability at stores,RC identified,70914,"Approximately 700 product locations had reduced stock allocations in RELEX, impacting the stock availability across some stores. Investigations revealed around 177k open purchase orders in RELEX due to potential operational issues in WMS (Foods NDCs) and CSSM (Stores). In WMS, the foods NDCs were advised to systemically zero ship the already picked containers, and in CSSM, a decision was taken to allow the stock in transit to continue to auto-clear within 7 days (as BAU) for ambient and 14 days for frozen products. In addition, an amendment was carried out to reduce the visibility of stock in transit volume from 7 days to 3 days was carried out in RELEX to minimise the impact on reordering. Further actions will be agreed between Food Supply Chain Business and D&T to monitor the stock in transit positions.",,12/4/2024,,,,,"No,No",,"Yes,Yes",No,Operational Process Issues,,Various operational issues being handled by business teams,N/A,N/A,TBC,1/29/2025,January,2025,Gokul,Closed,MIM,Y,2024,12,December,9/16/2025,286,>60
12/2/2024,12/2/2024,90458656,03:40,8:23,04:43,Foods,Food Supply Chain,"Food, Data","Food Supply chain, Data",Manage Allocation of Food Stock,SI,Delay in Foods Relex Overnight Batch impacting Foods Critical Flows,Relex,NA,"£9,500.00","Food Supplier orders, NDC allocations, RDC allocations, Frozen allocations, Food reporting",RC identified,70694,"Food supplier orders, NDC allocations were delayed due to an issue with the Relex overnight batch.  This impacted the supplier orders, 28 day order plan, picking operations at Bradford & Milton Keynes DCs,  Foods Compliance reports, FLIC and OTS reports. A contingency plan has been invoked to generate the NDC allocations which is currently in progress. Foods supplier orders were sent by 06:55 and 28 day order plan was sent to suppliers by 07:08. The root cause has been attributed to inflated data volume due to a combination of Delivery Flow Smoothing capability and user updates in Relex, however, we await details from Relex.",,12/2/2024,,,,,"Yes, No",Yes,"Yes, Yes",No,External Dependencies,No,"The root cause has been attributed to inflated data volume due to a combination of Delivery Flow Smoothing capability and user updates in Relex, however, we await details from Relex.",N/A,N/A,"FOATR-26670, FOATR-6136, FOATR-21975",2/20/2025,February,2025,Kavitha,Closed,MIM,Y,2024,12,December,9/16/2025,288,>60
12/1/2024,12/1/2024,90457701,00:04,6:02,05:58,Foods,Food Supply Chain,Foods,Food Supply chain,Manage Allocation of Food Stock,KI,Delay in Foods Relex Overnight Batch impacting FFO,Relex,NA,No Financial impact,Food Supplier orders,RC identified,70711,"Foods supplier orders were delayed due to an issue with the Relex overnight batch, however, no impact to RDC and NDC allocations. The batch was recovered by 23:26 and the Foods supplier orders were sent by 06:02 (beyond the target completion time of 05:15). The root cause was attributed to the processing of historical data (1.4M records) which was added to resolve a separate issue delaying the weekly Relex batch.",,12/1/2024,,,,,"Yes, No",Yes,"Yes, No",No,External Dependencies,No,The root cause was attributed to the processing of historical data (1.4M records) which was added to resolve a separate issue delaying the weekly Relex batch.,N/A,N/A,N/A,2/20/2025,February,2024,Naveen,Closed,MIM,Y,2024,12,December,9/16/2025,289,>60
11/30/2024,11/30/2024,90463097,03:30,3:50,00:20,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,SI,Digital Receipts sent to customers for historical transactions,Customer,DROS (Digital Receipts Orchestration System) ,No Financial impact,Poor customer experience,RC identified,70680,"Around 60K e-receipts were sent to customers for historical transactions resulting in poor customer experience and increase in calls to the customer service centre, however, payments were not taken from the customers. The root cause was attributed to an API issue within vendor Yocuda infrastructure on 29/11 who advised to reprocess the ~27k failed messages in DROS (Digital Receipts Orchestration system). Product team had inadvertently reprocessed all the messages from the DROS exception queue resulting in sending the historic e-receipts to the customers. ",,11/30/2024,,,,,"No, Yes",No,"No, No",No,Operational Process Issues,No,The root cause was attributed to an inadvertent processing (human error) of all digital receipt messages from the DROS (Digital Receipts Orchestration System) exception (non-retry) queue during a recovery action following an API issue within Yocuda resulting in sending 60k messages to customers.,N/A,N/A,RSS-3284,1/7/2025,January,2025,Saloni,Closed,MIM,,2024,11,November,9/16/2025,290,>60
11/28/2024,11/28/2024,90451656,07:24,8:18,00:54,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,SI,Store tills offline across the estate,TCS,Network,£440.00,Till payments,RC identified,70715,"Through alerting and store shouts, it was identified that store tills were offline between 07:24 and 08:18. Customers were unable to redeem gift cards and credit vouchers in store impacting 73 gift card transactions incurring a financial loss of ~£440, however, payments were unaffected. The root cause was attributed to a known firmware bug (memory leak) on the F5 network load balancer in Stockley Park resulting in an unexpected reboot of the active load balancer. Although services were failed over, configuration sync issues were identified on the standby load balancer which were manually synced to restore the services. A proactive restart of the Stockley F5 load balancer was performed to safeguard instore trading during the Golden Quarter. ",,11/28/2024,,,,,"Yes, No",Yes,"Yes, Yes",No,Software Defects/Limitation,No,"The root cause was identified as a memory leak caused by a known firmware bug on the Stockley Park F5 load balancer, leading to an unexpected reboot of the active load balancer followed by a configuration sync issues after the failover on the standby load balancer.
A case was raised with F5, and they recommended upgrading the firmware version. However, this upgrade will not be performed during the Golden Quarter as this incurs an outage across the estate.",N/A,N/A,NA,3/13/2025,March,2025,Gokul,Closed,MIM,Y,2024,11,November,9/16/2025,292,>60
11/28/2024,11/28/2024,90450816,03:10,11:08,07:58,Foods,Food Supply Chain,"Food, Data","Food Supply chain, Data",Manage Allocation of Food Stock,SI,Delay in Foods Relex Overnight Batch impacting Foods Critical Flows,Relex,NA,See note,"Food Supplier orders, NDC allocations, RDC allocations, Frozen allocations, Food reporting",RC identified,70578,"Food supplier orders,  NDC allocations were delayed due to an issue with the Relex overnight batch.  A workaround was applied to generate the NDC allocations by 06:00. A contingency plan was invoked to mitigate impact to supplier orders, 28-day order plan, Foods Compliance reports, FLIC and OTS reports. Foods supplier orders were sent at 08:01. The Relex batch was recovered by 09:05,and the pending NDC allocations for the impacted 21 products were sent to Bradford.  The root cause was attributed to a known bug within Relex production environment which was fixed on 28/11 through a planned release.",,11/28/2024,,,,,"Yes, No",Yes,"Yes, Yes",Yes,External Dependencies,No, The root cause was attributed to a known bug within Relex production environment which was fixed on 28/11 through a planned release.,N/A,N/A,"FOATR-26670, FOATR-6136, FOATR-21975",2/20/2025,February,2025,Kavitha,Closed,MIM,Y,2024,11,November,9/16/2025,292,>60
11/24/2024,11/26/2024,90458691,01:20,11:00,57:40,GTS,Digital Workplace Services,GTS,Digital Workplace Services,Infrastructure,KI,Some colleagues unable to access or use Microsoft 365 services and features,Microsoft,NA,No Financial impact,Microsoft (Outlook and Teams),RC identified,70695,"Some colleagues are unable to access Exchange Online using Outlook Web and the Outlook desktop client. Some colleagues may also be unable to access or modify their calendar in Microsoft Teams. This would include loading calendar, viewing, creating/updating and joining meetings. The root cause is attributed to a change deployed by Microsoft and this was reverted to restore services on 27/11.",,11/25/2024,,,,,"No, No",No,"No, No",Yes,External Dependencies,Global change,The root cause is attributed to a change deployed by Microsoft and this was reverted to restore services on 27/11.,N/A,N/A,NA,11/27/2024,November,2024,Rehan,Closed,MIM,,2024,11,November,9/16/2025,296,>60
11/22/2024,11/22/2024,90437625,04:15,12:40,08:25,Foods,Food Supply Chain,"Food, Data","Food Supply chain, Data",Manage Allocation of Food Stock,MI,"Delay in Relex overnight batch impacting the availability of foods supplier orders, NDC allocations",Relex,NA,"£15,600.00","Food Supplier orders, NDC allocations, RDC allocations, Frozen allocations, Food reporting",RC identified,70614,"Alerting indicated a delay in the food supplier orders, RDC & NDC allocations due to an issue with the Relex overnight batch.  As a workaround, suppliers were advised to use the previous day's 28-day order plan, and contingency plan was applied to generate RDC, NDC & Frozen allocations, foods compliance & FLIC reporting were also impacted. Further issues were identified in the NDC allocations generated for cross-dock orders which were resent to Milton Keynes by 11:37 and Bradford by 12:08 impacting DC operations. The root cause was attributed to a combination of a human error during a deployment by the Relex devops (Accenture) and an infrastructure issue within the Relex SaaS platform.",,11/22/2024,,,,,"Yes, No",Yes,"Yes, Yes",NA,External Dependencies,CR,"On 21/11, as part of the Relex fortnightly release to include multiple bug fixes and new features under CRQ212574. In it, the Relex DevOps (Accenture) had incorrectly executed the product assortment step (human error), resulting in Env1 products being active in Env2. The issue stemmed from the assortment restriction job — which determines which lines are calculated in each environment — being set to Env 1 instead of Env2 for Env2 products.",CRQ212574,Human error,NA,1/29/2025,January,2025,Gokul,Closed,MIM,,2024,11,November,9/16/2025,298,>60
11/21/2024,21/11/2024,90435629,02:15,7:15,05:00,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Network,KI,Castle Donington - Users unable to access internet/ applications through 'SALINA' network,Zscaler,NA,No Financial impact,Flow Room Operations,RC unknown,70806,"Castle Donnington colleagues were unable to access web based applications such as Power BI, BMC Helix (Remedy), etc via the SALINA wireless network between 02:00 & 07:15 on 21/11 and 21:40 (22/11) & 02:00 (23/11). As a workaround, colleagues were connected to the ""CHIEF"" wireless network to access the applications, there was no impact to DC operations. The root cause was attributed to an issue with the Zscaler traffic routing through the London 3 datacentre . Services were restored without any tech intervention on 21/11, however, a workaround was applied to route the traffic to London 5 (Primary) and Manchester 1 (Secondary) Data Centre to restore services on 22/11.",,21/11/2024,,,,,"No, Yes",No,"Yes , Yes",No,Root Cause Undetermined/Still Under Investigation,NA, The root cause was attributed to an issue with the Zscaler traffic routing through the London 3 datacentre ,NA,TBC,TBC,13/03/25,March,2025,Rehan,Closed,MIM,,2024,11,November,9/16/2025,299,>60
11/21/2024,11/21/2024,90436862,11:36,12:59,01:23,Customer Channels,Service Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,KI,Customers unable to use PayPal to pay for orders,PayPal,NA,"£17,000.00",Customer experience,RC identified,70597,"Alerting identified that customers were unable to make payments using Paypal between 11:36 and 12:59 resulting in poor customer experience, however other payment methods were unaffected. Approx. 1,361 orders were anticipated to be impacted when compared with previous day's sale through Paypal. The root cause was attributed to a central issue at Paypal, which was resolved to restore the services.  We await root cause from Paypal.",,21/11/2024,,,,,"Yes,No",Yes,"Yes, Yes",Yes,External Dependencies,NA," The root cause was attributed to a central issue at Paypal, which was resolved to restore the services.",NA,NA,NA,12/12/2024,December,2024,Saloni,Closed,.Com SM,,2024,11,November,9/16/2025,299,>60
11/21/2024,11/22/2024,90435749,00:24,20:00,43:36,Customer Channels,Platform & Store Ops ,"Customer Channels, Food","Platform & Store Ops, Foods Supply Chain",Manage Allocation of Food Stock,MI,CSSM Food Stock data corruption impacting store operations & reporting,Customer,CSSM,Finance impact unavailable,Food Stock Data,RC identified,55360,"Store operations were impacted due to a data corruption in the CSSM food stock. Foods reporting, ASO allocations, Store Stock Tool, and Shelf Edge Tickets for 2 UPCs were impacted. Stores were blocked centrally from counting to avoid further stock discrepancies. The root cause was attributed to human error resulting in a stock value amendment of all products in the CSSM database with null values. The stock values for majority of the products were corrected followed by the CSSM database sync and store counting was unblocked by 12:52. 
On 22/11, the stock position for another 72k impacted UPCs were corrected in CSSM. A full reconciliation was performed within CSSM and no issues were reported by the stores over the weekend.",,21/11/2024,,,,,"Yes, No",Yes,"No,No",TBC,Operational Process Issues,NA,The root cause was attributed to a combination of a known issue within CSSM (producing the null values for certain UPC’s) and a manual error while addressing the null value scenario.,NA,NA,NA,,,,Pavithra,Open,MIM,Y,2024,11,November,9/16/2025,299,>60
11/19/2024,11/19/2024,90430167,06:35,8:22,01:47,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,WMS – JDA dispatcher application inaccessible at Outlet DC,TCS,Network,"£3,185.07",DC operations ,RC identified,70945,"Peterborough (Outlet) DC colleagues reported issues whilst accessing JDA Dispatcher application on their HHTs (Hand held terminals) and workstations from 06:00 impacting the picking and packing operations at site. This resulted in productivity loss of circa 219.66 hrs incurring a financial loss of £3,185.07. The root cause was attributed to a known bug on the existing FortiGate firewall version impacting the network connectivity between the HHT's and the WMS application. Services were restored by resetting the network tunnel by GXO IT at 08:22. ",,22/11/2024,,,,,"No, Yes",No,"Yes , Yes",No,Software Defects/Limitation,NA,The root cause was attributed to a known bug on the existing FortiGate firewall version impacting the network connectivity between the HHT's and the WMS application.,NA,NA,NA,,January,1900,Kavitha,Open,MIM,Y,2024,11,November,9/16/2025,301,>60
11/17/2024,11/18/2024,90428181,22:45,0:33,01:48,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,SI,Ollerton DC - Label printing issues on 17/11,TCS,Linux/VMWare,No Financial impact,Carrier Label Printing,RC identified,70555,Ollerton DC colleagues were unable to print carrier labels between 22:45 and 00:33 impacting packing and despatch operations of 942 ecomm orders resulting in customer miss promises. The initial root cause has been attributed to an unexpected restart of Mule cluster servers. Services were restored by manually restarting the cluster services. Root cause investigations underway with Redhat.,,11/18/2024,,,,,"Yes,No",Yes,"Yes,No",No,Infrastructure or Platform Failures,NA,The root cause has been attributed to a hardware failure (storage controller fault) on one of the esxi hosts resulting in an unexpected reboot of the Ollerton Mule server hosted on a different esxi host within the common storage (VSAN) infrastructure. ,NA,NA,NA,3/3/2025,March,2025,Pavithra,Closed,MIM,Y,2024,11,November,9/16/2025,303,>60
11/15/2024,11/15/2024,90423246,17:00,20:40,03:40,C&H and Intl,C&H Commercial Trading,"C&H and Intl, Customer Channels","Selling Experience, C&H Commercial Trading",Manage Payment & Content Customer.Com,KI,Messages from PIM failing in Product Domain API,Customer,PDX,No Financial impact,Website,RC identified,70513,"Business colleagues reported messages failures between Product Information Management (PIM) system and Product Domain API. Around 270 products were unavailable for customers & 5.5k updates to existing products were not reflecting on ordering platforms. The initial root cause has been attributed to PDX in-memory change on 14/11 resulting in a null value on a mandatory field in the Product Domain API. Services were restored by reverting the change through a configuration rollback and application restarts by 20:40, 15/11. Root cause investigations underway.",,11/15/2024,,,,,"No, Yes",No,"No, No",Yes,Change Failure,CR,The issue is caused due to null values in the In-Memory database which is creating constraint errors while performing buyer approval. Customer STEP is running on 11.0MP4 which does not have capability of clearing null value from In-Memory Database.  PIM Product team and Stibo to understand the root cause of the issue. Stibo team gathered further information to continue the RCA.,CM-12496,TBC,NA,2/26/2025,February,2025,Gokul,Closed,.Com SM,Y,2024,11,November,9/16/2025,305,>60
11/14/2024,11/14/2024,90422088,01:30,2:17,00:47,C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Sorted issue impacting Donington packing operations,Sorted,NA,"£5,000.00",Carrier Label Printing in CD,RC identified,70705,"Through proactive monitoring, intermittent issues were identified with Sorted carrier label printing from 23:49 at Donington. Packing operations were impacted between 01:30 and 02:17 resulting in a capability loss of 13k singles at the site.  The root cause was attributed to a load balancer failure within Sorted infrastructure which was fixed by 02:17, 15/11.  Awaiting root cause and recovery details from Sorted.",,11/14/2024,,,,,,,,,External Dependencies,, The root cause was attributed to a load balancer failure within Sorted infrastructure which was fixed by 02:17,NA,NA,NA,11/20/2024,November,2024,Kavitha,Closed,MIM,NA - Not MIM managed incident,2024,11,November,9/16/2025,306,>60
11/14/2024,11/14/2024,90422069,19:51,20:31,00:40,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Welham DC - WMS JDA Application unavailable,TCS,Linux/VMWare,"£7,100.00",WMS,RC identified,70522,"JDA Dispatcher application was unavailable at Welham Green DC impacting site operations between 19:51 and 20:31. This resulted in a capability loss of 10.1k singles and productivity loss of 426 hours incurring a financial impact of £7.1k.  The root cause was attributed to an unexpected restart of the WMS database server due to a hardware fault on the ESXi host. Services were restored by migrating the database server to a healthy host.  JDA Dispatcher application services were restarted at 23:20 to resolve the residual issues with boxed automation, YMS and HHT connectivity.  Further investigations underway with vendor HP with a plan to replace the faulty hardware on 18/11. ",,11/14/2024,,,,,"Yes, No",Yes,Yes,Yes,Infrastructure or Platform Failures,,The root cause was attributed to an unexpected restart of the WMS database server due to a hardware (CPU) fault on the ESXi host. ,NA,NA,NA,1/6/2025,January,2025,Saloni,Closed,MIM,,2024,11,November,9/16/2025,306,>60
11/14/2024,11/14/2024,90422175,20:45,22:26,01:41,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Carrier label printing issues at Ollerton DC on 14/11,Customer,WMS,"£1,700.00",Carrier Label Printing,RC Identified,70520,"Ollerton DC colleagues were unable to print carrier labels between 20:45 and 22:26 impacting packing and despatch operations. This had resulted in a productivity loss of 104 hours incurring a financial impact of £1.7k, however, no impact to customer miss promises. The initial root cause has been attributed to a gradual increase in print jobs being piled up resulting in an overall breach of the print spooler capacity. Services were restored by clearing the piled up messages from the print queues. Root cause investigations underway.",,11/14/2024,,,,,"Yes, No",Yes,Yes,Yes,Operational Process Issues,No,The  root cause was attributed to an error made by the DC colleagues whilst replacing the lable film of the printer not closing the printer completely resulting in a gradual increase in print jobs being piled up eventually breaching the overall print spooler capacity.,NA,NA,NA,1/11/2025,January,2025,Saloni,Closed,MIM,,2024,11,November,9/16/2025,306,>60
11/14/2024,11/14/2024,90420458,07:19,7:40,00:21,GTS,Enterprise Integration, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Issue at Middleware (MULE) impacting CD Site pack operations,Customer,Mule,"£1,000.00",Packing operations,RC unknown,70577,"Through proactive monitoring, issues were identified within Mule audit layer resulting in the demand notification response files to pile up in the ""out reject"" folder. Therefore, the Mule server was proactively restarted between 07:19 and 07:40 to restore services resulting in a packing capability loss of 3k singles. The initial root cause has been attributed to an increase in open files in the middleware queue breaching the threshold resulting in message pile ups. Testing is completed to increase the threshold in Mule with a plan to deploy on 19/11, this involves a site outage and requires planning. Root cause investigations underway. ",,11/14/2024,,,,,"No, Yes",Yes,Yes,No,Root Cause Undetermined/Still Under Investigation,No,"The root cause was attributed to messages pileup in middleware queue, due an increase in the number of open files in the outreject folder. The reason behind the increase in the number of open files remains inconclusive.",NA,NA,NA,1/16/2025,January,2025,Naveen,Closed,MIM,,2024,11,November,9/16/2025,306,>60
11/11/2024,11/11/2024,90414184,13:43,14:07,00:24,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Digital Cafe issue with blank screens on the kiosks,Counter Solutions,NA,"£2,980.00",Digital Café,RC identified,70620,"Store colleagues reported blank screens on the kiosks across the 17 Digital Cafes stores between 13:43 and 14:07. Customers were unable to place orders on the kiosks impacting ~252 transactions resulting in loss of sales (£2,980) and poor customer experience. As a workaround, ""hidden tills"" were used to place orders. The root cause was attributed to one of the API services (IIS service) within Counter Solutions going into an unresponsive state, the impacted API service was manually restarted to restore the services. A permanent fix was implemented on 11/11. ",,11/11/2024,,,,,"Yes,No",No,Yes,NA,External Dependencies,NA,"Root cause has been attributed to an error on one of a the critical API services, hosted on Scale-Out File Server (SOFS) infrastructure within Counter Solutions.",NA,NA,NA,11/20/2024,November,2024,Pavithra,Closed,MIM,,2024,11,November,9/16/2025,309,>60
11/8/2024,11/8/2024,90405381,11:20,13:26,02:06,Customer Channels,Service Experience, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Delay in sending order fulfilment messages to Donington,Customer,Sterling,See note,CD OF Messages,RC unknown,70518,"At 11:20, alerting indicated a delay in sending the order fulfilment messages for some Donington orders impacting the site operations. Around 6.3k orders were impacted and proposition was moved for Click & Collect and Next Day Delivery orders to mitigate customer miss promises. The root cause was attributed to a connectivity issue between Control-M agent and the OF message processing jobs in Sterling resulting in the job status in ""Executing"" state. The impacted jobs were re-triggered to recover the services by 13:26. Further investigations underway.",PIR to be scheduled,11/8/2024,,,,,"Yes,No",Yes,No,NA,Root Cause Undetermined/Still Under Investigation,NA,"The root cause was attributed to a connectivity issue between Control-M agent and the OF message processing jobs in Sterling resulting in the job status in ""Executing"" state. The reason behind the connectivity issue could not be ascertained.",NA,NA,NA,11/21/2024,November,2024,Saloni,Closed,MIM,,2024,11,November,9/16/2025,312,>60
11/8/2024,11/8/2024,90405232,09:17,10:46,01:29,Customer Channels,Customer Engagement,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,SI,Sign in Pages not loading on Mobile Apps,Secure Auth,NA,"£4,840.00",Mobile App,RC identified,70565,"Customers were encountering errors whilst logging in on both iOS and Android mobile apps from 09:17, however, customers already logged in were unaffected. Approx. 4.4k errors were observed resulting in poor customer experience. The root cause was attributed to a change within the vendor Secure Auth environment. Services were restored after Secure Auth had reverted the change at 10:46.",PIR to be scheduled,11/8/2024,,,,,"No, ",No,No,,External Dependencies,NA,The root cause was attributed to a change within the vendor Secure Auth environment.,NA,NA,NA,1/16/2025,January,2025,Kavitha,Closed,MIM,,2024,11,November,9/16/2025,312,>60
11/8/2024,11/8/2024,90404927,08:45,10:00,01:15,GTS,Enterprise Technology Platform,Customer Channels,Platform and store ops,Manage Allocation of Food Stock,SI,Store colleagues unable to access Foods CSSM application functionalities,Customer,CSSM,See note,Foods CSSM,RC unknown,70617,"Store colleagues reported issues whilst accessing Foods CSSM applications impacting  their ability to view store stock information (Foods SST app) between 08:45 and 10:00. Also, Accuracy checker, Gap scan, Top up and Stock scan functionalities were unavailable impacting store operations. The initial root cause has been attributed to a CPU spike on the CSSM reporting database server. The SST application PODs were restarted to restore services. Root cause investigations underway.",PIR scheduled on 12/11,11/8/2024,,,,,"Yes, Yes",Yes,"No, No",No,Root Cause Undetermined/Still Under Investigation,NA,"Microsoft updated that they could not find any abnormalities reported and on analyzing the system health extended events, they could see a session ID which executes some select query has consumed 95% of CPU for 250 Milliseconds, however it has not been logged and any other detail about the query, so we cannot determine what was the query, who triggered it and from where it was coming from, other than this they could not find any other abnormalities reported. So, if the issue reoccurs then during the issue time, MS has suggested to execute a SQL_LogScout tool to generate further logs to investigate deeper on the issue",NA,NA,NA,12/20/2024,December,2024,Gokul,Closed,MIM,,2024,11,November,9/16/2025,312,>60
11/8/2024,11/8/2024,DN ref: 126154285,08:31,8:49,00:18,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,KI,Orders not being placed on the kiosks across all Digital Cafes,Counter Solutions,NA,£562.00,Digital Café,RC identified,70620,"Store colleagues reported that orders were not being placed on the kiosks across the 16 Digital Cafes between 08:31 and 08:49 resulting in poor customer experience. As a workaround, stores were advised to man the ""hidden tills"" causing inconvenience to store colleagues. The root cause was attributed to an issue within QSR (partner of Vendor counter solutions) which was resolved to restore services. Awaiting root cause analysis from QSR.",PIR to be scheduled,11/8/2024,,,,,"No,No",No,Yes,NA,External Dependencies,NA, The root cause was attributed to an infrastructural issue within QSR (partner of  Vendor counter solutions) which was resolved to restore services.,NA,NA,NA,12/20/2024,December,2024,Pavithra,Closed,MIM,,2024,11,November,9/16/2025,312,>60
11/7/2024,11/7/2024,90401735,05:44,6:48,01:04,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,KI,Ireland home page unavailable from some customers,Customer,.Com,No Financial impact,Ireland website,RC identified,70396,"Business Ops team identified that some of the customers were encountering a blank homepage on the Ireland website across both desktop and mobile web between 05:44 and 06:58 resulting in poor customer experience, however, there was no drop in orders. The initial root cause has been attributed to some requests pointing to a problematic configuration (header) resulting in cache pollution. Services were restored by clearing the Akamai cache on the homepage allowing the new build to take effect. Root cause analysis underway.  ",PIR to be scheduled,11/7/2024,,,,,"Yes,No",Yes,No,No,Software Defects/Limitation,NA,The root cause has been attributed to some requests pointing to a problematic configuration (header) resulting in cache pollution - this appears to be a known issue when using next-JS which returns a successful (200) empty 2-byte response when this header is received.,NA,NA,https://github.com/vercel/next.js/issues/52515,12/5/2024,December,2024,Saloni,Closed,.Com SM,,2024,11,November,9/16/2025,313,>60
11/4/2024,11/6/2024,90396012,22:30,11:30,37:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Issues with Knapp WCS automation in Welham DC impacting inbound boxed receipt,KNAPP,NA,£760.00,WG inbound receiving,RC identified,70901,"Welham Green DC colleagues reported issues receiving inbound stock via the KNAPP WCS automation system from 22:30 on 04/11.  This resulted in the capability loss of 85k singles, however, site had increased the number of colleagues to mitigate the operational impact. The initial root cause has been attributed to a change within KNAPP infrastructure on 04/11 which was reverted by 11:40 on 05/11. A full reconciliation was performed between KNAPP WCS and JDA Dispatcher WMS systems to identify the missing ASNs which were retriggered to restore services by 11:30, 06/11. Awaiting root cause details from KNAPP.",PIR scheduled on 12/11,11/5/2024,,,,,"No, Yes",No,"Yes, Yes",No,External Dependencies,NA,Knapp advised that they had carried out a change at 6am on 4th November.  The content of this change was a bug fix to resolve an issue with the dummy barcode when processing empty totes.  The side effect of the change was that it resulted in the closure of all containers in the warehouse and set error 8589934592 on all of them. ,NA,NA,NA,12/24/2024,December,2024,Rehan,Closed,MIM,,2024,11,November,9/16/2025,316,>60
11/3/2024,11/3/2024,90392152,19:34,21:21,01:47,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Unable to pick orders in CD due to a controller fault,SSI Schaefer,NA,"£5,000.00",CD Picking,RC unknown,70556,"Castle Donington DC colleagues were unable to pick orders between 19:34 - 21:21 due to a mechanical controller issue. This resulted in capability loss of 15K singles, CFR of 1026 orders and proposition was moved by +24 hours. The root cause was attributed to an error within the SCS system (responsible for the automation) resulting in a backlog of pending events impacting the WCS automation system. Services were restored by clearing the pending events and restart of WCS by Vendor SSI. ","PIR held  and the agreed problem actions are added to the ""problem actions tab"" for further tracking. ",11/3/2024,,,,,"No,No",No,"No,no",NA,Root Cause Undetermined/Still Under Investigation,NA,"The underlying cause remains undetermined -however it stems from either a mechanical malfunction or a potential manual intervention.

During the incident day, the light barrier on the conveyor of the SSI - SCHAEFER Carousel System failed to detect an item or tote while processing orders, resulting in a ""NO SCAN"" situation. This failure has created a backlog of pending events within the SSI automation system, leading to a complete halt in operations, which subsequently affected the picking processes on-site.
",NA,NA,NA,12/24/2024,December,2024,Pavithra,Closed,MIM,,2024,11,November,9/16/2025,317,>60
10/31/2024,11/22/2024,90387948,04:00,16:00,540:00,C&H and Intl ,C&H & Intl Supply Chain,"Customer Channels, C&H and Intl, Foods","Platform and Store Ops, C&H and Intl Supply Chain, Foods Supply Chain",Others,SI,Track My Vehicle application is unavailable due to a central issue at Microlise,Microlise,NA,See note,Track My Vehicle,RC identified,70644,"Due to a central issue at Microlise, the Track My Vehicle application is unavailable from 04:00 (31/10). This is affecting vehicle tracking and planning activates across multiple portfolios. However, there is no impact to DC operations or store deliveries. There is a manual workaround in place to remediate the inconvenience caused.  Vendor Microlise is prioritising service restoration to mitigate any business impact.",,10/31/2024,,,,,No,No,Yes,Yes,External Dependencies,NA,A cyber attack at Microlise had caused the issue. ,NA,NA,NA,12/5/2024,December,2024,Saloni,Closed,MIM,,2024,10,October,9/16/2025,320,>60
10/30/2024,10/30/2024,90381051,08:33,9:08,00:35,C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent carrier label printing issues at multiple C&H sites,Sorted,NA,£360.00,Packing operations,RC identified,70516,"Donington colleagues reported intermittent carrier label printing issues between 8:33 and 9:30. Similar issues were identified across Ollerton, Brands, C&H Bradford and Stoke DCs impacting packing operations, resulting in a capability loss of 600 orders at Donington, with no impact to other sites. The root cause was attributed to a change deployed by vendor Sorted, which was reverted to restore services. Awaiting root cause details from Sorted.",PIR scheduled on 5th Nov,10/30/2024,,,,,Yes,No,No,NA,External Dependencies,NA,"The root cause was attributed to a change deployed by vendor Sorted, which was reverted to restore services. Awaiting root cause details from Sorted.",NA,NA,JDATE-10749,12/5/2024,December,2024,Saloni,Closed,MIM,,2024,10,October,9/16/2025,321,>60
10/25/2024,10/25/2024,90369835,10:40,11:00,00:20,GTS,Enterprise Technology Platform,Foods,Foods,Manage Pick of Food Stock,KI,WMS application was inaccessible impacting Milton Keynes DC operations,TCS,Linux,"£1,980.00",WMS appliction at MK DC,RC unknown,70394,"Alerting indicated that the WMS application was inaccessible at Milton Keynes DC between 10:40 and 11:00 resulting in a capability loss of 3.3k singles. The initial root cause has been attributed to a sudden reboot of the primary server followed by an unsuccessful failover to secondary server due to a partial Oracle client installation after the Oracle 19C upgrade last year. Services were restored on the primary server, however, root cause investigations underway with Redhat.",,10/25/2024,,,,,"Yes,No",Yes,No,NA,Root Cause Undetermined/Still Under Investigation,NA,"The root cause was attributed to a Network File system (NFS) timeout on the Linux cluster, which triggered a reboot of the primary application server followed by an unsuccessful failover to the secondary server as the relevant oracle client was not installed on the secondary server. Redhat could not determine the underlying cause of the file system timeout on the day of the incident, therefore, root cause remains inconclusive
",NA,NA,NA,2/6/2025,February,2025,Saloni,Closed,MIM,Y,2024,10,October,9/16/2025,326,>60
10/23/2024,10/24/2024,DN ref: 125726009,10:55,13:19,  26:14:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,KI,Discounts and vouchers not being accepted at Digital Café stores,Customer,Digital café,£55.00,Staff discounts at Digital Cafes,RC unknown,NA (Issue with DN and Remedy integration - Actions agreed),Around 54 store colleagues reported issues with their 20% staff discount not working in digital cafes. The root cause was attributed to a modification of the discount from multi-use to single-use on 22/10.  Services were fully restored by reverting the discount to a multi-use offer on 24/10. Investigations continue for a small number of colleagues who briefly accessed the discount under the single-use setting to re-enable the multi-use offer.,NA,10/23/2024,,,,,NA,NA,NA,Yes,Root Cause Undetermined/Still Under Investigation,NA,"The initial root cause was attributed to a modification of the discount from multi-use to single-use in October 2022. However, the cause of this modification is unknown, and there is no evidence to confirm the change. As a result, the root cause remains undetermined.",NA,NA,NA,10/29/2024,October,2024,Saloni,Closed,MIM,,2024,10,October,9/16/2025,328,>60
10/20/2024,10/20/2024,90358279,01:20,3:30,02:10,Group Platforms,Finance, C&H and Intl,C&H and Intl Supply Chain,Manage Allocation of C&H Stock,KI,cSAP linux patching impacting retail orders flow & BEAM reports,Customer,SAP,No Financial impact,Demand and Fulfilment overnight batch and C&H critical reports,RC identified,70903,"Following the Linux OS patching on the CSAP system on 19/10, alerting indicated a delay in the completion of D&F (Demand &Fulfilment) orders processing to Donington and other DCs. There was a delay in updating the latest data in C&H daily sales report, however, there was no impact across all the DCs. The root cause was attributed to some SAP cyclic jobs responsible for processing orders into WMS not being released after the outage. These jobs were manually retriggered to process the orders into WMS by 04:20. Root cause analysis underway.",,10/20/2024,,,,,"Yes,No",Yes,No,NA,Change Failure,CR,"some of the cyclic jobs responsible for the processing Demand & Fulfilment (D&F) orders had gone to a wait condition after the release of the jobs (Force start) delaying the overnight batch.
 
The D&F jobs in SAP executes on a cyclic basis.
On the day of the outage, the initial executions of these jobs proceeded as expected, but subsequent instances entered into a wait condition in CONTROL M. The team subsequently retriggered the job by clearing the wait condition, resulting in a delay of 12 minutes to the overall SLA.",CRQ210534,Inadequate post validations,NA,3/26/2025,March,2025,Naveen,Closed,MIM,Y,2024,10,October,9/16/2025,331,>60
10/17/2024,10/17/2024,DN ref: 125579373,09:20,13:00,03:40,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Issues with Digital cafe orders not reaching kitchens from kiosks,Counter Solutions,NA,£100.00,Store colleaagues inconvenience,RC identified,NA (Issue with DN and Remedy integration - Actions agreed),"Store colleagues reported that the orders placed on the Digital Café kiosks were intermittently not sent to the kitchen between 9:20 and 13:00.  Around 500 orders were placed during the impacted window, however the actual impacted orders were minimal (i.e 8%), as a workaround the store colleagues had to manually take the orders to the kitchen resulting in operational inconvenience.  The root cause was attributed to a central issue within the vendor QSR’s infrastructure, and a fix was deployed to restore services.  Awaiting root cause details from QSR.",Actions are being tracked in the actions tab,11/7/2024,,,,,"No, Yes",No,"Yes, Yes",No,External Dependencies,NA,"Counter solutions updated that a failure in the underlying system to send an alert indicating an issue with 
the pod servicing the application. This pod failed to automatically restart after encountering out-ofmemory errors",NA,NA,NA,1/7/2025,January,2025,Gokul,Closed,MIM,,2024,10,October,9/16/2025,334,>60
10/14/2024,10/15/2024,90350936,09:00,6:00,21:00,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,SI,Ready to collect email comms issue,Customer,OMS,No Financial impact,Customer order collection,RC identified ,70433,"At 09:00, whilst comparing the ready-to-collect email communications from the past week, it was identified that the ready-to-collect emails for 14/10 were not sent to customers.  Around 61k emails were delayed and were sent on 15/10 resulting in poor customer experience. The root cause was attributed to a connectivity issue between Control-M and the middleware queue manager which caused the Sterling job to execute without any data.  Services were restored after the scheduled job ran at 06:00 on 15/10 and the Control M pods were restarted at 14:05. Root cause investigations underway.",,10/14/2024,,,,,"No, Yes",No,No,NA,Infrastructure or Platform Failures,NA,The root cause was attributed to a connectivity issue between Control-M and the middleware queue manager which caused the Sterling job to execute without any data,NA,NA,NA,11/18/2024,November,2024,Kavitha,Closed,MIM,,2024,10,October,9/16/2025,337,>60
10/14/2024,10/14/2024,90346086,15:10,20:30,05:20,Customer Channels,Service Experience,"Customer Channels, C&H and Intl, Foods","Selling Experience, Customer Engagement, C&H and Intl Supply Chain, Foods",Manage Payment & Content Customer.Com,MI,Worldine payment issue causing some Mastercard payments to be declined and/or go offline,Worldline,NA,"£35,000.00",Online order payments,RC identified,70431,"Alerting identified a drop in orders due to an issue with card payments at 15:10.  This impacted around 805 transactions in the first 15 minutes, 23% reduction in the order success rate and 114 Sparks Pay transactions resulting in poor customer experience.  Order proposition was moved for Next day delivery orders across all ecomm DCs with a customer miss promise of 338 orders at Donington.  The root cause was attributed to a change within the Worldline infrastructure affecting MasterCard payments only.   As a workaround, payments were switched to the offline mode and services were restored by 20:30 after the change was reverted by Worldline. Awaiting root cause details from Worldline",Actions are being tracked in the actions tab,10/14/2024,,,,,"Yes, No",Yes,"Yes, Yes",Yes,External Dependencies,CR,"The root cause was an Customer specific change associated with an incident fix carried out by the Worldline Level 1 (L1) Support team in Belgium. They checked the MID/TID configuration and noticed that the Merchant Category Code (MCC) for Retail was blank on Mastercard. They came to the (incorrect) conclusion that the blank MCC was a 
problem and decided to proactively fix the issue. The person carrying out the fix did not realise that the MID was associated with multiple TIDs. Instead of checking the number of TIDs associated with the MID, they mistakenly updated one TID instead of the required 350 TIDs. (Refer to Actions 3, 4 and 6).",NA,NA,NA,10/28/2024,October,2024,Gokul,Closed,.Com SM,,2024,10,October,9/16/2025,337,>60
10/12/2024,10/12/2024,90339674,07:48,10:21,02:33,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent carrier label printing issues at C&H Ollerton DC,Customer,WMS,"£5,000.00",Carrier Label printing,RC identified,70545,"Ollerton DC colleagues experienced intermittent carrier label printing issues from 01:31 impacting their picking, packing and dispatch of international orders, potentially impacting the CFR. Services were restored by restarting the WMS application services at 10:05. Detailed root cause investigations underway.",,10/12/2024,,,,,"No,Yes",No,No,NA,Data Integrity & State Issues,NA,"Root Cause has been attributed to the long running query (Lib_Extract - responsible for DNR data extraction from WMS to OMS) which caused the order daemon to stall. Based on the existing alerting setup, the long running query was manually cleared which impacted the print Deamon causing the issue.",NA,NA,NA,11/20/2024,November,2024,Gokul,Closed,MIM,,2024,10,October,9/16/2025,339,>60
10/11/2024,10/14/2024,90337961,05:38,17:04,07:12,GTS,Enterprise Integration,Customer Channels,Platform and store ops,Manage Payment & Content Customer.Com,SI,"Intermittently customers are seeing ""Something went wrong"" errors",Apigee,N/A,"Digital Cafes - £1,559",.Com Website,RC identified,70076,"From 05:38, customers experienced intermittent errors across various journeys on the website, impacting Availability Service, Store Checkout services and Digital Receipts resulting in poor customer experience. Additionally, Digital Cafes and the In-Store Assist app, Collect app, Parcel Scanning, and Intelligent Waste were affected, though there was no drop in orders across any channels. The root cause was attributed to an incorrect configuration of an IP address within the Apigee infrastructure, which was corrected to restore services by 11:41.
From 15:55, intermittent errors were again observed. Vendor Apigee was engaged and confirmed the above issue reoccurred.   A temporary fix was provided to mitigate the issue at 17.04 and a permanent fix has been identified, however the ETA for the implementation is yet to be finalised. Hypercare continues to be in place to ensure stability.",,10/11/2024,,,,,No,No,Yes,NA,External Dependencies,,"The root cause was attributed to an incorrect configuration of an IP address within the Apigee infrastructure, which was corrected to restore services by 11:41.The IP configs were incorrect due to issue in clean up process of external IPs from DNS record set as the  Apigee router AWS instances went unresponsive. In addition, a dependent service, used by Google Apigee's stale IP cleanup job, was returning errors to somequeries and hence, this failure  was not caught by the existing alerting system. The second instance of the issue on 11/1O occured when Google attempted to recreate the instance with IP address, for which the root  cause is not known.",NA,NA,NA,12/23/2024,December,2024,Kavitha,Closed,.Com SM,,2024,10,October,9/16/2025,340,>60
10/10/2024,10/10/2024,90335015,08:10,10:21,02:11,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,KI,Majority of customers cannot see Homepage content,Customer,Platform operation ,No Financial impact,.Com Website,RC identified,70557,"From 08:10, majority of customers were unable to view homepage content on the website affecting content cards on PLPs, fulfilment information editorial content on ""Inspire Me"" pages across desktop and mobile web. The root cause was attributed to a GitHub token expiry on 09/10, 19:00. Services were restored by reinstating the tokens and restarting the servers across Europe North & West regions by 10:21. ",PIR has been published - actions are being tracked under the actions tab,10/10/2024,,,,,"yes,No",yes,"No,No",NA,Operational Process Issues,NA,"Root cause was due to a Github token expiry at 19:00 on 09/10.  This token is used by Azure to retrieve the application container data.
There has been a working instance of Azure app service plan where they had an automated scaling/maintenace activity (as per Azure app portal). This had resulted with the containers not being able to retrive the data from Github, whilst the token also had expired resulting with the isuse. 

",NA,NA,NA,12/26/2024,December,2024,Pavithra,Closed,.Com SM,,2024,10,October,9/16/2025,341,>60
10/9/2024,10/9/2024,90331061,05:55,9:05,03:10,GTS,Digital Workplace Services,"C&H and Intl, Foods","C&H and Intl Supply Chain, Foods Supply Chain",Infrastructure,KI,Windows CE HHTs unable to connect to network across various DC's,Customer,Patching,No Financial impact,Windows CE HHT,RC identified,70547,"Since 05:55, various DC colleagues (both C&H and Foods) reported that they were unable to connect to Windows HHTs impacting site operations, however the android HHT's were working as expected. The initial root cause was attributed to a change in the GPO (Group policy object) on the radius server on 27/08 to disable the TSL (Transport layer security) 1.0 and 1.1. On the day of the incident, as part of patching activity, the radius server was rebooted at 01:26 and subsequently the GPO change took effect resulting in the connectivity issue. The change was reverted, and services were restored by 09:05.","PIR held - Actions are being tracked in the ""Actions Tab""",10/9/2024,,,,,"No, No",No,"Yes, Yes",No,Change Failure,CR,"As part of the ongoing security vulnerability remediation work (CRQ000000207053 - Vulnerability remediation reported in Qualys for Prod servers), the TLS version 1.0 and 1.1 (legacy protocols) were disabled on the radius servers on the following dates: MSHSRMNSUKPU202 - 27/08 & MSHSRMNSUKP8202 - 10/09.  After applying any change to the group policy object, a reboot should be done so that the changes can take effect.  We can see that this reboot was done but we need to investigate why the change did not take effect. At this stage the feeling is that the reboot was done too early and the change did not have time to replicate across the domain controllers.",CRQ#207053,Inadequate Testing,NA,2/20/2025,February,2025,Gokul,Closed,MIM,Y,2024,10,October,9/16/2025,342,>60
10/6/2024,10/6/2024,90327372,08:29,10:38,02:09,GTS,Enterprise Technology Platform,Customer Channels,Platform and store ops,Manage Allocation of Food Stock,SI,Delay in CSSM application availability followed by the SQL upgrade,TCS,SQL,Finance impact unavailable,CSSM,RC identified,70036,"During the overnight SQL upgrade on the CSSM database, there was a cloud connectivity issue which resulted in a two hour delay in the availability of CSSM to stores causing an impact to the store operations such as receiving deliveries, counting, backstage replenishment etc.  The root cause was attributed to a missing step during the change implementation. To mitigate the issue, The missing step was identified and a host-based routing was performed at the network layer to restore the connectivity at 08:29. After which checks were performed and the application was made available to stores at 10:38. Hypercare monitoring continues to be in place to ensure stability. ",PIR has been published - actions are being tracked under the actions tab,10/6/2024,10/9/2024,,,,"Yes,NO",No,"No,No",NA,Change Failure,CR,"There was a CSSM SQL upgrade #CR209635 on 6th October, where the CSSM database servers were upgraded from the existing 2012 Windows & 2014 SQL server to new Windows 2019 & 2019 SQL server. By default, during an upgrade the IP (Internet protocol) address is subjected to change and therefore it requires an additional step (i.e) a host-routing based routing to establish the connectivity between the upgraded SQL port and cloud services.
However, during the change deployment, this step was missed in the implementation plan impacting the connectivity from Swindon the external cloud.
Note: The On-prem services did not have an issue after the upgrade.",CR#209635,implementation error,NA,11/27/2024,November,2024,Pavithra,Closed,MIM,,2024,10,October,9/16/2025,345,>60
10/4/2024,10/4/2024,90321513,20:52,23:18,02:26,Customer Channels,Selling Experience,Customer Channels,"Customer Engagement, Selling Experience",Manage Payment & Content Customer.Com,SI,Order Drop on Customer.com Website,Customer,WCS,"£132,000.00",.Com Website,RC Unknown,70034,"From 20:52 , alerting indicated errors during signing-in and latency when loading the basket on the Customer website resulting in a drop of approx. 6k orders and poor customer experience. The initial root cause was attributed to high CPU utilisation on the WCS database, however around 23:18 services were restored without any intervention. Root cause analysis underway.  ",Actions are being tracked in the actions tab,10/4/2024,,,,,Yes,Yes,No,NA,Root Cause Undetermined/Still Under Investigation,NA,"There was an increase in traffic from the website for the wishlist service which slowed responses.  Due to the slow responses the GQL-Mesh further increased the queries causing further slowness and increase in PDP errors. WCS is the fallback mechanism for any issue in PDP, and was already facing latency at the time of this issue therefore couldn't cater to the new queries of availability services.  We found the network  pool ( servers are mapped in the POOL for load balancing) was down on during the issue window.  After the problem was reported  the network team enabled the pool from Infoblox to fix the issue.However, we don't any information in the logs to indentify the RCA.

",NA,NA,NA,1/16/2024,January,2024,Kavitha,Closed,.Com SM,,2024,10,October,9/16/2025,347,>60
10/4/2024,10/4/2024,90320229,12:30,15:50,03:20,C&H and Intl ,International Commercial Trading , C&H and Intl,International Commercial Trading ,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,IFOS application inaccessible for a brief period,Minster,N/A,No Financial impact,IFOS,RC unknown,70137,The IFOS (International Food Ordering System) application was inaccessible between 12:30 and 15:50. Bedworth and Bradford depot colleagues were unable to access IFOS to process invoices and dispatch loads. Vendor Minster restarted the com+ service (one of the sub components of IIS service) to restore services. Awaiting root cause details from Minster Logistics.,Actions are being tracked in the actions tab,10/4/2024,,,,,NA,NA,NA,NA,External Dependencies,NA,"The root cause was attributed to an issue with the COM+ service at Minster, which facilitates the connection between the IFOS web UI and the database. However, investigations on the IIS logs could not conclude the cause of the issue. Additional log captures have been added to the SOP to identify the issue in future instances",NA,NA,NA,11/21/2024,November,2024,Saloni,CLosed,MIM,,2024,10,October,9/16/2025,347,>60
10/3/2024,10/3/2024,90310072,11:57,13:20,01:23,Customer Channels,Platform & Store Ops,Customer Channels,Platform and store ops,Others,KI,ESEL (electronic shelf edge labels) not updating in a timely manner,Vusion,N/A,No Financial impact,ESEL,RC identified,70549,The ECS product team identified through alerting that all the 9 ESEL (Electronic Shelf Edge Labels) stores were receiving price label data with a delay impacting real time updates from 11:57.  The root cause was attributed to a signal transmitter instability within Vendor Vusion infrastructure causing connectivity issues. Vusion had deployed a fix to resolve the connection issues by 13:20. Root cause investigations continue.,Actions are being tracked in the actions tab,10/3/2024,,,,,"Yes, Yes",Yes,"Yes, No",Yes,External Dependencies,NA,"During the execution of a scheduled technical job in Vendor Vusion infrastructure, some endpoints encountered an issue that led to abnormally high resource consumption. As a result, endpoint performance degraded, eventually causing access points to disconnect due to resource exhaustion",NA,NA,NA,10/4/2024,October,2024,Gokul,Closed,MIM,,2024,10,October,9/16/2025,348,>60
10/3/2024,10/3/2024,90242015,10:00,16:00,06:00,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,SI,Chester contact centre agents experiencing degraded voice quality,Customer,.com,£480.00,Contact center ,RC Identified ,70033,"From 09/09, Chester contact centre colleagues were experiencing intermittent inbound and outbound voice issues (one-way audio, no audio, dropped calls, audio distortion, and audio clipping).  Capita Contact Centers, Colleague Services and colleagues working from home were not impacted. Chester handles circa 10% of voice contacts (high-value contacts such as exec complaints).  On 03/10, circa 80% of the calls were impacted resulting in poor customer experience. The root cause was attributed to the maximum utilization of the network link between Chester & Sabio Citrix exceeding its 10 Mbps bandwidth.  Investigations underway to ascertain the root cause and prevent re-occurrence.",No further actions pending,10/3/2024,,,,,No,No ,Yes,Yes,Capacity & Scalability Limitations,NA,The root cause was attributed to the maximum utilization of the network link between Chester & Sabio Citrix exceeding its 10 Mbps bandwidth which was probably caused by a product video on Customer website as that the first product video on the  site and issue was resolved once it was disabled. ,NA,NA,NA,10/17/2024,October,2024,Rehan,Closed,MIM,,2024,10,October,9/16/2025,348,>60
10/3/2024,10/7/2024,90317889,14:00,22:00,104:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Others,KI,Nominated day delivery issue,Customer,.com,No Financial impact,.com ordering ,RC unknown,69820,"Customers were unable to select their intended Nominated Day Delivery (NDD) option at checkout and were forced to select 10/10 for Home Delivery and Click & Collect (C&C) orders, the same issue occurred on 06/09. Around 1100 NDD and 5000 C&C orders were impacted resulting in poor customer experience. In addition, customers with NDD Home Delivery paid a charge of £5.99 for the delivery date as opposed to being charged £3.99 for Standard Delivery. Whilst the count of impacted customers should be decreasing from 03/10, the product team continue to replicate the issue to identify the root cause and resolution. ",Actions are being tracked in the actions tab,10/3/2024,,,,,,,,,Root Cause Undetermined/Still Under Investigation,NA,"Product team continue to replicate the issue to identify the root cause and resolution. 
Whenever CD calendar date is switched off during an activity, then this issue pops up. However, the root cause is still unknown and the teams are regrouping in Jan to discuss and agree on the next steps",NA,NA,NA,12/31/2024,December,2024,Gokul,Closed,.Com SM,,2024,10,October,9/16/2025,348,>60
10/3/2024,10/5/2024,90317186,10:57,16:30,17:33,GTS,Digital Workplace Services,Customer Channels,Platform and store ops,Infrastructure,KI,Store colleagues experiencing issue authenticating when accessing SSO applications,Kocho,N/A,No Financial impact,Store colleaagues inconvenience,RC identified,70031,"From 10:57, 18 stores reported authentication issues whilst trying to access some applications relying on SSO on their workstations.  A Sev-A case was logged with Microsoft for further investigation.  On 04/10, 14 stores reported the issue again and as a workaround, browser cache was cleared on the impacted workstations for the past 7 days to resolve the issue. The initial root cause has been attributed to an issue with the authentication token expiry.  Product teams continue to work with Microsoft to ascertain the root cause.",Actions are being tracked in the actions tab,10/3/2024,,,,,,,,,External Dependencies,CR,"On 3rd October a Conditional Access (CA) policy was renamed by third party vendor Kocho.
Once the policy was renamed, SharePoint refresh token had expired as the system had noticed the change in the policy name. ",NA,NA,NA,1/29/2025,January,2025,Rehan,Closed,MIM,,2024,10,October,9/16/2025,348,>60
10/1/2024,10/1/2024,90313416,12:12,14:06,01:54,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,KI,Free wi-fi provided by BT unavailable in stores,BT,N/A,No Financial impact,Free Wi-Fi,RC identified,70029,"Customers were unable to connect to free Wi-fi provided by BT across all stores between 12:12 and 14:06 and hence were unable to access Sparks card & offers, Scan & Shop mobile, resulting in poor customer experience. The root cause was attributed to a central issue within British Telecom (BT) infrastructure.  Awaiting resolution details and root cause from BT.",Awaiting resolution details and root cause from BT.,10/1/2024,,,,,,,,,External Dependencies,NA,The root cause was attributed to a central issue within British Telecom (BT) infrastructure.  ,NA,NA,NA,,,,Rehan,Closed,MIM,,2024,10,October,9/16/2025,350,>60
10/1/2024,10/1/2024,90312719,10:07,11:42,01:35,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Carrier label printing impacted across various C&H DC's,Customer,N/A,"£12,000.00",Carrier Label printing,RC identified,70133,"Colleagues reported issues with carrier label printing across Donington, Ollerton, Brands and Stoke DCs between 10:07 and 11:42 impacting packing and despatch operations.  No impact to customer orders, however, there was a capability loss of circa 20k singles at Donington. The root cause was attributed to a user account mapped to the Sorted API authentication key that was disabled impacting the connectivity between Mule and Sorted.  Services were restored by reinstating the user account. ",Actions are being tracked in the actions tab,10/1/2024,,,,,,,,,Operational Process Issues,NA,"On 30th September, as part of a housekeeping activity, the Donington Carrier Team, also known as the Ecom Transport Product Team, disabled a user account named Gary Bennett from the Sorted Console as the user had left MNS. This user ID was responsible for logging into the Sorted UI and was mapped to the Sorted authentication key across multiple C&H DCs, including Ollerton, Stoke, Donington, and Brands. When the account was disabled, the associated authentication key was also revoked, disrupting the connection between Mule and Sorted, which caused the issue.",NA,NA,NA,12/18/2024,December,2024,Saloni,Closed,MIM,,2024,10,October,9/16/2025,350,>60
9/24/2024,9/25/2024,90295590,11:11,11:24,00:50,Customer Channels,Customer Engagement,Customer Channels,"Customer Engagement, Selling Experience",Manage Payment & Content Customer.Com,MI,Customers encountering errors when attempting to sign in to the website,Secure Auth,.com,"£88,000.00",.Com Website,RC identified,69793,"Customers were encountering “Sorry, something went wrong” pages on the website impacting all ordering channels, CFTO for ~30 mins. There was a drop of circa 4k orders when compared to previous days resulting in loss of sales and poor customer experience. The root cause was attributed to an application bug within Cloud Entity infrastructure impacting their database performance. A permanent fix was implemented by Cloud Entity on 26/09.",,9/26/2024,,,,,"No,Yes",No,"No,No",NA,External Dependencies,NA,The root cause was attributed to an application bug within Cloud Entity infrastructure impacting their database performance. A permanent fix was implemented by Cloud Entity on 26/09.,NA,NA,NA,5/16/2025,,,Pavithra,Closed,.Com SM,,2024,9,September,9/16/2025,357,>60
9/23/2024,9/23/2024,90290812,16:00,18:02,02:02,Customer Channels, Selling Experience,Customer Channels, Selling Experience,Manage Payment & Content Customer.Com,KI,Issues in double discount in basket on desktop and mweb,Customer,Price Domain,See note,.Com Website on Desktop and mobile,RC identified,70132,"Through Contact Centre and colleague shouts, customers reported that Customer website on desktop and mobile was showing incorrect basket value with twice the expected discounts. This impacted around 500 products with percentage discount promotions resulting in poor customer experience. The initial root cause has been attributed to a change deployed on 19/09 to prevent a double dipping of promotions and it was reverted at 18:02 to fix the issue. Detailed root cause investigations underway. ","PIR held - Actions are being tracked in the ""Actions Tab""",9/23/2024,,,,,"No, Yes",No,No,Yes,Software Defects/Limitation,NA,"Root cause was a code fix which had been deployed the previous week to fix a problem with 8 products on 
promotion which were double dipping.",NA,NA,NA,11/4/2024,,,Gokul,Closed,.Com SM,,2024,9,September,9/16/2025,358,>60
9/21/2024,10/29/2024,90281344,05:10,6:00,112:50,Customer Channels,Service Experience,All BU's,All BU's,Manage Payment & Point of Sales in store,MI,Central network connectivity issue impacting various services,Customer,POS,"Digital Cafes - £93,403"," Pick, pack and Despatch operations at CD",RC identified,70406,"Multiple applications & services were impacted due to a central network connectivity issue between 21/09 and 24/09 resulting in poor customer & colleague experience.  Digital Cafes, pack benches & carrier label printing at Castle Donington and other ecomm DCs, In-store payments & gift card transactions (online and instore), store honeywell applications - CSSM, ECS, Collect, Parcel Scanning etc along with Czech Republic DC operations & Power BI reports were impacted. To mitigate the impact, the routed internet access (RIA) & Cloud Connect traffic along with SD WAN stores were routed via Swindon Fortigate firewall and the IPS engine was disabled to recover most of the services.  
On 25/09, internet traffic & Cloud Connect traffic was enabled through Stockley Park firewall after downgrading the firmware version.  The initial root cause was attributed to a CPU spike on the Stockley Park FortiGate firewall, further investigations underway with Fortinet vendor.","The PIRs for the impacted areas have been completed. However, due to ongoing network incident,the network specific PIR is yet to be planned.",9/21/2024,,,,,Yes,Yes,Yes,Yes,Change Failure,Change ," The initial root cause was attributed to a CPU spike on the Stockley Park FortiGate firewall. Further investigations revealed that the Dynatrce installation on the store tills had resulted in increase of session on SP firewall, which caused the issue.",CRQ209368,Inadequate post implementation checks,,,,,Saloni,Open,MIM,Y,2024,9,September,9/16/2025,360,>60
9/13/2024,9/13/2024,90261991,12:58,16:00,03:02,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),MI,Message processing issues from WMS to WCS impacting CD operations,Customer,WMS,"£40,000.00"," Pick, pack and Despatch operations at CD",RC identified,69892,"Alerting indicated a message pile up in middleware layer impacting the picking, packing and despatch operations at Donington between 13:00 and 16:00. This had resulted in a capability loss of ~99k singles and the order proposition was moved to +24 hours for NDD and C&C orders to prevent the CFR impact. The root cause was attributed to a script which had been automated on 10/09 however the stock received manually via HHT's caused the script to fail.  As a temporary fix, the piled up messages were cleared from the WCS middleware layer and retriggered to resolve the issue. The Control M job responsible for automation has been put on-hold to prevent reoccurrence.",Actions are being tracked in the actions tab,9/13/2024,,,,,"Yes,No",Yes,No,No,Change Failure,CR,"The root cause was attributed to an automated script introduced on Tue, Sep 10 to handle the purchase order creation for inbound split boxed receipt through automation (WCS). The script failed to address the manual receipt scenario and had resulted in the issue. ",CRQ000000208315,Inadequate testing,NA,10/31/2024,October,2024,Saloni,Closed,MIM,,2024,9,September,9/16/2025,368,>60
9/11/2024,9/11/2024,90254892,05:35,9:38,04:03,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Issues with Relex nightly batch impacting foods allocations and suppliers orders.,Relex,Foods,See note,"Relex User interface, RDC allocations and Supplier orders",RC identified,70130,"An overnight job failure within the Relex batch had resulted in minor discrepancies in the RDC allocations and unavailability of Relex UI. However, a contingency plan was applied to mitigate the impact to allocations. The Relex batch was retriggered to make the Relex UI available at 09:38. The root cause was attributed to a node failure within Relex infrastructure and we await further details.",No,9/11/2024,,,,,"No,No",NO,Yes,No,External Dependencies,NA, The root cause was attributed to a node failure within Relex infrastructure,NA,NA,NA,9/11/2024,Septemebr,2024,Rehan,Closed,MIM,,2024,9,September,9/16/2025,370,>60
9/11/2024,9/11/2024,90256307,08:40,8:59,00:19,Customer Channels,Platform & Store Ops,Customer Channels,Platform and store ops,Others,KI,Issues with Walkie Talkie application at stores,Microsoft,Honeywell,No Financial impact,Walkie Talkie application,RC identified,70069,"At 06:00, Microsoft sent email alerts notifying users of an issue with the Teams-based Walkie Talkie application. This impacted the ability of store colleagues to communicate with each other using the Walkie Talkie feature on their Honeywell devices and headsets. As a workaround, they had access to Ascom phones, Teams for communicating. The root cause was attributed to a coding issue in a standard change deployed by Microsoft intended to improve overall performance impacting multiple customers. The change was reverted by Microsoft to restore services at 08:59.", Due to traffic spike after a change the system got stuck infinte loop resulting in the issue.  We have also requested for any preventive measure that can put inplace to avoid such infinite loops which impacted the system ,9/11/2024,,,,,Yes,Yes,No,NA,External Dependencies,NA,The root cause was attributed to a coding issue in a standard change deployed by Microsoft intended to improve overall performance impacting multiple customers,NA,NA,NA,10/25/2024,October,2024,Kavitha,Closed,MIM,,2024,9,September,9/16/2025,370,>60
9/8/2024,9/8/2024,90249161,07:41,19:40,11:59,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent carrier label printing issues at Brands Thorncliffe DC,Customer,Mule,See note,Brands WMS,RC unknown,70025,"Following the capacity uplift activity at Brands DC, colleagues have been reporting intermittent issues with carrier label printing impacting the pack & despatch operations resulting in potential customer miss promises. Order proposition was moved to 24+ hours for C&C and NDD orders. The root cause has been attributed to intermittent connectivity issues between WMS application and primary Mule server. Services were restored by failing over the Mule services to the secondary server. Detailed root cause investigations underway.",Actions are being tracked in the actions tab,9/8/2024,,,,,"Yes, No",No,Yes,NA,Root Cause Undetermined/Still Under Investigation,NA,"The initial root cause was attributed to an intermittent connectivity issue between the WMS application and the Mule application running on the active server. With the assistance of RedHat, we attempted to capture additional logs for root cause analysis. As the issue did not recur during the failover activity, no further logs could be captured, and hence the root cause remains inconclusive.

 ",NA,NA,NA,12/18/2024,December,2024,Saloni,Closed,MIM,,2024,9,September,9/16/2025,373,>60
9/6/2024,9/9/2024,90245506,19:30,22:00,74:30,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Customer.com - Intermittent Issue With Nominated Day Delivery,Customer,.com,No Financial impact,.com ordering ,RC unknown,69820,"Since 05/09, customers reported issues that they were unable to select their intended Nominated Day Delivery (NDD) option at checkout and were forced to select 12/09 for Home Delivery and Click & Collect (C&C) orders, however investigations revealed that, issue was since 31/08 and the number of impacted customers has been decreasing since the 04/09. Circa 400 orders have been placed for NDD Home Delivery for 12/09 so far and circa 6.1k orders for NDD C&C orders (Total 6.5k Orders). Upon investigation, it was  identified that all impacted customers were presented with two calendars with September dates, when selecting a delivery slot for NDD on the checkout page. Teams were unable to find any error logs and are trying to reproduce this problem, so that it can be investigated further.","PIR held - Actions are being tracked in the ""Actions Tab""",9/10/2024,,,,,"No, No",No,No,No,Root Cause Undetermined/Still Under Investigation,NA,"Exact root cause has not been established, since no error logs were generated for the issue and technical teams/ops hub were 
unable to reproduce the issue after many attempts. It is strongly suspected that the cause of this issue is related to the 
calendar date of September 11th being switched off in relation to the EDC (Ecomm Disstribution Center - CD) outage",NA,NA,NA,10/14/2024,October,2024,Gokul,Closed,.Com SM,,2024,9,September,9/16/2025,375,>60
9/4/2024,9/4/2024,90240387,07:11,11:34,04:23,Foods,Food Supply Chain,"C&H and Intl, Foods","C&H and Intl Supply Chain, Foods Supply Chain",Manage & Maintain Labour Resource,KI,Central issue at Blue Yonder impacting SRD (Open Access) and WLM applications,Blue Yonder,NA,No Financial impact,Foods SRD (Open Access) and WLM,RC identified,70024,"Foods and C&H colleagues were encountering issues whilst accessing Foods SRD (Open Access) and WLM (Warehouse Labor Management) applications between 07:11 and 11:34. This impacted their ability to access planograms/ floor plans at stores and the resource planning capability through WLM at Stoke, Swindon, Bradford and MK.  To mitigate the impact, store colleagues were advised to use SST (Store Stock tool) for Open Access and DC colleagues were advised to use WLM application on the workstations. The root cause was attributed to a central issue within Blue Yonder infrastructure which was resolved by 11:34. Awaiting RCA from Blue Yonder.",Lisy to follow-up with BY to get the RCA details and to agree on the preventive measures. ,9/4/2024,,,,,"No,No",NO,"Yes, Yes",NA,External Dependencies,NA,The root cause was attributed to a human error within the BY infrastructure whilst deploying a network change at their end,NA,NA,NA,11/11/2024,November,2024,Pavithra,Closed,MIM,,2024,9,September,9/16/2025,377,>60
9/3/2024,9/3/2024,90239114,06:30,9:25,02:55,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Followed by a capacity uplift activity Ollerton WMS application was inaccessible,TCS,Linux,,Ollerton WMS,RC identified,70022,Ollerton WMS application was inaccessible between 06:30 and 09:25 after the capacity uplift activity on 03/09. This impacted e-comm orders picking and returns process. The root cause is attributed to a system resource constraints on two of the file systems within the server impacting the cluster services. Vendor RedHat was engaged and the cluster was recovered to restore the services. Detailed root cause investigations underway. ,,9/3/2024,,,,,,,,,Configuration Errors,NA,"After the capacity uplift, the nodes are supposed to be restarted and a cluster start needs to be done. When support teams restarted the cluster, a systemic health check of each node is carried out. When this was done the check found that the JDA application was running in both nodes - this actually was not the case and the application was not running on any node - Thus had caused an outage",NA,NA,NA,12-Dec,December,2024,Rehan,Closed,MIM,,2024,9,September,9/16/2025,378,>60
8/27/2024,8/27/2024,90224338,14:30,19:05,04:35,GTS,Digital Workplace Services,All BU's,All BU's,Infrastructure,KI,SALINA network connectivity issue across warehouses and support centers,Customer,Active Directory,,Salina Wi-Fi,RC identified,70021,"From 14:30, some colleagues were unable connect to Salina wireless network at Waterside office, Castle Donington and Foods Bradford DC.  As a workaround, impacted colleagues were able to connect to the other wireless networks.  The root cause was attributed to an internal server authentication certificate expiry on the radius servers, this certificate was renewed to restore the services by 19:05",All actions are closed,8/27/2024,8/27/2024,,,,"No, Yes",No,"Yes,Yes",Yes,Operational Process Issues,NA, The root cause was attributed to an internal server authentication certificate expiry on the radius servers,NA,NA,NA,9/15/2024,September,2024,Gokul,Closed,MIM,,2024,8,August,9/16/2025,385,>60
8/25/2024,8/26/2024,90220033,12:30,16:30,28:00,Customer Channels,Service Experience,Customer Channels,Service Experience,Others,SI, Delayed Parcels incorrectly being marked as Ready to Collect,Customer,.com,,BACS ,RC identified,69574,"Store colleagues reported issues with delayed parcels incorrectly being marked as ""Ready to Collect"" on the BACS application and store colleagues were being directed to old .com+ application from 23rd Aug. Around 610 orders were impacted, customers have received the ready to collect emails, however, the parcels were not available for collection resulting in poor customer experience.  The root cause was attributed to an issue with the BACS application certificate renewal in the DR region. To mitigate the impact, the DR region was disabled to handle the application traffic from prod region followed by increasing the number of pods from 4 to 6. The certificate was renewed on the DR region and traffic was moved back to 50/50 across both regions to restore services. ",Actions are being tracked in the actions tab,8/25/2024,8/26/2024,,,,Yes,No,"Yes,Yes",Yes,Certificate issue,Yes,The root cause has been attributed to a incorrect namespace details in the certificate update script resulting in the certificate renewal failure,CM-12060,NA,NA,10/17/2024,October,2024,Kavitha,Closed,MIM,,2024,8,August,9/16/2025,387,>60
8/20/2024,8/21/2024,90207738,12:35,16:00,27:25,GTS,Digital Workplace Services,All BU's,All BU's,Others,KI,Sync issues between MY HR and One Identity impacting colleagues profiles in Active Directory,Customer,One Identity,," Blue Yonder D&F, Medalia ",RC identified,70020," C&H colleagues reported issue in accessing Blue Yonder D&F application. Also, Medallia colleagues have reported issue with their single sign ons. Investigations revealed that there is a sync issue between My HR and One Identity database followed by job execution which has resulted in incorrect colleague details being updated in AD. It looks like all the colleague’s details are updated as it was on the day they joined Customer, any amendments with respect to their name, department etc are not updated in AD. The critical  AD attributes have been reinstated for all impacted user accounts to fix the issue. The non-critical attributes of the user accounts have been corrected.",It has been agreed to not run any sync jobs/ deploy changes within One Identity as there are historical stale data within One Identity. Andy Neilsen and Duane Bergh are owing to derive a plan to address these issues in future.,8/20/2024,,,,,NA,NA,Yes,NA,Customer tech change,Change ,"The current data sync between MyHR & OneIdentity only syncs data for new starters and not colleagues who have a change to their department, job title, or other attributes within MyHR resulting in backlog of stale data being held in a One Identity Table. As part of a planned change, a process to sync “Mover” data from One Identity to AD was triggered which resulted in the old data in One Identity overwriting certain attributes in AD.",CRQ206633,Inadequate testing,NA,8/30/2024,August,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,392,>60
8/20/2024,8/20/2024,90208820,11:00,13:50,02:50,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Delay in Donington planned release impacting operations,TCS,Linux,,Castle Donington Operations,RC unknown,69572,"Issues were identified whilst performing the Linux OS patching at Donington resulting in a delay to the agreed outage window and therefore impacting operations between 11:00 and 14:00. Order proposition was moved to 24+ hours for C&C and NDD orders along with a capability loss of 53.1k singles. The initial root cause has been attributed to an issue with the primary MQ and Mule servers impacting the relevant cluster services. Services were restored using a manual workaround to bring up the services on the secondary servers at 13:50. An outage was secured between 06:00 & 08:00 on 22/08, to implement the NFS timeout parameter as recommended by RedHat to restore the cluster services. Site operations have remained stable. Detailed root cause investigations underway.","All action have been tracked and closed, Please refer to the actions tab for details",8/20/2024,,,,,No,No,Yes,Yes,Customer tech change,Change ,"After the Linux OS patching activity, the primary MQ server encountered a NFS timeout issue on the cluster services. In addition, the primary Mule server had  disk issues impacting the cluster services. Root cause remains inconclusive",206208,Product bug,NA,10/16/2024,October,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,392,>60
8/18/2024,8/19/2024,90206833,23:00,11:00,12:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent issues with HHTs at Ollerton DC,Customer,WMS,,HHTs,RC identified,70018,Ollerton DC colleagues encountered intermittent HHT issues in the SYW (Shop Your Way) area impacting the outbound despatch operations resulting in a customer miss promise of 135 orders. The root cause was attributed to an incorrect process followed by Operations colleagues whilst processing Home delivery orders as SYW orders resulting in a capacity constraint on the WMS database. Services were restored by uplifting the database CPU capacity from 8 to 14,"PIR held - Actions are tracked and closed in the ""problem actions tab""",8/23/2024,8/23/2024,,,,Yes,No,NA,Yes,Human error - Business,NA,The root cause was attributed to an incorrect process followed by operations team whilst processing the standard home delivery orders  as Click and Collect orders (Shop Your Way) due to a knowledge gap..,NA,NA,NA,11/10/2024,November,2024,Pavithra,Closed,MIM,,2024,8,August,9/16/2025,394,>60
8/15/2024,8/15/2024,90199179,03:40,12:51,09:11,Group Platforms,Finance,Group Platforms,Finance and Data,Others,KI, EDW Day-1 delayed due to a cSAP BW job hung,Customer,SAP,,Day-1 reporting ,RC identified,69606,"At 03:40, alerting indicated that the General Ledger extractor job in cSAP BW was in hung state resulting in the delay in EDW Day-1 reporting flow completion. There was a delay in the availability of latest sales data in Business Objects and Power BI reports. Services were restored by retriggering the problematic job and the EDW Day-1 reporting flow had completed at 12:51. Root cause analysis underway.",PIR to be scheduled,8/15/2024,8/15/2024,,,,"Yes,NO",No,No,NA,Configuration issue,NA,The root cause was attributed to a job failure of the SAP BW extractor impacting its subsequent instances resulting in a significant delay to the overnight batch,NA,NA,NA,9/3/2024,Septemeber,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,397,>60
8/14/2024,8/14/2024,90197506,09:01,22:00,12:59,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,SI,Inflated supplier orders and allocations of Relex chilled products,Customer,ASO,,Foods Supplier orders and Allocatoins,RC identified,69648,"Relex orders and allocations for chilled products were inflated resulting in higher volume of supplier orders and over allocations for future store deliveries.  To mitigate the impact, suppliers were advised to revert to the previous day 28-day order plan and contingency plan was applied for RDC allocations. The root cause was attributed to a connectivity issue within the ASO application (Allocation Scheduling and Orchestration) resulting in a partial data load to Relex.  Services were restored by restarting the impacted components followed by data correction within ASO.","PIR held - Actions are being tracked in the ""Actions Tab""",8/14/2024,8/20/2024,,,,"No,Yes",No,Yes,NA,Infrastructure issue / Hardware failure,NA,"The root cause was attributed to a Azure connectivity issue within the ASO application. The secondary region (West Europe) of SAND lost connectivity to the Azure service bus due to a service disruption at Microsoft and therefore it was unable to push the SAND update to the appropriate database table. Due to inaccurate SAND positioning, Foods Purchase orders (chilled) were inflated during the overnight batch as per design within Relex.",NA,NA,NA,10/8/2024,October,2024,Pavithra,Closed,MIM,,2024,8,August,9/16/2025,398,>60
8/13/2024,8/15/2024,DN Ref: 123887531,09:59;10:44,10:15; 11:02,00:33,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,KI,Digital Cafe kiosks were unavailable on 13th Aug and 15th Aug,Counter Solutions,NA,"£4,478.00",Digital café,RC identified,NA (Issue with DN and Remedy integration - Actions agreed),Customers were unable to place orders across all the 14 Digital Cafes between 09:59 and 10:15 on 13th Aug and 10:44 and 11:02 on 15th Aug resulting in poor customer experience and potential loss of sales. The root cause was attributed to an issue with one of the virtual machine within Counter Solutions infrastructure  after a code release. Services were restored once the services were failed over to the other virtual machine. Root cause analysis underway. Hypercare monitoring in place.,Actions being tracked,8/13/2024,,,,,"No,Yes",No,Yes,NO,External Dependencies,NA,"The root cause was attributed to a code release at Counter Solutions on 12/08, resulting in a memory constraint on one of the virtual machines. We have asked Counter Solutions to provide further insight into the technical details of what went wrong with the code that caused the memory leak on the virtual machines",NA,NA,NA,1/11/2024,January,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,399,>60
8/12/2024,8/12/2024,90193515,09:00,23:30,14:30,Group Platforms,Finance, C&H and Intl,C&H Commercial Trading ,Others,KI,C&H weekly sales data understated by 3.7 M in MP and SSI reports,Customer,SAP,,MP and SSI applications,RC identified,69650,"C&H colleagues reported that weekly sales are understated by 3.7M in MP, SSI reports. This impacted the future planning of sales, stock and intake and overall merchandising planning activities. The initial root cause has been attributed to an overrunning job in SAP ECC impacting the flow of sales data into GMOR. To mitigate the impact, the understated sales data were reprocessed from GMOR into the downstream systems by 23:30, 12/08, Detailed root cause analysis underway. ",All actions are agreed,8/12/2024,TBC,,,,"Yes,NO",No,No,NA,Design issue,NA,"The root cause was attributed to an overrunning sales data processing job in SAP ECC due to high volume of idoc failures impacting the flow of sales data into GMOR resulting in an understatement of 3.7M data in the downstream systems – MP, SSI & BEAM.",NA,NA,NA,9/10/2024,September,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,400,>60
8/12/2024,8/12/2024,90191568,08:54,12:25,03:31,Customer Channels,Service Experience,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,KI,"
Waterside and Pantheon stores were unable to take card payments on 50% of tills",Worldline,Payments,,Till payments,RC identified,70125,"Waterside and Pantheon stores were unable to take card payments on 50% of the tills resulting in potential sales loss and inconvenience to the customers at stores. The initial root cause has been attributed to a change deployed by Worldline for these stores. To mitigate the impact, the TPV (Terminal identifier) configurations were amended for the impacted tills and the change was reverted by Worldline by 11:52 followed by which tills were rebooted by 12:25, 12/08. Detailed root cause investigations underway","PIR held - Actions are being tracked in the ""Actions Tab""",8/12/2024,TBC,,,,"No,Yes",No,Yes,Yes,External Dependencies,NA,The root cause has been attributed to a human error at Worldline (WL) whilst deploying a change to fix an underlying issue on their provisioning tool. ,NA,Human error,RPOS-14681,12/3/2024,December,2024,Pavithra,Closed,MIM,,2024,8,August,9/16/2025,400,>60
8/9/2024,8/9/2024,90481860,09:30,10:17,00:47,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,KI,Issues with multiple Foods and C&H functionalities in CSSM application,Customer,CSSM,," Gap scan, Accuracy checker, Counts, C&H KMR applications in CSSM",RC unknown,70927,"Some stores reported issues with multiple functionalities in CSSM application like Gap Scan, Accuracy Checker, Counts and C&H KMR between 09:30 and 10:17 impacting store operations. The initial root cause has been attributed to an issue with one of the CSSM web servers. Services were restored by performing an IIS reset on the problematic server. Root cause investigations underway. ",A SOP is in place to resolve issues with CSSM IS services,9/9/2024,,,,,Yes,No,Yes,NA,RC unknown,NA,The initial root cause has been attributed to an issue with one of the CSSM web servers,NA,NA,NA,9/9/2024,Septemeber,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,403,>60
8/6/2024,8/6/2024,90482041,08:56,18:49,09:53,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,KI,Issue with ECS ticket batching sent to stores.,Customer,CSSM,,ECS Ticket printing - illegal trading scenario,RC identified,70832,"P&P team had reported issues with emergency price tickets for 44 UPCs not being batched across all stores impacting around 44 UPCs across the estate. The root cause was attributed to the recent JDK upgrade in CSSM stock service resulting in a null stock value for each product sent to ECS. To mitigate the impact, adhoc batches were sent to the store. A permanent fix has been implemented to enable correct stock values to be populated against the UPC's in ECS. ",No actions pending ,8/6/2024,,,,,No,No,No,Yes,Customer tech change,Change , The root cause was attributed to the recent JDK upgrade in CSSM stock service resulting in a null stock value for each product sent to ECS,205558,Inefficient testing,NA,10/29/2024,October,2024,Saloni,Closed,MIM,,2024,8,August,9/16/2025,406,>60
8/6/2024,8/6/2024,90177992,11:48,17:20,05:32,GTS,Digital Workplace Services,Customer Channels,Platform & Store Ops,Manage Allocation of Food Stock,KI,Store colleagues unable to access CSSM and its C&H returns functionalities,Customer,CSSM,,"Collect, ISF, C&H returns, C&H SST",RC identified,69482,"Store colleagues reported issues whilst accessing multiple applications on the honeywells such as Collect, C&H SST and ISF impacting store operations between 09:00 and 10:10. The root cause was attributed to a human error resulting in amendment of the application user group name in active directory which was reinstated to restore services. 

Stores reported issues whilst accessing CSSM C&H returns functionality between 11:48 and 17:20. The root cause was attributed to the incorrect application user group name being cached within the CSSM servers after the scheduled IIS reset at 05:00. Services were restored by performing a IIS reset on the application servers. ",Actions are being tracked in the actions tab,8/6/2024,8/8/2024,,,,"No, No",No,"No, No",Yes,Human error - Customer Tech,NA,"Issue 1 Cause: SD got a request from a store colleague to be added into the relevant user group: ST APP CSSM GM Users in AD and the associate had inadvertently amended the user group SAM (Security Account Manager) account name, which impacted the authentication between the application and Active Directory at 11:45 on 5th Aug. 

Issue 2 Cause: During the scheduled IIS (Internet Information Service) reset of the 3 CSSM application servers at 01:00 and 05:00 on 6th Aug,  incorrect user group was cached on the app servers even after the correction of the group name at 09:18, which resulted in further authentication failures to the impacted functionalities. It is believed that as the C&H Returns functionality uses the security check LDAP (Lightweight Directory Access Protocol) to establish authentication with Active Directory, therefore, only C&H returns functionality was impacted.",NA,NA,NA,11/19/2024,,,Gokul,Closed,MIM,,2024,8,August,9/16/2025,406,>60
8/5/2024,8/5/2024,90181725,07:45,8:35,00:50,GTS,Enterprise Technology Platform,Group Platforms,"HR, Finance",Others,KI,Network connectivity issues at Salford Quays,Vodafone,NA,,Daily activities,RC unknown,69469,"Finance & HR colleagues in Salford Quays were experiencing connectivity issues between 07:45 and 08:35 impacting their daily activities. Services were restored without any tech intervention. 
Product teams have identified a connectivity issue from Salford Quays internet via the Vodafone link and a critical case was raised with Vodafone and relevant logs has been shared to ascertain the root cause. We await updates from Vodafone. "," Actions have been tracked and closed, refer to the ""actions tab"" for details ",8/5/2024,,,,,8/9/2024,No,No,NA,External Dependencies,NA,"Root cause remains inconclusive as Vodafone was unable to replicate the issue in their lower environment. Also the provided logs are insufficient for VF to further analyze the actual RCA. 

 Additoinal logging has been enabled to caputre the right logs for RCA investigatoins during the next re-occurence of the issue.",NA,NA,NA,9/15/2024,Septemeber,2024,Pavithra,Closed,MIM,,2024,8,August,9/16/2025,407,>60
8/3/2024,8/5/2024,90174166,20:30,15:44,44:14,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Castle Donington PLC issue,TCS,Linux,,PLC/ Automation,RC identified,69481,"Castle Donington Engineering team reported connectivity issues whilst rebooting one of the PLCs (Programable logic controller) at 20:30. There is no major impact to DC operations due to the issue however the site operations is at RISK where PLC's cannot be rebooted if it encounters any other faults. The root cause was attributed to the recent RHEL migration activity performed on CD TSM server on 30/07. The change has been reverted, however, the connectivity issues with the PLCs continue to persist, further investigations underway along with Redhat.",This requires a design change and is being worked with architect and being tracked with Jira [JDATE-10452] New design required for maintaining PLC servers.,8/3/2024,8/5/2024,,,,"No, No",No,"No, No",No,Customer tech change,CR,The root cause was attributed to the RHEL migration activity performed on CD TSM server on 30/07.,CRQ000000205206,Design issue,JDATE-10452,9/12/2024,Septemeber,2024,Gokul,Closed,MIM,,2024,8,August,9/16/2025,409,>60
8/1/2024,8/1/2024,90169969,16:37,17:47,01:10,Customer Channels,Customer Engagement ,Customer Channels,"Customer Engagement, Selling Experience",Manage Payment & Content Customer.Com,SI,Customers getting Something went wrong error message when attempting to sign-in,Customer,Akamai,"£4,950.00",Website journey,RC identified,69367,"Customers experienced difficulties while attempting to log in to the Customer.com website from 16:37 to 17:47. Approximately 4,500 customers were affected by this issue.  The root cause has been identified as an issue with Akamai, which was blocking a group of legitimate customer requests from specific IP addresses. Akamai resolved the issue by unblocking these IP addresses. Root cause investigations are currently underway.",All the actions are tracked in problem actions tab,8/1/2024,TBC,,,,Yes,No,Yes,TBC,Communication gap,NA," The root cause has been identified as an issue with Akamai, which was blocking a group of legitimate customer requests from specific IP addresses.",NA,NA,NA,1/16/2024,January,2024,Kavitha,Closed,.Com SM,,2024,8,August,9/16/2025,411,>60
7/31/2024,7/31/2024,90167863,17:30,20:50,03:20,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Message processing issues between WMS and WCS at Donington,SSI Schaefer,NA,,Picking and packing operations,RC identified,69478,"From 17:30, issues were observed in processing messages between WMS and WCS at Donington impacting picking, packing and dispatch operations. Order proposition has been moved for Click & Collect and Next Day Delivery orders to mitigate customer miss promises. No CFR impact to customers.
There was capability loss of approx 6k and proposition was amended to mitigate CFR impact.  The root cause is attributed to an issue with the WCS services. SSI Schaefer had restarted the affected services to restore services by 20:50. Root cause investigations underway. ",,7/31/2024,,,,,,,,,External Dependencies,NA, The root cause is attributed to an issue with the WCS services. Detailed root cause could not be ascertained due to inadequate logs and support from SSI.,NA,NA,NA,9/5/2024,Septemeber,2024,Saloni,Closed,MIM,,2024,7,July,9/16/2025,412,>60
7/30/2024,7/30/2024,90165269,12:55,15:25,02:30,GTS,Enterprise Technology Platform,All BU's,All BU's,Manage Payment & Content Customer.Com,MI,Customers getting an error message across various pages and channels,Microsoft,NA,"£435,600.00",Website journey,RC identified,69476,"From 12:55 customers were experiencing various issues including slow loading of pages and error messages when on the Customer website. Around 19.8k orders (based on forecast) were impacted which equates to £268k loss (worst case scenario) and poor customer experience. Services provided by Blue Yonder, Sorted label printing were also reported to have been impacted but there were no reports from our colleagues.  Microsoft advised that their infrastruture had been impacted by a DDOS attack and measures that they had implemented to protect their environment had not worked due to errors in the deployment.   Service was restored from 15:25, when Microsoft implemented network configuration changes and performed failovers to resilient paths to mitigate the issue.",All actions are closed,7/30/2024,,,,,Yes,Yes,No,Yes,External Dependencies,NA,Microsoft advised that their infrastruture had been impacted by a DDOS attack and measures that they had implemented to protect their environment had not worked due to errors in the deployment.,NA,NA,NA,11/14/2024,November,2024,Saloni,Closed,.Com SM,,2024,7,July,9/16/2025,413,>60
7/30/2024,7/30/2024,90165072,13:10,15:10,02:00,GTS,Enterprise Technology Platform,All BU's,All BU's,Others,KI,BMC Helix (Remedy) and Tech support portal inaccessible,TCS,Remedy,,BMC Helix,RC identified,69483,Colleagues were unable to access BMC Helix (Remedy) and Tech support portal between 13:00 and 15:10. This impacted their ability to create/update incidents and service requests. The root cause has been attributed issues during the SSO certificate renewal. A new certificate was shared with BMC vendor and it was updated in their platform to restore the services. Detailed root cause investigations are underway.  ,All the agreed actions are closed ,7/30/2024,7/30/2024,,,,"No,yes",no,Yes,Yes,Certificate issue,NA,The root cause has been attributed issues during the SSO certificate renewal due to a knowledge gap in knowing the dependcies of the  network F5 certificate within BMC infrastructure ,NA,NA,NA,8/15/2024,August,2024,Kavitha,Closed,MIM,,2024,7,July,9/16/2025,413,>60
7/27/2024,7/27/2024,90158804,06:00,10:30,04:30,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,Some suppliers had not received the foods 28-day order plan,Customer,CXM,,Delay in sending 28 Day Order Plan to suppliers,RC identified,69366,239 suppliers did not receive the foods 28-day order plan impacting future planning activities. The initial root cause was attributed to a reporting issue within CXM (Category Exception Management) application responsible to send the order plans to the suppliers. Services were restored by manually retriggering the order plan file from CXM to the suppliers by 10:30. ,Agreed PIR actions are tracked and closed (refer to the actions tab for details),7/27/2024,8/8/2024,,,,"No,Yes",No,Yes,Yes,Infrastructure issue / Hardware failure,NA,"The   root cause was attributed to a connectivity issue within Microsoft which had impacted the connectivity between CXM (Category Exception Management) application and Power BI Pagenated reports (hosted by MS) . Due to insufficient logs, MS is unable to further investigate on what caused the connectivity issue at their end. During the next issue occurence, additional log capturing will be enabled based on MS recommendations provided.",NA,NA,NA,8/27/2024,August,2024,Pavithra,Closed,MIM,,2024,7,July,9/16/2025,416,>60
7/26/2024,7/26/2024,90181725,08:30,10:34,02:04,GTS,Enterprise Technology Platform,Group Platforms,"HR, Finance",Network,KI,Colleagues in Salford Quays affected by Wi-Fi connectivity issues,TCS,Network,No Financial impact,Wifi connectivity,RC identified,69469,Colleagues in Salford Quays were experiencing Wi-Fi connectivity issues between 08:30 and 10:34 impacting their daily activities. The root cause was attributed to a connectivity issue between the access points and the wireless controllers after a power outage at the site. Services were restored by resetting the access points affected and the wireless controller. ,"PIR held - Actions are being tracked in the ""Actions Tab""",7/26/2024,,,,,"Yes,No", No,No,NA,Infrastructure issue / Hardware failure,NA, The root cause was attributed to a connectivity issue between the access points and the wireless controllers after a power outage at the site.,NA,NA,NA,,,,Pavithra,Closed,MIM,,2024,7,July,9/16/2025,417,>60
7/23/2024,7/23/2024,90149789,11:40,13:32,01:52,GTS,Enterprise Technology Platform,All BU's,All BU's,Infrastructure,SI,Control-M was unavailable for a brief period.,TCS,Systems Management Team,,Control M,RC unknown,69364,"Control M application was unavailable between 11:40 and 13:32. This impacted site operations at various DCs resulting in a capability loss of 50.2k singles at Donington and order prop move, delay in trailer shipment,  and multiple reports. The root cause was attributed to an issue during a planned failover activity of Control M service from primary to secondary server",All the agreed actions are closed ,7/23/2024,,,,,No,No,Yes,NA,Customer Tech change,CR,The root cause was attributed to an issue during a planned failover activity of Control M service from primary to secondary server. Due to Inadequate logs within OS and network layer Vendor BMC is unable to further investigate on the actual root cause of the issue .,199996,inadequate testing,NA,10/10/2024,October,2024,Pavithra,Closed,MIM,,2024,7,July,9/16/2025,420,>60
7/21/2024,7/21/2024,90144274,11:07,17:00,05:53,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Wireless connectivity issues at Welham Green DC impacting site operations,Locus,NA,,"Wifi connectivity, Locus automation",RC unknown,69310,"Welham Green DC colleagues reported wireless connectivity issues between 11:07 and 12:55 impacting picking, packing and despatch operations. This resulted in capability loss of 32k and 33k singles for boxed and hanging orders, respectively. The initial root cause has been attributed to a network loop between two Locus switch ports resulting in a broadcast traffic thus impacting Locus firewalls and the WLCs (Wireless LAN controller). The Locus Robots traffic was initially isolated to restore wireless network connectivity , however the locus automation remained impacted. To restore the service, one of switch port was disabled to break the loop by 16:56. Detailed root cause investigations underway.",All the agreed actions are closed ,7/21/2024,,,,,No,No ,Yes,NA,RC unknown,NA,"The initial root cause has been attributed to a network loop between two Locus switch ports resulting in broadcast traffic thus impacting Locus firewalls and the WLCs (Wireless LAN controller). The port on the back up switch was down, however on 21/07 at 10:28, the port came online and resulted in a network loop. However, the reason on how the port came online itself is inconclusive. ",NA,NA,NA,10/7/2024,October,2024,Kavitha,Closed,MIM,,2024,7,July,9/16/2025,422,>60
7/20/2024,7/20/2024,90147084,01:38,9:48,08:10,Customer Channels,Customer Engagement ,Customer Channels,Customer engagement,Manage Payment & Content Customer.Com,KI,Sparks Pay down for registration and account services across all channels,Customer,Wallet Experience,,Sparks Pay landing page,RC identified,69312,"Alerting indicated that Sparks Pay landing page was inaccessible between 01:38 and 09:48 impacting the customers to register for Sparks Pay or manage their Sparks Pay account. However, customers were able to make payments using sparks pay. The initial root cause has been attributed to the expiry of access token used to authenticate the application and it was renewed to restore services. A detailed root cause analysis underway.",Actions are being tracked in the actions tab,7/20/2024,,,,,,,,,Certificate issue,NA,"The Y account (Generic User account) is used to trigger the job and pull the images from the github container registry. The PAT (Personal Access Token) token for the Y account had expired and Kubernetes was trying to pull the image using expired PAT token. Due to this access issue, the Pods went to Image pull back off state and unable to serve the request making the application down. ",NA,NA,NA,8/28/2024,August,2024,Saloni,Closed,.Com SM,,2024,7,July,9/16/2025,423,>60
7/19/2024,7/19/2024,90140291,06:00,14:00,08:00,GTS,Digital Workplace Services,All BU's,All BU's,Infrastructure,MI,Global issue at CrowdStrike impacting multiple services hosted on Microsoft platforms,CrowdStrike,N/A,"£49,070 (Retail)","Card payments, Picking operations at Foods RDC, international order processing at CD and various applications like Assit, Bookable,Editrack etc.",RC identified,69310,"From 06:00, multiple services were impacted including colleagues/ suppliers using non Customer laptops, card payments at Digital Cafes, picking operations at Foods RDCs, international customers placing orders, packing international orders at Donington. Also Assist, Bookable, Editrack, Secure applications were impacted. The root cause was attributed to a global issue at CrowdStrike affecting Microsoft platforms. CrowdStrike had  deployed a fix by 09:00 and all our impacted services were gradually recovered by 14:00.",,7/19/2024,,,,,,,,,External Dependencies,NA,The root cause was attributed to a global issue at CrowdStrike affecting Microsoft platforms.,NA,NA,NA,,,,Rehan,Open,MIM,,2024,7,July,9/16/2025,424,>60
7/17/2024,7/18/2024,90137938,23:10,2:00,02:50,GTS,Enterprise Technology Platform,All BU's,All BU's,Network,MI,Multiple servers which are running as primary in Swindon moved to Stockley.,TCS,Network,,Various applications,RC identified,69412,"Following the Stockley Nexus switch reboot activity on 17/04, JDA Dispatcher application across multiple C&H and Foods distribution centres was not accessible between 23:00 and 2:00 impacting DC operations. The root cause was attributed to an overnight change which resulted in all the servers/apps hosted in the Swindon Data Centre to fail over to Stockley Park. Services were restored by manually repointing the impacted servers/apps to the Swindon Data Center.",All th agreed actions have been closed,7/17/2024,,,,,"No,yes",No,Yes,,Customer Tech change,CR,"The witness server sits in Waterside and determines when a failover is necessary.  When the Stockley Nexus 7k was rebooted as part of a planned change to fix a disk issue, this caused the connectivity between VSAN in Waterside and Swindon to break which caused the DR situation, hence the VM servers running as primary in Swindon moved to Stockley as they are supposed to do.  As many services have now been moved as part of the Stockley Park exit from Stockley to Swindon this explains the impact we observed during the incident",,,,10/1/2024,October,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,426,>60
7/17/2024,7/17/2024,90135982,04:10,7:44,03:34,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Foods Final Order (FFO) delayed due to delay in Relex nightly batch run,Relex,N/A,,FFO,RC identified,69183,"Foods final orders for Relex lines were delayed resulting with an impact to RDC allocations, finalised supplier orders, availability of 28 day order plan, Relex UI and OTS reports. The initial root cause was attributed to change which delayed the nightly run. Further root cause analysis underway.",Actions are updated in problem actions tab,7/17/2024,7/17/2024,,,,"No, No",No,"No, No",Yes,External Dependencies,NA,The root cause was attributed to a delay in processing the Relex files after the multi-tray change within Relex infrastructure.,NA,NA,NA,12/10/2024,,,Gokul,Closed,MIM,,2024,7,July,9/16/2025,426,>60
7/15/2024,7/15/2024,90169094,09:30,12:40,03:10,GTS,Digital Workplace Services,Foods,Foods Supply Chain,Others,KI,Find application inaccessible to users,Customer,Active Directory,,FIND,RC identified,69835,"Colleagues were unable to access FIND application between 09:30 - 12:40 impacting their planning activities on product lifecycle however, no impact to suppliers. The initial root cause was attributed to a secret key expiry on active directory. Services were restored by generating a new secret key and shared with vendor TraceOne. ","All the actions are closed - Please refer to the ""Problem actions"" tab.",7/15/2024,,,,,"No, No",No,"No, No",Yes,Certificate issue,NA,"The root cause was attributed to an expired secret key, responsible for application authentication. ",NA,NA,NA,8/14/2024,August,2024,Pavithra,Closed,MIM,,2024,7,July,9/16/2025,428,>60
7/12/2024,7/12/2024,90126150,05:15,9:41,04:26,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,SI,Some suppliers have received higher than expected volumes for certain Food product lines,Customer,Supply chain management ,N/A as prior to Nov 2024,FFO,RC identified,69406,"Through alerting, it was identified that circa 200 suppliers had received inflated volumes for 238 Food product lines. This impacted the generation of 28 day order plan to the suppliers. To mitigate the impact, contingency allocations were applied and suppliers were asked to amend their PO's in line with 28 day order plan. The root cause was attributed to an incorrect amendment of the event dates for August Bank Holiday and Christmas orders to 16th and 17th July by Relex business colleagues (super user). The event dates were corrected to reflect the expected order volumes.",PIR scheduled for 17/07,7/12/2024,,,,,,,,,Human error - Business,NA,The root cause was attributed to an incorrect amendment of the event dates for August Bank Holiday and Christmas orders to 16th and 17th July by Relex business colleagues (super user).,NA,NA,NA,10/15/2024,October,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,431,>60
7/11/2024,7/11/2024,122969603,13:37,16:10,02:33,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Service Experience",Manage Payment & Point of Sales in store,MI,Intermittent issues with Digital cafés,Customer,Digital café,"£13,083.00",Digital café,RC identified,69157,Intermittent issues were reported whilst placing orders on the Digital Cafés kiosks between 13:37 and 16:10.  This was impacting circa 500 customers placing orders on the kiosks without a drink resulting in poor customer experience. The root cause has been attributed to a change to remove the upselling of diet coke bottles from the product catalogue as they were out of stock. Counter Solution removed the diet coke bottles from the upselling database to restore services.,Actions are being tracked in the actions tab,7/11/2024,7/11/2024,,,,"No, Yes",No,No ,Yes,Customer Business change,Yes,The root cause has been attributed to a change to remove the upselling of diet coke bottles from the product catalogue as they were out of stock.,NA,NA, RDC-370,11/21/2024,November,2024,Kavitha,Closed,MIM,,2024,7,July,9/16/2025,432,>60
7/11/2024,7/11/2024,90124327,08:00,9:51,01:51,Customer Channels,Service Experience,Customer Channels,"Customer Engagement, Service Experience",Manage Payment & Point of Sales in store,MI,Issues with card payments at stores and online,Worldline,N/A,"£8,500 (Online)",Card Transactions,RC identified,69212,Card payments were declined in both stores and online between 08:00 and 09:51 impacting 135 transactions as the payments were declined on the website.  The root cause was attributed to a global issue within Worldline infrastructure impacting multiple customers including Customer. Awaiting detailed root cause and resolution details from Worldline.,,7/11/2024,,,,,,,,,External Dependencies,NA,"  The root cause was attributed to a global issue within Worldline infrastructure impacting multiple customers including Customer - Worldline had given a verbal confirmation that they were doing some maintenance work in their three data centres (DC1, 2, 3).  There seems to have ben an issue at DC1. The three waves of impact we experienced correlates with the reversion of these changes by Worldline. ",NA,NA,NA,12-Dec,December,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,432,>60
7/10/2024,7/10/2024,90123458,20:57,22:07,01:10,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Castle Donington customer orders issue,Customer,WMS,,CD customer orders,RC identified,69411,"Castle Donington DC colleagues encountered issues whilst processing customer orders between 20:57 and 22:07 impacting 1273 must-go orders resulting in a CFR impact of 1.7%. The root cause was attributed to a job failure responsible for order creation which was re-triggered to restore the services. As a permanent fix, a code is being developed and will be implemented during the next site outage.    ","PIR published and actions are tracked and closed  -Refer the problem actions Tab"" for details",7/10/2024,7/10/2024,,,,"Yes, No",Yes,No,No ,Configuration issue,NA,The root cause was attributed to the failure of a control-M job responsible for Order release. The problematic order release job has a dependency on the other critical order release jobs and these jobs are interdependent and will be executed as a loop during order creations from WMS to WCS. Due to an exception error within the “Offline reallocated order release job” the subsequent “New order release job” had also failed resulting in the incident.,NA,NA,NA,8/8/2024,August,2024,Pavithra,Closed,MIM,,2024,7,July,9/16/2025,433,>60
7/10/2024,7/10/2024,90121906,05:15,10:23,05:08,GTS,Digital Workplace Services,Customer Channels,Platform & Store Ops,Others,SI,Store colleagues unable to login to their honeywell devices,Customer,Honeywell,,Honeywell,RC identified,69410,Store colleagues were unable to login to their honeywell devices between 05:15 and 10:23 impacting stores operations. The root cause was attributed to connectivity issues between the honeywells and the Store Central Proxy server after a change to clone the server as part of SP Exit program.  Services were restored after the connectivity was re-established to the Store Central Proxy server in Stockley Park. Detailed root cause investigations underway.,All Actions closed,7/13/2024,,,,,Yes,No,No,Yes,Customer Tech change,CR,The root cause was attributed to connectivity issues between the honeywells and the Store Central Proxy server after a change to clone the server as part of SP Exit program,204296,Inefficient implementation plan,NA,10/7/2024,October,2024,Saloni,Closed,MIM,,2024,7,July,9/16/2025,433,>60
7/5/2024,7/5/2024,90111944,08:30,15:00,06:30,C&H and Intl ,C&H & Intl Supply Chain,Customer Channels,"C&H and Intl supply chain, Service Experience ",Others,KI,High rate of customer orders cancellation for Ollerton,Customer,WMS & .com,,DC orders,RC identified,69408,"A small number of customer orders fulfilled from Ollerton have been getting cancelled since late last year however the impact has been now magnified due to the volume ramp up in Ollerton. 5-10% of customer orders are impacted. The initial root cause has been attributed to systemic issue in WMS causing a stock mismatch between WMS and Availability Service resulting in order cancellation. As a workaround,  we are carrying out a clean-up in the availability service to avoid the stock mismatch. There were multiple scenario's causing negative DNR's (Demand Notification Response) which was resulting with a stock mismatch between WMS and Availability Service since 2023.Since the mismatch variance was low it was handled within WMS through a manual workaround. On 6th Aug, there was a change deployed to exclude the non-ecom zones in IAA logic the issue permanently",This incident was not a MIM handled incident - WE have got the document on the  fix details - The same has uploaded in the sharepoint,7/5/2024,7/5/2024,,,,"No,Yes",No,Yes,Yes,Design issue,NA,"
The  root cause has been attributed to systemic issue in WMS where IAA transactions were getting included in the adjustment extract for non Ecom SKU's (stock keeping unit) causing a stock mismatch between WMS and Availability Service resulting in order cancellation. ",NA,NA,NA,9/2/2024,August,2024,Pavithra,Closed,.Com SM,,2024,7,July,9/16/2025,438,>60
7/4/2024,7/6/2024,90109761,10:00,8:38,46:22,GTS,Enterprise Technology Platform,All BU's,All BU's,Network,SI, Intermittent CPU spikes in Stockley Park network firewall impacting multiple applications performance,Fortinet,N/A,,"LCS, Collect, ISF Picking, Backstage Counting, Returns, Digital Cafés and Donington pack screens",RC identified,69305,"From 10:00, intermittent performance issues were reported across multiple in-store applications – LCS (Local Count Service), Collect, ISF Picking, Backstage Counting and Returns. Digital Cafés and Donington pack screens were also impacted. Initial root cause is attributed to the intermittent CPU spikes observed in the Stockley Park FortiGate firewall. We have disabled the SandBox feature on 06/07 at 08:48 and the services have remained stable.",All the agreed actions are closed ,7/18/2024,,,,,NO,No,Yes,Yes,External Dependencies,NA,"Root cause summary: Whenever a new configuration change or any type of update package is received in the FortiGate, the OS will signal the IPS engine to always ‘reload’ the engine configuration.

In some cases, this engine configuration reload is absolutely necessary (e.g. if rule is added/deleted). However, in the case of Malicious URL DB or a FortiSandbox Block list change, this reload is not required. In circumstances where number of sessions is large, and/or when the firewall is processing many policies; and/or when configuration changes or AV/FSA update is too frequent – this can cause the IPS engine to become unnecessarily loaded, resulting in an observable CPU ‘spike’. At times this CPU spike would result in traffic ‘dropping’, or IPS engine crashes.

Fortinet are now working on a custom build for us that will reduce the number of reloads and hence minimise the risk of further IPS engine crashes. The build will be available by the end of July and will allow us to return back to the default number of updates which for now has been reduced to once a day at 1pm.

A separate discussion is being held regarding how to bring back Sandbox updates and to what exten",NA,NA,NA,9/17/2024,September,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,439,>60
7/3/2024,7/4/2024,90107367,15:50,2:02,10:12,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI, Issues with processing messages from Castle Donington WMS to WCS,Customer,WMS,"£18,000.00",DC operations ,RC unknown,69091,"Issues were observed with message processing between Castle Donington WMS and WCS impacting pick, pack and despatch operations for circa 70 minutes. This resulted in a capability loss of 42.9k singles and the order proposition was moved +24 hours for C&C and NDD orders. Services were restored without any manual intervention, mitigation actions both from application and infrastructure are in place to avoid re-occurrence. Detailed root cause investigations underway with vendor Redhat and Oracle. ",All the agreed actions are closed ,10/14/2024,,,,,Yes,Yes,Yes,NA,RC unknown,NA,"Despite extensive investigations with out teams and the assistance of Red Hat the root cause is inconclusive. The technical teams believe that due to the “back-to-school” campaign the number of orders doubled, and that required limit of the number of files that can be concurrently open on the DB server was increased from 10240 to 32800.",NA,NA,NA,8/8/2024,August,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,440,>60
7/2/2024,7/2/2024,90105509,16:30,19:25,02:55,Customer Channels,Service Experience,Customer Channels,"Platform & Store Ops, Service Experience",Others,KI,Post purchase fraud check issues with Accertify.,Customer,.com,,Ecom orders and DC orders,RC identified,69143,"Alerting identified that post purchase fraud checks from OMS to Accertify were failing between 16:30 and 19:25. This impacted Ecom distribution centre - Donington, Bradford, Ollerton, Stoke, Brands etc and supplier order fulfilment resulting in potential customer miss promises. The root cause has been attributed to a miscommunication resulting in inadvertent reset of the authentication keys in POG (Payment Orchestration Gateway) causing a connectivity issue between POG and Accertify. The authentication key was corrected and services were restored.",All agreed has been closed,7/2/2024,,,,,Yes,Yes,No,Yes,Human error - Customer Tech,NA, The root cause has been attributed to a miscommunication resulting in inadvertent reset of the authentication keys in POG (Payment Orchestration Gateway) causing a connectivity issue between POG and Accertify,NA,NA,NA,9/17/2024,September,2024,Kavitha,Closed,.Com SM,,2024,7,July,9/16/2025,441,>60
7/1/2024,7/2/2024,90102001,10:57,11:18,24:21,Group Platforms,Finance,Group Platforms,Finance,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,Proof of deliveries not created for food suppliers since the middle of last week,Customer,SAP,,"SAP, FITS,Foods Supplier Payments",RC identified,69141,"Finance colleagues reported a discrepancy of approx. £51 million between SAP stock position and the DC stock position data between 24th June and 1st July. Foods suppliers were not receiving proof of delivery and they could not issue an invoice, hence supplier payments were potentially delayed. The root cause was attributed to a certificate issue in the middleware layer which was fixed to restore connectivity within the flow. ",All actions are closed in the tracker,7/1/2024,,,,,"No, yes",No,No,Yes,Certificate issue,NA,The root cause was attributed to a certificate issue in the middleware layer which was fixed to restore connectivity within the flow. ,NA,NA,NA,11/4/2024,November,2024,Rehan,Closed,MIM,,2024,7,July,9/16/2025,442,>60
6/30/2024,6/30/2024,90099341,03:07,12:35,09:28,Customer Channels,Selling Experience,Customer Channels,"Platform & Store Ops, Service Experience",Others,KI,Customers & Store Colleagues unable to book Bra & Suit fit appointments both Online and In-Store,Customer,Assist,,Appointment,RC identified,69078,Store colleagues and customers were unable to book Bra & Suit fit appointments via Assist app and website respectively  between 03:07 and 12:35 resulting in poor customer experience. The initial root cause was attributed to an expiry of Booking application service (Y) account password. Services were restored by reinstating the Y account. Detailed root cause investigations underway.,All actions are closed,6/30/2024,6/30/2024,,,,"No, Yes",No,Yes,Yes,Human error - Customer Tech,NA,"The Y account for Booking service application was created in March 2024 and due to a human error the “password never expire” flag was not checked in Active Directory, which caused the password to expire on 26th June, however, the issue did not surface until 30th June, due to the usage of existing application cache.
It has been concluded that as part of Microsoft AKS patch update, the application pods were restarted which triggered the issue. ",NA,NA,NA,7/30/2024,August,2024,Gokul,Closed,MIM,,2024,6,June,9/16/2025,443,>60
6/30/2024,6/30/2024,90099610,07:00,9:54,02:54,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Manage Allocation of Food Stock,SI,Stores reporting issue with various functionalities in CSSM,Customer,CSSM,,CSSM,RC unknown,69139,"Store colleagues reported intermittent issues with various CSSM application functions between 07:00 and 09:54. This impacted their ability to perform foods price reduction, deliveries, counting etc. Services were restored by restarting the application service on one of the 3 CSSM webservers. Detailed root cause investigations underway.",Actions are updated in problem actions tab,6/30/2024,6/30/2024,,,,"Yes, No",No,"Yes, YEs",No,RC unknown,NA,CSSM team confirmed based on their detailed investigation and analysis that they have had similar incidents in the past and root cause was inconclusive and they will follow the SOP in case of such incidents,NA,NA,NA,12/6/2024,,,Gokul,Closed,MIM,,2024,6,June,9/16/2025,443,>60
6/29/2024,6/29/2024,90097755,12:51,13:21,00:30,GTS,Enterprise Technology Platform,Customer Channels,"Platform & Store Ops, Service Experience",Manage Payment & Point of Sales in store,SI,Network connectivity issue at stores,Vodafone,N/A,,Stores Network,RC identified,69138,"Alerting indicated that multiple stores went offline at 12:51. All stores would have experienced max 3mins network blips across tills, honeywells, wifi connectivity impacting payments and overall operations. The initial root cause is attributed to the failover of network traffic from SP DIA (Direct internet access) link to Swindon DIA link at 12:51. The traffic was failed back at 13:21 and have remained stable. Root cause investigations underway with Vodafone.",Actions are being tracked in the actions tab,6/29/2024,,,,,"Yes, No",Yes,Yes,Yes,External Dependencies,NA,The root cause was attributed to the failover and failback of internet traffic from Stockley Park DIA (Direct Internet Access) link to Swindon DIA links which had resulted in 2 network blips at 12:51 and 13:21,NA,NA,NA,12/12/2024,December,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,444,>60
6/28/2024,6/28/2024,90096085,14:32,16:00,01:28,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Carrier label printing issues at Stoke DC,Customer,Mule,,carrier label printing,RC identified,69137,"Stoke DC colleagues were unable to print carrier labels for ecomm orders between 14:32 and 16:00 impacting despatch operations, with no CFR impact. The root cause was attributed to a new certificate not being refreshed across pack benches at the site after the certificate installation on 27/06. Services were restored by restarting the pack benches to refresh the new certificate. Detailed root cause investigations underway. ",All actions are closed,6/28/2024,,,,,,,,,Certificate issue,NA,The root cause was attributed to a new certificate not being refreshed across pack benches at the site after the certificate installation on 27/06.,NA,NA,NA,7/31/2024,July,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,445,>60
6/24/2024,6/24/2024,90087203,15:19,15:46,00:27,Customer Channels,Selling Experience,Customer Channels,"Platform & Store Ops, Selling Experience",Manage Payment & Content Customer.Com,SI,Blank Home Page served to customers on desktop and mobile web,Microsoft,NA,"£11,000.00",Customer .com website,RC identified,68983,Customers were observing a blank homepage on the website on desktop and mobile web between 15:19 and 15:46. This impacted around 10k customers resulting in poor customer experience.  The initial root cause was attributed to a problematic Akamai cache which was cleared to restore services. An Akamai change was implemented to prevent recurrence of the issue. Root cause investigations underway.,Actions are being tracked in the actions tab,6/24/2024,TBC,,,,"NO,Yes",NO,"Yes, YEs",,External Dependencies,,  The initial root cause was attributed to a problematic Akamai cache which was cleared to restore services. The issue was caused due to an unaxpected behivour of the Microsoft patch due to  a condition when unpacking the app zip. Mictosoft case -2406250030009295,NA,NA,,9/4/2024,September,2024,Kavitha ,Closed,.Com SM,,2024,6,June,9/16/2025,449,>60
6/20/2024,6/20/2024,90079822,12:30,14:25,01:55,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,SI,Intermittent issues with click and collect kiosks and tablets at stores,Customer,C&C ,N/A as prior to Nov 2024,Click and collect,RC identified,68963,"Store colleagues were reporting intermittent issues with click and collect kiosks and tablets between 12:30 and 14:25. This is impacting order collection at stores resulting in poor customer experience. A workaround was applied where colleague can search for the customer by using the Collect app on honeywells. The initial root cause has been attributed to a memory constraint resulting in frequent restart of the application pods. The number of pods available to the application were increased to mitigate the impact. Also in an attempt to fix the issue, a change has been deployed  for 17 innovation stores on 21/06 and 22/06 to optimize logging, hypercare monitoring is in place.  Further investigation continues on root cause. ",PIR arranged for 27/06,6/20/2024,,,,,,,,,Capacity & Scalability Limitations,NA,The initial root cause has been attributed to a memory constraint resulting in frequent restart of the application pods,NA,NA,NA,8/7/2024,August,2024,Rehan,Closed,MIM,,2024,6,June,9/16/2025,453,>60
6/18/2024,6/18/2024,90073599,05:30,7:30,02:00,Group Platforms,Finance, C&H and Intl,International Commercial Trading,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,Czech Republic and Greece colleagues were unable to access e-SAP from the DC and Head office.,Customer,SAP Basis team,,E-Sap Application,RC unknown,69085,"Czech and Greece colleagues were unable to access ESAP application between 04:30 and 09:19. This impacted their ability to carry out pick and pack operations at the DC and activities such as financial reporting, purchase orders etc. The initial root cause was attributed an incorrect file format of the host file on the server and it was correct to restore the services",PIR has been published - actions were   tracked and closed in the actions tab,6/24/2024,NA,,,,"NO,Yes",No,"Yes, YEs",NA,Infrastructure issue / Hardware failure,NA,The  root cause was attributed an amendment of the local host file format  responsible for connecting eSAP application and router from .file to .txt. However the actual RCA on what caused the file format change in inconclusive. ,NA,NA,NA,7/15/2024,July,2024,Pavithra,Closed,MIM,,2024,6,June,9/16/2025,455,>60
6/17/2024,6/17/2024,90073897,16:50,17:40,00:50,GTS,Enterprise Integration, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Issues with carrier label printing at Castle Donington,Customer,Mule,,carrier label printing,RC identified,68777,Castle Donington DC colleagues were experiencing issues whilst printing carrier labels between 16:50 and 18:18 impacting packing and despatch operations. This resulted in capability loss of 9.3k singles with no CFR impact. The root cause was attributed to slowness in message processing between Mule and WMS database layers.  Services were restored by restarting the Mule services. A permanent fix to perform load distribution to other Mule servers and optimise Mule audit queues is being implemented in phases.,All actions are closed,6/17/2024,6/17/2024,,,,,,,,Configuration issue,NA,The root cause was attributed to slowness in message processing between Mule and WMS database layers.  Services were restored by restarting the Mule services.,NA,NA,NA,8/14/2024,July,2024,Gokul,Closed,MIM,,2024,6,June,9/16/2025,456,>60
6/17/2024,6/17/2024,90073503,10:30,13:20,02:50,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,Issues with hanging operations at Castle Donington,Customer,WMS ,,Handing Operations at CD,RC identified,69104,Castle Donington DC colleagues were experiencing issues with hanging operations between 10:30 and 14:12. This impacted inbound receipts and pack operations resulting in capability loss of 33k singles with no CFR impact. The order proposition for NDD and Click & Collect orders were moved due to this issue. The initial root cause has been attributed to an inefficient execution plan impacting the WMS database performance. Services were restored by manually executing the gather stats on the WMS database.,All the actions agreed has been closed,6/17/2024,6/17/2024,,,,Yes,Yes,Yes,Yes,Database issue ,NA,The initial root cause has been attributed to an inefficient execution plan impacting the WMS database performance,NA,NA,NA,7/12/2024,July,2024,Kavitha,Closed,MIM,,2024,6,June,9/16/2025,456,>60
6/17/2024,6/20/2024,90068324,12:00,19:50,43:50,Group Platforms,Finance,Foods,Foods Supply Chain,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,Food RDC Good Receipting missing in SAP for depot date 8th Jun,Customer,SAP,,Supplier payments,RC identified,69049,"Finance stakeholders had reported issues with missing good receipts in SAP for 8th June. This impacted £609k stock loss for supplier GRs, £689K outstanding stock in transit from NDCs to RDC/GIST and £420K missing PODs and delay in supplier invoice / payment. The root cause was attributed to a missing step after the cSAP migration activity whilst sending messages for RDC goods receipts into SAP. Services were restored after manually posting the data in SAP followed by reconciling the RDC good receipts across DCs.","All action have been tracked and closed, Please refer to the actions tab for details",6/17/2024,,,,,,,,,Human error - Customer Tech,NA, The root cause was attributed to a missing step after the cSAP migration activity whilst sending messages for RDC goods receipts into SAP,NA,NA,NA,7/24/2024,July,2024,Gokul,Closed,MIM,,2024,6,June,9/16/2025,456,>60
6/15/2024,6/15/2024,90069233,08:30,12:55,04:25,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent carrier label printing issues at C&H Bradford DC,TCS,Network,,Label printing,RC unknown,69009,"C&H Bradford DC colleagues were experiencing intermittent carrier label printing issues for ecomm orders between 08:00 and 12:55 impacting their packing and despatch operations. The order proposition for NDD and Click & Collect orders were moved to +24 hours for 75 mins, however there was no impact to CFR. The root cause was attributed to an issue with traffic flow between the Mule and Carrier gateway (Azure) layers through the primary (Stockley Park) load balancer. Services were restored by routing the entire traffic through the secondary load balancer (Swindon). Resilience has been restored and root cause investigations continue.",All actions are closed in the tracker,6/15/2024,,,,,"No, Yes",No,Yes,No,RC unknown,NA,The root cause was attributed to an issue with traffic flow between the Mule and Carrier gateway (Azure) layers through the primary (Stockley Park) load balancer,NA,NA,NA,10/16/2024,October,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,458,>60
6/14/2024,6/14/2024,90067277,08:30,13:00,04:30,C&H and Intl,C&H & Intl Supply Chain,"C&H and Intl, Foods","C&H and Intl Supply Chain, Foods Supply Chain",Manage & Maintain Labour Resource,KI,Central issue at Blue Yonder impacting Foods and C&H planning activities,Blue Yonder,NA,,"D&F, Foods SRD, WLM ",RC identified,69086,"Multiple applications hosted on Blue Yonder SAAS platform- D&F (Demand & Fulfilment), Foods SRD (Space, Range & Display) and WLM (Warehouse Labour Management) were inaccessible between 08:30 and 13:00. This impacted the colleague's ability to perform planning activities for C&H order allocation over the weekend, foods products and KPI management. DC colleagues were unable to produce colleague performance reports for finance and operations. The root cause was attributed to a central issue within Blue Yonder infrastructure which was resolved to restore services. Detailed root cause underway.",All actions have been completed,6/14/2024,,,,,,,,,External Dependencies,NA,"The root cause was attributed to a central issue within Blue Yonder infrastructure which was resolved to restore services.
On June 14th, an unscheduled routine network change was performed across multiple Azure region networks as part of ongoing efforts to enhance BY's security posture. It was to clean up a network rule identified as not adhering to restricted access requirements. The existing layered network rule framework relied on a secondary rule to ensure access to required networks, particularly purposed for egress firewall requirements. The removal of the identified rule, inadvertently caused a gradual and unforeseen restriction of access to the egress firewall. This impact was not immediately apparent. Initial indicators suggested that an external security service provider might be experiencing service degradation, which was affecting DNS lookups Network monitoring tools and egress connection point logs also indicated potential external service issues. The rule was subsequently rolled back without full awareness of the ongoing outage, which reinstated services to egress access points for DNS and data egress. During the incident, network monitoring and logs indicate a partial loss of connectivity to the firewall. The nature of the logging data and the unscheduled nature of the change, made it difficult to clearly link the firewall change as the root cause of the incident.",NA,NA,NA,7/30/2024,July,2024,Gokul,Closed,MIM,,2024,6,June,9/16/2025,459,>60
6/11/2024,6/11/2024,90062069,16:46,17:03,00:17,GTS,Enterprise Technology Platform,All BU's,All BU's,Manage Payment & Point of Sales in store,SI,Secondary (active) network firewall in Stockley Park was unavailable,Fortinet,NA,,"Payments, POS, Digital Café",RC identified,69043,"The active network firewall (secondary) in Stockley Park went into hung state between 16:46 and 17:03. This impacted instore payments through tills, digital cafes, pack screen connectivity and MI reporting at Donington and citrix application. Services were restored by manually failing over traffic to standby (primary) firewall in Stockley Park.  Root cause investigations underway with vendor Fortinet.",Actions are being tracked in the actions tab,6/11/2024,,,,,,,,,External Dependencies,NA,The root cause was attributed to a debug command being executed in the secondary firewall to triage a separate issue which resulted in the spike in CPU utilisation.,NA,NA,NA,7/31/2024,July,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,462,>60
6/11/2024,6/11/2024,90059839,07:30,9:00,01:30,GTS,Enterprise Technology Platform, C&H and Intl,International Commercial Trading,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,KI,Czech Republic unable to access e-SAP,TCS,Network,N/A as prior to Nov 2024,eSAP,RC identified,68946/66426,"Czech colleagues were unable to access ESAP application between 07:30 and 09:00. This impacted their ability to carry out pick and pack operations at the DC and activities such as financial reporting, purchase orders etc. The root cause was attributed to missing IP addresses in the secondary DIA circuit after the network failover activity on 10/06. These IP addresses were added to restore services. Root cause investigations underway.",PIR sent 21/06,6/11/2024,,,,,,,,,Human error - Customer Tech,NA, The root cause was attributed to missing IP addresses in the secondary DIA circuit after the network failover activity on 10/06,NA,NA,NA,12-Dec,December,2024,Rehan,Closed,MIM,,2024,6,June,9/16/2025,462,>60
6/10/2024,6/13/2024,90058950,11:30,7:20, 68:40:00,Group Platforms,Finance,"C&H and Intl, Digital & Data","C&H commercial Trading, Digital & Data",Others,MI,MP (Merchandise Planning) application was unavailable due to stock data discrepancy.,Customer,SAP,,MP (Merchandise Planning),RC unknown,"68946
","The Merchandise planning (MP) application was made unavailable from 11:30 due to missing stock feeds for store and DC from GMOR. Around 100 colleagues were unable to complete their planning activities for sales and stock intake within the deadline, BEAM and SSI reports were also impacted. The initial root cause is attributed to an issue with data processing in GMOR database tables resulting in partial data transfer to the downstream systems. The data was retrieved and was processed to MP (12/06), MP application was made available at 07:20 (13/08).","PIR published and actoins are  tracked under the ""Problem Actions tab""",6/10/2024,6/14/2024,,,,"No, Yes",No,No,NA,Human error - Customer Tech,NA,"As part of cSAP migration to Swindon, whilst performing batch catch-up activity in control-M all the relevant jobs were released in a controlled manner however, due to unknown reasons two GMOR jobs were executing in parallel resulting in partial data.",NA,NA,NA,7/23/2024,July,2024,Pavithra,Closed,MIM,,2024,6,June,9/16/2025,463,>60
6/8/2024,6/8/2024,90051798,08:00,17:40,09:40,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,Manage Payment & Point of Sales in store,MI,Issues with gift card transaction at the store tills,TCS,Network,NA,Gift Card,RC unknown,68933,"Store colleagues are experiencing issues with gift card transactions between 08:30 and 17:40, dot com transactions were not impacted. This impact to gift card transactions are being quantified. The initial root cause is attributed to an issue with traffic flow between POS beanstore (tills) and SVS (gift card) applications. Services were restored by modifying a network configuration to allow successful traffic flow. Services are currently running without resilience. Detailed investigations underway.",Actions are being tracked in the actions tab,6/8/2024,,,,,"Yes,No",Yes,No,TBC,RC unknown,NA,"The initial root cause is attributed to an issue with traffic flow between POS beanstore (tills) and SVS (gift card) applications. Due to lack of sufficient logs and recurrence of the issue, further root cause could not be determined.",NA,NA,NA,1/11/2024,January,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,465,>60
6/7/2024,6/7/2024,90049399,04:50,12:49,07:59,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,MI,Under allocation in WMS for Quantum orders,Customer,Quantum ,,"Quantum, WMS,ASO",RC unknown,68929,"At 04:50, Bradford and Milton Keynes DC colleagues reported discrepancy with orders allocated to the sites in WMS. This delayed the picking, packing and despatch operations at the sites, 28 day order plan generation and sending purchase orders (only Quantum lines) to the suppliers. As a workaround, allocations were regenerated using previous day's backup file from Quantum and sent to Bradford and Milton Keynes by 09:15, 28 day order plan and Quantum purchase orders were resent to the suppliers by 12:49. The initial root cause has been attributed to a supply chain order file formatting process issue in Quantum resulting in sending partial data to the downstream systems. ","PIR published and actoins were tracked and closed under the ""Problem Actions tab""",6/7/2024,6/7/2024,,,,"No, Yes",No,"Yes, Yes",No,Code/Product bug,NA,The root cause was attributed to issues whilst unzipping the file in SCRD to make it compatible in Quantum for further processing into downstream systems. The root cause behind the unzipping issue remains inconlcuisve due to the nature of the issue.,NA,NA,NA,7/10/2024,July,2024,Pavithra,Closed,MIM,,2024,6,June,9/16/2025,466,>60
6/6/2024,6/6/2024,90046807,06:00,11:27,05:27,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,MI,Bradford ambient DC was unable to pick due to over allocation in WMS,Customer,ASO,,"WMS, ASO",RC identified,69041,"Bradford DC reported issues with picking operations due to an over allocation in WMS between 06:00 and 11:27. This resulted in a delay in picking and despatch of 42K singles at the site.  The initial root cause was attributed to a change in ASO resulting in duplicate allocation. The duplicate orders were deallocated in WMS and allocations were retriggered to Bradford DC to restore services. For the permanent fix, the change was reverted on 06/06. Detailed RCA underway.",All actions are closed,6/6/2024,6/6/2024,,,,"No, Yes",No,"Yes,No",NA,Customer Tech change,CR, The root cause was attributed to a change in ASO resulting in duplicate allocation. ,#202435,inadequate testing,NA,10/31/2024,October,2024,Saloni,Closed,MIM,,2024,6,June,9/16/2025,467,>60
6/4/2024,6/4/2024,90043777,18:30,21:33,03:03,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,"	Issues with hanging operations at  castle Donnington DC",Schaeffer,NA,,WCS(SSI),RC identified,69065,Castle Donington DC were experiencing issues with hanging operations between 18:30 and 21:33. This resulted in a capability loss of 20.6k singles with 1.4% CFR. The root cause was attributed to a hotfix applied  as part of Hanging Optimization project on 23/05 causing a memory constraint impacting one of the WCS services. Services were restored by removing the hotfix followed by restarting the impacted WCS service by vendor SSI Schaefer,"PIR published and all actoins are tracked and closed- under the ""Problem Actions tab""",6/4/2024,6/4/2024,,,,"No, Yes",No,No,NA,External Dependencies,NA,The root cause was attributed to a hotfix applied  as part of Hanging Optimization project on 23/05 causing a memory constraint impacting one of the WCS services.,NA,NA,NA,7/30/2024,July,2024,Pavithra,Closed,MIM,,2024,6,June,9/16/2025,469,>60
6/4/2024,6/4/2024,90042330,03:19,11:50,08:31,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,KI,35 BP stores unable to access Customer applications on honeywells,Customer,Honeywell,,Honeywell,RC identified,68795,"35 BP stores were unable to access Customer applications on their honeywell devices between 03:19 and 11:50 impacting store operations, however, there was no impact to trading. The initial root cause has been attributed to a change deployed on honeywells impacting the wifi connectivity (2.4ghz)  on the honeywell devices. Services were restored by enabling a 5ghz frequency configuration on the honeywell devices. ",Actions are being tracked in the actions tab,6/4/2024,6/4/2024,,,,"Yes,No",No,No,Yes,Customer Tech change,CR,The  root cause has been attributed to a change deployed on honeywells impacting the wifi connectivity (2.4ghz)  on the honeywell devices,202100,design bug ,NA,7/1/2024,July,2024,Kavitha,Closed,MIM,,2024,6,June,9/16/2025,469,>60
6/1/2024,6/1/2024,90035888,10:38,23:50,13:12,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops,Others,ADHOC,Store user manager application issue,Customer,Cloud Ops,N/A as prior to Nov 2024,Store User Manager,RC identified,68927,"On 1/6, few stores were experiencing issues whilst accessing store user manager application impacting their ability to reset password, generating new store user id and granting assess to store applications, however there was no impact to trading. As a workaround service desk were able to assist the store colleagues to reset passwords. The initial root cause has been attributed to a human error during the deployment of a vulnerability fix on production VM's which was meant only for non-prod VMs. The fix was reverted on all 6 user manager application servers to restore services by 23:50. ",Actions are being tracked in the actions tab,6/1/2024,6/1/2024,,,,,,,,Human error - Customer Tech,NA,The root cause has been attributed to a human error during the deployment of a vulnerability fix on production VM's which was meant only for non-prod VMs.,NA,NA,NA,29//01/2025,January,2025,Rehan,Closed,MIM,Y,2024,6,June,9/16/2025,472,>60
5/30/2024,5/31/2024,90033215,13:42,3:45,14:03,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),MI,Unplanned power outage at Castle Donington DC,"National Grid , SSI Scheafer",NA,"£230,000.00","WCS(SSI), WMS",RC identified,68911,"Castle Donington site operations were impacted due to an unplanned power outage between 13:42 and 14:40 resulting in a capability loss of ~552.6K singles and an increased CFR (Customer Failure Rate) of 13%.  After the power was restored, message pileups were observed on the Warehouse Control System (WCS) application impacting site automation.  Vendor SSI had cleared the messages followed by an application restart to restore services at 03:45, 31/05.",,5/30/2024,,,,,,,,,External Dependencies,NA,"The core root cause of the issue was the local National Grid power outage.  Mains power was restored within the hour however not all areas of the DC have back up power. Areas LDC 1 and LDC2 have battery backup power.  The battery backup is available to cover the time period whilst the generators start up.  These start up within 6 seconds and take a short time to get to the correct frequency.  

Investigations found that the automation stopped because the actuator in substation 7 didn’t pull any power.  The purpose of the actuator is to activate and pull power from the generators – no root cause is available as this model of actuator is not in support any longer and has now also been replaced after the fault was found.
",NA,NA,NA,8/21/2024,August,2024,Rehan,Closed,MIM,,2024,5,May,9/16/2025,474,>60
5/24/2024,5/24/2024,90021007,09:07,10:12,01:05,Group Platforms,HR,All BU's,All BU's,Manage & Maintain Payroll,SI,My HR application was inaccessible on 24/05,Oracle,NA,,MyHR,RC identified,68909,"Colleagues were unable to access My HR application between 09:07 and 10:12. This impacted  colleague’s ability to all HR interactions such as view/change bank details, movers and leavers etc. The root cause was attributed to an issue with Oracle which was resolved to restore services. Awaiting root cause and resolution details from Oracle.",All actions are closed,5/24/2024,6/14/2024,,,,Yes,No,No,Yes,External Dependencies,NA,root cause was attributed to an issue with Oracle which was resolved to restore services. Awaiting root cause and resolution details from Oracle.,NA,NA,NA,6/17/2024,June,2024,Saloni,Closed,MIM,,2024,5,May,9/16/2025,480,>60
5/23/2024,5/24/2024,90018353,07:04,3:30,20:34,GTS,Enterprise Integration,"Foods, Customer Channels","Foods Supply Chain, Platform & Store Ops",Manage Allocation of Food Stock,KI,20 foods UPCs missing in CSSM,Customer,Enterprise Services,,CSSM,RC identified,68939,"Store colleagues were encountering invalid UPC error for 20 foods UPC’s whilst performing stock counting impacting store operations, stock reports, and interface of sales information's into CSSM. This also impacted Foods DC (GIST) operations as they were unable to receive stocks, Foods supplier ordering and allocations. The root cause was attributed to an hierarchy change made to the affected UPCs and its downstream flows due to a system design limitation. An action plan was made to correct the stock position in CSSM and these got interfaced into the downstream systems. Mandated counts were sent to stores perform stock counting as expected.",Actions are tracked and closed. Kindly refer to the Action tab for more details,5/23/2024,5/24/2024,,,,"Yes,No",Yes,No,No,Configuration issue,NA,The root cause was attributed to 5 new hierarchy changes made to 20 UPCs after the cut off of 15:00 in SAP. This had resulted in Enterprise services not picking up the hierarchical changes to be sent to downstream systems due to the system design limitation in ES - legacy configuration made for the days when we had mainframe systems.,NA,NA,NA,7/15/2024,July,2024,Pavithra,Closed,MIM,,2024,5,May,9/16/2025,481,>60
5/23/2024,5/23/2024,90019318,06:39,14:52,08:13,GTS,Enterprise Technology Platform,Customer Channels,"Selling Experience, Customer Engagement, Service Experience",Manage Payment & Content Customer.Com,KI,Customer.com Store finder not working because Bing Maps is down,Microsoft,NA,,Store finder app ,RC identified,68801,"Customers were experiencing issues with Store Finder across all web channels from 06:39.  This impacted customer's ability to search for stores in Store Finder, find stock for a product in store on “find in store”, return item to store on website return option and select store for collection on checkout. The root cause was attributed to a global outage with Microsoft's Bing Maps which was resolved by 14:52. Awaiting details from Microsoft on cause and resolution actions.","PIR actions are tracked and closed in the ""problem actions tab""",5/23/2024,5/23/2024,,,,"No,Yes",No,Yes,NA,External Dependencies,NA,The root cause was attributed to a global outage with Microsoft's Bing Maps which was resolved by 14:52.,MS: Ticket -2405230030004187,NA,NA,7/11/2024,July,2024,Pavithra,Closed,.Com SM,,2024,5,May,9/16/2025,481,>60
5/18/2024,5/18/2024,90008993,10:25,17:57,07:32,Customer Channels,Customer Engagement ,Customer Channels,"Selling Experience, Customer Engagement, Service Experience, Platform & Store Ops",Manage Payment & Content Customer.Com,MI,Sign in Pages not Loading on UK Website and Mobile Apps,Secure Auth,NA,"£1,034,000.00",.Com Website,RC identified,68907,"Alerting indicated that customers were receiving a blank page whilst signing into the Customer .com website and mobile application (both android & iOS) from 10:25. Customers were unable to sign in and place orders resulting in an order drop (47k orders vs last week) and poor customer experience. . Also, instore applications like assist app, scan and shop, .com+, online returns, sparks, Digital Receipts were impacted. The root cause was attributed to a contractual bill payment issue at Cloud Entity (3rd party provider for sign-in) with their DNS provider. The payment issue was resolved by Cloud Entity followed by configuration change to restore services by 18:00.",Actions are being tracked in the actions tab,5/18/2024,5/21/2024,,,,Yes,Yes,No,Yes,External Dependencies,NA,"The Cloudentity SaaS environment experienced a full outage due to the expiration of the cloudentity.io domain. The registration of the cloudentity.io domain, provided by their vendor Network Solutions, expired resulting in a complete service outage. The expiration was not detected in time due to outdated payment and contact information",NA,NA,,10/17/2024,October,2024,Kavitha,Closed,.Com SM,,2024,5,May,9/16/2025,486,>60
5/17/2024,5/18/2024,90004445,09:45,8:00,22:15,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,SI,stores reporting latency in honeywell notifications from Click and Collect application,Google,NA,,Click and collect,RC identified,68757,"Some store colleagues were observing latency in receiving honeywell notifications from Click & Collect application from 16/05, 09:00. This delayed the customer order collection at stores resulting in poor customer experience, therefore, store colleagues were advised to man the C&C kiosks. The root cause was attributed to an issue with Google firebase cloud messaging services. Services were restored by Google at 06:00, 18/05 by providing a temporary exemption on push notification approach until 30th June. Detailed root cause investigations underway.",All actions updated,5/18/2024,,,,,,,,,External Dependencies,NA, The root cause was attributed to an issue with Google firebase cloud messaging services facilitating the push notifications from C&C application to honeywell devices.,NA,NA,NA,7/18/2024,August,2024,Saloni,Closed,MIM,,2024,5,May,9/16/2025,487,>60
5/10/2024,5/10/2024,89992934,06:00,9:20,03:20,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Peterborough (Outlet) DC colleagues were unable to access JDA WMS application,Customer,WMS,,WMS JDA dispatcher,RC unknown,68721,"Peterborough (Outlet) DC colleagues reported issues whilst accessing JDA application on honeywells and workstations between 06:00 and 09:20. This impacted the picking and packing operations at site, with no residual impact. Services were restored by performing a full restart of application and database. Root cause investigations underway. ",All agreed has been closed,5/10/2024,"No,Yes",,,,Yes,No ,No,Yes,RC unknown,NA,The root cause remains inconclusive,NA,RC unknown,NA,6/27/2024,June,2024,Kavitha,Closed,MIM,,2024,5,May,9/16/2025,494,>60
5/3/2024,5/3/2024,89980920,07:30,8:35,01:05,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,KI,Issues with store backstage replenishment,Customer,Colleague Devices,,Backstage replenishment,RC identified,68631,"Store colleagues reported errors in ""Backstage C&H replenishment"" application on the honeywell devices between 07:30 and 08:35 whilst submitting the backstage scans. This affected 60 stores in the impacted hour and the counts were manually submitted in the backend to remediate the impact. The root cause was attributed to a change implemented on 02/05 to enhance the replenishment app which was reverted to restore services. Detailed root cause investigations continue.",Actions are being tracked in the actions tab,5/3/2024,5/3/2024,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,The root cause was attributed to a change implemented on 02/05 to enhance the replenishment app,#200715,inadequate testing,NA,6/14/2024,June,2024,Gokul,Closed,.Com SM,,2024,5,May,9/16/2025,501,>60
4/27/2024,4/27/2024,89970025,00:54,8:21,07:27,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Delay in the Foods final order flow,OpenText,NA,,FFO,RC identified,68491,"Through dashboard monitoring, it was identified that the Foods final orders had not interfaced into OpenText. There was a delay in sending the supplier orders and ASNs (Advance Shipment Note) to GIST impacting trailer shipment. The root cause was attributed to a message pile up issue at OpenText which were cleared to restore services by 08:49. Detailed root cause underway.",All actions are closed,4/27/2024,4/30/2024,,,,,,,,External Dependencies,NA,The root cause has been attributed to a message pile up at OpenText due to an unexpectedly large file originating from Customer shipping partner Maersk which blocked the flow of the processing.,NA,NA,NA,7/31/2024,July,2024,Gokul,Closed,MIM,,2024,4,April,9/16/2025,507,>60
4/25/2024,4/26/2024,89966778,11:00,16:30,05:30,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Manage Allocation of Food Stock,KI,No BP sales received into CSSM,BP,CSSM Foods,,CSSM,RC identified,68782,"Alerting indicated that CSSM had not received any sales from BP. This impacted circa 190+ stores operations due to inaccurate stock and therefore, store counting was blocked to avoid stock corruption. The root cause was attributed to one of the BP services facilitating the message flow between BP and Customer in stopped state, which was restarted to restore services. 
On 26/04, latency was observed in processing the sales message flow from BP to Customer due to a Mulesoft issue within BP infrastructure. Monitoring is underway to monitor the flow of messages before unblocking the store counting. ",PIR document has been provided by BP and it has been uploaded in the Sharepoint,4/25/2024,4/25/2024,,,,"Yes,No",Yes,yes,NA,External Dependencies,NA,"25/04 -  The root cause was attributed to one of the BP services facilitating the message flow between BP and Customer in stopped state, which was restarted to restore services.
26/04 -  latency was observed in processing the sales message flow from BP to Customer due to a Mulesoft issue within BP infrastructure.",NA,NA,NA,6/17/2024,June,2024,Pavithra,Closed,MIM,,2024,4,April,9/16/2025,509,>60
4/23/2024,4/23/2024,90048310/89963305,11:40,15:15,03:35,GTS,Enterprise Technology Platform," Customer Channels, Digital and data","Platform and Store Ops, Digital and data",Others,KI,DataStage services were unavailable after the RHEL upgrade activity,TCS,Unix,,"DataStage, EDW",RC identified,68801,"Following the DataStage RHEL upgrade, the DataStage services were unavailable between 11:40 to 15:15 and 22:49 to 23:30. This delayed the customer order fulfilment for Flamingo and MM flowers, availability of Retail Dashboard In Day UK franchise and stock integrity reports. The root cause was attributed to an incorrect configuration on the primary DS node which was corrected  to restore services. The impacted batch jobs were recovered and all the overnight batches got completed successfully. Detailed root cause analysis underway.",All agreed has been closed,4/23/2024,4/23/2024,,,,No,Yes,Yes,Yes,Customer Tech change,Change ,The root cause was attributed to an incorrect configuration on the primary DS node which was corrected  to restore services.,#198446,Design issue,NA,6/19/2024,June,2024,Kavitha,Closed,MIM,,2024,4,April,9/16/2025,511,>60
4/22/2024,4/22/2024,89961127,10:50,11:59,01:09,Customer Channels,Selling Experience,Customer Channels,"Customer Engagement, Selling Experience",Manage Payment & Content Customer.Com,KI,Order Drop on Customer.com Website,Customer,WCS,,Website- basket,RC identified,68607,"Alerting indicated a spike in 'sorry page errors' in baskets between 10:50 and 11:59. There was a drop in order volume of around 200 orders. The root cause was attributed to a combination of an incorrect network configuration and a certificate issue on North Europe region which was corrected and renewed. As a workaround, the traffic was routed to West Europe region until service restoration. ",All actions are tracked and closed.,4/22/2024,4/22/2024,,,,"Yes,NO",Yes,No,NA,Configuration issue,NA,The root cause was attributed to a combination of an incorrect network configuration and a certificate issue on North Europe region which was corrected and renewed,NA,NA,NA,6/9/2024,June,2024,Pavithra,Closed,.Com SM,,2024,4,April,9/16/2025,512,>60
4/22/2024,7/16/2024,89962225,17:00,18:00,85 days,C&H and Intl ,C&H & Intl Supply Chain," Customer Channels, C&H and Intl","Service Experience, Platform and store ops, C&H and Intl Supply chain",Manage Pick & Allocation of Stock and Orders (Castle Donington),MI,Multiple issues impacting Castle Donington DC operations after the dispatcher upgrade,Customer,WMS,,"WMS, WCS(SSI),MULE,OMS",RC identified,69892,"Following the Castle Donington WMS dispatcher upgrade activity (22/04), connectivity  issues were observed on the MULE (middleware) layer between WCS and WMS impacting site operations. The WMS database was restarted, a problematic database package was recompiled followed by a Mule service restart to improve the message flow between WCS and WMS. Tech team continues to work on further defects  and to stabilize the site operations.",Actions have tracked has part of project team backlogs ,4/22/2024,,,,,"Yes,no",Yes,No,Yes,Customer Tech change,Change ,Castle Donington WMS Dispatcher upgrade (22/04) had resulted with  multiple issues  impacting the operations,#199770,inadequate testing,NA,10/31/2024,October,2024,Saloni,Closed,MIM,,2024,4,April,9/16/2025,512,>60
4/18/2024,4/18/2024,89956269,20:00,20:05,00:05,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Customer Engagement",Manage Payment & Content Customer.Com,KI,Order Drop on Customer.com Website,Customer,WCS,,WCS ,RC unknown,68585,"Alerting indicated issues whilst customers trying to place orders on the Customer.com website between 20:02 and 20:04. This resulted in an order drop of 450 orders and inconvenience to customer order journey across website and mobile app. The root cause was attributed to a database contention, however, services were restored without any manual intervention. Detailed root cause investigations underway with vendor Oracle.",All actions are closed.,4/18/2024,NA,,,,Yes,Yes,No,NA,RC unknown,No,"High active session counts were observed on the context management table, however we cannot determine the cause for increase in the active sessions. Hence RC is inconclusive. ",N/A,N/A,N/A,5/1/2024,May,2024,Gokul,Closed,.Com SM,,2024,4,April,9/16/2025,516,>60
4/18/2024,4/18/2024,89955164,11:05,12:03,00:58,Customer Channels,Service Experience,Customer Channels,"Platform & Store Ops, Service Experience",Others,KI,Store colleagues unable to book in parcels on the collection app,Customer,OMS,,Collection app,RC identified,68452,"Multiple stores reported issues, whilst booking in parcels in the Customer.com store collection app between 11:05 and 12:03. This delayed the parcel book in, however there was no impact to customer collections scheduled after 12:00.  The root cause was attributed to a delay in processing the messages from OMS (order management system) to collection app. The order statuses were updated to restore the services. Detailed root cause investigations underway. ",All agreed has been closed,4/18/2024,4/18/2024,,,,No ,No,No,NA,Customer Tech change,Change ,"

An issue was identified where a transaction responsible for queuing shipment messages before triggering to Collection database was not invoked due to the ControlM job being on hold during a RHEL patching outage. This resulted in orders being moved to ""ready to collect"" status but not triggering to Collection DB from Sterling.
",#198446,Knowledge gap,NA,6/27/2024,June,2024,Kavitha,Closed,.Com SM,,2024,4,April,9/16/2025,516,>60
4/16/2024,4/16/2024,89949184,09:00,10:05,01:05,GTS,Enterprise Technology Platform,All BU's,All BU's,Manage Allocation of Food Stock,MI,Central network connectivity issue impacting various applications,Palo Alto,NA,,Various applications,RC identified,68457,"Alerting indicated multiple applications hosted on the Azure Cloud were not accessible between 09:00 and 10:05 impacting store operations, C&H & International order planning, Foods Supply Chain planning, Sparks Hub along with Beam reporting. The root cause was attributed to a bug in the current Palo Alto firewall version impacting the outbound Azure traffic to the internet, after a change in the Microsoft network.  Services were restored by rebooting the firewall.
On 17/04 and 18/04, issues impacting various applications were reported again between 09:16 - 09:55 and 00:51 - 01:52 respectively. This impacted 31 stores whilst booking in stocks in CSSM resulting in potential stock corruption. As a permanent fix, the firewall firmware version upgrade has been planned.",All actions are closed.,4/16/2024,5/22/2024,,,,"Yes, No",Yes,Yes,No,External Dependencies,No,"The root cause was attributed to a bug in the current Palo Alto firewall version impacting the outbound Azure traffic to the internet, after a change in the Microsoft network.",NA,NA,NA,5/22/2024,May,2024,Gokul,Closed,MIM,,2024,4,April,9/16/2025,518,>60
4/12/2024,4/12/2024,89944603,10:00,13:20,03:20,Customer Channels,Service Experience,Customer Channels,"Platform & Store Ops, Service Experience",Others,SI,"Twilio telephony issue, Inbound calls experiencing one-way audio",Customer,NLP ,,"Stores, Contact centers",RC identified,68562,"Contact Centre reported issues with external calls to and from stores (both ROI and UK) from 10:00. This impacted all external calls (customer, colleagues, suppliers, etc) to ROI and UK stores(intermittently). Store operations (such as City Maintenance team, IT SOC, etc) were also impacted as they could not call the stores. The root cause was attributed to a configuration change to enable new Twilio whitelisted IP addresses for routing calls which was reverted to restore services by 13:20. Detailed root cause investigations underway. ",All agreed has been closed,4/12/2024,4/12/2024,,,,No ,No,No ,Yes,Unauthorised Change ,NA,The root cause was attributed to a configuration change to enable new Twilio whitelisted IP addresses for routing calls.,Unauthorised change,Communication gap,Riceh -577 & 578,6/19/2024,June,2024,Kavitha,Closed,MIM,,2024,4,April,9/16/2025,522,>60
4/10/2024,4/11/2024,89942250,22:30,7:59,09:29,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Allocation of C&H Stock,SI, Delay in D&F batch impacting ordering and allocations for C&H DC's,Blue Yonder,NA,"£5,000.00",D&F,RC unknown,68573,"At 22:30 on 10/04, alerting indicated a delay in the overnight D&F batch due to a job overrun at Blue Yonder impacting the order generation across the C&H DCs. This resulted in a capability loss of 12.5K retail singles at Donington, 17.K singles at Welham Green DCs and cancellation of multiple trailers. The availability of DCO (DC orchestration) files to Donington and D&F data to BEAM and EIH reports were also delayed.  The root cause was attributed to a spike in memory utilization on the Blue Yonder database server. To mitigate the impact, the application and DB servers were restarted to process the orders into the DCs by 07:59. As a permanent fix, the database memory was uplifted on 11/04.",Actions are being tracked in the actions tab,4/11/2024,NA,Yes,Yes,NA,"Yes, No",Yes,Yes,Yes,External Dependencies,No, The root cause was attributed to a spike in memory utilization on the Blue Yonder database server.,NA,NA,NA,4/30/2024,April,2024,Pavithra,Closed,MIM,,2024,4,April,9/16/2025,524,>60
4/7/2024,4/7/2024,89934681,08:30,14:40,06:10,GTS,Enterprise Technology Platform,Customer Channels,"Customer Engagement, Service Experience, Selling Experience",Others,MI,Issues in accessing MyAccount features,Microsoft,NA,,My Account,RC identified,68545,"Customers and Contact Center colleagues were experiencing errors while trying to access My Account features between 08:30 and 14:40. Approx. 8k customers were intermittently unable to view, amend or return their orders resulting in poor customer experience. The initial root cause has been attributed to the removal of ciphers (used for encryption & decryption) to mitigate another issue on the Microsoft infrastructure on 06/04. The load balancer settings were changed by Microsoft to ignore the ciphers and re-establish connectivity to the My Account application.  Detailed root cause investigations underway with Microsoft.",All actions are closed,4/7/2024,5/13/2024,6/15/2024,6/15/2024,6/17/2024,"No, Yes",No,No,Yes,External Dependencies,NA,"Microsoft is implementing backend network changes to accept traffic only using the approved list of ciphers. This change is part of Microsoft's security and compliance measures to ensure a secure and standardized network environment. The FESK AKS cluster has custom configurations for allowed ciphers, which doesn’t include all the approved ciphers as per Microsoft's standards which caused the isue. ",NA,NA,,6/17/2024,June,2024,Saloni,Closed,.Com SM,,2024,4,April,9/16/2025,527,>60
4/7/2024,4/7/2024,89934699,06:00,12:20,06:20,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Issues with C&H Bradford site operations.,KNAPP,NA,,Picking and packing operations,RC identified,68531,"Bradford C&H DC colleagues were unable to perform picking operations between 6:00 and 12:30.  To mitigate the immediate impact, order proposition was moved to 24 hours for both Next Day Delivery (NDD) and Click & Collect (C&C) orders.  The root cause has been attributed to a security patching activity performed by Knapp on the WMS and WCS servers.  After the patching activity, the SRC (Storage Retrieval Controller) component was inadvertently powered on with an incorrect configuration resulting in an outage. The configuration was corrected to restore the services",All of the actions are tracked and closed,4/7/2024,,,,,"No,Yes",No,No,Yes,External Dependencies,No,The root cause has been attributed to a security patching activity performed by Knapp on the WMS and WCS servers.,NA,NA,NA,5/29/2024,May,2024,Pavithra,Closed,MIM,,2024,4,April,9/16/2025,527,>60
4/6/2024,4/6/2024,89934337,04:30,11:30,07:00,GTS,Digital Workplace Services,"C&H and Intl, Foods","C&H and Intl Supply Chain, Foods Supply Chain",Others,SI,SALINA WiFi network connectivity issues across various DC's,Customer,Active Directory,,Salina Wi-Fi,RC identified,68533,"Colleagues across multiple distribution centres were unable to access internet using Salina wireless network between 04:30 and 11:30.  No impact to site operations, however, as a workaround, colleagues were able to connect using Customer Guest or their local wireless network.  DC operations at Milton Keynes were impacted as the HHTs lost network connectivity.  The root cause was attributed to the expiry of the root CRL (Certificate Revocation List) which was renewed to restore the services.",Actions are being tracked in the actions tab,4/6/2024,4/6/2024,,,,Yes,Yes,No,Yes,Certificate issue,NA, The root cause was attributed to the expiry of the root CRL (Certificate Revocation List) which was renewed to restore the services.,NA,NA,NA,6/18/2024,June,2024,Kavitha,Closed,MIM,,2024,4,April,9/16/2025,528,>60
4/3/2024,4/3/2024,89929555,02:30,11:46,09:16,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,CD experiencing Metapack call failures at SYW for AnPost orders,TCS,Network,,Metapack,RC unknown,68414,"Castle Donnington DC colleagues reported issues whilst processing  AnPost orders via Metapack between 02:30 and 11:46 impacting ~12 AnPost orders. As work around, these orders were processed via Sorted. The root cause was attributed to an issue with Metapack VIP which was disabled and re-enabled to restore services. Detailed root cause investigations underway",All actions agred has been closed ,4/3/2024,NA,,,,"No,yes",No,Yes,NA,RC unknown,NA,"The cause is suspected to be an issue with Metapack VIP which was disabled and re-enabled to restore services,however RC is inclonculsive and risk is being raised against this. ",NA,NA,2132,7/15/2024,August,2024,Rehan,Closed,MIM,,2024,4,April,9/16/2025,531,>60
4/3/2024,4/3/2024,89928907,01:30,6:57,05:27,Foods,Food Supply Chain,Foods,Foods Supply Chain,Manage Allocation of Food Stock,SI,Issue with the Relex batch processing,Relex,NA,,Supplier orders,RC identified,68413,"Foods final orders for Relex lines were generated incorrectly impacting the RDC allocations and finalised supplier orders. There was a delay in the availability of 28 day order plan, Relex UI, OTS reports and commitment sheets. The root cause has been attributed to a connect job failure after an change at Relex end. The problematic job was fixed and the batch was resumed successfully. Awaiting root cause details from Relex.  - Supply chain Data delayed from FOA.",All actions are closed,4/3/2024,4/8/2024,,,,"No, N/A",No,Yes,Yes,External Dependencies,N/A,"Root cause is attributed to a hardware replacement done on 2nd within Relex SaaS, during the switchover to the new server, critical transactions connect job was missed inadvertently, impacting the Relex batch and causing the issue. ",NA,Inefficient testing,N/A,7/2/2024,June,2024,Gokul,Closed,MIM,,2024,4,April,9/16/2025,531,>60
4/1/2024,4/1/2024,89925701,04:00,16:30,12:30,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),SI,Internet connectivity issues across multiple DCs after PA license expiry,TCS,Network,,"DC Network, YMS",RC identified,68406,"Multiple issues were reported across Castle Donington, Welham Green and Foods Bradford DCs from 04:00. This impacted the till transactions at Donington cafeteria and colleague's ability to access Simple Compliance and T&A application. Welham Green and Foods Bradford DC colleagues were unable to access YMS (Yard management system) and Sharepoint sites, Power BI reports respectively. However, there was no impact to the site operations. Services were restored by applying a workaround. The root cause was attributed to the license expiry on the PA firewalls on 31/03 which has been extended until 15/04 by 16:30.",All of the actions are tracked and closed,4/1/2024,4/1/2024,,,,"Yes,No",No,No,Yes,Certificate issue,No,The root cause was attributed to the license expiry on the PA firewalls on 31/03.,NA,License renewal ,NA,5/29/2024,May,2024,Pavithra,Closed,MIM,,2024,4,April,9/16/2025,533,>60
3/28/2024,3/29/2024,89920510,10:00,6:00,20:00,Customer Channels,Service Experience,Customer Channels,"Service Experience, HR",Manage Payment & Point of Sales in store,SI,Staff discount cards not scanning on the tills,Customer,My Discount/ CDCS,,Staff discount cards,RC identified,68407,Stores colleagues reported that old blue staff discount card and Pension leavers discount card were not scanning at the tills. This impacted the old pensioners and staff's ability to claim their discounts. The root cause was attributed to an issue with one of the parameter introduced within SDS ( Staff Discount Services) on Jan'24 followed by a URL change in CDCS (Colleague discount cache service) on 27/03. The parameter was corrected to restore services. ,All actions are closed,3/28/2024,4/3/2024,,,,3/28/2024,NA,NA,NA,Customer Tech change,Unauthorised change,The root cause was attributed to a parameter (column name) introduced within My Discount application on Jan'24 followed by a URL change in CDCS (Colleague discount cache service) on 27/03 resulting in an issue with the file format sent to the tills.,NA,Inefficient testing,,9/16/2024,September,2024,Saloni,Closed,MIM,,2024,3,March,9/16/2025,537,>60
3/27/2024,3/27/2024,89918599,12:30,15:51,03:21,C&H and Intl ,C&H commercial Trading, C&H and Intl,C&H commercial Trading,Others,KI,C&H SSI application inaccessible,TCS,Network,,C&H SSI,RC identified,68302,"Colleagues reported that the C&H SSI and master data application was inaccessible between 12:30 and 15:51. This impacted colleague’s ability to perform merchandise planning, sales and stock intake planning and PO amendments. The root cause has been attributed to an inadvertent deletion of a public IP and a new public IP was configured to restore services.",All actions are closed,3/27/2024,3/27/2024,,,,"No, Yes",No,No ,Yes,Unauthorised Change ,NA,The root cause has been attributed to an inadvertent deletion of a public IP due to incorrect IP details in the prerequisite request raised for setting the new SSI application functionality.,NA,NA,CNWK-2132,5/8/2024,May,2024,Kavitha,Closed,MIM,,2024,3,March,9/16/2025,538,>60
3/26/2024,3/26/2024,89931618,08:00,11:04,03:04,GTS,Enterprise Technology Platform, C&H and Intl,International Commercial Trading,Manage & Maintain Financial Integrity - Raise PO's/Pay Suppliers etc,SI,Czech and Greece colleagues unable to access eSAP,TCS,Network,,eSAP,RC identified,68416 ,"Czech and Greece colleagues reported issues whilst accessing eSAP from 07:50 and 11:05 impacting finance, reporting and warehouse operations. The root cause is attributed to a DNS resolution issue through the Fortigate firewall after the network firewall migration of the DMZ router from Palo Alto to FortiGate. Services were restored by configuring a static IP address to allow the eSAP DNS resolution. Further issues have been identified with the external vendors - Cogent & SAP DNS resolution through the FortiGate firewall impacting the finance year end activities and fault find in our SAP systems respectively. Further investigations are underway.",All actions agred has been closed ,3/26/2024,3/26/2024,,,,"No,yes",No,Yes,Yes,Customer Tech change,, The root cause is attributed to a DNS resolution issue through the Fortigate firewall after the network firewall migration of the DMZ router from Palo Alto to FortiGate,,Inefficient testing,,5/29/2024,August,2024,Rehan,Closed,MIM,,2024,3,March,9/16/2025,539,>60
3/24/2024,3/24/2024,89913334,09:46,19:00,09:14,Digital & Data,Data,"C&H and Intl, Customer Channels","C&H and Intl Supply Chain, Customer Engagement",Others,KI,Bigdata platform issues impacting the EIH data availability,Customer,EIH,,EIH Platform,RC unknown,68089,"Latency was observed whilst running any queries on the EIH platform from 09:30. This impacted the data availability of C&H end of week reporting, and POS and ECOM sales transactional data for 23/03 was not available in EIH impacting any customer domain self-service reports fetching data from EIH. The initial root cause is attributed to an issue with the EIH Impala service responsible for query execution on three application servers. This service was disabled on the problematic servers to restore services. As a workaround, the weekly BEAM process was skipped and the EIH data is currently replicated to BEAM to mitigate any impact to the self-service report",EIH is getting decommissioned on 14th August. No further efforts being spent to identify root cause and configure alerts,3/24/2024,,,,,,,,,RC unknown,NA,"The root cause was attributed to the latency on the Impala service running on three application servers. However, investigations are underway to identify the reason behind the latency on those three servers.

Vendor Cloud era suspects the latency of the impala service could be due to high CPU utilisation on the three servers, however, from the OS logs it is evident that the CPU utilisation across the three servers is slightly higher but within the threshold. Further investigations are underway. A case has been raised with HP vendor to investigate the hardware component of the servers. As these servers are out of support, work is in progress to extend the vendor support with HP.",NA,NA,NA,7/31/2024,July,2024,Saloni,Closed,MIM,,2024,3,March,9/16/2025,541,>60
3/21/2024,3/21/2024,89908543,01:49,9:50,08:01,GTS,Enterprise Technology Platform,Customer Channels,"Platform & Store Ops, Service Experience",Manage Payment & Point of Sales in store,MI,70 stores hosted on BT (British Telecom) network were hard down,BT,NA,,Store Network,RC identified,68083,"Since 01:49 AM, some stores hosted on BT (British Telecom) infrastructure were hard down (both Primary and resilient links are down) impacting the contactless payments and store operations. The initial root cause has been attributed by a change implemented on BT core network. A fix was deployed to restore the services by 09:50. Awaiting detailed root cause from BT. ",All th agreed actions have been closed,3/21/2024,4/16/2024,,,,Yes,Yes,No ,Yes,External Dependencies,,A human error while implementing a change  on BT core network had caused the issue. A fix was deployed to restore the services by 09:50.,,Human error,,9/17/2024,September,2024,Rehan,Closed,MIM,,2024,3,March,9/16/2025,544,>60
3/20/2024,3/20/2024,89933591,17:48,19:55,02:07,GTS,Enterprise Technology Platform,Customer Channels,"Customer Engagement, Service Experience",Manage Payment & Point of Sales in store,KI,Colleagues reported that tills were having connectivity issues with Eagle Eye services for mobile coupons,TCS,Network,,Eagle Eye service,RC identified,68558,"Colleagues reported that tills were having connectivity issue with Eagle Eye services for mobile coupons from 12/03, this impacted the vouchers scanning to redeem items, such as free hot drinks resulting in 32K failed scan attempts including the retries. The issue was caused after the network firewall migration from Palo Alto to FortiGate. A fix was deployed to restore the services.",All actions are closed,3/20/2024,3/20/2024,,,,No,No,No,Yes,Customer Tech change,CR,The issue was caused after the network firewall migration from Palo Alto to FortiGate. ,#197407,inadequate testing,NA,4/3/2024,April,2024,Gokul ,Closed,MIM,,2024,3,March,9/16/2025,545,>60
3/18/2024,3/18/2024,89902881,10:55,12:17,01:22,Digital & Data,Data,"C&H and Intl, Foods, Group Platforms","C&H Commercial Trading, Foods, Finance",Others,KI,Colleagues were unable to run any reports through Business Objects,Customer,Data Platform,,BO Reports,RC identified,68084,Colleagues were unable to run any reports through Business Objects between 10:55 and 12:17. This impacted the availability of the critical Finance weekly process and foods supply chain & C&H reporting. The root cause was attributed to an incorrect configuration of the application network security group resulting in connectivity issue between application server and azure storage account. The configuration was corrected to restore services.,All actions are closed in the tracker,3/18/2024,3/22/2024,4/14/2024,5/14/2024,5/14/2024,No,No,Yes,Yes,Configuration issue,NA,The root cause was attributed to connectivity issues between the application servers (VMs – virtual machines) and its storage account as it was using a dynamic/public IP instead of a private endpoint,NA,NA,NA,5/29/2024,May,2024,Saloni,CLosed,MIM,,2024,3,March,9/16/2025,547,>60
3/16/2024,3/16/2024,89900521,05:45,9:30,27:15,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders,SI,Fire incident at Bradford C&H DC impacting site operations,KNAPP,NA,, DC operation,RC identified,67962,"A fire incident was reported at Bradford C&H DC around 05:45 which was contained by the sprinkler activation at the site, preventing extensive damage. There is no impact to ecomm orders, also the order proposition for click and collect orders have been moved to 24 hours. Around 20k retails orders could be impacted resulting in a delay to store deliveries. The initial root cause is attributed to the power supply from City FM to an unused Knapp Zebra printer at the site. The site has been cleared and teams are working on the prechecks before we resume the operations at the impacted zone. Detailed rootcause analysis underway.",further investigations will be carried out by GXO on determining the actual root cause. ,3/16/2024,4/20/2024,,,,Yes,yes,NA,NA,External Dependencies,NA,The initial root cause is attributed to the power supply from City FM to an unused Knapp Zebra printer at the site. It has been confirmed by the business team and Zebra team that the unit identified was under product recall and this now explains the actual root cause. ,NA,NA,NA,5/17/2024,August,2024,Rehan,Closed,MIM,,2024,3,March,9/16/2025,549,>60
3/13/2024,3/13/2024,89895108,05:00,9:00,04:00,Customer Channels,Service Experience,Customer Channels,"Selling Experience, Platform and store ops",Others,KI, Store colleagues unable to access Travel Money application (Customer Bank),Customer Bank ,NA,,Travel money app,RC identified,67955,"Store colleagues reported issues whilst accessing travel money application (Customer bank) on their Honeywell devices between 07:00 and 09:00, impacting colleagues ability to assist customers on the exchange of their currency (GBP) via the Travel Money app. The root cause has been attributed to a change at Essiell (third party supplier to Customer bank) and it was reverted to restore the services. We await detailed root cause",All actions have been completed,3/13/2024,3/13/2024,,,,N/A,NA,Yes,Yes,Change Failure,NA,The root cause has been attributed to a change at Essiell (third party supplier to Customer bank) and it was reverted to restore the services,NA,NA,NA,6/14/2024,June,2024,Gokul,Closed,MIM,,2024,3,March,9/16/2025,552,>60
3/12/2024,3/12/2024,89893425,05:00,10:30,05:30,GTS,Enterprise Technology Platform,Customer Channels,"Service Experience, Platform and store ops",Manage Payment & Content Customer.Com,SI,Colleagues were unable to view product images on the website,TCS,Network,N/A as prior to Nov 2024,"website, ISF, Assist",RC identified,68064,"From around 08:00, support centre and store colleagues reported that they were unable to view product images on the Customer website.  Customers using normal internet were unaffected and there was no drop in orders.  Stores reported that they were unable to book orders on the Assist app and they could not log in to .com+ preventing them from booking orders on the workstations. It was found that the FortiGate firewall was blocking some key domains by default after the firewall migration change from Palo Alto to Fortigate. The domains were configured to be ""allowed"" and service was restored from 10:30.",Actions are being tracked in the actions tab,3/12/2024,3/12/2024,,,,"Yes,No",Yes,No,Yes,Customer Tech change,Yes ," Following the Firewall migration from Palo Alto to Fortigate at SP on 11/03, It was found that the FortiGate firewall were blocking some key domains by default after the firewall migration change from Palo Alto to Fortigate. The domains were configured to be ""allowed"" and service was restored from 10:30.",197366,implementation error,,12/12/2024,December,2024,Rehan,Closed,MIM,,2024,3,March,9/16/2025,553,>60
3/7/2024,3/7/2024,89884794/89885099,01:37,9:00,07:23,GTS,Enterprise Technology Platform,Foods,Foods Supply Chain,Manage Allocation of Food Stock,KI,Delay in Foods Bradford NDC allocation,Microsoft,NA,,ASO,RC identified,68037,There was a delay in sending around 21% orders to foods Bradford DC within the target completion time of 6 AM. It impacted the picking and dispatch of the impacted orders. The root cause was attributed to an issue with the AKS cluster in west Europe causing the orders to get stuck in ASO. The impacted orders were manually triggered and the orders interfaced by 9 AM. RCA underway with MS.,Actions are being tracked in the actions tab,3/7/2024,3/7/2024,,,,Yes,No,Yes,Yes,External Dependencies,No,The root cause was attributed to an issue with the AKS cluster in west Europe causing the orders to get stuck in ASO,No,No,No,6/19/2024,June,2024,Gokul,Closed,MIM,,2024,3,March,9/16/2025,558,>60
3/6/2024,3/6/2024,89883764,11:00,13:43,02:43,Customer Channels,Service Experience,Customer Channels,Customer Engagement,Manage Payment & Content Customer.Com,KI,Sparks pay payment failures,Worldline,NA,,SparksPay,RC identified,68035,Online and In stores sparks pay transactions were declined between 11:00 and 13:43 impacting ~57 orders. The root cause was attributed to a change at Worldline which was reverted to restore the services. Awaiting detailed RCA from the vendor Worldline.,Awaiting RCA from Worldline,3/6/2024,3/6/2023,,,,Yes,Yes,No ,NA,External Dependencies,NA,The root cause was attributed to a change at Worldline which was reverted to restore the services,NA,NA, PFOUND-599,4/16/2024,April,2024,Kavitha,Closed,.Com SM,,2024,3,March,9/16/2025,559,>60
3/5/2024,3/5/2024,89880759,01:55,5:26,03:31,GTS,Enterprise Technology Platform,All BU's,All BU's,Manage Pick of C&H stock,MI,"Internet connectivity issues across stores, DCs and support centres after a network change",TCS,Network,,"DC network, stores network, Ignio, JIRA, ROD,MIRO, MYIT portal, reporting",RC identified,68038,"Around 01:55, Castle Donington DC colleagues reported issues with their internet connectivity via LAN and WIFI upon further investigations, the cause of the issue was identified to be due to an overnight change on the Stockley firewall which had impacted all the DC (Welham,Thorncliffe, Ollerton,Brands,stoke etc) on their operations. Parallely Stores were also reporting issues with their HHT’s and workstations not connecting to internet. 

Also, Multiple applications like Ignio, JIRA, ROD,MIRO, MYIT portal were impacted and multiple Beam reports, Power BI reports, 28- Day order plan were impacted.

 To fix the issue, the change was reverted and the services were restored by 05:26. Detailed root cause analysis underway",All actions are closed,3/5/2024,3/6/2024,,,,"yes,No",No,No,No,Customer Tech change,CR,"On 05/03, a change implemented to migrate Stockley Vsys (virtual systems) from Palo Alto to FortiGate firewall had resulted in issues with external DNS resolution, causing the incident

Note: After the Swindon Migrations aswell, as the external traffic has been using SP DNS VIP for external forwarding the issue was not identified in the post validations and it was failed to be identified until the incident occurred on 05/03.",#196693,inappropriate testing and validations,,8/27/2024,August,2024,Gokul,Closed,MIM,,2024,3,March,9/16/2025,560,>60
3/2/2024,3/4/2024,89933408,08:50,4:30,43:30,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops,Others,SI,Significant number of  duplicate ECS tickets to be printed at stores ,Customer,NA,,ECS ticket printing,RC identified,68420,"Foods operations colleagues reported that various stores had received a significant number of duplicate tickets to be printed for certain products, even though there were no actual price changes. This had caused an inconvenience to the store colleagues, however there was no impact to trading. The root cause was attributed to a change made on the ticket design and to fix the issue, a batch was created to revert the change and another batch was created to reflect the actual price changes for today (04/03). ",Actions were  tracked and closed in the actions tab,3/4/2024,3/4/2024,,,,"No,yes",No,No,NA,Customer Business change,No,"The root cause was attributed to a combination of product attribute change in SAP and design change in ECS which resulted in duplicate tickets batched to stores.
",NA,inadequate testing,NA,6/17/2024,June,2024,Pavithra,Closed,MIM,,2024,3,March,9/16/2025,563,>60
3/1/2024,3/1/2024,89874461,02:13,5:21,03:08,Infosec & Tech Risk,Infosec & Tech Risk,All BU's,All BU's,Manage Payment & Content Customer.Com,MI,"Customer Website not available and no orders placed UK, IE & International",IP Twins,NA,,Customer .com website,RC identified,68011,"Customer UK website, International websites and Customer Bank sites were inaccessible between 02:13 - 05:21 resulting in poor customer experience. The root cause was attributed to a bug on the IP Twins portal which removed multiple DNS entries inadvertently during a planned change.  Services were restored after reinstating the missing DNS entries on the IP Twins portal. Detailed root cause investigations underway with vendor IP Twins",All the agreed actions are closed ,3/1/2024,3/5/2024,,,,No,YEs,No,NA,External Dependencies,NA,The root cause was attributed to a bug on the IP Twins portal which removed multiple DNS entries inadvertently during a planned change,NA,NA,NA,5/15/2024,August,2024,Rehan,Closed,.Com SM,,2024,3,March,9/16/2025,564,>60
2/28/2024,2/28/2024,89870729,11:30,12:00,00:30,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,KI,Digital cafe stores customers unable to place orders using the kiosks,Counter Solutions,NA,,Digital café,RC identified,68009,Digital Café stores reported an issue with the lunch menu cutover and customers unable to place orders on the kiosks between 11:30 and 12:00. The issue was intermittent across all the 14 stores significantly impacting the customer order journey. The root cause was attributed to a manual error whilst deploying a change by Counter Solutions. A permanent fix was implemented.,All actions are closed,2/28/2024,2/28/2024,,,,No,No,No,Yes,External Dependencies,NA,"The root cause was attributed to a manual error whilst deploying a change by Counter Solutions. A new feature was deployed on the breakfast to lunch cutover menu to reflect on the kiosks at 11:30, one field was updated correctly at 11:30, however the other field was updated incorrectly at 12:00 which caused the issue.",NA,NA,NA,5/13/2024,May,2024,Saloni,Closed,MIM,,2024,2,February,9/16/2025,566,>60
2/26/2024,2/26/2024,89867645,16:40,21:48,05:08,Infosec & Tech Risk,Infosec & Tech Risk, C&H and Intl,C&H and Intl Supply Chain,Others,KI,Issues with C&H SFTP file transfers,Customer,Infosec,,SFTP,RC identified,68013,"Alerting indicated that the virtual machine responsible for C&H supply chain SFTP file transfers were unavailable from 16:40. This potentially impacted D&F ordering and transport planning activities, however no impact was incurred. The root cause was attributed to the isolation of the SFTP file transfer VM's after detecting a hacking tool in the primary SFTP servers to prevent any security threat. Services were restored by reinstating these VMs after ascertaining that there is no security threat.",Actions are being tracked in the actions tab,2/26/2024,29/02/2023,,,,Yes,Yes,No,Yes,Process Gap,NA,The root cause was attributed to the isolation of the SFTP file transfer VM's after detecting a hacking tool in the primary SFTP servers to prevent any security threat,NA,NA,INTSVCS-53919,5/8/2024,May,2024,Kavitha,Closed,MIM,,2024,2,February,9/16/2025,568,>60
2/23/2024,2/24/2024,89858563,10:00,4:00,20:00,Business Process Services,Business Process Services,Customer Channels,Platform & Store Ops,Others,MI,Multiple foods UPC lines deactivated in SAP causing stock loss in store,Customer,NA,,CSSM,RC identified,67763,"Retail stakeholders reported multiple foods UPCs were deactivated in SAP resulting in stock corruption at stores.  Store colleagues are unable to perform systemic count due to the stock mismatch. There is no impact to sales, however CSSM will no records of these UPCs. The root cause is attributed to incorrect deactivation and reactivation of these UPCs in SAP by multiple business teams. The impacted 109 UPCs have been reactivated and these will be processed into CSSM overnight.  A decision was taken with the Retail stakeholders to manually correct the stock position for few UPCs tomorrow morning. Further investigations are underway. ",All actions are closed in the tracker.,2/23/2024,2/23/2024,,,,No,No,Yes,Yes,Human error - Business,NA,The root cause was attributed to an inadvertent deactivation & reactivation of the foods UPCs by multiple business teams,NA,NA,BPSFOOD1,8/20/2024,August,2024,Saloni,Closed,MIM,,2024,2,February,9/16/2025,571,>60
2/22/2024,3/18/2024,89849978,14:00,16:50,602:00,GTS,Enterprise Technology Platform,Digital & Data,Digital & Data,Others,SI,"Issues with EDW data refresh, reporting and queries on new HP EDW system ",TCS,Database,,EDW  ,RC unknown,67858,"Colleagues have been reporting issues with EDW report refresh, reporting and queries after the migration of the EDW PDOA database to new HP solution. This is impacting the critical Golden reports on weekend and delaying the data availability for the S4 SAP replacement programme followed by a potential impact to Month end financial reporting activities. A comprehensive action plan has been devised to drive the issue to resolution through twice daily MIM calls. Further investigations underway.",All actions are closed,2/22/2024,NA,,,,NA,NA,Yes,NA,RC unknown,NA,"From the initial analysis, it looks like the EDW database on the HP solution is not utilizing the available system resources on the new platform – disk storage, GPFS (general parallel file system) capacity etc. Therefore, investigations are underway to fine tune the database on the HP platform to restore the performance issues with vendor IBM. The root cause remains inconclusive, however, various optimisations are being performed on the database or Business objects to fine tune the query performance - being handled by as part of the EDW PDOA to HP project.",NA,NA,NA,4/22/2024,April ,2024,Saloni ,Closed,MIM,,2024,2,February,9/16/2025,572,>60
2/18/2024,2/18/2024,89854118,02:45,11:03,08:18,Customer Channels,Service Experience,Customer Channels,Customer Engagement,Manage Payment & Content Customer.Com,KI,"Customers were receiving an error while accessing order history, returns and account settings pages across platforms.",Customer,.com,,My account,RC identified,67841,"Customers and contact centre colleagues reported a 503 error whilst accessing their order history, returns and account settings pages across web, mWeb and application from 02:45. Around 11k customers were unable to track their orders & returns and edit their account details. The root cause was attributed to a token expiry of the FESK-My Account application server. Services were restored by regenerating the application server token by 11:03. ",All the actions are tracked and closed in the actoins tab,2/18/2024,2/18/2024,,,,"Yes,No",Yes,No,NA,Certificate issue,No,The root cause was attributed to a token expiry of the FESK-My Account application server.,NA,NA,NA,5/9/2024,May,2024,Pavithra,Closed,.Com SM,,2024,2,February,9/16/2025,576,>60
2/14/2024,2/14/2024,89848633,16:40,18:50,02:10,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,Manage Payment & Content Customer.Com,SI,Gift card page was unavailable to customers,Customer,Wallet ,,E-gift card,RC identified,67745,Customers were unable to view their gift card balance or redeem a digital gift card balance was applied to their account between 16:40 - 18:50 resulting in poor customer experience. The cause of the issue was inadvertent removal of a DNS entry whilst provisioning a new application. The deleted DNS entry was manually re-instated to fix the issue. Root cause analysis underway.  ,All actions are closed,2/14/2024,2/14/2024,,,,"No,Yes",Yes,No,NA,Human error - Customer Tech,NA,The cause of the issue was inadvertent removal of a DNS entry whilst provisioning a new application. Root cause analysis underway.  ,NA,NA,NA,5/12/2024,August,2024,Rehan,Closed,.Com SM,,2024,2,February,9/16/2025,580,>60
2/13/2024,2/14/2024,89846290,17:52,7:04,13:12,GTS,Enterprise Technology Platform,Digital & Data,Digital & Data,Others,KI,Issue in EDW prod vs DR database replication impacting critical reports,TCS,Database,,"Power BI,  EDW ",RC identified,67843,"The EDW Prod and DR DB were not in sync after the restart of the EDW database for separate issue, delaying the data availability for EAH and analytics users accessing the DR environment. To remediate the impact the critical Power BI and Beam reports were pointed to Production. With the assistance of IBM the replication was restarted at 07:04 to restore service. We await root cause details from IBM.",Actions are being tracked in the actions tab,3/16/2024,4/22/2024,,,,Yes,Yes,Yes,Yes,Database issue ,NA,The issue was caused due to a replication issue on one of the database table which was disabled as a permanent fix.,NA,NA,NA,4/22/2024,April,2024,Saloni,Closed,MIM,,2024,2,February,9/16/2025,581,>60
2/11/2024,2/12/2024,89843461,17:15,1:28,08:13,Group Platforms,Finance,"C&H and Intl, Group Platforms","C&H and Intl Supply Chain, C&H Commercial Trading, Finance",Manage Allocation of C&H Stock,SI,SAP POSDTA application unavailable,SAP,NA,,SAP POSDTA,RC identified,67818,SAP POSDTA application was unavailable between 17:15 (11/02) – 01:28 (12/02) impacting availability of sales data for 11/02 impacting some finance reports and D&F allocations resulting in under allocation for some stores. The initial root cause is attributed to connectivity issue between SAP POSDTA database and application.  We await a detailed root cause from SAP vendor.,Actions are being tracked in the actions tab,2/12/2024,4/4/2024,,,,"Yes,NO",Yes,No,No,External Dependencies,No,"A network glitch in the hosted environment which led to instability to the POSDTA application - necessitating a restart of the hosts on that day (all within the hosted stack, so in SAP control). """,NA,NA,NA,4/4/2024,April,2024,Gokul,Closed,MIM,,2024,2,February,9/16/2025,583,>60
2/10/2024,2/11/2024,89843143,20:17,15:13,18:56,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,SI,Sofa and Armchair PDPs showing a 503 error to customers ,Customer,WCS,,PDPs on website,RC identified,67820,"Customers reported that there were unable to access Sofa and Armchair PDP’s (Product Display Page) on website from 20:17, 10/02. This impacted around 60 orders leading to potential loss of maximum £40K. The initial root cause was attributed to an Azure V2 permission issue which prevented the access to furniture PDP’s and it was corrected to restore the services by 15:13.",Actions are being tracked in the actions tab,2/11/2024,2/11/2024,,,,"No,Yes",No,No,Yes,Human error - Customer Tech,NA,There was a permission issue on Azure V2 which prevented the relevant code from being accessed on the furniture PDP.,NA,NA,NA,4/11/2024,April,2024,Saloni,Closed,.Com SM,,2024,2,February,9/16/2025,584,>60
2/9/2024,2/9/2024,89839262,06:30,8:40,02:10,GTS,Enterprise Technology Platform,All BU's,All BU's,Others,MI,Intermittent network connectivity issues were observed between Stockley and Swindon data centres ,TCS,Network,,"Netcool, SAL, Safeguard, Scan tower",RC identified,67732,"From 04:18, intermittent network connectivity issues were observed between Stockley and Swindon data centres. This impacted the store colleagues whilst accessing applications on the honeywells and workstations, Welham Green scan tower functionality, Netcool alerting dashboard and the synchronisation between EDW PROD and DR databases. The problematic DEA (direct ethernet access) link was isolated, which restored services at 10:07. Services are currently running without resilience. Vodafone are continuing urgent root cause investigations and will propose a plan for restoring the link. Hypercare is in place over the weekend.",All actions are closed,2/9/2024,2/21/2024,,,,"No,Yes",No,Yes,Yes,Infrastructure issue / Hardware failure,NA,The root cause was attributed to faulty network devices in Swindon which was replaced to restore the on 21/02.,NA,NA,NA,5/13/2024,May,2024,Saloni,Closed,MIM,,2024,2,February,9/16/2025,585,>60
2/6/2024,2/7/2024,89833987,09:30,3:43,18:05,C&H and Intl ,C&H commercial Trading,Customer Channels,Platform & Store Ops ,Others,SI,Store colleagues reported issues whilst carrying out RFID counting,Customer,STIBO,,RFID Counting,RC identified,67699,Store colleagues reported issues whilst carrying out RFID counting for C&H departments T70 & T14. This impacted the stock counting on 06/02 causing inconvenience to the colleagues as they had to recount the next day.  The root cause was attributed to setup issues causing duplicates in CSSM. Services were restored after the duplicates were removed at 03:00 on 07/02.,All actions are closed,2/6/2024,2/6/2024,,,,"Yes,NO",No,Yes,Yes,Code/Product bug,NA,"Frequently due to user product set up issues, in the CSSM reference data, the system is receiving two different UPC’s numbers, but with exactly the same attribute – when a new product is created existing attributes can be chosen, but there is no validation to prompt that, there is already an identical product in the system. The cause is therefore errors caused by complexed GUI and design.",NA,NA,NA,4/3/2024,April,2024,Gokul,Closed,MIM,,2024,2,February,9/16/2025,588,>60
2/1/2024,2/5/2024,89809846,05:10,0:00,80:50,C&H and Intl ,C&H commercial Trading, C&H and Intl,C&H commercial Trading,Others,KI,Brands articles go live on the website impacted due to missing RRP,Customer,RP,,Brands,RC identified,67709,Around 400 Brands articles set up in either Range Planner and STIBO could not go live on the website due to missing RRP (selling price) in SAP.  Brands merchandisers updated the missing prices directly in SAP to mitigate the impact ready for go-live on the website 06/02.  The Brands team will continue to use an existing report filtered to £0.00 RRP to identify any further impacted articles until root cause is established. SAP team continue to investigate the root cause.,All actions are closed,2/6/2024,2/9/2024,,,,"No,No",No,Yes,Yes,Human error - Business,NA," Range Planner products – It happens due to copy and paste the price from excel. Users have been advised to type in the field manually to avoid issue. No systemic fix as RP is legacy and no development is possible. 
 STIBO products – An issue has been found where ”child/variant” products (eg different sizes) are not updated in SAP, if the price differs by size.",NA,NA,NA,4/3/2024,April,2024,Gokul,Closed,MIM,,2024,2,February,9/16/2025,593,>60
1/31/2024,1/31/2024,89824383,12:25,13:35,01:10,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,Others,MI,Store colleagues reporting blank ECS pricing tickets,ECS,NA,,ECS,RC unknown,67702,From 12:25 store colleagues reported that ECS pricing tickets were printing as blank with the UPC number.  Vendors ECS were engaged and carried out a restore of ECS data from yesterday’s ECS backup and service was restored at 13:35. Root cause analysis underway.,All actions are closed ,1/31/2024,NA,,,,No,No,Yes,NA,External Dependencies,,The stye override function is cumbersome to use and prone to the user making errors.  Work done to make changes to the style override for the deposit return scheme at the time of the issue could have led to the corruption of ECS data.   It has been agreed that the issue cannot be replicated and it is highly unlikely that the root cause for this issue will be found.  The ticketing solution product owner has confirmed this.,,,,4/12/2024,April,2024,Rehan,Closed,MIM,,2024,1,January,9/16/2025,594,>60
1/30/2024,1/31/2024,89822495,12:13,13:15,25:02,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,Others,KI,"	Stores colleagues unable to log in to single sign on applications",Customer,Active Directory,,ECS,RC unknown,67703,Since 12:13 (30/01) BP stores colleagues were observing single sign on log in issues whilst accessing the ECS price ticketing application. There was a work around available as they could enter their credentials and proceed into the applications. From 13:15 on 31/01 the issue stopped occurring and root cause investigations are in progress,"All action have been tracked and closed, Please refer to the actions tab for details",1/30/2024,2/8/2024,,,,"No,No",No,Yes,NA,RC unknown,No,"The root cause remains inconclusive. However, it was anticipated that the issue could have been caused due to central issues with MS impacting their Teams,Outlook and various MS service.",NA,NA,NA,5/2/2024,May,2024,Pavithra,Closed,MIM,,2024,1,January,9/16/2025,595,>60
1/30/2024,1/30/2024,89823312,14:48,15:41,00:53,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Customer Engagement",Manage Payment & Content Customer.Com,KI,Customers unable to load or view Onyx hosted online baskets.,Customer,Onyx,,Customer .com website,RC identified,67684,"From 14:48 – 15:41 customers throttled to an Onyx hosted online basket saw a ‘sorry’ error message and were unable to load/view their basket or checkout.  50% of customers on desktop and mobile web were impacted (because Onyx basket throttle is set to 50%).  However, if the affected customer had tried to load the basket again or checkout, they may have been directed to non-Onyx basket (i.e. WCS) and this would have allowed them to ‘see’ the basket and checkout.  There was a drop of approximately 600 orders during the impacted time.  A deployment by the Price Domain team caused a problem with all customers who are throttled to Onyx basket. The change was reverted in order to restore service.",Actions are being tracked in the actions tab,1/30/2024,2/5/2023,,,,Yes,Yes,No,Yes,Customer Tech change,Change ,The root cause was attributed to a Price Domain change deployed in order to clean-up redundant price fields using the information generated by a reporting tool based on an incorrect usage statistics report resulting in deletion of mandatory production fields. ,CR,Data issue,,4/5/2024,April,2024,Kavitha,Closed,.Com SM,,2024,1,January,9/16/2025,595,>60
1/27/2024,1/27/2024,89817942,07:35,11:22,03:47,GTS,Enterprise Technology Platform,"C&H and Intl, Foods","C&H and Intl Supply Chain, C&H Commercial Trading, Foods Commercial Trading ",Others,KI,cSAP ECC database running out of memory,TCS,Database,,SAP,RC identified,67673,"Alerting indicated high memory utilisation on the SAP ECC database server causing a potential threat to the overnight critical batches. A restart of the cSAP ECC database was carried out between 10:15 and 11:22, the server remains stable since then. A case has been raised with IBM to analyse the logs for the root cause. All the overnight critical batches have been completed without any issues.",Actions are being tracked in the actions tab,1/27/2024,1/31/2024,,,,Yes,Yes,Yes,No,Capacity & Scalability Limitations,No,High memory utilisation on the ECC DB server (huxp0036) caused the system to pageout and the system was not allowing any new process to be triggered and it went into hung state as the memory and swap was fully utilised.,NA,NA,NA,2/22/2024,February,2024,Gokul,Closed,MIM,,2024,1,January,9/16/2025,598,>60
1/27/2024,1/27/2024,89792763,00:00,2:52,02:52,Customer Channels,Service Experience, C&H and Intl,C&H and Intl Supply Chain,Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,CD DC received Ecom nominated orders beyond the cut off time,Customer,Sterling ,,WMS,RC identified,67671,"Castle Donington DC colleagues reported that they were receiving the Ecom orders beyond their cut off time for 27/01, causing an inconvenience to operations at site. In readiness for a planned outage orders for 28/01 CPD (Customer Promise Date) orders were to be sent as 27/01 CPD. To remediate the impact, propositions were moved (+24 hours) for the C&C and NDD orders at 02:52. The impacted 1992 orders were dispatched as promised without impacting the CFR. Detailed root cause underway.",Actions are being tracked in the actions tab,1/27/2024,1/31/2024,,,,"No, yes",No,Yes,No,Human error - Business,NA,"The issue was caused as C&H Online Fulfilement Ops team had turned off the shipping calendar in Sterling as part of a prerequisitive activity, however, it was missed to turn back on. 
",NA,NA,JDATE-10322,2/20/2024,February,2024,Kavitha,Closed,MIM,,2024,1,January,9/16/2025,598,>60
1/27/2024,1/27/2024,89817622,00:32,5:03,04:31,Customer Channels,Platform & Store Ops,Customer Channels,"Selling Experience, Customer Engagement",Manage Payment & Content Customer.Com,SI,Issues with sign-in and log-out pages on the website ,Customer,NA,,Customer .com website,RC identified,67593,"Alerting indicated issues with sign-in and log-out pages on the website from 00:32 causing inconvenience to customers on their website journey. Initial root cause was attributed to an onyx deployment 26/01, where an empty file has been updated on the website causing the issue. To mitigate the impact, traffic was failed over from Onyx to WCS and services have remained stable since 05:03. A fix has been identified and it is planned to be implemented on Monday (29/01). Detailed root cause analysis underway. ",Actions are being tracked in the actions tab,1/27/2024,2/5/2024,,,,Yes,Yes,Yes,Yes,Customer Tech change,Change ,"The root cause was attributed to a deployment in Onyx on 26/01, where an empty file was updated on the website causing the issue.",,Human error ,,4/5/2024,April,2024,Kavitha,Closed,.Com SM,,2024,1,January,9/16/2025,598,>60
1/26/2024,1/27/2024,89814960,14:45,12:20,21:35,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,Others,KI,Some store colleague accounts were deleted,Customer,Stores,,Store colllegue accounts ,RC identified,67668,"Some store colleagues reported that their store accounts are inaccessible since 24/01. The root cause is attributed to the deletion of store accounts as part of store closures process under a change impacting 58 active store users accounts. As a mitigation step, all these accounts were re-instated in AD. However, few impacted colleagues are unable to access their old emails which is currently being investigated.  ",Actions are being tracked in the actions tab,1/26/2024,1/26/2024,,,,No,No,Yes,NA,Process Gap,No,"As part of the regular store closure process, all the associated store colleagues accounts were deleted from the OU (Organisational Unit) in AD (Active Directory). However, out of the 74 deleted colleagues, 58 of them had been retained to work for the other open stores and they were not able to login. Also, before the store closure process starts, it is expected to move these active/retained colleagues account to their respective new stores which was also missed resulting in the issue",NA,NA,NA,3/23/2024,March,2024,Gokul,Closed,MIM,,2024,1,January,9/16/2025,599,>60
1/26/2024,1/26/2024,89814764,10:30,12:15,01:45,GTS,Enterprise Technology Platform, C&H and Intl,C&H commercial Trading,Others,KI,Merchandise Planning (MP) application was inaccessible,Customer,C&H,,MP,RC identified,67666, C&H colleagues reported that Merchandise Planning (MP) application was inaccessible between 10:30 and 12:15 impacting their planning activities for sales and stock intake.,All actions are closed,1/25/2024,2/12/2024,,,,No,No,No,Yes,Customer Tech change,CR,"As part of a change, the DNS entries for the Merchandise Planning application on prem servers were removed. However, when the on prem servers for MP were decommissioned and migrated to the cloud in July 2023, the host name in the cloud remained the same, and therefore depended on the DNS entry that was deleted.",#193513,inefficient deployment plan,NA,2/5/2024,February,2024,Gokul,Closed,MIM,,2024,1,January,9/16/2025,599,>60
1/22/2024,1/22/2024,89809963,20:14,20:40,00:26,Customer Channels,Service Experience,Customer Channels,Service Experience,"Manage Payment & Point of Sales in store, Manage Payment & Content on Customer.com",SI,Issues with card payments at stores and online,Worldline,NA,,Card Transactions,RC identified,67653,"Website and instore card payment transactions were failing between 20:04 and 20:40. To mitigate the impact, offline transactions were enabled for website payments. Around 3.5k in store and 970 online payment transactions were declined. The root cause has been attributed to an issue with one of Worldline's acquirers. Detailed root cause awaited from Worldline.",All actions are closed,1/22/2024,2/19/2024,,,,Yes,Yes,No ,NA,External Dependencies,NA,"The root cause has been attributed to an global issue with one of Worldline's acquirers.During a Visa change period, Worldpay traffic was required to be moved to a
route that would be unaffected by the changes. Worldpay inadvertently moved some traffic to an incorrect route, resulting in the impact to transactions.",NA,NA,NA,2/19/2024,February,2024,Rehan,Closed,MIM,,2024,1,January,9/16/2025,603,>60
1/21/2024,1/21/2024,89806638,05:20,9:00,03:40,C&H and Intl,C&H & Intl Supply Chain," C&H and Intl, Data","C&H and Intl Supply Chain, Data",Manage Pick & Allocation of Stock and Orders (Castle Donington),KI,A central Azure issue at Microsoft impacting Sorted label printing at CD and Beam Services. ,Microsoft,NA,,Label printing,RC identified,67647,"Castle Donington DC colleagues were unable to print sorted carrier labels (Evri and Anpost) between 05:20 to 09:00 impacting the packing and dispatch operations resulting to a capability loss of 19.5K singles, the CFR is yet to be ascertained.","All action have been tracked and closed, Please refer to the actions tab for details",1/21/2024,1/30/2024,,,,No,No,"yes, No",NA,External Dependencies,NA," Root Cause –  
 
Sorted: The issue was caused by a global Azure cloud platform outage (Microsoft) due to which multiple Azure platform services were impacted including “Key Vault” which is used by Sorted.  
 
Note: Key Vault is an Azure service for safeguarding cryptographic keys and other secrets. 
  
Root cause from Microsoft:  
Azure engineering determined that Azure resource manager experienced an outage as the backend had a configuration change causing the ARM web roles to crash. ",MS - #2401210050000068,NA,NA,2/2/2024,February,2024,Pavithra,Closed,MIM,,2024,1,January,9/16/2025,604,>60
1/19/2024,1/19/2024,89803522,03:30,8:00,04:30,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,Others,SI,ECS application not launching in store application launcher ,Customer,Stores Infra,,ECS,RC identified,67645,Store colleagues were unable to print price labels as the ECS application was not accessible from 03:30 AM on 19/01.  A total of 915 workstations were impacted across 300 stores.  The root cause was attributed to a change on the Store Application Launcher (SAL) during which an incorrect package was deployed on the Windows 10 workstations causing the issue.  The change was reverted to restore the services around 8:00 AM.,Actions are being tracked in the actions tab,1/19/2024,1/19/2024,,,,No,No,No,Yes,Customer Tech change,Yes,The root cause was attributed to a change on the Store Application Launcher (SAL) during which an incorrect package was deployed on the Windows 10 workstations causing the issue. ,19420,implementation error,,2/6/2024,February,2024,Kavitha,Closed,MIM,,2024,1,January,9/16/2025,606,>60
1/18/2024,1/19/2024,89802641,10:47,6:00,19:13,Foods,Food Commercial Trading,"Foods, Customer Channels","Foods Commercial Trading, Service Experience, Platform & Store Ops",Others,KI,Pricing ops colleagues reported that some homecare products were being traded at the incorrect prices on tills,Customer,POS,,POS,RC identified,67567,"Pricing ops colleagues reported that some homecare products were being traded at the incorrect prices on tills. The tills were trading at a price higher than what would have been displayed on the pricing label. The potential impact would be 9 UPC's were traded at incorrect prices across 213 promotions in a maximum of 188 stores. As a workaround, the prices on tills have been corrected via POS.",Actions are closed. ,1/18/2024,1/26/2024,,,,No,No,Yes,NA,Process Gap,NA,The rootcause was attributed to the combination of system design limitation in ECS and business process gap while setting promotional prices.  ,NA,NA,"PAP-7198,PAP-6593,PAP-6597,FSO-56",4/11/2024,April,2024,Kavitha,Closed,MIM,,2024,1,January,9/16/2025,607,>60
1/17/2024,1/20/2024,89799773,11:11,9:00,70:00,Customer Channels,Service Experience,Customer Channels,Service Experience,Manage Payment & Content Customer.Com,MI,Issues in refunding online returns at ROI stores,Customer,POS,,Online Order refunds,RC identified,67566,"18 ROI stores were experiencing issues when refunding some online returns from 17/01 at store tills, resulting in a significant drop in refunds. The initial root cause is attributed to a incorrect config file which was updated during the recent ROI release (16/01). A fix was deployed across all the ROI Stores (18 stores) on 19/01 and the store colleagues confirmed that they are able to process the online refunds without any issues.",All actions are closed,1/19/2024,1/23/2024,,,,NO,No,No,NA,Customer Tech change,,The initial root cause is attributed to a incorrect config file which was updated during the recent ROI release (16/01).,,,,2/13/2024,February,2024,Rehan,Closed,MIM,,2024,1,January,9/16/2025,608,>60
1/16/2024,1/16/2024,89798659,10:28,11:27,00:59,GTS,Enterprise Technology Platform,Customer Channels,Selling Experience,Others,KI,Issues with Assist app and SPPD application.,TCS,Network,,"Assist, SPPD",RC unknown,67556,"Store colleagues reported intermittent issues whilst accessing Assist and SPPD applications between 10:28 – 11:27. Store colleagues were unable to help the customers to place orders, the picking and packing operations of ISF (In store fulfilment) orders were also impacted. The initial root cause was attributed to a network connectivity issue on the core internet path. Due to the core internet issue with some 3rd party like Vodafone, XXX  - the Akamai, Cloudflare, other platforms would have been impacted - causing the issue ",Actions are being tracked in the actions tab,1/16/2024,1/25/2024,,,,Yes,No,No,No,RC unknown,No,"The initial root cause was attributed to a network connectivity issue on the core internet path, however what caused the network connectivity issue cannot be determined ",NA,NA,NA,1/29/2024,January,2024,Biparnak,Closed,MIM,,2024,1,January,9/16/2025,609,>60
1/11/2024,1/11/2024,89790541,12:37,12:49,00:12,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Digital cafe stores customers unable to place orders using the kiosks,Counter Solutions,NA,,Digital café,RC identified,67634,Digital cafe stores customers were unable to place orders using the kiosks from 12:40 - 12:56. All cafes recovered without any known recovery action. Counter Solutions confirmed that they observed an outage on their alerting platform from 12:37 to 12:49. They are investigating the root cause and remediation options.,all the actions are  tracked  and closed in the actions tab,1/11/2024,1/11/2024,,,,"Yes,No",Yes,Yes,NA,External Dependencies,No,"The root cause was attributed to aa fault within Counter Solution infrastructure resulting in a failover of the application server within a cluster. 
Note:  Vendor Counter Solutions have migarted all their services to Azure Cloud from On-prem. This will now provide the resiliency
",NA,NA,NA,4/12/2024,April,2024,Pavithra,Closed,MIM,,2024,1,January,9/16/2025,614,>60
1/10/2024,1/11/2024,89789015,17:15,0:43,07:33:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Intermittent issues whilst processing C&C  orders at Thorncliffe DC,Blue Yonder,NA,,WMS,RC unknown,67550,Thorncliffe DC colleagues are intermittently unable to access WMS application on the RDTs impacting the SYW (Shop your way) orders from 17:15 on 10/01. The order proposition for 10/1 were moved by +24 hrs resulting in no CFR impact.  The initial root cause is attributed to a Blue Yonder code issue related to the systemic handling of the pallet locations.  Further root cause analysis underway with Blue Yonder.,All actions are closed ,1/11/2024,N/A,,,,No,No,No,NA,External Dependencies,No,Root cause not fully identified.  An issue with the location ID is suspected.  A logging patch developed by BY has been installed.  This will allow for better root cause analysis should the issue reoccur.  The click & collect order processing at the site is ongoing as expected since 00:43 on 11/01.,NA,NA,NA,4/22/2024,April,2024,Rehan,Closed,MIM,,2024,1,January,9/16/2025,615,>60
1/8/2024,1/10/2024,89787609,23:00,3:48,28:48,Customer Channels,Customer Engagement ,Customer Channels,Selling Experience,Manage Payment & Content Customer.Com,SI,Few products were not displayed on SRP and PLP pages,Customer,.COM,,Product Listing Pages (PLP) and Search Results Pages (SRP) ,RC identified,67628,"Colleagues reported that some products across Womenswear, Menswear & Furniture's were accessible via PDP, but not searchable or displayed on SRP and PLP pages. Up to 1351 products were impacted resulting in potential loss of sales. The root cause was attributed to a change filtering out the impacted products to Bloomreach feeds due to a code logic issue. Services were restored by reverting the change. Detailed RCA underway.",Actions are being tracked in the actions tab,1/9/2024,1/17/2024,,,,"Yes, Yes",No,No,NA,Customer Tech change,Yes," A  code change was deployed which introduced a new attribute to enable the PLPs and SRPs to display a filter  for Sale products. In some instances, products with the new attribute failed a validation check during the selection of products for Bloomreach Full Feed processing. This resulted in the affected products not being  processed by the Bloomreach Full Feed and thus not appearing on the PLPs and SRPs",CM-11064,inadequate testing,NA,2/17/2024,February,2024,Kavitha,Closed,.Com SM,,2024,1,January,9/16/2025,617,>60
1/4/2024,1/4/2024,89779090,13:03,14:35,01:32,C&H and Intl,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Manage Pick of C&H stock,KI,Performance issues in WMS application at C&H Bradford DC,KNAPP,NA,,Kisoft WMS,RC identified,67528,C&H Bradford DC colleagues reported performance issues in the WMS application between 13:03 and 14:13 resulting in a packing capability loss of 3.5k ecomm singles. The root cause was attributed to a colleague generating a report containing 300 million entries resulting in system resource constraint on the Kisoft WMS database. Services were restored by restarting the WMS database server by vendor KNAPP.,Preventive measures  have been agreed with KNAPP and the actions are closed,1/4/2024,1/4/2024,Yes,NA,NA,NA,Yes,NA,NA,External Dependencies,NA,The root cause was attributed to a colleague generating a report containing 300 million entries resulting in system resource constraint on the Kisoft WMS database.,NA,NA,NA,1/16/2024,January,2024,Saloni,Closed,MIM,,2024,1,January,9/16/2025,621,>60
1/2/2024,1/3/2024,89775205,08:30,9:30,25:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,Manage Payment & Point of Sales in store,SI,Multiple issues impacting the Digital Café stores,Counter Solutions,NA,,Digital café,RC identified,67616,"On 02/01, store colleagues reported an issue with the Digital Cafe kiosks as multiple products were showing out of stock.  Customers were unable to place orders resulting in loss of sales and wastage as the products could not be sold.  Counter Solutions advised that the new phase change products had not loaded onto the Digital Cafe kiosks due to an issue with the description on some of the products in the batch which was resolved around 15:00.  A further issue was reported on 03/01, where random products on the kiosks were intermittently showing out of stock due to a design issue on how the kiosk communicates with the stock management system.  A workaround has been implemented to record any item that is out of stock at the start of the day until a permanent fix is in place.",Actions are being tracked in the actions tab,1/2/2024,1/3/2024,,,,"Yes,No",Yes,Yes,NA,External Dependencies,No,"02/01 - Counter Solutions advised that the new phase change products had not loaded onto the Digital Cafe kiosks due to an issue with the description on some of the products in the batch
03/01 - Root cause has been attributed to a design issue on how the kiosk communicates with the stock management system",NA,NA,NA,4/18/2024,April,2024,Pavithra,Closed,MIM,,2024,1,January,9/16/2025,623,>60
12/23/2023,12/24/2023,89763232,22:49,13:00,14:11,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,"	Missing Bradford end of day stock file impacting allocations	",Customer,WMS,,"ASO,Quantum, Relex",RC identified,67516,"Alerting indicated a file archival failure as the Foods Bradford ISM (Inventory stock management) file was not sent from WMS into the downstream systems - Datastage, Quantum, Relex etc. This resulted in an allocation discrepancy for both Quantum & Relex products. To mitigate the impact, manual orders were raised for Quantum and missing Relex lines were generated and sent to the DC. The root cause was attributed to the ISM file not being generated in WMS due to a system resource constraint.",Actions are being tracked in the actions tab,12/23/2023,12/28/2023,,,,Yes,Yes,Yes,NA,Capacity constraint ,NA,"On 23/12, a long running HHT session was identified, and this session was terminated in WMS. However, as per the SOP, it is advised to identify the corresponding WMS daemon generating the long running session followed by a restart of the daemon to terminate the sessions systemically. This was not followed, and the relevant session kept on writing logs in the WMS application server eventually filling up the disk space.
At 22:58 on 23/12, an alert was triggered indicating the spike in the disk space utilization, the corresponding HHT was restarted followed by a manual clearance of the disk space. In parallel, the ISM (Inventory stock Management) file generation job was executing in WMS, although the Bradford file was not generated, the relevant job had completed successfully in Control M which led to the confusion that the files for both Bradford and Milton Keynes were successfully generated. ",NA,NA,NA,5/5/2024,May,2024,Saloni,Closed,,,2023,12,December,9/16/2025,633,>60
12/22/2023,12/22/2023,89728098,07:00,9:44,02:44,GTS,Digital Workplace Services,"Foods, Digital & Data","Foods Supply Chain, Digital & Data",,KI,Delay in sending the 28DOP to the suppliers,TCS,Messaging Services,,CXM,RC identified,67822,"A number of foods suppliers reported that they had not received the 28 day order plan by email potentially impacting their planning or delivery activities. The email issue also impacted the daily consignment reports to the Brands suppliers for planning activities. To mitigate the impact, the reports were sent manually to the suppliers. The root cause was attributed to an issue in the messaging mimesweeper server resulting in email delivery failures. Services were restored by migrating the messaging server to a different server. Hypercare has been agreed until the end of peak.",All actions are closed,12/22/2023,NA,,,,NA,NA,NA,NA,Code/Product bug,NA,The root cause was attributed to an issue in the messaging mimesweeper server resulting in email delivery failures.,NA,NA,NA,2/12/2024,February,2024,Gokul,Closed,,,2023,12,December,9/16/2025,634,>60
12/21/2023,12/23/2023,89759673,08:00,18:00,58:00,Foods,Food Supply Chain,Foods,"Foods Supply Chain, Service Experience",,KI, Allocation variances observed for a handful of CFTO products.,TCS,ASO,,ASO,RC identified,67293,"Through hypercare, it was highlighted that allocation volumes for a handful of CFTO products were not responding back to Depots with the expected store volumes. There was a shortage of 10 trays of 1 UPC on 21/12 impacting the customer order fulfilment at stores resulting in poor customer experience. To mitigate the impact, a contingency was applied to use the excess, stock, buffer stock store shelf availability and ordering from the suppliers. The root cause was attributed to the speed of goods receipting at depots. ",All actions are closed,12/21/2023,12/21/2023,,,,No,No,Yes,Planned for Q1 2024,Design issue,NA,"The root cause was attributed to the speed of goods receipting at depots, which was difficult for the system to handle. For the permanent fix, this needs a design change. ",NA,NA,NA,2/20/2024,February,2024,Gokul,Closed,,,2023,12,December,9/16/2025,635,>60
12/19/2023,12/19/2023,89753757,08:07,14:15,06:08,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Label printing issues at Ollerton & Stoke DCs,Sorted,NA,,Label printing,RC identified,67513,Ollerton and Stoke DC colleagues were unable to print E-com dispatch labels between 08:07 and 14:15 potentially impacting 1.5k orders due to be dispatched on 19/12. There was no impact to Stoke DC. The root cause was attributed to a failure in processing the service availability file for EVRI with a delivery date of 2024. Services were restored after deploying a fix. Awaiting detailed RCA from vendor Sorted. ,All actions are closed,12/19/2023,12/21/2023,,,,"No,No",No,Yes,Yes,3rd party issue,NA,"RCA from Sorted : There has been dormant bug since February 2023 within the service allocations that did not correctly allow for the year-rollover (2023 ? 2024) when calculting delivery dates. 
",NA,NA,NA,3/5/2024,March,2024,Gokul,Closed,,,2023,12,December,9/16/2025,637,>60
12/14/2023,12/14/2023,89744704,15:43,15:58,00:15,GTS,Enterprise Technology Platform,Customer Channels,"Customer Engagement, Selling Experience",,MI,Restart of WCS database preventing customers from placing order,TCS,Cloud Ops,,Customer .com website,RC identified,67270,Alerting indicated a drop in orders on the website between 15:43 and 15:58. Around 2600 customers were unable to place orders resulting in a poor customer experience. The root cause was attributed to the WCS database being unavailable and a manual restart was performed to restore services. Detailed root cause investigations underway with vendor Oracle.,Actions are being tracked in the actions tab,12/14/2023,12/14/2023,,,,Yes,Yes,Yes,NA,Human error - Customer Tech,,The root cause was attributed to an inadvertent deletion of files on the backup mount folder holding the critical process - Microsoft Defender and ASMB (Automatic Storage Manager background Process) which is responsible for running the WCS database. ,,,,3/25/2024,,,Rehan,Closed,,,2023,12,December,9/16/2025,642,>60
12/13/2023,12/18/2023,89740877,10:03,8:53,22:10,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops,,ADHOC,Barnstaple store  was hard down,BT,NA,,Click and collect,RC identified,68542,"Barnstaple store was hard down as both the network links hosted by BT(British Telecom) were unavailable impacting the store operations. As a workaround, a 4G cradlepoint router was provided to the store to resume operations, however, there was some slowness reported whilst accessing Click & Collect apps due to lack of bandwidth. The root cause was attributed to an incorrect IP address configuration (set to “inactive”) by BT resulting in radius server authentication issues to fix a separate issue within their infrastructure. The IP address configuration was corrected to restore services.  ",All actions are closed,12/18/2023,12/18/2023,,,,"Yes, No",Yes,"Yes, Yes",Yes,3rd party issue,NA,"To fix a previous DDoS attack within the BT infrastructure, the IP Address was configured as inactive within the live network. As the EBB and BB1 IP addresses are of same ranges, the systems had incorrectly shown this IP address as part of a range that could be deployed as part of BB1 migration which triggered the radius server authentication issues to the Barnstaple store circuits.",NA,NA,NA,8/16/2024,August,2024,Gokul,Closed,,,2023,12,December,9/16/2025,643,>60
12/12/2023,12/12/2023,89739134,09:45,11:15,01:30,Customer Channels,Customer Engagement ,Customer Channels,"Customer Engagement, Service Experience",,SI,Some Sparks promotions are not working on tills in stores,Customer,NA,,Store tills,RC identified,67272,"As part of the 12 days of Sparks, store colleagues reported that the 20% off coats promotion was not working on tills. Around 1.5k customers were unable to redeem the offer resulting in poor customer experience. The initial root cause was attributed to changes performed on the 20% off Sparks coats promotion on 11/12 resulting in the promotions to go into cancelled state. Services were restored by recreating the promotion followed by an emergency release to the tills. A PIR has been scheduled for detailed root cause investigations. ",Actions are being tracked in the actions tab,12/12/2023,12/14/2023,,,,"No, Yes",No,No,NA,Human error - Business,No,"The initial root cause was attributed to changes performed on the 20% off Sparks coats promotion on 11/12 resulting in the promotions to go into cancelled state. In the process of amending existing promotions, due to the number of steps required to be carried out and the tick boxes that needed selecting, the promotion was unintentionally cancelled.",NA,NA,NA,3/19/2024,March,2024,Rehan,Closed,,,2023,12,December,9/16/2025,644,>60
12/8/2023,12/8/2023,89731236,08:16,9:43,01:27,GTS,Enterprise Technology Platform,All BU's,All BU's,,SI,Hardware failure impacting various DC operations and Retail Dashboard report,TCS,Wintel,,DC operations ,RC identified,67164,"DC colleagues reported issues at Castle Donington and Swindon DCs resulting in a capability loss of 34.8k singles at Donington, the impact to Swindon is being ascertained. This also impacted the Milton Keynes operations for a brief period and the Retail In day sales report was not updated between 07:00 and 08:00. The root cause was attributed to a hardware failure on one of the linux replicated hosts impacting various cluster and middleware services across the estate.  Services were restored by restarting the cluster services followed by  application restarts. A critical case has been raised with HP for the hardware replacement. Hypercare is in place over the weekend.",Actions are being tracked in the actions tab,12/8/2023,12/8/2023,,,,"Yes, No",Yes,Yes,Yes,Infrastructure issue / Hardware failure,No,The root cause was attributed to a hardware failure on one of the linux replicated hosts impacting various cluster and middleware services across the estate.,NA,NA,NA,3/28/2024,March,2024,Gokul,Closed,,,2023,12,December,9/16/2025,648,>60
12/6/2023,12/6/2023,89775926,08:24,8:42,00:18,Customer Channels,Service Experience,Customer Channels,"Selling Experience, Customer Engagement",,SI,Order drop on Customer Website between 8:24-8:42PM,Worldline,NA,,.com ordering ,RC identified,67601,"Alerting indicated that there was a dip in orders on Customer Website between 20:24 to 20:42 impacting only card & Apple Pay payments. Around 1200 customers were unable to place their orders. The root cause was attributed to an issue at Worldline impacting the order authorisation flow. Services were restored by Worldline, and we await further updates.  ",The action needs to b address are already part of previous Problem hence no new actions agreed to track,12/6/2023,12/14/2023,,,,Yes,Yes,Yes,NA,3rd party issue,,"The root cause was attributed to an issue at Worldline impacting the order authorisation flow. Services were restored by Worldline, and we await further updates.  ",,,,12/14/2023,December,2023,Saloni,Closed,.Com SM,,2023,12,December,9/16/2025,650,>60
12/4/2023,12/4/2023,89723808,17:33,19:08,01:35,Customer Channels,Customer Engagement ,Customer Channels,"Selling Experience, Customer Engagement",,SI,Order decline on the Customer Website ,Customer,NA,,.com ordering ,RC identified,67171,"Alerting indicated - Latency and timeout errors were observed for various WCS and website API calls  due to WCS database contention and blocking sessions between 17:33 and 19:20. This resulted in 1.5K order declines and 18 customers called contact center reporting the issue. To mitigate the impact, the customer session API threshold was reduced to 5k from 20K after which the blocking sessions and order declines were cleared. The API threshold was increased to 10K by 20:10 to remediate the cascading impact to customer logins whilst the threshold was reduced. Hypercare monitoring and root cause investigations underway",Actions are being tracked in the actions tab,12/4/2023,12/8/2023,,,,Yes,Yes,Yes,No,Code/Product bug,,The root cause was attributed to a combination of ~1M marketing push notifications sent to the Android app and an underlying Android app design issue which was unable to handle bulk customer requests resulting in breaching its threshold limit,,,WLCM-2707/2106,2/12/2024,February,2024,Kavitha ,Closed,,,2023,12,December,9/16/2025,652,>60
12/3/2023,12/6/2023,89723033,13:00,13:05,72:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Orders placed in the digital cafes not systemically processing to the kitchens to prepare,Counter Solutions,NA,,Customer Inconveince,RC identified,67616,"Between 3rd & 5th December, store colleagues reported that the orders placed in the 18 Digital Cafes were intermittently not being sent to the kitchens, therefore the kiosks were manned to mitigate customer impact. The root cause was attributed to a global issue at vendor QSR - Counter solutions where an API connection limit was breached.  To restore service, QSR flushed out the API connections and increased the threshold limit.  On 7th December, two stores reported seeing the similar issue.
On 6th December, customers were unable to intermittently place orders on the kiosks in the Digital Cafe stores between 12:30 and 13:00. Kiosks were rebooted to restore service. A database connection timeout value was increased to prevent a recurrence. ",Actions are being tracked in the actions tab,12/3/2023,12/5/2023,,,,"No,Yes",No,Yes,NA,3rd party issue,No,"03rd  -5th Dec - The root cause was attributed to a global issue at vendor QSR - Counter solutions where an API connection limit was breached.
06/12 - A database connection timeout value was increased to prevent a recurrence.",NA,NA,NA,2/15/2024,February,2024,Pavithra,Closed,,,2023,12,December,9/16/2025,653,>60
12/3/2023,12/4/2023,89721258,11:00,2:59,15:59,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Allocation failures across all Foods depots,TCS,SCRD,,Next Gen FOA,RC identified,67249,RDC Network (GIST) reported allocation failures for ~100 UPCs across all the foods depots from 10:00. This resulted in operational delay and impact to stock replenishment to stores. A contingency plan was applied to manually allocate at depots in case of any failures. The root cause is attributed to an issue with the outbound file sent from SCRD to NextGen FOA (Relex).,All actions have been completed,12/3/2023,12/3/2023,,,,"No,yes",No,Yes,NA,Code/Product bug,NA,"There was a missing lead time scenario in the SCRD outbound file sent to Relex for 99 UPCs which caused the zero-allocation issue. 
Background: During last PEAK, it was observed that the job responsible for processing the SCRD network extract file for the supply chain products was taking around 2.5 hours to complete which was causing a cascading impact to the overnight critical batch completion – supplier order finalization, NDC allocation, Quantum UI etc. This was also identified as a PEAK risk. Therefore, this job was optimized to bring down the run time of this job to in less than an hour (~45mins) during October 2023. This optimization introduced a bug in SCRD resulting in the system to not refer to historical lead times data (Lead time – time taken for a supplier to make goods ready for delivery) on one of the specific batches of allocation (Saturday into Sunday).
On 03/12, supply chain colleagues amended the supplier order pattern for 2 suppliers in SCRD which exposed the bug. As a result, Relex failed to generate the allocations for 2 suppliers as expected causing a zero-allocation scenario. Therefore, manual allocation was performed by the GIST colleagues with the help of data provided by the tech team to mitigate the impact. ",NA,NA,NA,2/4/2024,February,2024,Pavithra,Closed,,,2023,12,December,9/16/2025,653,>60
12/2/2023,12/2/2023,89718748,11:06,14:25,03:19,Business Process Services,Business Process Services,"Foods, Customer Channels","Foods Commercial Trading, Service Expereince",,MI,Tills trading at reduced prices across the Irish stores,Business,NA,,Store Tills,RC identified,67141,Store colleagues highlighted that foods products were trading incorrectly at reduced prices in the tills across all Irish stores between 08:00 and 14:25. This has resulted in illegal trading and loss of revenue and poor customer experience. The root cause was attributed to an incorrect store set up in SAP overriding the price files for all Irish stores in Datastage. Services were restored by manually updating the correct prices in POS. Hypercare in place. Detailed root cause investigations underway.,Actions are being tracked in the actions tab,12/2/2023,12/5/2023,,,,"No,yes",No,Yes,,Human error - Business,NA,"The issue was caused as this dummy store was set up incorrectly in SAP with IE (Ireland) as the country code, however, all other key entities were set up as a UK store (company and sales organization) which caused a conflict in a downstream system. This occurred because there was an error in the store set up form which reflected UK for all the attributes instead of IE + there was an incorrect setup into SAP as IE was used as the country instead of UK as per the form and UK used for the remaining two attributes. As the prices for Ireland are sent on a country level in the Datastage application it has taken priority based on the existing logic set in the system, this incorrect store set up therefore overrode the prices across all the Irish stores.",NA,NA,NA,4/16/2024,April,2024,Saloni,Closed,,,2023,12,December,9/16/2025,654,>60
11/30/2023,11/30/2023,89716033,07:08,17:24,10:16,Customer Channels,Customer Engagement ,Customer Channels,"Selling Experience, Customer Engagement",,KI,"	Promotional badges showing after promotion has ended",TCS,.com,,PLP,RC identified,67150,Promotions were displayed on the website for furniture and home products on the website even though the promotion had ended. Any orders however were placed with correct prices. This resulted in poor customer experience and 4 customers reported this to the contact centre.  The issue was caused due to a failure in processing of the full data feed due to memory constraints in our environment and the delta feed was also impacted due to a separate issue with Bloomreach.  The memory was increased to process the full feed data and Bloomreach processed the delta feed. The data processing completed by 17:08 after which the promotions were removed from the site successfully by 17:24.,"All the actions have been tracked and closed, please refer to the problem actions tab",11/30/2023,12/8/2023,,,,Yes,Yes,Yes,Yes,Certificate issue,NA,The issue was caused due to a failure in processing of the full data feed due to a Certificate error/connection problem encountered during Delta Feed processing,NA,NA,NA,2/2/2024,February,2024,Pavithra,Closed,,,2023,11,November,9/16/2025,656,>60
11/29/2023,11/29/2023,89712765,09:15,16:00,02:00,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,SI,PLM unavailable,PTC,NA,,PLM,RC identified,67231,"Colleagues were unable to access PLM application from 09:15 to 09:45 and 14:30 to 16:00 impacting their ability to amend/set up any C&H product details in PLM. The root cause was attributed to an issue at PTC. Services were restored by restarting the application nodes. 

Issue at PLM : There has been multiple queue entries (over 6mil entires) on the server along with multiple instances of integration running in parallel causing a performance degradation on the production server. ","RCA document has been shared by PTC and preventive measures have been agreed,",11/29/2023,1/3/2024,,,,"No,yes",No,NO,NA,3rd party issue,NA,There has been multiple queue entries (over 6mil entires) on the server along with multiple instances of integration running in parallel causing a performance degradation on the production server. ,NA,NA,NA,1/2/2024,January,2024,Pavithra,Closed,,,2023,11,November,9/16/2025,657,>60
11/28/2023,11/28/2023,89710913,01:10,4:22,03:12,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,Allocation of C&H stock,KI,Delay in D&F order processing,Customer,NA,,D&F,RC identified,67133,"At 01:12, alerting indicated that D&F orders were not processed for the C&H DCs which delayed the retails orders to Donington by 20 mins.  The root cause was attributed to a job failure at Blue Yonder due to incorrect order set up by the Inventory Planners. Services were restored by correcting the order set up to recover the job at 01:29. Detailed investigations underway.",Actions are being tracked in the actions tab,11/28/2023,11/29/2023,,,,"Yes, No",Yes,No,Yes,Human error - Business,NA, The root cause was attributed to a job failure at Blue Yonder due to incorrect order set up by the Inventory Planners. ,NA,NA,NA,12/19/2023,December,2023,Gokul,Closed,,,2023,11,November,9/16/2025,658,>60
11/27/2023,11/27/2023,89709305,05:30,6:30,01:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Multiple issue at Welham Green DC impacting operations,TCS,WMS,,WMS,RC identified,68547,Welham Green DC colleagues were unable to access WMS application between 05:30 & 06:30 impacting the DC operations. Also there was an issue with boxed and x-dock stock receipt impacting the picking operations and few trailer shipments. The root cause was attributed to an inadvertent configuration in WMS resulting in the daemons to be unavailable. Services were restored by reinstating the configuration followed by a daemon restart. ,Engineers have been educated and refresher sessions on the process has been delievered to prevent the human errors ,11/27/2023,,,,,"No,yes",No,Yes,NA,Human error - Customer Tech,NA,The root cause was attributed to an inadvertent configuration in WMS resulting in the daemons to be unavailable,NA,NA,NA,1/1/2024,January,2024,Pavithra,Closed,,,2023,11,November,9/16/2025,659,>60
11/24/2023,11/24/2023,89705822,14:15,15:00,00:45,Customer Channels,Service Experience,Customer Channels,"Selling Experience, Customer Engagement",,SI,CFTO ordering slots were not available for customers,Customer,NA,,CFTO Ordering,RC identified,67116,Store colleagues highlighted that there were no available slots for customer to place CFTO orders 14:15 and 15:00. Around 60 customers were unable to place orders during the issue. The root cause was attributed to an manual error during a change in the shipping calendars. Services were restored by reverting the change. Detailed root cause analysis underway.,Actions are being tracked in the actions tab,11/24/2023,11/24/2023,,,,"No, Yes",No,Yes,,Human error - Business,NA, The root cause was attributed to an error during a change in the shipping calendar,NA,NA,,12/19/2023,December,2023,Gokul,Closed,,,2023,11,November,9/16/2025,662,>60
11/22/2023,11/22/2023,89700848,02:45,4:55,02:10,GTS,Enterprise Technology Platform,Foods,Foods Supply Chain,,KI,SAP PO (Process Orchestration) unavailable due to file system space issue,TCS,Backup,,FFO,RC unknown,67205,"During hypercare, team identified that SAP PO was inaccessible. Upon investigation, team identified a TSM issue due to which the database archive logs were not being moved to TSM resulting in the one of the file system being full. The Foods final orders were sent to suppliers  with a delay of approx. 8 min (5:23 AM) against the target completion time of 5:15 AM. The underlying TSM issue was resolved and the housekeeping was done on the database server to restore the services. Detailed root cause investigations in progress.",Actions are being tracked in the actions tab,11/22/2023,NA,,,,"No,Yes",No,Yes,NA,RC unknown,NA,"The issue was caused due to the database archival file system filling up the backup processes had failed due to connectivity (handshake) issue between the TSM client and the server. This caused the database to go into hung state as no further logs could be written, resulting in the issue. The root cause behind the handshake issue between the TSM (backup) server and the agent remains inconclusive as the backup agent is out of support and therefore there will be no support from vendor IBM.",NA,NA,,4/11/2024,April,2024,Kavitha,Closed,,,2023,11,November,9/16/2025,664,>60
11/16/2023,11/16/2023,89689701,04:05,12:10,08:05,GTS,Digital Workplace Services,All BU's,All BU's,,SI,Power apps hosted applications unavailable due to a central Microsoft issue,Microsoft,NA,,Power Apps,RC identified,67039,"Multiple applications hosted on Power apps -  HFSS, Closer to Customer, Store Help Portal, Labelling Compliance etc were inaccessible between 04:05 and 12:10. This resulted in inconvenience to the colleagues and a manual workaround was applied in stores for compliance application, therefore, no operational impact. The root cause was attributed to a change at Microsoft and was reverted to restore services. Awaiting detailed root cause analysis.c",Actions are being tracked in the actions tab,11/16/2023,11/23/2023,,,,No,No ,No,NA,3rd party issue,No,"Microsoft were deploying two updates to different parts of the service, which combined together and were related to Transport Layer Security (TLS) 1.3.

1. A configuration change to an internal service which manages Power Platform connectors
2. A Windows operating system (OS) change on the underlying infrastructure to process the new (1) dependency configurations
These changes are deployed through different deployment pipelines and those processes are unconnected. The plan was to deploy the configuration change (1) in a disabled state, and once the Windows operating system change (2) had fully deployed, Microsoft would slowly enable the (1) configuration change. 
A miss in the deployment process resulted in the configuration change to the internal service being deployed in an activated state. As this update completed deployment quicker than the required Windows operating system change, this caused an incompatibility and prevented Power Platform connections from being processed, resulting in impact.",N/A,N/A,WPTS-9392,2/20/2024,February,2024,Kavitha,Closed,,,2023,11,November,9/16/2025,670,>60
11/12/2023,11/12/2023,89681664,08:50,17:36,08:46,GTS,Enterprise Technology Platform,Foods,Foods Supply Chain,,KI,Quantum User Interface (UI) intermittently unavailable due to a file system space issue,TCS,Quantum DevOps,,Quantum,RC unknown,66911,"Alerting indicated that the weekly long-range FOP (Forecasts and Order Planning) job was failing with a java exception error and filled up one of the filesystems on the Quantum app server resulting in the Quantum User Interface (Q UI) being unavailable intermittently. Foods colleagues were unable to carry out the forecast and planning activities. To mitigate the impact, the AIX upgrade on the Quantum servers was reverted after which the problematic job completed successfully and Quantum UI has been stable from 16:30, 12/11. Root Cause investigations underway.",The next steps are in the problem actions tab,11/14/2023,11/14/2023,,,,Yes,Yes,No ,Yes,RC unknown,Yes,"During the testing, the Standard testing procedure was followed with 6k UPC’s (Mixture of chilled and Ambient products). However, on Sunday during the weekly job all the UPC’s were ambient UPC’s causing compatibility issues. Also, it is observed that quantum is not compatible with higher java versions and as it is a legacy application, we are not able to drill down on its compatibility issue. 

As a preventive measure, new test case with significant load volume will be added as part of the standard testing procedure. For the permanent fix, Relex will be replacing quantum in next 9months – 1 year. ",189214,inadequate testing,,2/6/2024,February,2024,Kavitha,Closed,,,2023,11,November,9/16/2025,674,>60
11/10/2023,11/10/2023,89678930,08:50,11:10,02:20,Customer Channels,Selling Experience,Customer Channels,"Selling Experience, Customer Engagement",,KI,PLPs and PDPs impacted by Microsoft DNS issue,Microsoft,NA,,Product Listing Pages (PLP) and Product Description Page (PDP) ,RC identified,67011,"Alerting indicated a spike in 404 errors on the PLPs and PDPs on the website from 08:50. 
This impacted the customer experience, reviews and stock values on the PDPs, retrieval of recommendations and Find In Store functionality. The root cause has been attributed to a central issue at Microsoft. Services were restored by 11:10 and we await detailed root cause analysis. ",Actions are being tracked in the actions tab,11/10/2023,11/22/2023,,,,"Yes, No",yes,NO,NA,3rd party issue,NA, Microsoft updated that there was a sudden spike in traffic resulting in Azure Traffic Manager service reaching its threshold,NA,NA,NA,12/12/2023,December,2023,Gokul,Closed,,,2023,11,November,9/16/2025,676,>60
10/30/2023,10/30/2023,89656303,15:26,20:14,04:48,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,SI,WMS application unavilable at Ollerton DC,TCS,Wintel/ Linux,,DC operations ,RC identified,66645,"Alerting indicated that WMS application was inaccessible at Ollerton DC between 15:26 and 20:14. This impacted the overall DC operations resulting in cancellation of 42 singles, incurring a 1.5% CFR. The root cause was attributed to a connectivity issue between the application server and the network gateway due to duplicate IP addresses mapped to newly built servers. These servers were shut down to resolve the IP conflict. Detailed RCA underway.",All the actions are closed,10/30/2023,10/30/2023,TBC,TBC,TBC,Yes,Yes,No,Yes,Human error - Customer Tech,NA,The issue was caused due to the IP conflict hindering the connectivity between the application server and network gateway which triggered the file system corruption resulting in application cluster restart failure.,NA,Human error ,MTSM 74,2/26/2024,February,2024,Saloni,Closed,,,2023,10,October,9/16/2025,687,>60
10/29/2023,10/29/2023,89652851,09:40,20:14,10:34,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Label printing issue at Welham Green DC,TCS,Linux,,Label printing,RC identified,66529,Welham Green DC reported that they were not able to print picking and packing labels between 09:40 and 20:14 impacting 100k boxed and 32k hanging pick and dispatch. Cross dock and inbound operations were not impacted.  The initial root cause has been attributed to missing config files from the application server and these we restored to fix the issue. Detailed root cause underway. ,Actions are being tracked in the actions tab,10/29/2023,10/31/2023,,,,Yes,Yes,Yes,Yes,Human error - Customer Tech,NA,"On 17/10, an alert was received in the WMS layer indicating one of the file directories holding the archival files (older than 7 days) was full resulting in a capacity constraint. A known workaround was implemented by manually executing a command to delete any files older than 5 days to ensure new files are successfully getting stored in the archival file directory.  However, this command was executed in the home directory holding the system files of the WMS application in error which deleted the print service files. On 29/10, an alert was received indicating the print daemon in stale state, therefore a daemon restart was performed which deleted the existing cache resulting in the issue.",NA,Human error,NA,11/30/2023,November,2023,Saloni,Closed,,,2023,10,October,9/16/2025,688,>60
10/29/2023,10/29/2023,89652705,17:49,17:58,00:09,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,"Brief Outage for Checkout, Sparks and myAccount",TCS,WCS,,Online orders,RC identified,66647,Alerting indicated a drop in orders and issues in signing into MyAccount and Sparks between 17:49 and 17:58. This impacted around 1350 orders and the customer's ability to access MyAccount and Sparks. Services were restored without any manual intervention and the root cause is being investigated.,"The workaround provided by Oracle to resolve JDBC problem, has been tested on the lower environment where we had observed a similar problem at a smaller scale during the regression testing and it successfully removed all the instances of the error. So, it has been deployed in production and currently live",10/29/2023,11/14/2023,,,,"Yes, No",Yes,No,Yes,Code/Product bug,NA,The root cause could be due to an issue with a version of the JDBC driver used rather than a network problem. This is a known issue with the version the JDBC driver that is currently used which is reportedly responsible for causing blocking behaviour intermittently (Which is what was observed during the time of this incident).,NA,NA,NA,11/14/2023,November,2023,Gokul,Closed,,,2023,10,October,9/16/2025,688,>60
10/28/2023,10/28/2023,89650991,01:10,3:15,02:05,Customer Channels,Service Experience,"Customer Channels, C&H and Intl","Service Experience, C&H and Intl Supply Chain",,KI,Orders being placed offline & Sterling application unavailable,TCS,Database ,,Sterling,RC identified,66631,"Alerting indicated that sterling application was unavailable between 1:10 and 3:15.  Customers were placing offline orders along with CFTO across UK, IE, International. Also, Ollerton DC were unable to complete returns and refunds for online orders. The root cause was attributed to database blocking sessions which were cleared without any manual intervention. Detailed root cause investigations underway.",All actions are being tracked in Problem Actions tab,10/28/2023,10/28/2023,,,,"Yes, No",Yes,No,Yes,Capacity constraint ,NA,A sterling release (recovery point) which had issue was used to recover. This restore point kept on gathering logs and ultimately the database FRA (Fast Recovery Area) storage consumption reached 99.9% and database stopped accepting new transactions/connections resulting in hung threads and application servers becoming unresponsive.,NA,Capacity constraint ,NA,11/24/2023,November,2023,Gokul,Closed,,,2023,10,October,9/16/2025,689,>60
10/27/2023,10/27/2023,89649814,12:30,13:50,01:20,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,KI,PLM Issue,PTC,NA,,PLM,RC identified,66628,"From 09:40, PLM (Product lifecycle Management) users reported 'Save Button' wasn't working in the 'Line Sheet' which impacted the daily operations to carry out their development work on C&H products. Vendor PTC identified that the add column utility that ran last time made some modifications and had caused the issue. PTC team taken the system down at 12:30 to recreate the index and made the PLM system available by 13:50.","Actions have been agreed by vendor PTC to ensure that after every add columns utility execution, they would perform the required validations",10/27/2023,10/27/2023,10/27/2023,10/27/2023,NA,"No, N/A",No,No,Yes,3rd party change,NA,Vendor PTC had implemented a change recently to create a flexible attribute in FlexPLM and during the change a step was missed to create a database table space,NA,NA,NA,10/30/2023,October,2023,Gokul,Closed,,,2023,10,October,9/16/2025,690,>60
10/27/2023,10/27/2023,TBC,07:30,16:47,09:17,Foods,Food Supply Chain,"Foods, C&H and Intl","C&H and Intl Supply Chain, Foods Supply Chain",,KI,Central connectivity issue at OpenText.,OpenText,NA,,"ASNs, PO etc.,",RC identified,NA,"B2B messages such as ASNs, Invoices, purchase orders etc were not flowing from Customer to suppliers and vice versa due to a global connectivity issue at OpenText affect multiple clients from 07:30. This impacted the Ocado order files transmission via EDI (manual workaround utilised) Delays in ASN interfacing for both Foods and C&H DCs impacting operational efficiency. x3 Foods shipments had to be rejected and sent back to suppliers due to missing ASNs needed for goods receiving.  Services were restored by OpenText at 16:47.",NA,10/27/2023,10/27/2023,,,,Yes,Yes,No,Yes,3rd party issue,NA,Global connectivity issue at OpenText,NA,NA,NA,11/21/2023,November,2023,Saloni,Closed,,,2023,10,October,9/16/2025,690,>60
10/27/2023,10/28/2023,89649648,07:34,7:16,23:42,Group Platforms,Finance,Digital & Data,Digital & Data,,KI,"RD vs BI variance of 1.3M, where BI is understated",TCS,SAP BW,,EDW reporting ,RC identified,66531,"Alerting indicated a variance of 1.3M between RD and BI, where BI was understated. The root cause was attributed to an inadvertent release of the successor job in SAP BW layer, whilst the predecessor job was in a held state. The missing sales were processed as part of the overnight batch.","The issue was caused to due to manual error,  products have advised to follow the process with out deviation to avoid such miss",10/28/2023,10/28/2023,,,,Yes,Yes,NA,NA,Human error - Customer Tech,No,"The root cause was attributed to an inadvertent release of the successor job in SAP BW layer, whilst the predecessor job was in a held state.",NA,NA,NA,10/31/2023,October,2023,Kavitha,Closed,,,2023,10,October,9/16/2025,690,>60
10/27/2023,10/27/2023,89648657,02:50,5:20,02:30,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,HHT issues at Stoke DC,TCS,WMS ,,HHTs,RC identified,66632,"Stoke DC colleagues were experiencing issues with the HHT devices between 02:50 and 05:20. The root cause was attributed to a memory constraint in the WMS application layer. Services were restored by restarting the daemons. As a permanent fix, the RDT queues were rebuilt in WMS with additional memory.",All the actions are  tracked and closed under the Problem actions tab ,10/27/2023,10/27/2023,,,,Yes,No,No,yes,Capacity constraint ,NA,"The root cause was attributed to a memory constraint in the WMS application layer due to the recent addition of Ecomm orders in Stoke DC, causing the transactions to exceed the queue depth limit of 16kb resulting in message pile-up’s. ",NA,Capacity constraint ,NA,11/30/2023,November,2023,Pavithra,Closed,,,2023,10,October,9/16/2025,690,>60
10/27/2023,10/27/2023,89649412,03:03,6:10,03:07,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Quantum Job failure and WMS slowness impacting Bradford NDC allocations,Customer,NA,,"Supplier orders, NDC allocation",RC identified,66534,"Alerting indicated a job failure in Quantum potentially impacting critical flows like FFO, Quantum, NDC and RDC allocations, 28 DOP etc. Bradford orders were interfaced into WMS with a delay of ten minutes (target completion time is 6am).  The root cause was attributed to a problematic UPT value which was removed from the batch to recover the job. ",All the actions are  tracked and closed under the Problem actions tab ,10/27/2023,10/27/2023,,,,Yes,Yes,Yes,Yes,Human error - Business,NA,The root cause was attributed to a human error by Supply Chain colleagues whilst setting up an order quantity in Quantum,NA,NA,NA,11/30/2023,November,2023,Pavithra,Closed,,,2023,10,October,9/16/2025,690,>60
10/26/2023,10/26/2023,89647767,12:32,14:25,01:53,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Latency observed in HHTs at Swindon DC ,TCS,WMS,,HHTs,RC identified,66940,"Swindon DC colleagues were experiencing issues with HHT devices between 12:32 and 14:25, however, there was no impact. The initial root cause was attributed to blocking sessions in the database layer.  Services were restored by restart of the application and database. ",All actions are closed,10/26/2023,10/26/2023,,,,Yes,No,No,Yes,Human error - Customer Tech,NA,"As part of combined deliveries project, a newly introduced code was implement which had package logging enabled and it was writing large number of records in database whenever the receipt message from GIST DC was processed in Swindon DC. On 26/10, there was a slight increase in the volume which clubbed with other transaction being processed at same time, the DB reached its threshold on committing the DB transaction and started to slow down in processing these transactions, causing slowness in DC operations. 
",NA,Capacity constraint ,NA,11/21/2023,November,2023,Saloni,Closed,,,2023,10,October,9/16/2025,691,>60
10/25/2023,10/25/2023,89645456,12:14,12:35,00:21,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,,KI,Issues in contactless payments at stores,TCS,Network,,Payments,RC identified,66633,A spike was observed in the number of contactless transactions at 446 stores between 12:14 to 12:35.  Around 4.8k payment transactions were impacted. The root cause was attributed to the network traffic (SD WAN hub) failing over from Stockley park primary to secondary. ,Network team have been advised to perform any activity outside the business hours.,10/25/2023,10/25/2023,,,,No,No,No,Yes,Customer Tech change,NA,"There was a bug identified in the exisitng SDWAN hub and a fix was provided by vendor Fortigate to downgrade the device. During the activity,  the network traffic (SD WAN hub) had failed over from Stockley park primary to secondary, resulting in a blip - which is normal.",NA,Inefficient deployment plan,NA,11/21/2023,November,2023,Saloni,Closed,,,2023,10,October,9/16/2025,692,>60
10/22/2023,10/22/2023,89640418,23:00,0:30,01:30,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,KI,Multiple store applications inaccessible,TCS,Retail Applications,,Store Applications,RC identified,66506,"Multiple store applications like CSSM, E ticketing, SST, Intelligent waste etc were inaccessible between 23:00 and 00:30, due to a connection issue with internal load balancer IP during the AKS upgrade. The initial root cause has been attributed to an issue with Ngnix version and it was upgraded to resolve the issue. Detailed root cause investigations underway.",Actions are being tracked in the actions tab,10/22/2023,10/25/2023,,,,"Yes, No",No,No,Yes,Customer Tech change,NA,"In the process of carrying out the AKS upgrade, the IP address of the load balancer was very quickly reassigned to a newly spun up node which resulted in an IP conflict",NA,inadequate testing,https://confluence.marksandspencer.app/display/DS/Options+to+Bulk+Restart+Intune+Managed+Devices,12/25/2023,December,2023,Saloni,Closed,,,2023,10,October,9/16/2025,695,>60
10/21/2023,10/21/2023,89637079,11:40,12:20,00:40,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Issues with card payments at stores,Worldline,NA,,Card Transactions,RC identified,66609,"Store colleagues reported issues with both contactless and chip & pin payment transactions between 11:40 and 12:20, there was no impact to website transactions. The impact to stores payment transactions were quantified. The root cause has been attributed to a central issue at Worldline. Detailed root cause awaited from Worldline.","Below action, worldline has updated by 2nd Nov and awaiting further update from Worldline.
P2PE Decryption module auto recovery mechanism enhancement is being worked upon by WL
All actions based on our PIR are being tracked in Problem Actions tab",10/21/2023,10/27/2023,,,,"No, Yes",No,No,NA,3rd party issue,NA,"The issue was caused due to a threshold effect resulting from some latency in the decryption components response times, and triggering a snowball effect on such components, leading to an unforeseeable availability decrease of the decryption solution.",N/A,N/A,NA,2/29/2024,November,2023,Gokul,Closed,,,2023,10,October,9/16/2025,696,>60
10/18/2023,10/18/2023,89630816,00:45,7:20,06:35,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Issue with the Relex batch processing,Relex,NA,,Supplier orders,RC identified,66914,"Foods final orders for Relex lines were generated incorrectly impacting the RDC allocations and finalised supplier orders. There was a delay in the availability of 28 day order plan, Relex UI, OTS reports and commitment sheets. The root cause was attributed to human error resulting in a data issue in the overnight batch. Service were restored by correcting the data followed by rerunning the batch.",All actions are being tracked in Problem Actions tab,10/18/2023,10/18/2023,,,,"No,yes",No,Yes,No,3rd party issue,NA,"The root cause was attributed to human intervention resulting in a data issue in the overnight batch.


Relex is designed not to consider all dynamic overrides from the calculation and continue batch – this is because the quality of the calculation from Relex without overrides should be good enough to provide accurate store Allocations and supplier Orders.  However there should not have been intervention from the FOA support team (Accenture), given the design.",NA,NA,FOATR-19510,1/3/2024,January,2024,Rehan,Closed,,,2023,10,October,9/16/2025,699,>60
10/15/2023,10/15/2023,89624760,09:16,15:43,06:27,Customer Channels,Service Experience,Customer Channels,Selling Experience,,KI,Significant drop in 3rd party brands  Dropship products on website,TCS,OMS,,.com ordering ,RC identified,66353,Dotcom Business operations identified a 40% drop in 3rd party branded drop-ship products on the website between 09:16 and 15:43. This impacted 500 products resulting in potential loss in revenue and poor customer experience. The root cause has been attributed to the products being out of stock in the Order Management System (OMS) whilst the Bloomreach feeds were executed. Detailed rootcause investigations underway with vendor Bloomreach.,All actions are closed,10/15/2023,10/25/2023,,,,"No,yes",Yes,No ,No,Code/Product bug,No,"The issue is due to individual DS supplier setup configuration (Node notification and shipping calendar) over the weekend clashing with old piece of Sterling code from 2015. The impacted DS suppliers are non-Working over the weekend and this piece of code is ignoring the historic supplier stock, instead overriding the SOD file to ""0"" stock from these impacted suppliers",NA,,,2/26/2024,February,2024,Kavitha,Closed,.Com SM,,2023,10,October,9/16/2025,702,>60
10/11/2023,10/12/2023,89617346,08:55,9:00,24:05,GTS,Enterprise Integration,"Foods, Customer Channels","Foods Commercial Trading, Platform & Store Ops",,KI,Inaccuracy in 'dropped & locked' products promotional tickets,TCS,RET,,ECS,RC identified,TBC,Foods P&P colleagues identified inaccurate pricing information for x9 'dropped & locked' promotional products where tickets were displaying £7 for both the full and promotional price. The root cause was attributed to a combination of business process and a system design logic which prioritizes promotional prices over full price changes. A manual workaround was applied followed by triggering an adhoc ticketing batch for stores allowing them to print the corrected tickets on 12/10.,Next actions will be taken care within the product team,10/12/2023,10/12/2023,,,,No,No,Yes,No,Design issue,NA,The root cause was attributed to a combination of business process and a system design logic which prioritizes promotional prices over full price changes.,NA,Process gap,,11/21/2023,November,2023,Sravan,Closed,.Com SM,,2023,10,October,9/16/2025,706,>60
10/9/2023,10/10/2023,89613207,08:00,15:30,31:30,Infosec & Tech Risk,Infosec & Tech Risk,GTS,Digital Workplace Services,,KI,Some colleagues were unable to access O365 applications,TCS,Infosec,,O365,RC identified,66346,"Colleagues were unable to access the O365 applications like Outlook, teams etc from 08:00. This impacted 63 colleagues across the estate. The root cause was attributed to an issue during the overnight synch between Azure AD and One Identity on 06/10 resulting in revoking the user's o365 licenses. Services were restored by reinstating the impacted user accounts by 15:30 on 10/10. ",Actions are being tracked in the actions tab,10/9/2023,10/9/2023,,,,"No,yes",No,Yes,NA,Code/Product bug,NA,The root cause of the issue was that bad practices in the housekeeping of user ID’s over the years has led to the new application One Identity revoking some user accounts.,NA,Data issue,,11/21/2023,November,2023,Saloni,Closed,,,2023,10,October,9/16/2025,708,>60
10/7/2023,10/8/2023,89610270,18:30,8:32,14:02,GTS,Enterprise Technology Platform,Foods,Foods Supply Chain,,KI,Delay in Q UI availability and Pre-allocations due to hardware failure,TCS,Storage ,,Quantum,RC identified,66367,Alerting indicated that multiple jobs were overrunning due to a database connectivity issue in Quantum. This resulted in a delay to the Quantum user interface availability and RDC backup allocations. The root cause was attributed to a hardware failure on the virtual tape library. Services were restored by restarting the database followed by a storage uplift. ,All the actions are  tracked and closed under the Problem actions tab ,10/8/2023,10/8/2023,,,,"Yes,No",Yes,Yes,Yes,Infrastructure issue / Hardware failure,NA,"The root cause was attributed to a hardware failure on the virtual tape library. Services were restored by restarting the database followed by a temporary storage uplift. For the permanent fix, the faulty hardware has been replaced. ",NA,NA,NA,11/30/2023,November,2023,Pavithra,Closed,,,2023,10,October,9/16/2025,710,>60
10/7/2023,10/7/2023,89609334,04:42,12:21,07:39,Foods,Food Supply Chain,"Foods, Customer Channels","Foods Supply Chain, Platform & Store Ops",,KI,Frozen delivery messages not received into CSSM,GXO,NA,,CSSM,RC identified,66368,Alerting indicated that foods frozen delivery messages were not received into CSSM from 11:23 on 06/10 potentially impacting stock receipt at stores. An issue at GXO was resolved to restore services at 12:21. We await root cause and recovery details from GXO. Hypercare monitoring is in place.," Actions and next steps agreed with GXO
1. Agree with GXO on sending proactive comms to Customer for future scenarios. - RDC Network (used to be called GIST) and GXO (Bedworth Deport/IT) have stated they will send comms to us if they see issues.
2. Configure alerts at GXO to proactively identify these interface failures - They have said they have now configured the alerts they are able to, in that flow.",10/7/2023,10/7/2023,,,,"No,Yes",No,No,NA,3rd party issue,NA,The GXO interface which process the messages to OpenText was blocked due to a data issue in one of their customer file resulting in the issue. ,N/A,N/A,NA,11/1/2023,November,2023,Gokul,Closed,,,2023,10,October,9/16/2025,710,>60
10/4/2023,10/4/2023,89603749,11:00,14:48,03:48,C&H and Intl ,C&H & Intl Supply Chain,Customer Channels,Service Experience,,SI,Ecomm refund issue at Ollerton DC,TCS,WMS,,WMS,RC identified,66321,".com Ollerton DC refunds were not initiated from WMS to Sterling as the refund processing jobs were on hold due to a data transition activity between 22/09 and 25/09. On the day of the go live, due to an oversight it was not realised that the orders would move to an archive table after one day. After a successful sample of missing orders were tested, all pending return orders have been processed from WMS Ollerton and received in Sterling successfully. ",All actions are being tracked in Problem Actions tab,10/4/2023,10/9/2023,,,,"No,Yes ",No,No,NA,Human error - Customer Tech,yes,"The issue was caused due to a change where  the refund processing jobs were put on hold as part of the change  which had resulted in data archival. Though the job was released after the change, there was no proper documentation/step in the cutover plan to check on the archival logs. Once noticed, all the Refunds on the archive was re-processed. 

",CRQ000000188782,Human Error,,11/21/2023,November,2023,Kavitha,Closed,,,2023,10,October,9/16/2025,713,>60
10/2/2023,10/2/2023,89599990,15:07,19:45,04:38,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,Welham Green DC power outage causing wi-fi issues,National Grid,NA,,WMS,RC identified,66194,Welham Green DC were experiencing Wi-Fi connectivity issues between 15:07 and 19:45 impacting the site operations. Around 45k retail dispatches were impacted. The root cause has been attributed to an unplanned power outage at the site. A network switch was rebooted to restore the connectivity. Awaiting detailed root cause from vendor Juniper.,All actions are being tracked in Problem Actions tab,10/2/2023,10/2/2023,,,,"Yes, No",Yes,No,NA,3rd party issue,NA,The root cause has been attributed to an unplanned power outage at the site. A network switch was rebooted to restore the connectivity.,NA,NA,NA,10/10/2023,October,2023,Gokul,Closed,,,2023,10,October,9/16/2025,715,>60
9/30/2023,9/30/2023,TBC,04:00,10:17,06:17,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Drop in Relex supplier order volume,Relex,NA,,Supplier orders,RC identified,NA,"Foods supply chain colleagues highlighted a 25% drop in the  Relex supplier order volume impacting the RDC allocations and finalised supplier orders. Suppliers were advised to use previous day's 28 day order plan and PO amendments were made in OFP (One Foods Platform) to mitigate the impact. The root cause was attributed to a data issue resulting in overnight batch failure. Whilst recovering the batch failure, allocations were generated with duplicates which were removed by 10:00.",All actions are  tracked and closed. Please refer to the actions tab for more details,9/30/2023,9/30/2023,,,,"yes,No",No,No,No,3rd party issue,NA,There was an issue in the Relex batch overnight - a Dynamic override with an end date in the past (7th Sep) caused batch to fail. Intended date was 7th October – human error,NA,NA,NA,11/11/2023,November,2023,Pavithra,Closed,,,2023,9,September,9/16/2025,717,>60
9/27/2023,9/27/2023,"
89589952",01:02,4:44,03:42,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Payment issues causing an order outage,TCS,Payments,,Online orders,RC identified,66306,Alerting indicated that no orders have been placed on the website from 01:02.  Customers were unable to place orders on the website resulting in poor customer experience and potential loss of sales (circa 1000 orders).  Root cause attributed to a payment certificate expiry which was renewed to restore the services at 4:44.,All actions are being tracked in Problem Actions tab,9/27/2023,9/27/2023,,,,"Yes, No",Yes,No,Yes,Customer Tech change,CR,Root cause was attributed to a payment certificate expiry which was renewed to restore the services at 4:44.,CR#CM-10489,Human error,NA,11/6/2023,November,2023,Gokul,Closed,,,2023,9,September,9/16/2025,720,>60
9/24/2023,9/24/2023,89585266,13:00,15:30,00:29,Customer Channels,Service Experience,Customer Channels,"Selling Experience, Customer Engagement",,KI,Customer Charge Card & Budget Card orders stuck in WCS,Worldline,NA,,Online orders,RC identified,66302,Alerting indicated that orders paid using a Customer Budget or Charge card were stuck in WCS from 17:16. This resulted in cancellation of 307 orders with delivery date of 25/09 and delay in sending 8k order confirmation emails to customers. The root cause attributed to an issue at Worldline which was resolved by 14:30 on 24/09. Detailed root cause investigations underway.,All actions are being tracked in Problem Actions tab,9/24/2023,9/24/2023,,,,Yes,Yes,No,Yes,3rd party issue,NA,"The WL Charon KAS database supports the tokenisation and detokenisation process – in this instance, 
performance on the main nodes reduced due to a memory error (there are two main nodes with two 
secondary nodes, supported by multiple additional nodes), triggering a reaction to fail over to the secondary 
nodes. 
Due to a memory error, the performance of the main nodes on WL Charon DB has reduced and failed over to secondary but 
This should have been able seamlessly failover however the secondary nodes did not respond as 
expected and did not process the full load of traffic. This resulted in the process of detokenisation for 
Chargecard / Budgetcard transactions being interupted, and so the detokenisation did not occur. Meaning 
transactions appeared to be accepted to the customer but were not actually authorised because the 
detokenisation did not occur (this process happens after the point the customer has had the ‘order accepted’ 
message). 

The reason the secondary nodes did not respond as expected was due to the monitoring rule not indicating 
that both main nodes had an error and had failed over – so the secondary nodes were responding to the load 
of one main node, instead of two.",NA,NA,PFOUND-258,12/11/2023,December,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,723,>60
9/22/2023,9/23/2023,89582869,10:00,6:58,21:02,Group Platforms,Finance,"Group Platforms, Digital & Data","Finance, Digital & Data",,SI,Issue in POS sales flow in SAP POSDTA,TCS,SAP,,POSDTA,RC identified,66181,"As part of the hypercare after the SAP monthly release on 22/09, it was identified that POS sales message were not processing in POSDTA causing message pileups. This resulted in a late sales variance of £23.3M between RD vs BI along with a delay in the availability of latest daily sales report. The issue was caused due to a change introduced during the release which was reverted to restore services. ",All actions are tracked and closed  in Problem Actions tab,9/22/2023,9/29/2023,,,,"Yes,No",Yes,Yes,Yes,Customer tech change,Yes,"As a part of the SAP monthly release on 22/09, a change was introduced to address an existing issue of duplicate records insertion (combination of promotion lines with multiple discount types) in the POSDTA database tables causing message failures. After the change, the POS sales messages were not processed in POSDTA tables due to additional complex combination of promotion lines and discount types which was not tested in the lower environment.",CR-185894,Lack of the lower environment,NA,11/21/2023,November,2023,Pavithra,Closed,,,2023,9,September,9/16/2025,725,>60
9/22/2023,9/22/2023,89580465,15:30,7:00,15:30,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,MI,CSSM performance issues due to hardware failure,TCS,Wintel,,CSSM,RC identified,66293,"Stores colleagues reported slowness whilst accessing the CSSM applications impacting the store operations from 06:00. The root cause was attributed to a hardware failure on the ESXi host on 21/09. Services were restored after the data rebuild completion as a part of the automated recovery process at 09:10.
At 11:20, alerting indicated a delay in processing the POS sales messages in CSSM resulting in potential stock corruption. Store counting was blocked between 12:00 and 16:30. Services were restored by restarting the CSSM servers followed by a memory uplift. Detailed root cause investigations underway.",All actions are closed in the tracker,9/22/2023,10/6/2023,,,,Yes,Yes,"Yes, Yes",Yes,Infrastructure issue / Hardware failure,NA,"CSSM performance degradation:

A hardware failure was detected in the ESXI host which serves the CSSM application virtual machines on 21/09 and 15:30. As part of the automatic recovery process, the CSSM data was rebuilt into the alternate host. It is believed that due to an increased usage of the CSSM application due to the time of the day, the data rebuild took longer resulting in the overrunning of multiple jobs and performance issues in CSSM.

Note: The faulty hardware was attributed to a memory module and array controller failure in the infrastructure. The hardware was replaced by HP on 23/09.

Delay in POS message flow in CSSM – The initial root cause is believed to be due to an increase in memory usage resulting in system resource contention on the CSSM application servers.

05/10: It is believed that during the data replication in the CSSM database servers on the alternate host, whist the web servers continuously tried to connect to the database servers for processing the messages, it resulted in the spike in memory consumption causing the delay in the message flow. As a precaution, the CSSM web server memory is being uplifted on 08/10 from 6gb to 8gb to support PEAK.",NA,Hardware failure,NA,10/16/2023,October,2023,Saloni,Closed,,,2023,9,September,9/16/2025,725,>60
9/22/2023,9/22/2023,89582208,08:00,17:50,09:10,Customer Channels,Platform & Store Ops,Customer Channels,Service Experience,,KI,Multiple store tills  rebooted after Microsoft patching activity,TCS,Platform operation ,,POS,RC identified,66178,"Following the recent Microsoft patching activity, around 1600 store tills were rebooted during business hours causing inconvenience to store colleagues. The root cause was attributed to an inadvertent configuration set up during the patch installation which was corrected to fix the issue.  ",Internal PIR was organsied by the product teamand all the agreed actions are closed. ,9/22/2023,9/22/2023,,,,"No, yes",No,Yes,No ,Customer Tech change,No, The root cause was attributed to an inadvertent configuration set up during the patch installation which was corrected to fix the issue.  ,,Human error,,10/9/2023,October,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,725,>60
9/21/2023,9/21/2023,89580029,17:07,18:53,09:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Issues placing orders via old safari browser version,TCS,Onyx,,Online orders,RC identified,66177,Customers were unable to select the product size on the Product Details Page (PDP) on the Customer.com website from 11:29.  13% of customers using older version of Safari browsers on mobile web and desktop were unable to add a selected product to the basket. The issue was caused due to a code change at Onyx which was fixed to restore services at 18:53 on 21/09. Detailed root cause investigations underway.,All the agreed actions are closed ,9/21/2023,9/28/2023,,,,"No, yes",No ,No,Yes,Customer Tech change,No,"
A change was deployed on Onyx browse at 11:01am 20th September to resolve an exiting issue on the PDP pages. The change included a pattern to breakdown product details received from an API into an Array. Due to the incompatability issues between the Old Safari and the product breakdown patter, the sizing selector becoming either unresponsive or non-selectable and therefore the customer would have been unable to add to bag. ",,Code bug,,10/9/2023,October,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,726,>60
9/21/2023,9/21/2023,89579766,14:30,22:15,09:10,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Latency in order processing in WMS for Brands DC,TCS,WMS,,WMS,RC identified,66173,"Following the Brands sale launch, Brands DC reported latency whilst processing the ecomm orders in WMS from 14:30. Around 130 singles had missed customer promises and the order proposition was moved to 24 hours. The root cause was attributed to an order merge code in the WMS layer resulting in inefficient database indexing. The performance was improved by optimizing the code at 22:30. ",All actions are  tracked  and closed in Problem Actions tab,9/21/2023,10/6/2023,,,,"No,No",No,No,NA,Customer Tech change,NA,"As part of the Sports Edit packaging change for Brands on 14/08, the order daemon was performing a full database table scan (order merge rule) while processing the orders from interface table to the main tables within the WMS layer there was a Brands sales launch on 21/09 and the order daemon was facing a spike in order volume for the first time after the change, resulting in performance issues.",NA,NA,NA,11/11/2023,November,2023,Pavithra,Closed,,,2023,9,September,9/16/2025,726,>60
9/19/2023,9/19/2023,89575205,08:28,9:03,00:29,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Spike in contactless payment refusals,Worldline,NA,,Contactless transactions,RC identified,66172,Alerting indicated spikes in the contactless payment refusals (both in store and online) between 08:28 & 08:41 and 08:47 & 09:03. This impacted 4730 in-store transactions with 70% on the mobile device and 300 online orders. The root cause was attributed to a central network issue at Worldline. Detailed root cause investigations underway.,,9/19/2023,9/19/2023,,,,Yes,Yes,No,NA,3rd party issue,NA,The issue was caused due to an influx of traffic posted by one of the ISP at Worldline - Verixi impacting their services,NA,NA,NA,10/7/2023,October,2023,Sravan,Closed,,,2023,9,September,9/16/2025,728,>60
9/18/2023,9/18/2023,89574094,16:20,17:18,00:58,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Drop in orders on website,TCS,WCS,,IOS & Android Apps,RC identified,66287,"Alerting indicated a drop in online orders (both IOS & Android Apps) and slowness in application like Sparks, Onyx etc. This resulted in a potential revenue loss of £26.8k and in store applications like Assist and Sparks were also impacted. The root cause was attributed to an increase in the rate limit (20k/min) for requests coming from the Customer Session API. Services were restored by reducing the rate limit (4k/min) by 17:18. Detailed root cause investigations underway.",,9/18/2023,9/22/2023,,,,yes,yes,yes,no ,Customer Tech change,No,"RC was a combination of factors causing App push notification (to iOS and Android apps). The slowdown/order drop occurred because there was an excessive number of API 
requests which were not handled properly.
Change in rate limit - Performance test was not done after a rate limit change to 20k/min  
Increase in push notification size - First time 500K was sent along with rate limit of 20k/min
Running on one hall/half capacity because of WCS deployment
Android App sending excessive number of requests - as it was not handling 500 requests correctly and kept retrying",,,AMNS-11445,11/8/2023,November,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,729,>60
9/11/2023,9/12/2023,89560574,16:32,12:32,19:00,Group Platforms,Finance,"Group Platforms, Foods","Finance, Foods Commercial Trading",,SI,Stock discrepancy of £24m affecting supplier payments,TCS,Foods Supply chain,,GIST Sites,RC identified,66147,"Finance business colleagues highlighted a £24M stock discrepancy across the GIST sites due to missing goods receipts in SAP. This was impacting the financial stock accounting and supplier payments, no impact to stock movement or allocation. The issue was caused due to a change resulting in UPC formatting mismatch whilst processing the GR messages into SAP. A permanent fix was implemented on 12/09 and the recovery actions have been completed to mitigate the stock discrepancy.",Actions are being tracked in the actions tab,9/10/2023,9/21/2023,,,,,,,,Customer Tech change,CR,The issue was caused due to a change resulting in UPC formatting mismatch whilst processing the GR messages into SAP,CR#186577,inadequate testing,NA,10/9/2023,October,2023,Gokul,Closed,,,2023,9,September,9/16/2025,736,>60
9/10/2023,9/11/2023,89558638,02:30,11:44,33:14,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Delay in Relex overnight batch,Relex,NA,,"FOA, FFO, 28 day Order plan, OTS reports, commitment sheets, CSSM SST",RC identified,66137,"Foods Final orders were delayed to suppliers on 10/09 and 11/09 due to an issue in the overnight Relex batch. The 28-day order plan of 09/09 was used for Relex products to generate the supplier orders for both days. The availability of the 28-day order plan, OTS Reporting & Commitment sheets are also delayed. The root cause was attributed to a bug within the Storage Grid at Relex (managed by their vendor NetApp). The overnight batch completed successfully on 12/09 and the supplier orders and allocations were generated without any issues. Relex and Accenture teams continue to investigate further on the root cause.",,9/11/2023,9/13/2023,,,,Yes,Yes,Yes,Yes,3rd party issue,NA,"The distributed storage platform (Storage Grid) at Relex consists of 38 database nodes. One of the service responsible for replicating the data across the nodes according was stuck on one of the 38 nodes on 30/08. The stuck service exhausted available nodes by 6:00am BST 9/9, which resulted in the performance/latency in RELEX system.",NA,NA,NA,11/10/2023,November,2023,Saloni,Closed,,,2023,9,September,9/16/2025,737,>60
9/9/2023,9/10/2023,89557480,02:00,10:30,32:30,Foods,Food Supply Chain,"Foods, Customer Channels","Foods Supply Chain, Platform & Store Ops",,KI,Delay in frozen delivery receipt from GXO to CSSM,GXO,NA,,Bedworth (Ultima),RC identified,66139,"Alerting indicated that foods frozen deliveries were not received between 2:00 and 11:02, potentially impacting the stock receipt for 3 stores. The root cause was attributed to a central network issue at GXO impacting the warehousing management system at Bedworth (Ultima). The Ultima system was restarted, and the message flow was resumed to send the systemic frozen deliveries to CSSM. ",All actions are  tracked and closed in Problem Actions tab,9/9/2023,9/19/2023,,,,"yes,No",yes,No,NA,3rd party issue,NA,"The root cause was attributed to a central network issue at GXO due to a bug in their Infrastructure, impacting the warehousing management system at Bedworth (Ultima). ",NA,NA,NA,3/12/2024,March,2024,Rehan,Closed,,,2023,9,September,9/16/2025,738,>60
9/7/2023,9/7/2023,89553489,07:00,8:25,01:25,GTS,Enterprise Technology Platform,Foods,Foods Supply Chain,,KI,Multiple Foods applications were unavailable.,TCS,Network,,"ASO,SCRD,ORCA,OFP",RC identified,66120,"Multiple applications like ASO, SCRD, ORCA and OFP were unavailable between 07:00 to  08:25 impacting the colleague's ability to access these applications. Also the RDC allocations from ASO to GIST were impacted. The issue was caused due to a network change which was reverted to restore services. Detailed root cause analysis underway.",Actions are being tracked in the actions tab,9/5/2023,10/31/2023,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,"To reduce the attacks and vulnerability issues, a network change was deployed to add routes in the production foods route tables thus allowing the traffic to go via either the azure firewall or PA firewall based in the flow which resulted in the issue. The root cause attributed to inadequate testing.",187888,inadequate testing,NA,10/31/2023,October,2023,Gokul,Closed,,,2023,9,September,9/16/2025,740,>60
9/6/2023,9/6/2023,89551822,08:24,8:29,00:05,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Spike in contactless payment refusals,Worldline,NA,,Contactless transactions,RC identified,66221,"A spike was observed in the contactless payment refusals between 08:24 and 08:29 impacting the contactless transactions at stores. Also, 361 online orders had failed, out of which 341 orders were reprocessed successfully. We await detailed root cause from Worldline.","Below actions have been agreed and this will be driven by product teams

Align management over sight of any maintenance mode activation even for standard automated updates – this will ensure an additional check is in place before updates can progress to the next phase - Retraining of AHUB technical support team to underline the importance of pre-checks before any standard updates or future changes, to ensure the environment is stable and healthy. - Prepare and recommunicate the checklist for all AHUB components health status. - For all non-automated update steps, create a shared document with a checklist including highlighting all check points where additional checks and approvals are required before moving forward.",12/9/2023,12/9/2023,,,,Yes,Yes,NA,Yes,3rd party issue,NA,"The incident occurrred due to an error in first phase of standard AHUB update operation. The operation was to completed in two steps. After the automated tasks in the first step were completed, the final task to disable the maintenance mode was missed on the load balancer instance 1 from DC1. This led to the second phase - memory upgrade - failing and causing service impact. This incident was due to a unique user error – the user who was overseeing the standard automated operation missed the final step to disable maintenance mode for Load balancer 1 in DC1 and so when the operation moved to the second phase, transactions could not flow through load balancer 1 and so services were impacted.",NA,NA,NA,9/10/2023,September,2023,Sravan,Closed,,,2023,9,September,9/16/2025,741,>60
9/6/2023,9/6/2023,89551947,07:15,9:10,01:55,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Issue with SparksPay,TCS,Sparks Journey,,Sparks,RC identified,66121,"Customers were encountering an error on the 'Sparkspay' page between 07:15 and 09:10 impacting the existing customers to settle/manage their account balance. Also, new customers were unable to apply for a Sparkspay account. The issue was caused due to a miss in the certificate renewal process. Service was restored by updating the new certificate.",All the actions are being tracked under Problem actions section ,9/6/2023,9/13/2023,,,,"No,yes",No,No ,Yes,Customer tech change,Yes,"The new certificate was missing host header for one of the prod regions in the SAN (Subject Alternative Names) value. Prod consists of two domains fesk-sparkspay-prod1.mac.apps.mnscorp.net  and fesk-sparkspay-prod.mac.apps.mnscorp.net.  However, the new certificate was generated only with fesk-sparkspay-prod1.mac.apps.mnscorp.net due to which the sparkspay page was routing to ‘Sorry Something went wrong’.  There was a communication gap due to which team had missed to update the both the domains in SAN values  while requesting for new certificate instead they had included only one domain. Akamai requires both the domains for routing.  ",CM-10489,Communication gap,WALLET-229,11/27/2023,November,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,741,>60
9/5/2023,9/5/2023,89550056,12:38,14:15,01:37,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,POS back office unavailable,TCS,POS 2nd Line Support Beanstore,,POS Backoffice,RC identified,66224,"Store colleagues were unable to access POS back office between 12:38 and 14:15 impacting their ability to acknowledge the red alert product removal.  Also, stores were unable to create new accounts, carry out till password resets, create or amend product restrictions etc with no impact to trade. The issue was caused due to multiple long running queries  causing system resource contention on the back office database. Services were restored by optimising the database query followed by application cluster restart.",Actions are  tracked and closed  in the actions tab,9/5/2023,9/6/2023,,,,"Yes,No",Yes,yes,Yes,Database issue ,NA,There were multiple long running queries (red alert related) causing contention on the POS back-office database leading to the CPU spiking to 100%.  A hint was added to make the data extraction for the queries more efficient and the POS back-office cluster was restarted.,NA,NA,NA,10/30/2023,October,2023,Pavithra,Closed,,,2023,9,September,9/16/2025,742,>60
9/2/2023,9/2/2023,89545728,19:32,20:40,01:08,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Timeout issue with Accertify,Accertify,NA,,order fulfilment,RC identified,65908,"From 19:32, alerting indicated timeout errors during customer order journey whilst transactions being authenticated by Accertify (Fraud Prevention Supplier). This impacted 3.3k orders, the order confirmation emails to customers and orders to fulfilment partners were delayed. Also 265 orders were cancelled due to payment retries. A workaround was applied to remediate the immediate impact by 20:40. The root cause has been attributed to an issue at Accertify which was resolved by 20:48 and we await further updates.",All the agreed actions are closed ,9/8/2023,9/8/2023,,,,yes,yes,yes,NA,3rd party issue,No,The root cause has been attributed to an issue at Accertify which was resolved by 20:48 and we await further updates.,,,,10/6/2023,October,2023,Kavitha,Closed,,,2023,9,September,9/16/2025,745,>60
9/2/2023,9/2/2023,89548390,05:18,8:00,02:42,GTS,Digital Workplace Services,Customer Channels,Platform & Store Ops ,,SI,Issue in the CSSM Foods reporting,TCS,Active Directory,,CSSM,RC identified,66107,Alerting indicated authentication errors between the CSSM OLTP (Online Transaction Processing) and CSSM Foods reporting servers between 03:00 and 08:00 resulting in report replication failures. Store colleagues were unable to view  the latest foods delivery adjustments and stock data in the reports impacting gap scan and target counts. The issue was caused due to a change in the application service account name responsible for active directory authentication. Services were restored by reinstating the service account. Detailed root cause investigations underway.,Actions are being tracked in the actions tab,9/2/2023,9/2/2023,,,,"Yes, No",Yes,No,Yes,Human error - Customer Tech,NA,The issue was caused due to a change in the application service account name responsible for active directory authentication. Services were restored by reinstating the service account.,NA,NA,NA,9/28/2023,September,2023,Gokul,Closed,,,2023,9,September,9/16/2025,745,>60
8/31/2023,8/31/2023,CR#186332,16:00,16:30,00:30,Digital & Data,Data,Digital & Data,Digital & Data,,KI,Issues with the FAQT reports,TCS,Big Data Support,,FAQT,RC identified,186332,"After the FAQT (Foods Analytics Query tool) decommission activity on 31/08, 4 franchise colleagues had lost access to FAQT universe impacting their ability to refresh the self-managed reports. User access was reinstated to fix the issue. The FAQT migration team will provide appropriate guidelines to the stakeholders to prevent the recurrence of the issue. ",No pending actions.,8/31/2023,8/31/2023,,,,No,No,Yes,NA,Customer Tech change,Yes,"After the FAQT (Foods Analytics Query tool) decommission activity on 31/08, The impacted users are not regular FAQT users and they are not aware of  report migration process to BEAM because of which the users where unable to refresh the report in older FAQT universe ( business layout)",186332,Knowledge gap,NA,9/3/2023,September,2023,Kavitha,Closed,,,2023,8,August,9/16/2025,747,>60
8/30/2023,8/31/2023,TBC,09:26,15:35,29:09,Customer Channels,Platform & Store Ops,"Foods, Customer Channels","Foods Commercial Trading, Platform & Store Ops",,KI,P&P price changes reflecting incorrectly in the tills,TCS,Promotion Team,,"POS,ECS",RC identified,NA,Foods P&P (pricing and promotion) colleagues highlighted that the price changes for 26 UPCs were reflecting incorrectly in the tills resulting in potential illegal trading.  The issue was caused due to an user error as duplicate promotion activations and deactivation were caried out on the same UPCs on 22/08.  The immediate impact was mitigated with a workaround in POS (tills) and promotional prices were retriggered to reflect correctly in the tills.,Actions have been agreed to discuss with the Business team on how to prevent the duplication. No  other pending actions,8/30/2023,8/31/2023,,,,No,No,Yes,NA,Human error - Business,N/A,The issue was caused due duplicate promotions that were created and deactivated for the same UPCs on 22/08 by an user,N/A,N/A,N/A,9/6/2023,September,2023,Pavithra,Closed,,,2023,8,August,9/16/2025,748,>60
8/26/2023,8/26/2023,89527560,08:42,12:45,04:03,Group Support,HR,Group Platforms,HR,,KI,My HRe application inaccessible,Sdworx,NA,,HRe,RC identified,66005,"My HRe application was inaccessible between 08:42 to 12:45 impacting colleague's ability to view their payslips. The issue was caused due to a problematic network component at Sdworx which was recovered, awaiting root cause details from Sdworx. ","networking firewall was down and SdWorx Service Provider, Navisite worked on applying the fix
on the primary firewalls. The technical team have applied the fix on the secondary firewall as well.",8/26/2023,8/26/2023,,,,No,No,No,NA,3rd party issue,NA,The issue was caused due to a problematic network component at Sdworx which was recovered,NA,NA,NA,10/25/2023,October,2023,Gokul,Closed,,,2023,8,August,9/16/2025,752,>60
8/24/2023,8/30/2023,89524064,09:00,12:00,147:00,GTS,Enterprise Technology Platform,Customer Channels,"Platform & Store Ops, Service Experience",,MI,Intermittent network connectivity issues at multiple BT hosted stores,BT,NA,,Store Network,RC unknown,66003,"Since 9 am, few stores hosted in BT (British Telecom) infrastructure were experiencing intermittent network connectivity issues impacting the contactless payments and store operations.  Connectivity has been restored for all the impacted 36 stores by rebooting the in-store router.  No further issues observed since 25/08.  Detailed Root cause analysis underway. ",We await detailed root cause from the vendor ,,,,,,"yes,No",yes,yes,No,RC unknown,NA,Root cause inconclusive.,NA,NA,NA,10/20/2023,October,2023,Saloni,Closed,,,2023,8,August,9/16/2025,754,>60
8/23/2023,8/23/2023,89521783,10:36,13:45,03:09,Group Platforms,HR,Group Platforms,HR,,SI,MyHR application was inaccessible,Oracle,NA,,MyHR,RC identified,65456,"MyHR application was inaccessible between 10:36 and 13:45 impacting the colleague's ability to raise service requests, holiday requests, update personal/bank details. Also, line managers would have been unable to record promotions, new hires, leavers etc. The issue was caused due to a planned outage within the Oracle cloud environment, awaiting further details from Oracle vendor ",Preventive actions have been agreed by the Vendor,8/23/2023,8/25/2023,,,,No,No,No,NA,3rd party issue,N/A,"Oracle determined that the application services were impacted after a planned maintenance activity. Mitigation of the issue occurred by rolling back the change
Actions have been agreed by Oracle to check to better understand the various parts of this event and determine how to make further changes to improve our services and processes.",N/A,N/A,N/A,8/30/2023,August,2023,Pavithra,Closed,,,2023,8,August,9/16/2025,755,>60
8/18/2023,8/18/2023,89511346,08:36,9:00,00:24,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Issues with Pricing & Promotions,TCS,RET team,,Shelf edge tickets,RC identified,"70624
 ","Foods Pricing & Promotion colleagues reported that ~ 300 tickets had been re-batched to stores without any actual promotional changes, resulting in inconvenience to the store colleagues. The root cause has been attributed to a design limitation at ECS",Preventive actions have been agreed by the team,8/18/2023,8/21/2023,,,,"No,yes",No,No,No,3rd party issue,N/A,"Multipe 'design' related changes had been made for 83 promotions last week, and the suppression box was left 'unchecked' - As a result, ALL of these promotions batched and were sent to store as the changes recorded a new hash but were not suppressed. The root cause attributed to a design limitation in ES to send prices to POS.",N/A,N/A,N/A,8/30/2023,August,2023,Pavithra,Closed,,,2023,8,August,9/16/2025,760,>60
8/16/2023,8/16/2023,89509453,07:30,8:44,01:14,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Store colleagues unable to login to the Customer bank application.,Customer Bank ,NA,,Travel money app,RC identified,66101,"Store colleagues reported issues whilst logging into the Click & Collect app for foreign exchange on the honeywells between 07:30 and 08:44. Potentially, colleagues were unable to assist customers to exchange their currency (GBP) via the Travel Money app. The root cause has been attributed to a server issue at Essiell - third party supplier to Customer bank. We await detailed root cause.",No further details obtained from Vendor,8/16/2023,NA,,,,NA,NA,NA,Yes,3rd party change,NA, The root cause has been attributed to a server issue encountered after a planned change at Essiell - third party supplier to Customer bank.,NA,NA,NA,10/24/2023,October,2023,Gokul,Closed,,,2023,8,August,9/16/2025,762,>60
8/15/2023,8/28/2023,89505033,11:30,17:00,221:30,Foods,Foods Commercial Trading ,Foods,Foods Commercial Trading ,,SI,FIND application is inaccessible,FIND,NA,,FIND,RC identified,66007,"FIND application is inaccessible from 11:30 15th August impacting foods colleague's ability to perform product lifecycle management activities and product launches, however, a manual workaround is in place.  The issue has been caused due to a cyber incident within the Trace One infrastructure which is currently being investigated and we await updates.",NA,8/16/2023,8/16/2023,,,,No,No,"Yes, Yes",Yes,3rd party issue,NA,The issue was cause due to a cyber attack at TraceOne,NA,Cyber attack,NA,9/25/2023,September,2023,Saloni ,Closed,,,2023,8,August,9/16/2025,763,>60
8/11/2023,8/11/2023,89496762,08:45,9:42,00:57,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Store tills offline while processing refunds.,TCS,POS,,POS,RC Identified,65427,"Some store tills were showing offline while processing the refund transactions between 07:30 and 09:42. There was no impact to trading, however, this had resulted in inconvenience to the store colleagues. The root cause attributed to an issue in one of the webserver services which was manually restarted to restore services. Detailed root cause investigations underway.",All actions are closed,8/11/2023,8/11/2023,,,,"No, yes",No,No ,Yes,Capacity constraint ,NA,The tills were showing offline as one of the POS IIS servers (Internet Information service – an extensible webserver software for the Windows server) had stopped running due to a memory constraint. This was caused due to the simultaneous execution of the Defender updates at 03:07 on the impacted server and the daily TSM (Tivoli Store Manager) system backup at 03:00.,NA,Capacity constraint ,RPOS-10222,10/16/2023,October,2023,Saloni,Closed,,,2023,8,August,9/16/2025,767,>60
8/9/2023,8/9/2023,89507893,12:56,14:46,01:50,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Store Finder not working.,Microsoft ,NA,,Store finder app on the .com website,RC identified,,"Alerting indicated errors in the Store finder service in Desktop, mobile web, and Android app between 12:56 and 14:46, iOS was unaffected. Customers were unable to find a product in store via PDP and select a store for delivery at checkout resulting in a small drop in orders. The root cause was attributed to a central issue at Microsoft.  ","In order to stop issues like this from re-occurring, the Microsoft Maps Platform & Operations teams are
focusing their efforts on the following action items:
• Additional monitoring for excessive customer traffic and more efficiently rate limiting on Bing Maps
Platform side - Implemented
• Move Bing Maps Platform front end to Azure for greater capacity and capabilities to throttle large
volumes of usage – The product team has set up a health check which checks whether the API is up or down",8/9/2023,8/9/2023,,,,Yes,Yes,No ,NA,3rd party issue,Na,"Microsoft have confirmed that Root Cause was determined to be excessive usage that was immediately blocked. Automatic mitigation doubled the capacity at the
remaining DCs and, once the API requests to the overloaded DCs came back to normal levels, traffic was returned to those endpoints",NA,NA,NA,11/23/2023,November,2023,Sravan,Closed,,,2023,8,August,9/16/2025,769,>60
8/3/2023,8/3/2023,89481611,11:30,12:44,01:14,Customer Channels,Service Experience,Customer Channels,"Platform and Store Ops, Service Experience, Selling Experience",,MI,Multiple drop in orders during August Sale launch,Akamai,NA,,Customer .com website,RC identified,65431,"Between 11:30 - 12:44, multiple stores and website applications were impacted due to a Palo Alto (PA) firewall data plane reaching its 100% utilization. There was a drop in orders, increase in errors on the website and loyalty checkout offers resulting in poor customer experience. This also impacted in-store applications like Assist, parcel scan and C&H returns. The root cause is being investigated by vendors Palo Alto, Microsoft and hypercare monitoring is in place.",Actions are tracked and closed. Kindly refer to the Actoins tab for more details,8/4/2023,8/8/2023,,,,Yes,Yes,No,No,3rd party issue,NO,"The cause of the issue is ascertained due to high CPU utilization (100%) and high network bandwidth on the firewall as Akamai was having issues in connecting to WCS creating additional sessions (retry sessions) causing high traffic from Akamai to WCS.

This increase was observed between the OMS Cluster Worker Nodes and the Azure Kubernetes Master Node (Supported by Microsoft).  The Root cause on the spike is inconclusive, however Palo Alto had provided few recommendations which has been applied after which the environment remains stable",NA,NA,CM-10809,11/1/2023,November,2023,Pavithra,Closed,,,2023,8,August,9/16/2025,775,>60
7/25/2023,7/26/2023,89462143,07:13,7:45,24:32,Group Platforms,HR,Group Platforms,HR,,SI,Performance issues in the Time & Attendance application,Blue Yonder,NA,,T&A,RC identified,65328,"From 07:30, store colleagues reported performance issues while accessing the Time & Attendance application impacting their ability to apply leaves for year 24/25 and enter timesheet and overtimes. The root cause attributed to an increase in the application usage resulting in system resource constraints. Blue Yonder had uplifted the CPU and memory on the application and database servers to resolve the issue by 07:00 on 26/07.",All actions are being tracked in Problem Actions tab,7/25/2023,8/3/2023,,,,"No, yes",No,No,Yes,3rd party issue,NA,The root cause attributed to an increase in the application usage resulting in system resource constraints,NA,Configuration issue,N/A,9/27/2023,September,2023,Gokul,Closed,,,2023,7,July,9/16/2025,784,>60
7/23/2023,7/23/2023,89459808,11:40,19:30,07:50,GTS,Enterprise Technology Platform,All BU's,All BU's,,KI,Customer outbound messages were piled up in Cloud B2B middleware layer,TCS,Cloud Frameworks,,"Costa,  Foods ASN flow, ISF flow ",RC unknown,65307,"Message pile up was observed in Cloud shared gateway between 11:40 and 19:30. This impacted various third party flows across portfolios - Foods ASN, Costa orders, Flamingo flower orders etc. The issue was due to the Cloud shared gateway channel being in retrying state causing 85k message pile ups and it was restarted to restore the services. Detailed root cause investigations underway. ",Actions are being tracked in the actions tab,7/23/2023,7/23/2023,,,,"No, yes",no ,No,Yes,RC unknown,No,A potential network connectivity issue between the cloud B2B queue managers impacted the flow of messages resulting in the shared gateway channel to go into retry state. The reason behind the network connectivity issue is being investigated. ,,RC unknown,,9/12/2023,September,2023,Kavitha,Closed,,,2023,7,July,9/16/2025,786,>60
7/22/2023,7/22/2023,89458208,17:03,21:55,04:52,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,"	Castle Donington WCS services were unavailable between 17:03 and 21:55 on 22nd July",TCS/SSI,Network/SSI,,WCS ,RC unknown,65306,Castle Donington WCS services were unavailable between 17:03 and 21:55 impacting the picking capability for 54k singles. 1572 orders were miss-promised and the global proposition for Click & Collect and Nominated Day Delivery orders was moved by 24 hours. The root cause was a network switch failure due to power issue at the site resulting in the unavailability of the WCS services. Services were restored by a restart of the WCS services.,Actions are being tracked in the actions tab,7/22/2023,8/3/2023,,,,Yes,Yes,No ,Yes,RC unknown,No,"Issue1: Reboot of multiple switches. 
 Around 32 switches across 7 cabinets and 3 chambers were rebooted due to a possible power issue at the site. However, the root cause could not be ascertained due to insufficient logs, City FM continues to investigate further. 

Issue 2:  WCS picking service unavailable.
Once the network switches were functional, WCS application services were failed over from application server 2 to  server 1. As the WCS debugging log was enabled it resulted in system capacity constraints impacting the recovery of WCS picking services. Also, after the reboot of application server 2, both the virtual IPs were pointing to one server instead of being load balanced preventing the WCS services to be available. ",,RC unknown,,9/26/2023,September,2023,Kavitha,Closed,,,2023,7,July,9/16/2025,787,>60
7/19/2023,7/20/2023,89449493,09:29,8:42,23:13,GTS,Enterprise Technology Platform,Customer Channels," Service Experience, Selling Experience",,MI,Tills are offline across the estate,TCS,POS,,POS,RC identified,65296,"Store tills were trading offline across the estate followed by a patching activity on POS (Point of Sale) infrastructure impacting the POS sales flow from 06:05. The following were affected -  Refunds, gift card transactions, stock counting, instore online order payments, understated POS sales figures in various reports etc. A manual workaround was applied to mitigate the impact to supplier orders and NDC allocation. The issue was caused due to a database service being incorrectly enabled resulting in the CPU utilisation spike on the application servers. Services were restored by disabling the database service at 08:52, 20/07. All the pending transactions were processed out of POS by 16:53, 20/07. A PIR has been planned for 24/07.",All actions are closed,7/19/2023,7/25/2023,,,,"No, Yes",No,No,Yes,Capacity constraint ,NA,"After the wintel security patching on the POS Beanstore servers on 19/07, a reboot was performed which automatically enabled one of the database services – SQL server analysis, resulting in 100% utilization of the system resources, causing the issue. As per the design, this service reserves 20% of memory by default and the other SQL processes use more than 80% which resulted in memory contention on the servers and the operating system went into a hung state. It has been confirmed that the cause behind this service to be enabled automatically after a restart could not be ascertained and no further actions can be carried out to find the reason.",NA,Capacity constraint ,NA,10/10/2023,October,2023,Saloni,Closed,,,2023,7,July,9/16/2025,790,>60
7/18/2023,7/19/2023,89453324,11:45,14:33,26:58,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Customers unable to place order on iOS App,OneTrust,NA,,iOS app,RC identified,65300,"From 11:45, alerting indicated that customers were unable to place orders using iOS App, the Android App, mweb and desktop remains unaffected. This impacted 2600 customers, however 60% of the customers had retried to place the orders in other ways. The issue was caused due to a change on the iOS App related to a cookie consent via One Trust. A fix was implemented to restore services by 14:33, 19/07",Most of the actions are part of future enhancement and being tracked by Customer in the confluence page With the confluence page https://confluence.marksandspencer.app/pages/viewpage.action?spaceKey=DS&title=Blameless+review%3A+iOS+mweb+sev2,7/18/2023,7/31/2023,,,,"Yes, No",Yes,No,Yes,3rd party change,NA,"iOS App v8.3 had a query parameter change relating to cookie consent (One Trust). However, the query parameter was not working in the correct way and it resulted in a ‘sorry’ error page being presented to customers.",NA,NA,NA,10/18/2023,October,2023,Gokul,Closed,,,2023,7,July,9/16/2025,791,>60
7/15/2023,7/16/2023,89441651,09:15,18:35,09:20,Customer Channels,Service Experience,Customer Channels," Service Experience, Selling Experience",,SI,Delay in .com customer orders and order flow to the fulfilment partners,TCS,order fulfilment,,order fulfilment,RC identified,65279,"From 09:15, alerting indicated timeout errors during customer order journey whilst transactions being authenticated by Accertify (Fraud Prevention Supplier) . There was a delay to the order placement and 4.6K order confirmation emails to customers, sending 8k orders to the fulfilment partners and 2518 orders were cancelled due to token expiry, insufficient funds resulting in poor customer experience. A change at Accertify on their datastorage had caused the issue and services were restored by 10:00 on 16/07.",Actions are being tracked in the actions tab,7/15/2023,7/18/2023,,,,Yes,Yes,"Yes, Yes",Yes,3rd party change,Yes,"Accertify during the scheduled maintenance to upgrade its storage environment, there was a challenge in bringing the storage components back. This forced the Interceptas, ADI, and the Accertify Device platform to shut down due to the dependence on storage infrastructure to house virtual backend stores for the virtual machines that power the Accertify platform.
Eventually requiring a redeployment of infrastructure components to their previous version and restarting all of the virtual machines restored the services.",,Infrastructure issues,,10/6/2023,October,2023,Kavitha,Closed,,,2023,7,July,9/16/2025,794,>60
7/14/2023,7/14/2023,89440261,14:45,16:45,02:00,Customer Channels,Customer Engagement ,Customer Channels,"Customer Engagement, Selling Experience",,KI,Errors on Product Listing Pages (PLP) and Search Results Pages (SRP) in Customer .com,TCS,.COM ordering ,,.com ordering ,RC unknown,65169,"Alerts indicated errors on Product Listing Pages (PLP) and Search Results Pages (SRP) whilst accessing the Customer.com website between 14:45 and 16:45. Customers would have experienced errors/slowness while browsing and searching, however there was no drop in orders. Bloomreach (search provider) had identified an increase in errors while processing an index feed file from Customer resulting in an increase in traffic and the capacity was increased to restore services.",Actions are being tracked in the actions tab,7/14/2023,7/24/2023,,,,Yes,Yes,No,Yes,RC unknown,No," Bloomreach identified that Customer was sending multi-term queries along with BAU traffic resulting in high latency and an increased error rate. This caused  Bloomreach to limit Customer traffic with a 429 rate limit response.
Investigations revealed that the queries were client-side requests to GQL-Mesh and therefore unable to obtain the IP address to identify the source of these requests. The requests did all share the same user-agent (Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36)
 As the instance is hosted on a shared cluster with other clients within Bloomreach Infrastructure, the increase in traffic had a knock-on impact resulting in experiencing performance degradation.",NA,NA,,9/12/2023,September,2023,Kavitha,Closed,,,2023,7,July,9/16/2025,795,>60
7/14/2023,7/15/2023,89441018,00:00,7:21,31:21,Customer Channels,Customer Engagement ,Customer Channels,"Customer Engagement, Selling Experience",,KI,Customers unable to apply for Sparks Pay,TCS,Sparks Journey,,Sparks,RC identified,65277,"Customers were unable to apply for Sparks Pay from 00:00 impacting 25 newly opened accounts, no impact to existing customers. The issue was caused due to a change at HSBC which was reverted to restore services by 07:21, 15/07",The agreed action  have been closed ,7/14/2023,7/31/2023,,,,Yes,Yes,No,Yes,3rd party change,Yes,"There was a project to reduce fraud on Customer credit cards. One of the changes relates to the new account generation logic , instead of the numbers being sequential, the account number is randomised. However, the bin number that is used has a different numbering format to that which Ingenico use on their card system. Everything works ok until the tokenisation request, because Ingenico do not have the new range, the journey was failing. To restore service the change which was done by HSBC was rolled back.",NA,inadequate testing,WALLET-229,11/9/2023,November,2023,Kavitha,Closed,,,2023,7,July,9/16/2025,795,>60
7/14/2023,7/14/2023,89439216,07:45,9:20,01:35,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,KI,Stores unable to connect the honeywells and printers to the wireless network,TCS,Network,,Honeywell,RC identified,65162,Store colleagues reported that Honeywells and MTP (mobile ticket printers) were not connecting to the wireless network between 07:45 and 09:20. This resulted in an operational impact across 18 SDWAN migrated stores. A change to upgrade the Stockley firewall firmware version from 7.0.7 to 7.0.12  which was intended to fix a memory issue on the firewall device was reverted to restore the services.,A change to upgrade the Stockley firewall firmware version from 7.0.7 to 7.0.12  which was intended to fix a memory issue on the firewall device was reverted to restore the services.,7/14/2023,7/14/2023,,,,Yes,No,"Yes, Yes",No,Customer Tech change,Yes,A change to upgrade the Stockley firewall firmware version from 7.0.7 to 7.0.12  which was intended to fix a memory issue on the firewall device was reverted to restore the services.,CRQ000000184384,Customer Tech Issue,NA,10/9/2023,October,2023,Sravan,Closed,,,2023,7,July,9/16/2025,795,>60
7/5/2023,7/5/2023,89420873,09:30,20:30,11:00,Digital & Data,Data,All BU's,All BU's,,KI,Microsoft Azure Databricks services unavailable,Microsoft ,NA,,BEAM Reports,RC identified,65242,Microsoft Azure Databricks services were unavailable between 09:30 and 16:40. The BEAM reports - Foods stock movement tool and other reports were not updated with the latest data from 09:30. A global network issue at Microsoft due a fibre break had caused the issue resulting in  20% of the Databricks customers including Customer.,"1. Microsoft have advised to configure new workspaces pointing to North Europe. This will help mitigate the impact if either of the datacentres become unavailable - This is a long term actions and is being dealt by BEAM platform Manager - Munesh Patel. Awaiting a backlog id.

2. Alerting - Microsoft confirmed that network alerting services indicated a fiber cut at 07:22 UTC and a congestion alert triggered at 07:46 UTC. Our networking on-call engineers engaged and began to investigate. All required alerts are configured at Microsoft and hence no additional alerting required.",7/5/2023,7/5/2023,,,,"Yes, No",Yes,"No, No",TBC,3rd party issue,No,"Due to a fiber cut caused by severe weather conditions in the Netherlands, 25% of network links between two West Europe datacenters at Microsoft became unavailable resulting in traffic congestion eventually becoming inaccessible, causing the issue.",N/A,N/A,,8/15/2023,August,2023,Saloni,Closed,,,2023,7,July,9/16/2025,804,>60
7/3/2023,7/5/2023,89415385,08:03,9:55,01:52,Customer Channels,Platform & Store Ops,"Group Support, Commercial Trading","Finance, Foods Commercial Trading",,SI,Foods waste value understated across various reports,TCS,CSSM,,EDW reports ,RC unknown,65143,"Finance colleagues reported that foods waste value was understated in golden reports and other analytics/BI/Beam reports resulting in last week’s gross profit figures being overstated. The issue was caused due to 81k messages being stuck in the retry queue in CSSM between 27/06 and 30/06. The messages were retriggered to reflect the correct values. However, it was identified that the 81k messages had inadvertently retriggered again resulting in duplicates. Recovery actions were carried out to address the duplicates and adjustments were made by Finance to correct the figures. Detailed root cause analysis underway.",All actions are being tracked in Problem Actions tab,7/3/2023,7/4/2023,,,,"No, Yes",No,"Yes, Yes",No,RC unknown,No,"1. The root cause behind the 81k messages being stuck in the retry queue remains inconclusive
2. The second retrigger of the messages occurred because the team, did not manually clear the retry queue after the messages were retriggered on 03/07.  With the alert being triggered on 04/07, the product team member had inadvertently released the 81k messages again. ",N/A,N/A,N/A,7/24/2023,July,2023,Gokul,Closed,,,2023,7,July,9/16/2025,806,>60
6/30/2023,7/2/2023,89412457,12:00,1:00,37:00,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Message failures in exceptional delivery from GIST to CSSM,TCS,CSSM,,CSSM,RC identified,65124,"At 12:00, alerting indicated 134 message failures in exceptional delivery from GIST to CSSM impacting 900 UPCs store combination. Stores would have received these deliveries physically in store resulting in potential stock corruption. The issue was caused due to one of the database parameters (identifier primary key column) breaching its threshold limit which prevented the message processing. A decision has been taken with the business to increase the parameter value and process the failed message on 02/07 at 01:00. ",The PIR has been held and the actions will be tracked within the portfolio,6/30/2023,6/30/2023,,,,Yes,Yes,Yes,Yes,Configuration issue,No,The issue was caused due to one of the database parameters (identifier primary key column) breaching its threshold limit which prevented the message processing. We are awaiting a detailed root cause from the Service Lead Valli,N/A,NA,,8/7/2023,August,2023,Saloni,Closed,,,2023,6,June,9/16/2025,809,>60
6/29/2023,6/29/2023,89410912,13:36,16:00,02:24,Infosec & Tech Risk,Infosec & Tech Risk,Customer Channels,"Customer Engagement, Selling Experience",,MI,"Customers cannot place orders via desktop, mobile , apps",TCS,.com,,.com ordering ,RC identified,65226,"Alerts indicated a drop in order volume between 13:36 and 16:00. Customers were unable to place orders via desktop, mobile, apps resulting in poor customer experience and loss of sales (7k orders). The issue was caused due to a security change related to Page Integrity Manager/In Browser protection and was reverted to restore services.",All actions are closed,6/29/2023,7/6/2023,,,,Yes,Yes,No,NA,Customer Tech change,Unauthorised change,"Customer use the Page Integrity Manager (PIM) from Akamai to block potential malicious activity and the Tealium Customer Data Platform 
Mechanism for releasing Third Party scripts to customer browsers and applications. PIM generates alerts when specific sensitive fields are requested, however, these alerts were deactivated based on the domains through reconfiguration within the PIM tool. During the PIM security outage, multiple changes were made to deactivate all these alerts which resulted in a bug to block all the requests within the tool. This blocked the Adobe Target script from Tealium from loading into the PIM resulting in issues with customer sign in and check out pages.",NA,Product bug,,10/10/2023,October,2023,Saloni,Closed,,,2023,6,June,9/16/2025,810,>60
6/29/2023,6/29/2023,89409612,07:00,17:00,10:00,Infosec & Tech Risk,Infosec & Tech Risk,All BU's,All BU's,,MI,Multiple users reported that they are not able to login to Outlook and applications via SSO (Single Sign-On),TCS,Active directory,,O365,RC identified,65222,"After a cyber security change, it was identified that the system username for 3900 colleagues had changed and 500 user accounts status were disabled. This impacted majority of the Head office, DC, contact centre and HR colleague's ability to access Outlook, Teams and any SSO (single sign on) applications. Services were gradually restored by re-enabling the user's accounts by 17:00. Investigation are underway to understand why the change had impacted the wider user community.",All actions are closed,6/29/2023,7/3/2023,,,,No,No,Yes,No,Customer Tech change,Change ,"On 29/09, as part of go-live to process four joiners and four leavers overnight a customer script was run at 01:24 after a full sync between Customer Active Directory and One Identity Manager OnDemand (IMOD). Due to an issue in the script, the AD accounts were deleted resulting in the accounts in IMOD inheriting an “Account Definition” which IMOD then reconciled against AD and updated with new values it derived from templates containing the CATE UPN (Unique Principle Name)
Also, as a part of this script. IMOD was to update the OU (Organizational Unit), but as this template was empty, it put in a null value in the OU field. ",CR#183591,Configuration issue,,10/16/2023,October,2023,Saloni,Closed,,,2023,6,June,9/16/2025,810,>60
6/28/2023,6/29/2023,89413024,14:00,23:55,33:55,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Customers getting errors while tracking orders,TCS,Returns Team,,.com returns,RC unknown,65224,"Customers were encountering errors whilst initiating returns and issues were observed in the Kafka services used for tracking returns. 16,811 out of 17524 returns have been impacted with no barcode/QR code information in email between 14:00,  28/06 and 23:55, 29/06. The issue was caused due to an problematic message in the return services. As a workaround, this message was skipped whilst the fix is being identified.  Detailed root cause analysis underway.",All actions are closed,6/29/2023,NA,,,,No,No,Yes,Yes,RC unknown,NA,The root cause remains inconclusive,NA,NA,NA,10/16/2023,October,2023,Saloni,Closed,,,2023,6,June,9/16/2025,811,>60
6/25/2023,6/27/2023,89402066,16:00,16:54,48:54,Group Platforms,HR,Group Platforms,HR,,SI,My T&A incorrectly excluding breaks for colleague timesheet punches. ,Blue Yonder,NA,,Incorrect payments,RC identified,65209,"Some stores using the Blue Yonder Time & Attendance application reported incorrect exclusion of breaks for colleague timesheets from 16:00. There was no impact to the colleague payments, however, the payslips would be delayed to Thursday. The issue was caused by a fix for another incident which prevents colleagues from manually punching for breaks. A fix was implemented to restore services by 16:54, 27/06.",Actions have been tracked and closed in the actions tab - Kindly refer to it,6/25/2023,6/28/2023,,,,"No, Yes",No,No,NA,3rd party issue,N/A,A configuration change within the T&A application to prevent colleagues from manual break punches was overridden by an unknown job at Blue Yonder which hindered the auto generation of breaks resulting in incorrect colleague payments calculation.,N/A,NA,NA,8/7/2023,August,2023,Pavithra,Closed,,,2023,6,June,9/16/2025,814,>60
6/21/2023,6/22/2023,89395055,10:30,6:00,19:30,Group Support,Finance,Digital & Data,Digital & Data,,KI,Ecom sales understated by £835K in GMOR BO and BEAM reports,TCS,SAP BO,,GMOR BO and BEAM Reports,RC unknown,65707,"E-Comm sales were understated by £835K in GMOR BO and BEAM reports. The issue was caused due to unprocessed transactions within the SAP POSDTA system which impacted the overnight sales data flow into GMOR. A manual workaround was applied and the missing sales were updated in the relevant reports by 06:00, 22/06.",All the actions are being tracked under Problem actions section ,6/25/2023,7/14/2023,,,,No,No,Yes,NA,3rd party issue,No,"Unprocessed transactions were stuck in the inbound queue in POSTDTA.
 Database locks are normal and the system locks tables during the normal course of data processing to ensure that other processes cannot access the tables being used and interrupting the process.  However, once the process has stopped running the system should release its lock on the table and allow other processes to start.  However, in this case the incident was caused by the system not releasing the lock. The root of the issue remain inconclusive , as the team is not able to replicate the issue in the non-prod environment as requested by SAP for investigations. ",N/A,RC unknown,SAP-SLS-00144,7/14/2023,July,2023,Kavitha,Closed,,,2023,6,June,9/16/2025,818,>60
6/15/2023,6/16/2023,89385821,10:00,10:48,18:42,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,B2B customers unable to use their e-codes/e-gift cards,Adobe C7 (Scene 7) ,NA,,B2B e-gift cards,RC identified,64877,"B2B customers were unable to access their e-codes or e-gift cards between 10:00 and 15:06 on 15/06 and 21:10 and 10:48 on 16/06, B2C e-gift cards were not impacted. Around 10.5k e-gift cards were impacted resulting in a poor customer experience. As a workaround, a fix has been implemented to restore services. Awaiting detailed root cause from vendor Adobe C7.",All actions are being tracked in Problem Actions tab,6/15/2023,7/5/2023,,,,No,No,Yes,NA,3rd party issue,N/A,Vendor Scene 7 confirmed that the B2B e-gift card URL using HTTP was not enabled in the recently built Adobe datacentre due to insufficient testing resulting in the 500 internal server errors.,N/A,NA,NA,7/10/2023,July,2023,Gokul,Closed,,,2023,6,June,9/16/2025,824,>60
6/14/2023,6/14/2023,89382363,08:29,9:16,00:47,Customer Channels,Platform & Store Ops,"Foods, Customer Channels","Foods Commercial Trading, Platform & Store Ops ",,KI,P&P price changes reflecting incorrectly in ECS,TCS,RET team,,Shelf edge tickets,RC identified,64978,"Foods P&P (pricing and promotion) colleagues highlighted that the price changes for 56 UPCs were reflecting incorrectly in ECS resulting in potential illegal trading. As part of an ongoing Retail project, the test price change file was inadvertently loaded into ECS, causing the issue. The price changes were manually processed in ECS to restore services.",All actions has been  tracked and closed,6/14/2023,6/14/2023,,,,No,No,Yes,Yes,Human error - Customer Tech,N/A,"As a part of ongoing Retail project, the test price change file was inadvertently loaded into ECS production environment, causing the issue. ",N/A,Human error,NA,7/10/2023,July,2023,Pavithra,Closed,,,2023,6,June,9/16/2025,825,>60
6/5/2023,6/5/2023,89367334,14:30,15:45,01:15,GTS,Enterprise Technology Platform,"Customer Channels, C&H and Intl","Selling Experience, Service Experience, Customer Engagement, Platform & Store Ops, International Commercial Trading",,MI,Global Apigee issue impacting multiple applications using API services ,Apigee,N/A,,"Collect App, ISF, Intelligent Waste, VOD applications  Pay With Me & Mobile Pay Go, IBT and IFO",RC identified,64854,"Alerting indicated issues with the API services impacting multiple applications across various portfolios between 14:30 and 15:45. Store colleagues were unable to access Intelligent Waste, ISF and Collect applications impacting the store operations. The VOD applications - Scan & Shop, Pay With Me & Mobile Pay Go along with International Buying Tool, International Food Ordering applications were also unavailable. Some customers were unable to place orders on the website, redeem Sparks TMO offers. The root cause was attributed to a central database issue at APIGEE impacting multiple customers. ",All the actions are being tracked under Problem actions section ,6/9/2023,6/19/2023,,,,Yes,Yes,No ,Yes,3rd party issue,N/A,"
Apigee confirmed that an increase in the heap memory had impacted one of their Cassandra database nodes servicing a subset of Apigee customers in the eu-west-1 region. This resulted in latency across multiple API services, causing the issue.",N/A,N/A,1856,3/8/2023,March,2023,Sravan,Closed,,,2023,6,June,9/16/2025,834,>60
6/4/2023,6/4/2023,89364597,14:20,15:51,01:31,Group Support,HR,Customer Channels,Platform & Store Ops ,,KI,Time and Attendance application was inaccessible via desktop,Blue Yonder,N/A,,T&A,RC identified,64839,"From 14:20, store colleagues were unable to access Time and Attendance application on their workstations. This impacted 385 stores where this application is currently rolled out - impacting store labour scheduling, timesheet etc. The issue seemed to have self-recovered at 15:51.",Actions have been tracked and closed in the actions tab - Kindly refer to it,6/5/2023,6/5/2023,,,,"No,Yes",No ,Yes,No ,3rd party issue,N/A,"The database query responsible for user authentication had picked up an inefficient execution plan due to a huge volume of data in the database, causing the issue. The Blue Yonder purge job responsible for the database housekeeping did not run from 31/03 due to a conflict with the schedule restart of the APE engine resulting in the huge data volume.",N/A,N/A,NA,8/7/2023,August,2023,Pavithra,Closed,,,2023,6,June,9/16/2025,835,>60
6/2/2023,6/3/2023,89362084,06:40,2:08,19:28,Commercial Trading,Foods Commercial Trading ,"Customer Channels, Store Operations","Service Experience, Run the store",,KI,P&P changes reflecting incorrect pricing at POS and ECS,Customer,NA,,Tills and Ticketing,RC identified,64860,Foods P&P (pricing and promotion) colleagues highlighted issues with the price changes of certain products at the store tills. Incorrect prices were reflecting on the tills resulting in potential illegal trading. The issue was caused due to a change performed in SAP. A manual workaround was performed to remediate the immediate impact. The prices for 14 UPCs have processed through as BAU and are reflecting as expected in POS and ECS from 04/06.,All the actions are being tracked under Problem actions section ,6/2/2023,6/7/2023,,,,"No,Yes",No,Yes,NA,Human error - Business,N/A,"On 01/06, while making the price changes, the reactivation step was missed for certain Reduced to clear products in SAP, causing the issue.",N/A,N/A,NA,8/30/2023,August,2023,Pavithra,Closed,,,2023,6,June,9/16/2025,837,>60
6/1/2023,6/1/2023,89360258 ,09:10,10:55,01:45,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI, Issues with gift card transactions in the tills,TCS,Retail SVS Team,,POS,RC identified,64958,Store colleagues reported issues with gift card transactions in the tills. Customers instore and online were unable to redeem gift cards between 08:23 and 10:55. The root cause was attributed to a configuration issue in the deployment pipeline during a change. A workaround was applied to restore the services followed by a permanent fix.,All the actions are being tracked under Problem actions section ,6/2/2023,6/2/2023,,,,"No, Yes",No,Yes,Yes,Customer Tech change,Yes,"It has been confirmed that one of the configuration - “Enable F5” was inadvertently set up as “false” in the DR environment during the built of SVS (gift card) change deployment tool last year. On 01/06, when the network timeout change was deployed in the SVS DR layer, this erroneous configuration was not identified due to a human error which impacted the inflow of traffic between the POS and the gift card services.  ",CR#182244,Configuration issue,,7/3/2023,July,2023,Kavitha,Closed,,,2023,6,June,9/16/2025,838,>60
5/31/2023,5/31/2023,89358542,08:26,16:30,08:04,GTS,Enterprise Technology Platform,All BU's,All BU's,,KI,Colleagues unable to access the shared files after the power maintenance activity at Waterside.,TCS,Storage ,,Shared Drives,RC identified,64924,"Some colleagues would have been unable to access certain Waterside shared drives between 08:26 and 16:30. This caused inconvenience to a few colleagues in their daily tasks, however there was no residual impact. The root cause was attributed to a storage device failure after the power maintenance activity at Waterside and these were restarted to restore services.",All the actions has been tracked under the problem action section and they are closed  ,5/31/2023,6/2/2023,,,,Yes,Yes,No ,No,Infrastructure issue / Hardware failure,No,"During the planned power maintenance activity at Waterside, one of the power distribution unit (PDU) was shut down, subsequently all of the connected devices including the HNAS (Hitachi Network attached storage) devices went offline. There is a DR in place within the controllers of the storage device to failover to the active controller (connected to the other active PDU) unfortunately, the available DR controller was already in an unhealthy state, which led to a total outage. ",No,Infrastructure issue / Hardware failure,NA,10/3/2023,October,2023,Pavithra,Closed,,,2023,5,May,9/16/2025,839,>60
5/30/2023,5/30/2023,89359796,12:46,17:40,28:54,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Incorrect returns policy lead-time for full price products returns,TCS,Refund Team,,Product returns,RC identified,64957,"From 26/05, an order containing both full price products and sales price products was incorrectly showing as a sales price order impacting their return refund time. Around 11k customers were unable to return the full price products beyond 14 days instead of the normal 35 days, resulting in poor customer experience. An interim solution was applied on 31/05 and next steps are being discussed.",All the actions are being tracked under Problem actions section ,5/30/2023,6/6/2023,,,,"No, Yes",No,No,NA,Customer Tech change,Yes,The issue was caused due to an incorrect configuration in WCS  (Sale flag  =  'True') for both sale and non-sale items for mixed baskets.,CM-9041,inadequate testing,FULRE-1669/FULRE-1579,6/21/2023,June,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,840,>60
5/30/2023,5/30/2023,89359175,06:42,10:45,04:03,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Returns Label Generation API service is down,TCS,Returns Team,,Returns,RC identified,64830,Customers were unable to initiate returns from the website between 06:42 and 10:45. This impacted around 2.5k returns when compared to previous week. The root cause was attributed to an expired service after a change and was updated to restore servic,All the actions are being tracked under Problem actions section ,5/30/2023,5/30/2023,,,,No,No,No,Yes,Certificate issue,Yes,"During  the planned deployment on 30/05, the Returns pods restart failed as the Azure service principle (Identity for Azure access) got expired on 26/05. The new Azure key was not updated in the returns label generation API cluster, causing the issue. ",NA,Service principle/token  expiry ,,8/4/2023,August,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,840,>60
5/27/2023,5/27/2023,89351882,11:30,13:30,02:00,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Android App Not Loading Account Details,TCS,Your account ,,Andorid app ,RC identified,64920,"Customers encountered errors when selecting the “Your Account” option from the hamburger Menu in all versions of the UK Android App from 22/05 14:00, however IE and iOS app was working fine . Android order volumes were not impacted by this issue. The issue was caused due to a change and a fix was deployed to restore services by 12:57. Detailed RCA underway.",All the actions are being tracked under Problem actions section ,5/27/2023,5/27/2023,5/27/2023,,,"Yes, No",No,Yes,NO ,Customer Tech change,Yes,"During the Akamai change under CM-9976,CM9950 to handle the mobile App path to redirect the customer directly to WCS instead of MCP (Mobile Commerce Platform),  the query with the unique customer identification details was not passed to WCS as the value in the Akamai configuration was set to ""No"" , instead of ""yes"" . This caused WCS to present a version of the My Account page which did not contain any user specific information.

",CM9976,Configuration issue,,7/4/2023,July,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,843,>60
5/26/2023,5/26/2023,89351281,23:00,4:00,05:00,Customer Channels,Service Experience,Customer Channels,Selling Experience,,KI,Orders Cancelled After Stuck In Awaiting Reservation Status,TCS,Sterling ,,Sterling,RC identified,64822,"After the Sterling release on 25/05, it was identified that the customer orders were placed in offline mode and stuck in awaiting reservation status. Around 933 orders were cancelled with no impact. A snow report was shared with contact centre to proactively communicate to the customers. Detailed root cause investigations underway. ",All the actions are being tracked under Problem actions section ,5/26/2023,5/26/2023,,,,No,No,No,Yes,Customer tech change,Yes,"As part of Sterling release, when Sterling is offline, orders would not go into this ""Awaiting reservation"" status, once Sterling is back online another stock check would be carried out at order creation and these orders would be cancelled as stock not available cancellations

· Within the Sterling Release build CMWA decommissioning changes were included which introduced a bug and caused these orders to go into this status and refunds not be triggered for these orders and sent to customers.",CM9985 ,Code/Product bug,,6/20/2023,June,2023,Saloni,Closed,,,2023,5,May,9/16/2025,844,>60
5/26/2023,5/26/2023,89350726,04:00,12:50,08:50,Customer Channels,Service Experience,Customer Channels,Platform & Store Ops ,,SI,Messages sent from sterling to Collection DB failed in Sterling,TCS,Sterling ,,Collection app,RC identified,64914,Store colleagues reported issue with Collect app as the messages sent from sterling to Collection DB were failing between 04:00 and 10:20.  Colleagues were unable to locate the customer’s parcel in stores to mark them as collected. The root cause attributed to a configuration issue and it was updated to restore the services. ,All the actions are being tracked under Problem actions section ,5/26/2023,5/26/2023,,,,No,No,No,Yes,Customer tech change,Yes,The messages from Sterling to Collection DB had failed due to a space in the service configuration introduced during the Sterling Release. ,CM9985 ,Human error,,8/30/2023,August,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,844,>60
5/26/2023,5/27/2023,89350725,11:20,12:15,24:55,Foods,Foods Commercial Trading ,Foods,Foods Supply Chain,,SI,Incorrect Foods SRD ranging file sent to Quantum and Relex,TCS,SRD,,Quantum & Relex,RC identified,64812,"A significant drop in foods ranges was identified in SRD which resulted to the generation of an incorrect ranging file being sent to Quantum and Relex. This is impacting the RDC allocation, stock movement to stores and finalized supplier orders. The issue is caused due to a change implemented in SRD on 24/05. A fix has been identified and is currently being applied. As a workaround, we have advised GIST to proceed with systemic manual allocation and discussion are in progress to address the impacted supplier orders.",All the actions are being tracked under Problem actions section ,5/26/2023,5/26/2023,,,,"No, Yes",Yes,Yes,Yes,Customer Tech change,Yes,"As a part of the SRD change to introduce additional columns in the Planogram product extract, the query used to perform the default one-time data load into the additional columns filtered the products with planogram name as ‘Null’, which caused the drop in foods ranges.",CRQ181781,Incorrect script,NA,7/31/2023,July,2023,Pavithra,Closed,,,2023,5,May,9/16/2025,844,>60
5/24/2023,5/25/2023,89346002,08:55,5:00,20:05,Digital & Data,Data,"Commercial Trading,
Group Support","Foods Commercial Trading, Finance",,SI,"Understated POS sales data in FAQT, SWAT, Retail Dashboard and other reporting system",TCS,EDW ,,"FAQT, SWAT, Retail Dashboard and other reporting systems",RC identified,64810,"POS sales was understated by £20M in FAQT, SWAT, Retail Dashboard, Analytics reporting , FAP, Commitment sheets and all analytics reporting for 23/05. The issue was caused due to a human error while recovering the cyclic job responsible for processing POS sales data. Service was restored and the reports were made available by 5:25 on 24/05.",No futher actions pending ,5/25/2023,5/25/2023,,,,"Yes, No",No,No,No,Capacity constraint ,NA,"The job EDWDTRN010 which sends POS sales from SAG to EDW HDW table had its last successful run from 11:45-11:56 23/05.  During its next 12:00 the job was stuck for the whole day and was not triggered manually upom receiving an alert.
 This is a known issue as there were other job failures in the past as the LSF (load sharing facility) was unable to submit jobs in DS whilst connecting to the DS node. IBM has advised to increase a grid parameter value and this has been done.",NA,Capacity constraint ,,5/26/2023,May,2023,Sravan,Closed,,,2023,5,May,9/16/2025,846,>60
5/23/2023,5/23/2023,89344497,10:40,11:18,00:38,GTS,Digital Workplace Services,All BU's,All BU's,,SI,Performance issues with multiple applications hosted on Microsoft 0365 and Azure services,Microsoft ,NA,,O365,RC identified,64811,"Colleagues reported intermittent performance degradation while accessing multiple Microsoft applications like Teams, SharePoint, Power BI, Outlook and Azure services between 10:40 and 15:35. The root cause is attributed to a network connectivity issue at Microsoft which impacted multiple customers.  Awaiting detailed root cause from Microsoft.",All actions are being tracked in Problem Actions tab,5/26/2023,5/26/2023,,,,No,No,No,Yes,3rd party issue,NA,The issue was caused due to a central connectivity issue between Microsoft and their Internet Service Providers (ISP) impacting multiple customers including Customer. This also interrupted the connectivity between Microsoft and Zscaler via their London V and Frankfurt datacentres. We are extensively following up with Microsoft and Zscaler for a detailed root cause analysis.,NA,NA,,8/7/2023,August,2023,Saloni,Closed,,,2023,5,May,9/16/2025,847,>60
5/22/2023,5/22/2023,89341385,07:00,9:11,02:11,Group Platforms,HR,Group Platforms,HR,,KI,People System application unavailable,SD Worx,NA,,Peoplesystem,RC identified,64910,Colleagues were unable to access People System between 07:00 and 09:11. This impacted their ability to view pay slip and claim additional payments etc. The issue was caused due to a database storage crisis at Sdworx. Service was restored by adding extra storage by their hosting partner Navisite. Detailed root cause investigations underway.,They have identified actions to run the backup job more frequently to allow the quicker completion of the process. Hypercare while the backup process in running,5/22/2023,6/2/2023,,,,No,No,No,Yes,3rd party issue,NA,"The backup job at Sdworx was stuck due to database storage issue at Navisite, causing the issue. Navisite confirmed that the database storage disk was full which caused the issue. ",NA,NA,NA,6/6/2023,June,2023,Saloni,Closed,,,2023,5,May,9/16/2025,848,>60
5/22/2023,5/22/2023,89342062,09:00,9:30,00:30,Group Support,Finance,Customer Channels,Platform & Store Ops ,,KI,Prices in two stores are trading at 5% higher than the tickets are reflecting,Customer,NA,,Store Tills,RC identified,64815,"All Food products at two stores (Ashtead and Dartford) were scanning through tills at 5% higher than the advertised prices on tickets. On 21/05, a price change performed by the Pricing and Trade team  on these two stores via the BSC Master Data team into SAP had caused the issue.  As a workaround, the price list priority was changed in POS allowing the tills to pick up the standard pricelist in stores.",No pending actions. ,5/22/2023,5/22/2023,,,,No,No,No,Yes,Human error - Business,NA," On 21/05, a price change performed by the Pricing and Trade team  on these two stores via the BSC Master Data team into SAP had caused the issue.",NA,NA,NA,8/7/2023,August,2023,Pavithra,Closed,,,2023,5,May,9/16/2025,848,>60
5/22/2023,5/22/2023,89343069,14:08,14:13,00:05,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Issues with contactless payments at stores,Worldline,NA,,POS,RC identified,64913,A spike was observed in the contactless payment failures between 14:08 and 14:13. This impacted the contactless transactions at stores. The issue was caused due to a change at Worldline on their acquirer hub. Awaiting detailed root cause analysis.,Worldline has not yet came back with the RC document. Hence Service lead has confirmed to deal with it within the product,5/22/2023,5/22/2023,,,,Yes,Yes,No ,NA,3rd party change,NA,The issue was caused due to a change at Worldline on their acquirer hub,NA,NA,,7/18/2023,July,2023,Saloni,Closed,,,2023,5,May,9/16/2025,848,>60
5/22/2023,5/22/2023,89342875,13:30,14:07,00:37,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,MTO sofa PDPs leading to error message,TCS,Platform operation ,,Customer .com website,RC identified,64923,Customers encountered errors on the Made to Order Sofa PDP page in the website between 13:30 and 14:07. Store colleagues were unable to place order on behalf  of customers resulting to poor customer experience and potential loss in sales. The issue was caused due to a traffic migration change and services were restored after the completion of the change. Detailed RCA underway.,Agreed action has been closed,5/22/2023,5/22/2023,,,,"No, Yes",No,No,Yes,Customer Tech change,Yes,"The Akamai change was deployed to Production in order to move the FESK PDP Sofa traffic from the V1 origin to the V2 origin. There was no feasibility to throttle the traffic in the production environment which was missed during the testing and hence all the traffic was moved together, causing the outage.",CM9950,Inefficient deployment plan,NA,6/28/2023,June,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,848,>60
5/19/2023,5/19/2023,89338386,13:05,13:27,00:22,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Spikes in the card payment refusals between 13:05 and 13:27,Worldline,NA,,POS,RC identified,64886,Worldline observed spikes in the card payment refusals between 13:05 and 13:27. 5 stores had reported the issue to the desk. Worldline advised an issue at their Acquirer and we are awaiting detailed root cause analysis.,Worldline has not yet came back with the RC document. Hence Service lead has confirmed to deal with it within the product,5/19/2023,5/19/2023,,,,Yes,Yes,No,NA,3rd party issue,NA,Worldline advised an issue at their Acquirer and we are awaiting detailed root cause analysis.,NA,TBC,,7/18/2023,July,2023,Saloni,Closed,,,2023,5,May,9/16/2025,851,>60
5/18/2023,5/18/2023,89309464,10:00,20:00,10:00,Group Support,Finance,Customer Channels,Platform & Store Ops ,,KI,P&P product changes not reflecting in POS and ECS,TCS,SAP,,"POS,ECS",RC unknown,64823,The P&P (Pricing & Promotion) product changes were not reflecting in POS (tills) and ECS (ticketing) due to an issue in SAP. The products which were 'removed' off the 3 for £8 Bonus Buy VFP Promotion in SAP were trading illegally across the UK mainchain stores. An emergency adhoc batch was triggered to restore services. Root cause analysis underway.,No pending actions,5/18/2023,5/25/2023,,,,No,No,No,NA,RC unknown,NA,"  SAP has advised that the cause of the issue was due to  duplicate record insertions in a custom table. However, the issue could not be replicated in non prod after multiple discussions with the vendors and stakeholders. The root cause remains inconclusive.",NA,NA,NA,7/4/2023,July,2023,Saloni,Closed,,,2023,5,May,9/16/2025,852,>60
5/16/2023,5/17/2023,89334769,21:30,15:30,18:10,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Dropship order confirmation messages failing in Sterling,CommerceHub,NA,,Dropship orders,RC identified,64684,"The order confirmation messages were failing in Sterling as the dropship orders were rejected by CommerceHub from 21:20. This impacted the delivery of 28 orders dated 22/05. The issue was caused due to a deployment at CommerceHub and was reverted to 
restore the services by 15:30, 17/05. Awaiting detailed root cause from CommerceHub. ",All actions are being tracked in Problem Actions tab,5/17/2023,5/19/2023,,,,Yes,Yes,No,Yes,3rd party issue,No,The internal project structure for the validation engine was overhauled and included in another project and has been for many months. An unrelated change was implemented in a different branch of the new project structure that had an unexpected impact on the validation engine.,NA,NA,,8/3/2023,August,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,854,>60
5/15/2023,5/15/2023,89331108,11:18,13:05,01:47,GTS,Enterprise Technology Platform,"Customer Channels, C&H and Intl","Service Experience, C&H and Intl Supply Chain",,SI,Network connectivity issues impacting gift card transactions and carrier label printing at Donington and C&H Bradford,TCS,Network,,.com ordering ,RC identified,64574,"From 11:18, issues were reported with gift card activities at stores and carrier label printing at Donington and C&H Bradford DCs. This impacted the packing and picking capability of 27k singles at Donington. Also, customers in stores and online were unable to redeem their gift cards. The network VIP (virtual IP address) that hosts traffic and sends out to the third party vendors was in disabled state and was re-enabled to restore services at 13:05. Root cause analysis underway with vendor F5. ",NA,5/16/2023,5/16/2023,,,,No,No,Yes,Yes,Customer Tech change,Yes,"During the AKS upgrade (CRQ000000181097) on 15th May, there was requirement to move B2B DR traffic to Prod and back again. During the activity, the wrong command was issued (PROD instead of PROD.) to view the relevant nodes that needed to be disabled.  These nodes were then also disabled without a secondary check to see that the correct nodes were being displayed.  The cause was therefore human error.",181097,Human error,NA,7/4/2023,July,2023,Saloni,Closed,,,2023,5,May,9/16/2025,855,>60
5/12/2023,5/12/2023,89327395,05:00,18:15,13:15,Customer Channels,Service Experience,Customer Channels,Selling Experience,,KI,C&H products showing OOS on the website,TCS,Sterling,,.com ordering ,RC identified,64666,"Colleagues reported that C&H products were showing out of stock (OOS) on the website between 05:00 and 18:15. This had potentially impacted the sales and caused poor customer experience. The root cause has been attributed to an issue in the inventory picture after the daily manual sync across the Availability service & Sterling, which was manually corrected to restore the services.",All actions are being tracked in Problem Actions tab,5/12/2023,5/15/2023,,,,No,No,Yes,Yes,Data issue,NA,"A daily manual sync activity had impacted the inventory picture between Sterling and availability services, thus causing the issue. After the Sterling release, the workaround was inadvertently applied to 50k items instead of 22.7K of items because IAA’s post 5am would have automatically corrected the inventory picture., resulting in the synch issue between Sterling & AS.
",NA,,NA,8/21/2023,August,2023,Gokul,Closed,,,2023,5,May,9/16/2025,858,>60
5/12/2023,5/12/2023,89327393,00:00,9:30,09:30,Customer Channels,Service Experience,Customer Channels,Selling Experience,,KI,Brands depts showing OOS on the website,TCS,Brands,,.com ordering ,RC identified,64578,Colleagues highlighted that multiple Brands products were showing out of stock (OOS) on the website. Around 7.7k products were impacted resulting to a drop in order volume causing poor experience to the customers. The issue was caused by the missing country node master collection configuration of the Brands DC in the Availability service and was added to restore services. ,All actions are being tracked in Problem Actions tab,5/12/2023,5/12/2023,,,,No,No,Yes,TBC,Configuration issue,NA,The issue was caused by the missing country node master collection configuration of the Brands DC in the Availability service,NA,Configuration issue,NA,8/21/2023,August,2023,Gokul,Closed,,,2023,5,May,9/16/2025,858,>60
5/11/2023,5/11/2023,89324621,12:40,15:25,02:45,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,SI,Issues in processing PLM messages,PTC,NA,,PLM,RC identified,64565,"Since 28/04, around 75k PLM (C&H product lifecycle management) messages were not processed due to an authentication issue between Customer and PTC (PLM vendor).  Colleagues were unable to create new products and update article contracts, selling price and cost price, suppliers were unable to order packaging and labelling.  PTC identified that the Customer user profile had expired causing this issue.  A work around has been implemented and messaging processing is now in progress.",No pending actions. ,5/11/2023,5/11/2023,,,,"No,yes",No,Yes,,3rd party issue,NA," The issue was caused due to an expired password as a result of the password expiry policy at Traceone set to 90 days after the upgrade of PLM Flex on 28th January 2023.
",NA,NA,NA,7/24/2023,July,2023,Pavithra,Closed,,,2023,5,May,9/16/2025,859,>60
5/10/2023,5/10/2023,89289859,10:15,16:00,05:45,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Contact centre emails sent to customers not reaching recipients,Sabio,NA,,Customer emails,RC identified,64888,Contact centre identified errors while sending the customer emails. This impacted the receipt of 1200 emails causing poor experience to the customers. Vendor Sabio resent the impacted emails without any manual intervention. Detailed root cause analysis underway.,No futher actions pending ,5/10/2023,5/10/2023,,,,"No,yes",No,Yes,TBC,3rd party issue,No,The issue was caused due to number of delicate processes  in the email work flow. Mutiple workshops are being organised to address and reitrate the work flow to avoid any gaps. ,NA,NA,NA,6/13/2023,June,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,860,>60
5/9/2023,5/9/2023,89311347,17:30,18:00,00:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Connectivity issues between Sabio and Medallia,Sabio,NA,,Contact center ,RC identified,64984,"The secure connection between Sabio and Medallia had failed from 29/04. Customer feedbacks were lost during this period and the reporting was incomplete. The issue was caused due to a security certificate expiry at Sabio and was renewed to restore the services by 17:30, 09/05.",No futher actions pending ,5/9/2023,5/9/2023,,,,"No,yes",No,NA,Yes,3rd party issue,No, The issue was caused due to an expired security certificate at Sabio,NA,NA,NA,5/26/2023,May,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,861,>60
5/9/2023,5/9/2023,89320290,07:35,23:00,15:25,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,KI,Performance degradation while accessing the Customer application at BP stores,TCS,Network,,BP Stores,RC identified,64559,"Some BP stores experienced performance degradation while carrying out any Customer related activities, impacting their daily operations. A memory issue was identified in one of the network component which was restarted on 9th and other network components also have been restarted and the services remain stable, hypercare continues.",All actions has been  tracked and closed,5/9/2023,5/16/2023,,,,NO,No,Yes,TBC,Infrastructure issue / Hardware failure,NA,"It has been identified that there were no memory issue on the concentrators. The reason behind the memory error is the high network throttling at BP end due to limited bandwidth between Customer and BP, causing constant movement of stores from one concentrator to another.",NA,NA,NA,,January,1900,Pavithra,Closed,,,2023,5,May,9/16/2025,861,>60
5/8/2023,5/8/2023,89318800,13:06,13:52,00:46,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Stores were not able to access My Compliance app.,Accenture,NA,,My compliance app,RC identified,64562,"On 08/05, stores were not able to access my compliance app from around 13:05. 10 stores reported the issue.  The MY compliance app is used to check the compliance process such as Labelling, temperature in the stores and stores wouldn't have been able to check the compliance process. Compliance team redeployed fresh build of the app in prod around 13:15. Stores confirmed about issue resolution around 13:45.",All actions are being tracked in Problem Actions tab,5/8/2023,5/8/2023,,,,"No,yes",Yes,NA,Yes,Unauthorised Change ,NA,An issue with the image cache while deploying a change related to store dropdown had caused the issue.,NA,NA,NA,5/24/2023,May,2023,Sravan,Closed,,,2023,5,May,9/16/2025,862,>60
5/6/2023,5/6/2023,89316223,08:45,9:24,00:39,Foods,Foods Commercial Trading ,Foods,Foods Commercial Trading ,,KI,One Customer Foods Platform application inaccessible,TCS,OFP ,,OFP,RC identified,64641,"On 06/05, Colleagues reported issues while accessing the OFP (One Customer Foods Platform) application. The user authentication service token had expired and was renewed to restore the services by 09:20. Detailed root cause analysis underway.",All actions are being tracked in Problem Actions tab,5/8/2023,5/8/2023,,,,"No,yes",No,No,Yes,Certificate issue,No,The user authentication service token had expired and was renewed to restore the services by 09:20,NA,Certificate/Service Principal Issue,NA,6/6/2023,June,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,864,>60
5/3/2023,5/3/2023,89310371,17:40,23:20,05:40,Foods,Foods Commercial Trading ,Foods,Foods Commercial Trading ,,KI,Purchase Orders to ROI suppliers and the accuracy of allocation responses in RDC’s and stakeholder forecasting activities were impacted.,TCS,OFP ,,"Quantum, CSSM, ASO ",RC identified,64981,On 03/05 Purchase Orders to ROI suppliers and the accuracy of allocation responses in RDC’s and stakeholder forecasting activities were impacted due to an integration logic scenario that was uncovered by a new ROI STG (Store Group) creation in SAP. The new STG has been temporarily deleted and services have since been restored across impacted systems.,As a agreed process for key incident where MIM is not involved the PIR was held internally and actions are tracked within the team ,5/3/2023,5/3/2023,,,,"No,yes",No,yes,no ,Design issue,No,"As per current design of the Store group hierarchy in systems, a store group named ‘ROI’ (ZROI) cannot be used to consolidate stores connected to ROI1 and ROI 2 because it already exists as an STG (Class) used for Alcohol Minimum Pricing (see below table of existing STG’s in the ROI space). This new mapping has caused a conflict in pre-existing system logic design and the STG file data failed to send the data to downstream impacting the ordering/allocations ",NA,Design issue,NA,7/10/2023,July,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,867,>60
5/3/2023,5/3/2023,89325577,15:57,16:39,00:42,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Customers were unable to collect coffee stamps with new purchases.,TCS,Loyalty Product,,Loyalty Product,RC identified,64593,"On 02/05, customers were unable to collect coffee stamps with new purchases from 07:36, however the issue was identified only at 15:57 on 03/05. This has impacted 40k transactions and caused poor customer experience. The issue was caused due to the inadvertent deletion of a database table holding the coffee stamp information which was subsequently reinstated to restore services at 16:39. Detailed root cause analysis underway.",All actions are being tracked in Problem Actions tab,5/3/2023,5/3/2023,,,,"No,yes",No ,Yes,Yes,Human error - Customer Tech,No,The issue was caused due to the inadvertent deletion of a database table holding the coffee stamp information.,NA,Human error,CMPN-137/CMPN-138/CMPN-139,7/10/2023,July,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,867,>60
5/2/2023,5/2/2023,89310031,15:22,16:16,00:54,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Payments through Apple Pay were not successful,Apple Pay ,NA,,.com ordering ,RC unknown,64643,"On 02/05, alerting indicated that payments through Apple Pay were not successful resulting in poor customer experience from 15:22. Payments were successful via alternative methods. Apple Pay reported a central issue and it has been fixed by 16:16. Detailed root cause pending from Apple Pay.",No actions to be tracked as the issue was fixed apple pay ,5/2/2023,5/2/2023,,,,Yes,Yes,NA,NA,3rd party issue,No,As Apple pay is global company and its not specific to Customer and they wont be sharing RCA with us and hence there is no actions ,NA,RC unknown,NA,5/9/2023,May,2023,Kavitha,Closed,,,2023,5,May,9/16/2025,868,>60
4/27/2023,4/27/2023,89300491,13:41,14:10,00:29,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,WMS application unavailable at Castle Donington and Ollerton DCs,TCS,Storage ,,WMS,RC identified,64613,"On 27/04, WMS application was unavailable at Castle Donington and Ollerton DCs between 13:41 and 14:10. This impacted the packing and despatch operations resulting to a capability loss of 7k singles at Donington and productivity loss at Ollerton. The primary database server went into an unreachable state and the database services were failed over to the secondary server. As the services are currently running without resilience, regular catch-up calls have been scheduled over the weekend, whilst, the teams continue to triage the issue on the primary server. ",No pending actions,4/28/2023,5/2/2023,,,,Yes,Yes,Yes,No ,Infrastructure issue / Hardware failure,NA,"The primary database server went to an unreachable state due to faulty SFP switches, causing the issue.",NA,Infrastructure issue / Hardware failure,,7/24/2023,July,2023,Saloni,Closed,,,2023,4,April,9/16/2025,873,>60
4/25/2023,4/25/2023,89295245,03:54,11:30,07:36,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,KI,Performance issue with Customer application in BP stores,TCS,Network,,Customer Application,RC identified,64536,"On 25/04, Some BP stores reported slow responses when carrying out Customer related activities.  The issue was found to be a heavily utilised 11MB link due to SCEP (anti-virus) updates being sent down the link for all the 204 stores at the same time. The updates were cancelled and the utilisation reduced resulting in normal performance. For a permanent fix team to check on the feasibility to upgrade the link with higher bandwidth.",No pending actions. ,4/25/2023,5/6/2023,,,,Yes,Yes,Yes,NA ,Unauthorised Change ,NA,"Background: By default, the weekly SCEP anti-virus patches are deployed in batches across 1000 stores in the estate anytime between 22:30 – 4 am. In March, the first batch ran at 12:00 based on the number of stores scheduled for the patch update.

 On 25/04, the SCEP (Anti-Virus) update planned for 204 BP stores along with other stores was randomly scheduled at 02:30, as there were a smaller number of stores scheduled for the patch update. As the network bandwidth between Customer and Hughes is 11MB, however, it is 1MB between Hughes and BP, it caused high utilization resulting to latency while accessing the applications.",NA,Inefficient deployment plan,NA,6/20/2023,June,2023,Pavithra,Closed,,,2023,4,April,9/16/2025,875,>60
4/23/2023,4/23/2023,89291867,01:50,4:35,02:45,GTS,Enterprise Integration,Customer Channels,Platform & Store Ops ,,SI,CSSM had not received the store location information files for 378 stores,TCS,CSSM,,MQFTE,RC identified,64601,Alerting indicated that CSSM had not received the store location information files for 378 stores. Stores would not have been able to perform any CSSM food functionalities between 01:50 and 7:11. The partial file was sent to CSSM causing the issue. The previous day's reference file was loaded into CSSM as a workaround. A PIR is planned for 24/04.,All actions are closed,4/24/2023,4/24/2023,,,,Yes,Yes,yes,Yes,Design issue,NA,"The CLCAS job responsible for generating the store location file had failed on 21/04 due to a file contention, as the file was picked up by the MQFTE file transfer job which runs every 1 min as per design. This resulted in the generation of the partial file which was then picked up by the datastage job on 22/04 and sent to CSSM, causing the issue. ",NA,Design issue,CPEG-8292 - Honeywell display feature,7/18/2023,July,2023,Saloni,Closed,,,2023,4,April,9/16/2025,877,>60
4/21/2023,4/21/2023,89293860,07:40,14:52,07:12,Group Support,FInance,Supply Chain,Foods Supply Chain,,KI,COSTA purchase order interface was unavailable,SAP,NA,,Day-1 Report and Trailer processing in Hydepark DC,RC identified,64527,"On 21st April, after the SAP release, it was identified that the COSTA purchase order interface was unavailable impacting the order processing in SAP. This caused a delay in sending the orders to suppliers and the file was sent manually as a workaround. SAP vendor identified invalid characters in the release deployment package which was removed and redeployed to fix the issue. Also the SAP outage was delayed by 1 hour which caused a delay in the EDW day-1 report availability and trailer processing at Hydepark DC. ",All actions are being tracked in Problem Actions tab,4/27/2023,4/27/2023,,,,Yes,Yes,Yes,Yes,3rd party change,NA,"1.	Delay in sending COSTA orders to the suppliers: 
The release package description on the COSTA purchase order interface in SAP had invalid characters (non-ASCII characters) which caused the issue. It has been confirmed that an interim change was performed in the SAP cloud solution (by SAP vendor) to exclude the non-ASCII characters in the deployment package description, after the package testing was completed in the Customer environment. 
 
2.	After the completion of the regular pre-requisite activities, it was identified that some unknown transactions were still getting posted in the system, although there was no system usage. The clearance of these transactions took time which contributed to an hour extension of the SAP release. There are no logs/evidence identified to understand the source of these transactions, however, tech teams are in process of replicating the issue in the non-prod environment.
 
3.	Trailer impact at Hydepark DC: The SAP job responsible creating the picklist (outbound delivery documents) and send to WMS had executed its first cycle and was waiting for a resource, which caused the issue. Due to a miscommunication between the SAP Release management and the wider teams during the job release, the previous day’s job instance was active in Control M. This did not allow the current day’s job to run its next iterations. Actions have been agreed to address this in future.",NA,NA,NA,7/4/2023,July,2023,Saloni,Closed,,,2023,4,April,9/16/2025,879,>60
4/20/2023,4/20/2023,89287810,03:26,18:00,14:34,GTS,Enterprise Integration,Supply Chain,Foods Supply Chain,,KI,Foods final orders were duplicated for 60 suppliers.,TCS,MQFTE team,,FFO,RC identified,64602,"On 20/04, supply chain colleagues reported that foods final orders were duplicated for 60 suppliers resulting in an operational impact. These duplicates were cancelled via One Foods Platform to mitigate the impact. The root cause has been attributed to a missing configuration in the middleware system as a part of a change. A permanent fix has been deployed.",All actions are being tracked in Problem Actions tab,4/25/2023,4/25/2023,,,,"No,yes",no,Yes,Yes,Customer Tech change,Yes,A human error while deploying the MQFTE monitor in middleware as part of the change CR#180028 had caused the issue. The end of line character in the MQFTE monitor was configured as LF instead of CRLF which corrupted files that was sent to CXM.   ,CR#180028,Human error,INTSVCS-50019/FSORP-20292,6/13/2023,June,2023,Kavitha,Closed,,,2023,4,April,9/16/2025,880,>60
4/14/2023,4/19/2023,89277635,21:48,11:00,,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Errors were encountered in WMS hanging adapter services at Castle Donington,SSI Scheafer,NA,,Handing Adapter services,RC identified,64542,"On 14/04, at Castle Donington DC encountered fatal error in WMS hanging adapter services. This impacted the packing and dispatch operations resulting to a hanging capability loss of 115k and C&C preposition was moved to 48+ hrs. On 19/04, SSI vendor identified that the issue was caused as all the receiving order lines were deleted by a clean-up service during deployment for a distribution order related to a trailer that was unloaded. The errors have been cleared and DC operations is in progress of unloading the trailers.  Service is being monitored for stability. ",All actions are closed,4/14/2023,4/14/2023,,,,Yes,yes,No,Yes,3rd party change,NA,SSI determined the cause of hanging fatal to be due to all receiving order lines being deleted for a distribution order by the clean-up service during deployment. ,NA,NA,NA,5/8/2023,May,2023,Saloni,Closed,,,2023,4,April,9/16/2025,886,>60
4/10/2023,4/11/2023,89270388,21:45,7:26,09:41,Digital & Data,Data,Foods,Foods Commercial Trading ,,KI,Delay to the EDW SWAT reporting batch,TCS,EDW,,SWAT and FAQT,RC identified,64490,"On 11/04, alerting indicated multiple job failures in the EDW SWAT critical flow due to missing system folders and permission issues. This delayed the availability of latest sales and stock data in FAQT reports (key analytical reports - Commitment sheets). The missing folders were restored to fix the issue at 07:16. ",All actions has been  tracked and closed,4/17/2023,4/17/2023,,,,Yes,Yes,Yes,Yes,Human error - Customer Tech,NA,The issue was caused as folders were archived by weekly job. While recovering a weekly job ‘EDWDFSP012’ failure on 10th April 2023 the parameter was set to archive the files at a directory level instead of archiving the specific folder to recover the job failure due to a human error had caused the issue. ,NA,Human error,NA,5/2/2023,May,2023,Pavithra,Closed,,,2023,4,April,9/16/2025,890,>60
4/10/2023,4/25/2023,89268522,09:08,17:00,367:52,Group Platforms,HR,Group Platforms,HR,,MI,Peoplesystem application is inaccessible,Sdworx,NA,,Peoplesystem,RC identified,64506,"Store colleagues reported that the People system portal is inaccessible. This is impacting the ability to access payslips and view holidays, weekly payroll of 3500 colleagues, T&A cutover for 346 stores etc. A central issue at Sdworx has caused the issue and a backup plan is being devised by the colleague services to mitigate impact. ","Rebuild of AD environment from scratch and reset of all accounts including the AD KRBGT account (SD Worx did not want to take any riskand chose to immediately rebuild our AD environment to allow us to 100% trust the AD environment prior to logging onagain by our engineers into the isolated environment).
Rebuild of application hosting environment including database servers and restore of required databases",4/10/2023,5/29/2023,4/25/2023,NA,NA,NA,NA,NA,NA,3rd party issue,NA,"A cyber attack at Sdworx has caused the issue. 
Detailed information:  The root cause identified by the Sdworx forensic analysis points to a service account of which the password was leaked or obtained in some unknown way and that was used to logon to the environment. ",NA,NA,NA,5/29/2023,May,2023,Saloni,Closed,,,2023,4,April,9/16/2025,890,>60
4/10/2023,4/10/2023,89268516,05:35,8:05,02:30,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,Castle Donington DC users unable to print carrier labels intermittently,TCS,Network,,Label printing,RC unknown,64487,"Colleagues reported intermittent label printing issues between 5:30 and 9:10 on the ground floor (Chamber 2) at Castle Donington. This impacted the packing and dispatch operations resulting to a capability loss of 12,652 singles, however, with no CFR impact. The majority of printers recovered without any specific recovery actions. Root cause investigation is underway.",All actions has been  tracked and closed,4/12/2023,4/24/2023,,,,No,No,No,No,Infrastructure issue / Hardware failure,NA," The root cause is inconclusive as there is no evidence to indicate any issues with the network components and the printing infrastructures.
We identified that the majority of the devices on the ground floor are connected to a network switch which is out of support, therefore, it could not be directed towards further investigation with the vendor CISCO. Actions have been agreed to reboot or replace the switch if the issue manifests itself.",NA,NA,NA,4/24/2023,April,2023,Pavithra,Closed,,,2023,4,April,9/16/2025,890,>60
4/5/2023,4/5/2023,89262468,13:49,16:49,03:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,"Dine In, Bank and Help pages broken on website",Customer,NA,,"Dine In, Bank and Help pages on website",RC identified,64272,"Colleagues from Contact Centre and Stores reported that the Dine In, Bank and Help pages on Customer website were crashing in mobile and tablet devices between 00:00 and 12:30, causing inconvenience to the customer website journey. The content changes made for Top Nav was reverted to restore services. Root cause investigations are underway. ",All actions are being tracked in Problem Actions tab,4/5/2023,4/5/2023,,,,No,No ,No ,Yes,Human error - Business,NA,"A new process was introduced to help colleagues with changes to the navigation to try and minimise user-error. Within AEM, users have to amend a different component depending upon whether they are making desktop/mweb navigation changes or App navigation changes. Unfortunately, the person who made the navigation changes selected the wrong component which resulted in making changes for desktop navigation in the App component. Once the content was published overnight, the ‘broken’ navigation caused problems on the WCS pages. (Note: the FESK pages were not affected by this",NA,NA,NA,9/13/2023,September,2023,Gokul,Closed,,,2023,4,April,9/16/2025,895,>60
3/29/2023,3/31/2023,89249625,12:00,16:30,52:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Payment settlement for Customer Your School Uniform website orders not processed,VisionNet,NA,,Website order payment settlements,RC identified,64292,"The payment settlement for Customer Your School Uniform website orders had been pending from 7th Feb 2023, although the orders were dispatched. This impacted the payment settlement for 1800 orders worth £73,944, however customers were not impacted. Microsoft identified that the payment type was blank instead of the value ‘Credit card’ in the terms of payment setting in Microsoft Dynamics 365 portal and the value was updated to resolve the issue. Further investigations underway with Microsoft.",All actions are being tracked in Problem Actions tab,3/29/2023,5/2/2023,,,,"No,Yes",No,No, Yes,3rd party issue,No,The payment type was blank instead of the value ‘Credit card’ in the terms of payment setting in Microsoft Dynamics 365 portal due to a system bug and the value was updated to resolve the issue. Awaiting detailed RCA from Visionet,NA,NA,,6/14/2023,June,2023,Kavitha,Closed,,,2023,3,March,9/16/2025,902,>60
3/25/2023,3/25/2023,89241000,06:55,10:00,03:05,GTS,Digital Workplace Services,"C&H and Intl, Foods","C&H and Intl Supply Chain, Foods Supply Chain",,SI,HHT connectivity issues across Foods and C&H sites,TCS,Active directory,,HHTs,RC identified,64213,"DC Colleagues reported connectivity issues with HHTs across Bradford Foods, Milton Keynes, Welham Green and Thorncliffe between 06:10 and 10:00. Welham: XD Stores delivery impacted - hanging 3573 singles across 5 stores. Thorncliffe lost 10 hours of productivity.  For Milton Keynes – the impact was 13k singles & 5 hours loss of productivity.  It was found that the Windows Certificate service was in an inactive status and interrupted the regular auto renewal of the CRL (certificate revocation list).  Service was resumed by renewing the CRL.",All actions are being tracked in Problem Actions tab,3/25/2023,3/28/2023,,,,"No,Yes",No,Yes,Yes,Infrastructure issue / Hardware failure,NA,"The windows Certificate Authentication (CA) service facilitates the auto renewal of the CRL (Certificate Revocation list) which is responsible for HHT authentication. The auto renewal will be successful only when the CA service is in active state.

The CA service went into a hung state as it was not rebooted/patched for the last 90 days due to an issue in the CATE environment, which interrupted the auto renewal of the CRL, thus causing the issue.",NA,Infrastructure issue / Hardware failure,NKB-1440,7/4/2023,July,2023,Saloni,Closed,,,2023,3,March,9/16/2025,906,>60
3/23/2023,3/23/2023,89238115,11:11,13:40,02:29,Group Platforms,Enterprise Technology Platform,Group Platforms,Finance,,KI,Intermittent issues while accessing SAP Enterprise portal,TCS,network,,SAP Enterprise portal,RC identified,64215,"On 23/03, Colleagues reported intermittent issues while accessing the SAP Enterprise portal via Chrome or Edge impacting their ability to perform daily tasks like supplier invoices, pricing, stock movement etc. The issue was caused due to a Zscaler change implemented on 20/03 which resulted in issues with the user DNS (domain name search) resolution. The change was reverted to fix the issue. ",All actions has been  tracked and closed,3/23/2023,3/25/2023,,,,No ,No ,No,Yes,Customer Tech change,Yes ,"After the Zscaler CName change for SAP BPM application under CRQ000000178836, the user's authentication requests are getting processed via zscaler(But the expectation is that it should be processed via global protect), since the users are not mapped/provided access in zscaler, the connectivity is dropped at zscaler and the authentication has failed",178836,Process gap,NA,4/25/2023,April,2023,Pavithra,Closed,,,2023,3,March,9/16/2025,908,>60
3/21/2023,3/21/2023,89219900,09:00,9:30,00:30,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,KI,Proactive restart of the SSI application and ignite PODs ,Gridgrain,NA,,SSI,RC identified,64205,"On 21/03, a proactive restart of the SSI application and ignite PODs was performed between 09:00 to 09:30 to avoid any application performance issues. Investigations for a permanent fix are ongoing with the Ignite vendor Gridgain.",All the actions are tracked by Service Leads internally,3/21/2023,3/21/2023,,,,Yes,Yes,Yes,No,3rd party issue,NA,RCA is awaited from Gridgain and it will be tracked under the incident '89231000',NA,NA,NA,4/3/2023,April,2023,Saloni,Closed,,,2023,3,March,9/16/2025,910,>60
3/18/2023,3/18/2023,89228347,12:09,11:45,23:36,GTS,Digital Workplace Services,Customer Channels,Platform & Store Ops ,,SI,Stores unable to access some applications on workstation,TCS,Active directory,,"Peoplesystem, MYHR, TSL, Simple compliance, Myschedule , Myworldstore",RC identified,64066,"On 18/03, Some store colleagues experienced intermittent authentication errors while accessing some applications on their workstations. This impacted their daily activities, however, no impact to store trading. The issue was caused due to a recent patching activity on store domain controllers which was reverted to restore services by 11:45,19/03. Root cause investigations underway.",All actions are being tracked in Problem Actions tab,3/19/2023,3/19/2023,,,,No ,No ,No ,Yes,Customer Tech change,Yes ,"As part of the monthly patching activity, Microsoft patches were applied on production domain controllers on 18/03. The patch ‘KB5022840’ was introduced to address security issues for windows operating system which is incompatible with the legacy 2003 proxy server used for authentication in store domain, thus causing the issue.

The Microsoft link of KB article is below for reference.

https://support.microsoft.com/en-us/topic/february-14-2023-kb5022840-os-build-17763-4010-e914539f-d2bc-4af9-bc01-5964c0ab3903 ",178827,Infrastructure issue,NA,9/12/2023,September,2023,Gokul,Closed,,,2023,3,March,9/16/2025,913,>60
3/14/2023,3/19/2023,89231000,09:00,16:00,07:00,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,KI,Intermittent performance issues within SSI application,Gridgrain,NA,,SSI,RC identified,64205,"On 14/03, Colleagues experienced intermittent performance issues within the SSI Application impacting their ability to load data for whole departments between 09:00 and 16:00. However, the data load worked fine for smaller scopes. Services were restored after restarting the application by 16:00. An uneven distribution of data amongst the Ignite memory PODs had caused issue. Investigation continue with Microsoft for a permanent fix.",Cleanup logic change from 'destroy' to 'clear' (multiple cache names down to one). Extended the JWT token expiry. Few Ignite related changes implemented as per the recommendations.,3/19/2023,3/19/2023,,,,"Yes, No",Yes,Yes,No,3rd party issue,NA,"Vendor Gridgrain confirmed that the performance degradation was cacused due to a cache issue, JWT Token Expiry and Ignite issues.",NA,NA,NA,5/19/2023,May,2023,Gokul,Closed,,,2023,3,March,9/16/2025,917,>60
3/8/2023,3/8/2023,89209183,12:25,17:03,04:38,Group Platforms,Finance,"Foods, C&H and Intl ","Foods Commercial Trading, C&H and Intl Supply Chain",,SI,Cloud applications are unable to connect to SAP as the SAP cloud integration is down. ,SAP,NA,,All SAP Cloud Application,RC identified,64058,There was a connectivity issue between cloud applications and SAP from 12:25 due to a central issue at vendor SAP.  Foods purchase order amendments were unavailable to the suppliers on the One Foods platform and stock information to Czech DC were delayed.  A fix was rolled out to restore the services by 17:03. We await a detailed RCA from vendor SAP.,All actions are being tracked in Problem Actions tab,3/7/2023,3/28/2023, , , ,Yes,Yes,No,Yes,3rd party issue,No,"Due to an issue in the certificate expiration monitor at SAP, an intermediate certificate which establisted the connection between Cloud applications and SAP was not renewed , causing the issue.",NA,NA,NA,4/13/2023,April,2023,Kavitha,Closed,,,2023,3,March,9/16/2025,923,>60
3/7/2023,3/7/2023,89207562,14:45,15:15,00:30,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,"Onyx is down impacting Home Page, some PLPs and PDPs",Customer,NA,,Customer web experience,RC identified,64150,"Onyx (replacement of FESK) was down between 14:45 and 15:15 resulting in errors on the home pages, PDPs and PLPs causing inconvenience to the customer website journey. The issue was caused by a human error whilst making changes to the ""draft"" content",All actions has been  tracked and closed,3/7/2023,3/7/2023,,,,No,No,Yes,Yes,Human error - Business,NA,The issue was caused due to a human error whilst making changes to the global navigation within Contentful where draft content was added to one part of the global (GNAV) /tier 1 navigation and was inadvertently published whilst the draft content was still in place.,NA,NA,NA,4/14/2023,April,2023,Pavithra,Closed,,,2023,3,March,9/16/2025,924,>60
3/2/2023,3/2/2023,89196955,10:00,13:06,03:06,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Incorrect prices on some store tills,TCS,POS,,POS,RC identified,64140,"Store colleagues reported that prices were incorrect on store tills from 08:40. 109 tills had corrupt caches and were predominantly across 3 stores - Altrincham, Taunton, Torbay. An updated cache was applied to the impacted tills to restore the services by 13:06. Root cause has been identified as part of the till renumbering activity.",All actions has been  tracked and closed,3/2/2023,3/2/2023,,,,No,No,Yes,No,Configuration issue,NA,"As a part of till renumbering activity, an exception had occurred which prevented the tills and SCO in the stores to accept the delta prices, causing the issue.
",NA,Configuration issue,RPOS - 7726/RPOS - 7727,3/24/2023,March,2023,Pavithra,Closed,,,2023,3,March,9/16/2025,929,>60
3/2/2023,3/2/2023,89197087,14:17,14:37,00:20,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Stores unable to perform waste reduction,Retail Insight,NA,,Intelligent Waste,RC identified,64139,Alerting indicated that the connection to Retail Insights from CSSM was failing from 11:00 affecting stores performing waste reduction. Stores were not able to carry out waste reduction from 13:00 to 14:40 resulting in potential loss off sales for that period. Vendor Retail Insight advised that there was an issue with their DB indexes which caused timeout errors and this was fixed by 14:37 to restore service.  We await further details from Retail Insight.,All actions are being tracked in Problem Actions tab,3/2/2023,4/10/2023,NA,NA,NA,Yes,Yes,No,No,3rd party issue,NA,"Retail Insight confirmed an indexing issue with a backup table in their database, causing performance degradation and thus resulting in timeout errors. Also, there was a data sync issue at 11:00 causing the problematic backup table to function as primary in the database.",NA,NA,NA,5/29/2023,May,2023,Saloni,Closed,,,2023,3,March,9/16/2025,929,>60
2/28/2023,3/1/2023,89193627,16:17,6:50,14:33,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,MI,Unplanned Power Outage at Castle Donington DC,City FM,NA,,Castle Donington DC Operation,RC identified,64028,"Castle Donington DC had experienced an unplanned power outage between 15:20 and 17:10 impacting their operations. 44K retail order singles were delayed and Hemel trailer was cancelled due to this. City FM confirmed that a manual error caused the outage. After the power was restored, the services were brought up systemically to resume DC operations. However there were issues with hanging operations and it started working after the WCS App server interface restart.  A full service restart was done in sequential order with the help of SSI Vendor. Awaiting detailed RCA from City FM.",All actions are closed,2/28/2023,2/28/2023,,,,Yes,Yes,No ,Yes,3rd party issue,NA,The issue was caused due to activity performed at the time of the outage on the battery charger system resulting in the failure on the main circuit. ,NA,NA,,7/18/2023,July,2023,Saloni,Closed,,,2023,2,February,9/16/2025,931,>60
2/21/2023,2/21/2023,89179544,17:22,18:55,01:33,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,Label printing issues at Castle Donington and Ollerton DCs,TCS,Active directory,,Label printing,RC unknown,64027,"On 21/02, Castle Donington and Ollerton DC users were unable to print labels between 17:22 and 18:55. This caused packing capability loss of 14.9k singles with C&C proposition movement at Donington and miss-promise of 157 orders at Brands DC. The primary domain controller was shut down due to data replication issue, hence the connectivity between the application and database servers was delayed. The faulty domain controller was isolated to restore the connectivity.",All actions are being tracked in Problem Actions tab,2/24/2023,3/13/2023,,,,Yes,Yes,No,No ,RC unknown,No,"The domain controller responsible for server authentication was down due to a database corruption. The application server is usually connected to four domain controllers. As the primary domain controller was down, the server when tried establishing connection with the faulty domain controller, it ultimately timed out, causing performance issues. ",NA,RC unknown,,5/15/2023,May,2023,Kavitha,Closed,,,2023,2,February,9/16/2025,938,>60
2/20/2023,2/20/2023,89171700,15:00,17:25,02:25,Group Platforms,HR,Group Platforms,HR,,SI,Issues in viewing payslips and claiming additonal payments in Peoplesystem,Sdworx,NA,,HR ,RC identified,64128,"Following the decommissioning of Internet Explorer by Microsoft, the single sign on links at Sdworx were inaccessible in Microsoft Edge. Colleagues were unable to view payslips and claim additional payments in Peoplesystem. Also, colleague services were operating with limited abilities to support pay queries. Microsoft was engaged and a fix was implemented across all Customer Windows 10 machines to restore services. ",All actions has been  tracked and closed,2/24/2023,2/24/2023,,,,NA,NA,Yes,Yes,3rd party issue,NA,"This issue has manifested itself following the decommissioning of Internet Explorer by Microsoft. Despite the full disclosure of this to all stakeholders since early last year and preparing for this via obtaining all affected URLs and enabling them to be Microsoft Edge compatible, an SSO (single -sign on) restriction at Sdworx has caused an issue.",NA,NA,NA,3/14/2023,March,2023,Pavithra,Closed,,,2023,2,February,9/16/2025,939,>60
2/18/2023,2/18/2023,89171650,01:56,15:53,13:57,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,ADHOC,Cheshunt-0097- Store Offline ,TCS,Network,,Stores Network,RC identified,64402,"Cheshunt Store lost complete network from 01:56 until 15:30 on Saturday 18th February. Store traded offline, and hence contactless was not working, Honeywells were unavailable and so the store couldn’t complete Waste or counts, booking in their delivery and all other customer functions such as order collections had to be done manually. They had to manually monitor their fridge temperatures so as not to lose stock. ",All actions are being tracked in Problem Actions tab,3/3/2023,3/3/2023,,,,NA,NA,NA,TBC,Infrastructure issue / Hardware failure,NA,"Following a power cut during the night, the power to the Vodafone routers (ADVA modem) was lost. Upon investigation it was later identified that the PDU’s to which the routers are connected had no power and with the stores help the cables were moved to another PDU which resolved the issue",NA,Infrastructure issue / Hardware failure,NA,7/20/2023,July,2023,Gokul,Closed,,,2023,2,February,9/16/2025,941,>60
2/13/2023,2/13/2023,89160374,02:56,12:20,09:24,Group Support,Finance, C&H and Intl,C&H Commercial Trading ,,KI,Delay to the GMOR batches ,Customer,NA,,SSI,RC identified,64129,Alerting indicated that one of the GMOR jobs overran while processing a significant volume of data. This impacted the GMOR reporting batches completion of 12/02 and delay in updating GMOR intake data in SSI reports. Proactive actions are being performed to avoid recurrence of the issue. Root cause investigations underway.,All actions are being tracked in Problem Actions tab,2/13/2023,2/23/2023,,,,"Yes, No",Yes,No,Yes,Human error - Business,NA,"The GMOR back posting job responsible for processing the Purchase Orders with goods receipt information was overrunning due to a significant volume of purchase orders with amended delivery date, causing the issue.

Ideally, there should be no change in delivery date once the POs are goods receipted. However, on 12/02, a change in delivery date was performed on a bulk number of goods receipted POs due to the delivery of PO's beyond their delivery date as per business confirmation.",NA,Human error - Business,NA,4/13/2023,April,2023,Gokul,Closed,,,2023,2,February,9/16/2025,946,>60
2/6/2023,2/6/2023,89146854,10:38,11:43,01:05,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Sparks unavailable for iOS App,TCS,Sparks Hub Product Support,,Sparks,RC identified,63862,Customers were unable to access Sparks hub to view their sparks offers on iOS devices between 10:38 and 11:43 resulting in a poor customer experience. The issue was caused due to a security change implemented by Customer bank which was reverted to restore the service. Root cause investigation underway.,All actions are being tracked in Problem Actions tab,2/10/2023,2/10/2023,,,,Yes,No,No,Yes,Customer Tech change,Yes,"Following a vulnerability review of the corporate customer device service the corporate services team were requested to apply security checks as part of their API requests to minimise security risks. As part of this activity, a change was made on Apigee on a shared API component used by the Mobile Apps to validate a customer's sparks account. The change on Apigee implemented a validation to ensure a token passed by the app is present prior to sending the request to their backend application. The Mobile App was passing the token correctly, however, the implemented validation was not fetching the token correctly and this resulted in requests from the Mobile App receiving an error. This error had a more substantial impact on iOS App in particular, where the customer experience was that the sparks hub would not load correctly due to the error, highlighting an additional bug in the iOS code. The issue was fixed by reverting the Apigee code to the version prior to the security fix after which responses to this service were returning a successful response and the sparks hub was then loading correctly",CR#176423,Design issue,NA,5/1/2023,May,2023,Gokul,Closed,,,2023,2,February,9/16/2025,953,>60
2/1/2023,2/1/2023,89134640,05:56,8:26,02:30,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,MI,Customer.com - Drop in mobile orders ,TCS,Ecomm platform,,Moblie and IOS orders,RC identified,63822,"Alerting identified a significant drop in mobile orders between 05:56 to 08:26 were identified in both Android and iOS devices. This impacted around 3k orders (based on a comparative day), however, there were no loss in revenue. The issue was caused by a human error where 2 Microsoft Azure IP addresses were blocked, thus causing the issue. Services were restored by unblocking the IP addresses. Detailed RCA underway.",All actions are being tracked in Problem Actions tab,2/3/2023,2/3/2023,,,,Yes,Yes,No,Yes,Human error - Customer Tech,No,"During the investigation of the threshold breach of the 404 error alerts, if any IPs are found to be suspicious, it is then blocked with the help of Infosecurity team, by default. Though the threshold alerts were valid in this case, the IPs were inadvertently blocked by Infosec as per tech advice.",NA,Human error,NA,2/28/2023,February,2023,Kavitha,Closed,,,2023,2,February,9/16/2025,958,>60
1/31/2023,2/1/2023,89119732,17:20,12:00,18:40,C&H and Intl ,C&H Commercial Trading , C&H and Intl,C&H Commercial Trading ,,KI,Issues in processing the 3rd Party wholesale Brands PO’s ,TCS,Brands,,Brands,RC identified,63931,"Editrack were unable to send stock to Customer affecting approx. 30,000 singles across 9 Brands until 12:00,01/02.  Due to a configuration issue, the wrong business unit ID for Brands were sent to Editrack. A work around was applied to correct the business unit id and resend the purchase orders to Editrack.",All actions are being tracked in Problem Actions tab,2/6/2023,2/6/2023,,,,No ,No,Yes,No ,Design issue,No,"The Brands Business unit was introduced in 2021 and departments moved into that BU – but Editrack cannot handle PO’s for that new BU. This was hidden until 2023 due to one SAP Shadow table having stale data (which meant that the PO’s still went to middleware and Editrack with their old BU such as Menswear/Womanswear rather than Brands).When the stale data was refreshed in SAP on 16th January, the PO’s were sent with Brands BU (BUID 100) as per design – and could then not be processed by business in Editrack.",NA,Design issue,,4/24/2023,April,2023,Kavitha,Closed,,,2023,1,January,9/16/2025,959,>60
1/28/2023,1/29/2023,89125592,21:10,9:00,11:50,GTS,Enterprise Technology Platform,All BU's,All BU's,,SI,Multiple network devices in Swindon data centre were unreachable,Vodafone,NA,,Network,RC identified,63811,"At 21:10 on 28/01, alerting indicated that multiple network devices in Swindon data centre were unreachable due to a connectivity issue. The goods receipts flow between GIST and ASO and Quantum responses to ASO were impacted between 21:10 and 22:50, CSSM Foods app was affected by intermittent errors from 06:17 to 06:30. The EDW prod and disaster recovery replication and the availability of Foods commitment sheet was delayed as well. As a part of a decommission activity at Vodafone for another customer, some network cables were inadvertently disconnected from the Customer devices, thus causing the issue. The connectivity was restored by 09:00. Hypercare continues to monitor stability.",All actions are being tracked in Problem Actions tab,1/29/2023,1/29/2023,,,,Yes,Yes,No ,Yes,3rd party issue,NA,The issue was caused by a human error where multiple cables were removed from the Customer racks as part of STP2 decommissioning.,NA,NA,,5/30/2023,May,2023,Saloni,Closed,,,2023,1,January,9/16/2025,962,>60
1/25/2023,1/25/2023,89118309,07:55,8:30,00:35,GTS,Enterprise Technology Platform,All BU's,All BU's,,KI,Colleagues were unable to access multiple applications hosted on Microsoft o365 and Azure services,Microsoft,NA,,"SSI, Microsoft Teams, MS Outlook, PIM, Splunk, Azure express route etc.",RC identified,63905,Microsoft central issue impacted multiple applications and services across various portfolios between 07:00 and 08:25. Microsoft advised that a recent WAN update have caused the issue and it was rolled back to restore services. Azure express route (connection between Stockley and the Azure cloud) was running on secondary until the issue was resolved.  Awaiting RCA from Microsoft. ,All actions have been completed,1/25/2023,2/1/2023,,,,Yes,Yes,Yes,Yes,3rd party change,NA,Microsoft confirmed a change in their WAN infrastructure had caused the issue,NA,NA,,2/3/2023,February,2023,Saloni,Closed,,,2023,1,January,9/16/2025,965,>60
1/24/2023,1/28/2023,89116617,09:18,18:13,104:55,Group Platforms,HR,Group Platforms,HR,,KI,Sickness absences entered into the system were missing.,SDworx,NA,,MyHR,RC identified,63934,On 24/01 Colleague Services reported that they had received Service Requests from stores informing them that sickness absences entered into the system were missing. SD Worx investigated and on 26/01 the incident was escalated to sev 1 as the calls into CS increased. On 27/01 SD Worx confirmed that a change implemented on 20/01 to resolve an issue with absences interfacing from MyT&A had caused the issue. The change was reverted to resolve the issue and a workaround was reinstated for the T&A issue. Over the weekend SD Worx and Colleague Services worked together to manually correct the missing absences pre-reversion. Further calls with both parties taking place today to complete the actions and confirm resolution. Correction for weekly payroll needed completion by eod 30/01 to avoid payroll impact.,All actions have been completed,1/27/2023,2/10/2023,,,,No ,No ,Yes,No,3rd party change,CR,"This issue occurred following a change deployed on 20th January to MDTA Plug-In where a fix was applied to production environment for another issue. The defect is related to the Blue Yonder TNA deployment. The fix was to allow tags for Support Centre employees to interface to WFM. 
Scenarios tested as part of the fix were: 
1. Absence data interface from Blue Yonder to WFM 
2. Absence data interface from Oracle MYHR (Support Centre) to WFM The absence modification via screen was not covered as part of the test scenarios. 

This change was deployed as part of Hypercare and therefore under the current process this change was not subject to system testing and UAT.",RFCP48367,inadequate testing,NA,2/28/2023,February,2023,Sravan,Closed,,,2023,1,January,9/16/2025,966,>60
1/24/2023,1/24/2023,89116913,12:30,17:30,05:00,Foods,Foods Commercial Trading ,Foods,Foods Commercial Trading ,,KI,FIND application was inaccessible,TraceOne,NA,,FIND,RC identified,63935," 
On 24/01, FIND application was inaccessible between 12:30 and 17:30, however, the existing sessions were working as expected.  This impacted product lifecycle activities for Customer, Suppliers and our 3rd party Food Regulatory partner. A central problem at vendor TraceOne had caused the issue, impacting multiple customers. Awaiting root cause and resolution details.
 ",All actions are being tracked in Problem Actions tab,1/24/2023,2/1/2023,,,,No,No,No ,Yes,3rd party issue,No,"A network hardware issue at Traceone had caused a failure on SQL cluster, resulting the inaccessibility of the FIND application",NA,NA,NA,5/11/2023,May,2023,Kavitha,Closed,,,2023,1,January,9/16/2025,966,>60
1/24/2023,1/24/2023,89114796,08:30,19:00,10:30,Group Platforms,Finance, C&H and Intl,C&H and Intl Supply Chain,,KI,The stock at Hemel to the international partners was not present in WMS,TCS,SAP,, DC operation,RC identified,63809,Stock at Hemel due to be despatched to our international partners does not have any onward instructions on the WMS system and hence the stock is sitting at DC from 23/01  3 trailers are currently held at Hemel (and have missed their deadline). There was a SAP mapping change made over the weekend which has caused an issue within the middleware. A work around was performed to manually delete the order references and outbound delivery instructions were re-triggered to WMS.  The 3 impacted trailers have now interfaced in WMS. DC has been advised to dispatch the stock.,All actions are being tracked in Problem Actions tab,1/24/2023,1/26/2023,,,,Yes,Yes,Yes,TBC,Customer Tech change,Yes,"There was a SAP mapping change made to map the internal reference numbers from our partner ordering platform (IBT). The same reference field is captured against the Hemel Hub orders and was made available for the cross-dock orders.

In addition, an issue in the MULE mapping logic was highlighted by the change causing message failures in MULE.  

There are 2 fields available to store reference numbers that are sent under a single segment in the I177 messages from SAP.  They are differentiated by a QUALI value (002 for pick sheet reference, 001 for other reference).  For Hemel Hub orders they are mapped without issue however for Hemel cross dock orders only the 002 pick sheet reference is required for WMS. The MULE code looks for 002 qualifier in the reference segment and is incorrectly considering the whole segment as an array and getting a conflict when both ref values were present in the segment due to the inclusion of the 001 qualifier and ref value.
",CRQ173671,Design issue,,2/14/2023,February,2023,Kavitha ,Closed,,,2023,1,January,9/16/2025,966,>60
1/22/2023,1/22/2023,89111954,03:10,13:57,10:47,GTS,Operations,Customer Channels,Service Experience,,MI,Tills are slow across few stores due to full cache refresh,TCS,PCM ,,POS ,RC identified,63575,"Stores reported slow performance/freezing of tills. A full cache refresh job was released inadvertently which caused the issue. 5 to 10 % of tills across some stores experienced slowness due to this, however there was no overall impact to transactions.",All actions are being tracked in Problem Actions tab,1/22/2023,1/22/2023,,,,Yes,Yes,No,Yes,Human error - Customer Tech,No,A full cache refresh job was released inadvertently while releasing the jobs for a planned change and this had caused the issue,NA,Human error,,5/3/2023,May,2023,Kavitha ,Closed,,,2023,1,January,9/16/2025,968,>60
1/21/2023,1/23/2023,89110302,00:00,7:18,55:18,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Castle Donington WCS unable to process retail orders.,SSI Scheafer,NA,,CD WCS,RC identified,63796,"Castle Donington is unable to create batches for retail hanging orders in WCS system since 21st Jan. This impacted 49.8K hanging retail orders on 21st and 22nd Jan. SSI have identified that a backend flag is functioning incorrectly within the consolidation order process, thus causing the issue. Therefore, a manual intervention was made to process the order volume of 22/01. Currently, today’s order volume are being processed",All actions are being tracked in Problem Actions tab,1/24/2023,1/24/2023,,,,No,No,Yes,NA,3rd party issue,No,The ‘ordertargettype’ field in ‘wmsconsolidationorder’ was somehow being set incorrectly by WMS (change from hanging to boxed) which was preventing retail routes from batching. ,NA,NA,NA,2/10/2023,February,2023,Gokul,Closed,,,2023,1,January,9/16/2025,969,>60
1/21/2023,1/22/2023,89110869,22:00,22:10,24:10,Group Platforms,Finance,"C&H and Intl, Group Platforms","C&H and Intl Supply Chain, C&H Commercial Trading, Finance ",,SI,Intermittent performance issues in SAP after SAP upgrade,TCS,SAP,,"D&F, EDW, MP ",RC identified,63779,"The planned SAP ECC upgrade was delayed by 2 hours and intermittent performance issues were observed during the start of the Saturday overnight batches. As an impact, the D&F orders to Castle Donington and other DCs missed the cut off time, causing a delay to the DC picking operations. In addition, 15% of retail sales were understated in GMOR (Saturday sales) along with a variance of £640K between RD and BI.  A parameter change was implemented in SAP ECC followed by the application server restart as suggested by the SAP vendor. SAP systems have remained stable and overnight batches of 22/01 (Sunday) completed without impact.",All actions are being tracked in Problem Actions tab,1/22/2023,1/22/2023,,,,Yes,Yes,No,NA,Customer Tech change,Yes,"SAP outage extended: The SAP outage was delayed due to a combination of a delay in the pre - requisite activity due to high volume of messages processing in production. Further to this: a technical error occurred due to a bug while copying a table in the new software and hence the tables had to be regenerated resulting to additional delay.

Intermittent performance issues: There was a database slowness observed after the upgrade in message processing due to intermittent connectivity issues – this was resolved by a communications blocks parameter change.

A further latency issue occurred when multiple batches were released by PCM in error which caused the batch processing to be overloaded.",CRQ173468,Code/Product bug,,2/3/2023,February,2023,Saloni,Closed,,,2023,1,January,9/16/2025,969,>60
1/19/2023,1/19/2023,89106250,13:31,14:21,00:50,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Drop in Android App Orders ,Customer,NA,,Android app ,RC identified,70551,"There was a drop in order volume and revenue on Android App from 14:00, 18/01. Customers were unable to place orders & pay on the app.  Errors related to a Tealium Tag (Tag management system used on the website) were observed at the checkout stage. The last known change made on this tag was reverted to restore services by 14:21, 19/01. ",All actions are being tracked in Problem Actions tab,1/19/2023,1/19/2023,,,,Yes,No ,No ,No ,Customer Tech change,Yes,"Caused by a Tealium Tag deployment. Tealium is the Tag management service used by website to add script components used for either collecting data or providing functionality such as webchat to the website. Periodically Tealium provide new versions of their core script called Utag.js (A container script) as a template and request to remain up to date. Along with the recent Template changes, a consent manager tag comprising of 4 files was also added to the change, so in effect 2 changes were being made as part of a single publish. Usually new Utag templates are tested via a chrome extension which loads the Tealium changes from dev and runs it against the live website to check whether any issues occur. This was checked and the tags seemed to be firing correctly and consenting. As a result, the template was then updated and published on Tealium at 18/01 13:41. Following this publish the place order & pay button became unresponsive on Android App.",TBC,Inefficient deployment plan,NA,5/16/2023,May,2023,Gokul,Closed,,,2023,1,January,9/16/2025,971,>60
1/18/2023,1/18/2023,89104009,16:09,17:05,00:56,GTS,Enterprise Technology Platform,Customer Channels,"Platform & Store Ops, Service Experience",,SI,Multiple stores hard down due to BT Data Centre power outage ,BT,NA,,Store Network,RC identified,61721,A power outage within a BT local Data Centre occurred causing 27 stores to trade offline and 43 to operate on their primary or secondary link. Offline stores traded with impact to contactless payments and store operational procedures. PIR held with actions assigned to understand the number of stores with no resilience and plan to address.,All actions are being tracked in Problem Actions tab,1/18/2023,1/18/2023,,,,Yes,Yes,No,No,3rd party issue,NA,The issue was caused due to a power outage in Faraday Data Centre.,NA,NA,NA,5/17/2023,May,2023,Sravan,Closed,,,2023,1,January,9/16/2025,972,>60
1/8/2023,1/9/2023,89082631,17:35,12:25,18:50,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Issues in performing counting for F19a Grocery and F20a Wine stock at stores,TCS,"CSSM, Honeywell",,Count app,RC identified,63734,"Stores reported that the CSSM count app had stopped responding while counting F19A and F20A (Grocery and Wine) products during the foods major stock take activity. However, the majority of the main chain stores had completed their counting on 08/01. Out of 577 stores, 513 completed counting for F19a and 565 completed for F20A.  It is believed that a bug present in the version of the android software tool kit on which the app is built is causing the issue. Investigations continue.",All actions are being tracked in Problem Actions tab,1/19/2023,1/19/2023,,,,No,No,No ,Yes,Customer Tech change,CR,"A bug present in the target SDK version 26 (software development kit used to build applications in Android) in CSSM count app released in the Nov 2022 had caused the issue. As per Android developer document, if the target Sdk Version >= 24 and the saved state goes beyond a certain size, it will cause a runtime exception crash. The volume of counts performed across F15a and F20a products were significantly higher on 08/01, thus causing the issue.",CR#172013,Code/Product bug,,2/20/2023,February,2023,Saloni ,Closed,,,2023,1,January,9/16/2025,982,>60
1/3/2023,1/3/2023,89070787,01:00,2:55,01:55,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,WMS performance issues at Castle Donington DC,Customer,NA,,CD WCS,RC identified,63733,"WMS performance issues impacted Castle Donington across the site between 01:00 and 02:25. Due to the time of day, there were 4k missed promises. Root cause under investigation.",It was observed huge amount of locks performed in HBW causing the Queue pile up and a netcool alert was triggered around 00:29 The automation daemon processed and completed by 02:25. Site was back to BAU around 02:55.,1/5/2023,1/5/2023,,,,Yes,No,yes,Yes,Human error - DC Operations,No," Root cause identified to user error, locking stock for PPM in HBW crane caused excess of WMS messages slowing down the server.",NA,NA,NA,1/5/2023,January,2023,Gokul,Closed,,,2023,1,January,9/16/2025,987,>60
12/23/2022,12/23/2022,89054239,08:36,12:08,03:32,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI, Stores reporting issues with scanning CFTO QR codes in the tills.,TCS,"CFTO, VOD",,CFTO collection App,RC unknown,63508,"Stores reported intermittent delays when scanning the QR code in the tills for CFTO collections between 08:36 and 12:08. This caused some inconvenience to our customers and colleagues, but, no collections were missed.  It was identified that pod restarts, latency in response time, db sessions with duplicate till ids, had caused an overall performance degradation. As a workaround, stores were advised to take payments through the tills. To improve performance, the number of pods were uplifted from 9 to 14 and no further issues were reported from the stores until 15:00, 24/12, end of CFTO collection. Root cause analysis continues with the vendors Mongo DB, Flooid & Microsoft.",All actions are being tracked in Problem Actions tab,12/29/2022,NA,,,,Yes,No,Yes,TBC,RC unknown,No,"It is currently believed that a combination of pod restarts, latency in response times, db sessions with duplicate till ids, had caused an overall performance degradation. Detailed root cause is currently under investigation. ",NA,RC unknown,RSS- 2609,2/13/2023,February,2023,Saloni,Closed,,,2022,12,December,9/16/2025,998,>60
12/13/2022,12/13/2022,89034867,14:12,15:27,01:15,GTS,Enterprise Integration,Supply Chain,Foods Supply Chain,,KI,Foods DCs reporting issues in GR (goods receipts) flow,TCS,Integration Services,,ASO,RC identified,62076,"On 13/12, Foods DCs reported a delay of GRs (goods receipts) flowing into ASO impacting their picking operations and thereby delaying the replenishment into stores between 14:12 and 15:45. Message pile ups were observed in the middleware due to significant .com order volume combined with CFTO and peak level GIST messages, thus causing the issue. The CFTO finalization messages between Sterling and ORCA were paused for an hour to clear the piled up messages. In addition, the ASO inbound traffic was diverted to Swindon.",All actions are being tracked in Problem Actions tab,12/19/2022,44914,,,,Yes,Yes,Yes,No,Design issue,No,"We had high dotcom ordering volume on 11th and 12th December (225K and 229K respectively), due to which OMS was triggering large amounts of Order Events to BEAM/CDP and other consumers during business hours for the whole week.

Message flows from Dotcom to other cloud applications were routed through OnPrem Gateway as part of EM v2 migration in October. This caused pileups in the common queue transferring messages from OnPremises MQ Gateway to Cloud consumers.  GR messages from GIST to ASO was one among the impacted applications due to this pileup issue.

 

A change to route the Order Events messages through a dedicated High NFR queue needs to be implemented.",NA,NA,1801,2/6/2023,February,2023,Kavitha,Closed,,,2022,12,December,9/16/2025,1008,>60
12/9/2022,12/9/2022,89026544,10:30,11:55,01:25,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,MI,Stores reporting issues with Accuracy Checker when doing target counts,TCS,CSSM,,Accuracy Checker,RC unknown,63282,"Store colleagues were unable to carry out target counts using the Accuracy Checker application on their Honeywells between 10:30 and 11:55. Stores were unable to check the possible discrepancy between the physical stock available in store and in the system. Any other CSSM functions could have been impacted as well. The issue was caused due to the SQL database server reaching its 100% CPU utilization along with multiple long running queries on the database. Therefore, the CPU was uplifted on the CSSM database followed by a service restart on 10/12 and hypercare was put in place. A PIR is planned on 13/12. ",All actions are being tracked in Problem Actions tab,12/12/2022,,,,,Yes,No,No,No,RC unknown,NA,"Initially a change carried out on 30/11 to the way API calls are made for Accuracy Checker was investigated as a potential cause of the issue, however it has since been proven that those API calls are not relevant to the CSSM SQL database.
 
We have raised a log with Microsoft to get assistance in the diagnosis of why the CPU utilisation increased to 100%.  We have also increased our levels of alerting and logging to be able to react quicker in the event of a recurrence before stores are impacted. To mitigate the impact we have increased CPU by 25% on the CSSM SQL DB server. Microsoft confirmed that they could not identify the cause of the issue and requested to capture additional logs if the issue re-occurs",NA,RC unknown,1802 ,2/6/2023,February,2023,Sravan,Closed,,,2022,12,December,9/16/2025,1012,>60
12/5/2022,12/5/2022,89016407,04:10,4:49,00:39,Group Support,Finance,Commercial Trading,C&H Commercial Trading ,,KI,Delay to the GMOR critical batches,TCS,GMOR,,GMOR,RC identified,63334,"On 05/12, alerting indicated one of the GMOR jobs was overrunning causing a slight delay in the overnight GMOR batches. The job got completed at 04:49 without any tech intervention. The root cause was attributed to bulk changes of PO (purchase orders)delivery date made by colleagues.",All actions are being tracked in Problem Actions tab,12/6/2022,12/6/2022,,,,"Yes, No",Yes,Yes,No,Process Gap,NA,"The RPA (Robot process automation) colleagues had been performing their SAP PO (purchase order) amendments (back postings) at PO levels with a maximum limit of 3k in SAP ECC and the number of line items was not considered. On 05/12, whilst the PO count was within the limit, the line items count was considerably higher (75k). Therefore, the GMOR job overran while processing a significant amount of back posting PO data volume causing a delay to the overnight batches",NA,NA,NA,12/19/2022,December,2022,Gokul,Closed,,,2022,12,December,9/16/2025,1016,>60
12/3/2022,12/3/2022,89011800,10:30,20:00,09:30,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Issues with customer FTO orders,TCS,"SCRD, ORCA",,"SCRD, ORCA",RC identified,63233,Foods Logistics reported issues with Cake collection as products were sent to incorrect depots. This impacted 108 customer orders(Cakes and Sandwich Platters). The store movements in SAP were not reflecting in SCRD due to a caching issue in SCRD. The list of impacted orders were shared with Customer logistics to manage the order collection. Further actions will be managed between Foods tech and Customer logistics.,All actions are being tracked in Problem Actions tab,12/5/2022,12/5/2022,,,,"No, yes",No,Yes,NA,Code/Product bug,NA,"The store movements in SAP was not reflecting in SCRD (Supply Chain Reference data) as the application pod cache was not cleared in a timely manner. There is an existing bug in the cache clear logic and therefore the cache clear function did not work as expected. This was not identified as the AKS cluster housekeeping activity was periodically restarting the pods every week which cleared the cache until 23/10. 
 
Written instructions were given to the suppliers (GREENCORE & PARK CAKES) to send the orders for impacted store moves to the correct depots (Hemel instead of Barnsley), however, due to a misunderstanding between Suppliers & Customer Dotcom business operations, the entire stock (FTO) for all stores were sent to Hemel instead of Barnsley.",NA,NA,"SCLT2-4053 - Recon between ORCA and SCRD for store moves
SCLT2-4054 - Recon between ORCA SAP and FITS for FCO POs",1/17/2023,January,2023,Saloni,Closed,,,2022,12,December,9/16/2025,1018,>60
12/1/2022,12/1/2022,89007834,11:52,12:31,00:39,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Contactless and Chip & pin payments were declined at stores,Worldline,NA,,POS,RC identified,63327,Store colleagues reported that contactless and chip & pin payments were getting declined intermittently between 11:52 - 12:31 and 13:39 - 13:43. Worldline was engaged who advised a hardware issue impacting multiple customers. Awaiting root cause and recovery details from Worldline.,All actions are closed,12/15/2022,12/15/2022,,,,"No, Yes",No,Yes,No,3rd party issue,NA,Latency was observed on a storage array due to an issue with one of the disk on the global infra network which had impacted the transactions intermittently. The impacted disk was isolated to restore the service,NA,NA,SCLT2-4054 - Recon between ORCA SAP and FITS for FCO POs,6/13/2023,June,2023,Sravan,Closed,,,2022,12,December,9/16/2025,1020,>60
12/1/2022,12/1/2022,89008298,15:30,16:30,01:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,12 Days of Sparks - broken customer journey and incorrect number of offers,TCS,Sparks Journey,,Sparks,RC identified,63230,"As part of the 12 days Sparks campaign, customer validations highlighted an issue in the Sparks web journey. 500 customers had experienced broken website journey. The issue was caused due to the pre-purchase delivery pass banner and it was therefore removed. A code fix was implemented to fix the issue permanently and the banner was switched on at 07:30, 02/12",All actions agreed are closed. ,12/6/2022,12/9/2022,,,,"No,yes",NO,No,No ,Customer Tech change,TBC,"1. Broken customer journey relating to delivery pass on web. 
End to end regression testing for the Sparks Advent Calendar did not cover the full customer journey on web  e.g. Delivery pass, sparks plus. 
2. Customers with missing offers. 
This was caused by the Offer team moving  the offer type from Advent calendar to Bonus for testing purposes . 
3. Customers with more than 12 offers 
 Sparksplus customers incorrectly got the ”Snowman” offer applied.  This was caused by human error on Planning Inc’s side (3rd party)   ",TBC,Human error,,12/19/2022,December,2022,Kavitha,Closed,,,2022,12,December,9/16/2025,1020,>60
11/29/2022,11/30/2022,89006243,20:30,14:02,17:32,Group Platforms,Data,Group Platforms,Finance,,SI,Retail Dashboard sales flow was delayed,TCS,EDW,,Retail dashbaord ,RC identified,63228,"Alerting indicated a significant variance in POS sales data between SAGG (Sales Aggregation) and EDW. The Retail dashboard sales figures were updated at 14:02 (a delay of seven hours).  A surrogate key (used to identify a unique record in a database table) had breached its decimal limit, and hence no sales data was loaded from 17:00, 29/11. This was reset to fix the issue and the sales data was loaded successfully. ",All actions are being tracked in Problem Actions tab,12/1/2022,12/1/2022,,,,"Yes, No",Yes,No,No ,Configuration issue,NA,"1st issue: The POS job failed while loading the sales data in the table, as the surrogate key value had reached 11 decimal places (10000000000). This is the first time after the EDW GO-Live, where this value had gradually reached to its maximum, thus causing the issue.A temporary table was created to use the minimum available key value (starts from 1) followed by a code change in the main table to reset the key value to its minimum.

2nd issue: In addition, one of the EDW reporting jobs that loads the data from the problematic table (issue 1) overran due to invalid records as they were carrying the same key value that was processed by the POS job, thus causing delay in job completion.

These invalid entries were systemically masked to recover the job.",NA,NA,PBI000000063718,44925,December,2022,Gokul,Closed,,,2022,11,November,9/16/2025,1022,>60
11/27/2022,11/28/2022,89001060,21:00,8:00,11:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI, Stock mismatch between PDPs and checkout for free gift promotional product,TCS,WCS/OMS,,.com ordering ,RC identified,63209,"Customer contacts and internal stakeholders were unable to place orders for bedding products associated with a promotion enabling a free gift product (Free Pillow Spray). 1.4k orders were impacted due to this. The issue was caused due to a mismatch between the Stock Service inventory and the WCS/OMS inventory. To restore service, both the promotion and the free gift were removed from the website. ",All actions are being tracked in Problem Actions tab,12/1/2022,44896,,,,"No, Yes",No,No,No,Customer Tech issue,NA,"The available stock quantity for the promotional item was not the same in OMS and in Stock Service (SS),   stock was not available in OMS but was available in SS. This meant that the free gift could be successfully added to the basket but when the customer attempted to checkout, a call was made to OMS where it was found that there was no stock available thus preventing the customer from checking out. 

The issue was caused by the free gift having negative demand. Although SS is designed to send 0 demand quantity if the stock is negative, further inventory updates (demand adjustments) incremented the stock due to negative value. 
 
The reason for negative demand stems from an issue which occurred before Departments went live in SS. This related to demands against Advised stock, which, when items were despatched, was not handled correctly and impacted the whole supply/demand picture.

The free gift was not a part of the clean-up activity because only items with demand existing at the point of the SS database data population were taken from OMS and transferred into the  SS database.  Going forward the data clean up will occur irrespective of holding demands",NA,NA,NA,1/3/2023,January,2023,Gokul,Closed,,,2022,11,November,9/16/2025,1024,>60
11/27/2022,11/28/2022,88997922,20:00,15:21,19:01,Digital & Data,Data,Commercial Trading,Foods Commercial Trading ,,SI,Delay to Foods SWAT and C&H GML Reporting flows,TCS,EDW,,SWAT and FAQT,RC identified,63208,"Overnight, a delay was observed in the EDW SWAT Reporting batch due to an issue with its cyclic predecessor job. Foods SWAT reporting SLA of 07:00 was missed delaying the availability of latest sales and stock data in FAQT reports (key analytical reports - Commitment sheets). In addition, the C&H GML reporting SLA of 09:00 was missed. The predecessor cyclic job had started late yesterday and hence did not run all its cycles which caused the SWAT batch to not start on time, this is currently being investigated",All actions are being tracked in Problem Actions tab,11/29/2022,11/29/2022,,,,Yes,Yes,No,No ,Human error - Customer Tech,NA,"The 12 EDW cyclic jobs that normally run from 8am to 8pm started at 12pm on 27/11.  This delay was because the SUNDAY weekly predecessor job (EDW_STG_PURGE) before the cyclic jobs, which normally starts at 7am was long running due to volume.  This purges one week’s data from staging tables and the run time depends on data volume.  This job usually completes by 11am and on the day it completed 12:03pm.  This resulted in only EIGHT cyclic jobs running when the mandatory number of jobs for the SWAT flow to start is NINE between 8am-8pm.",NA,NA,,12/25/2022,December,2022,Saloni,Closed,,,2022,11,November,9/16/2025,1024,>60
11/21/2022,11/21/2022,88980082,11:30,14:00,02:30,Group Platforms,Enterprise Technology Platform,Group Platforms,Finance,,SI,cSAP BW and BPC was inaccessible.,TCS,SAP,,SAP,RC identified,63000,"Whilst investigating why Day -1 jobs had been over running, an issue was found with the primary network adaptor (VIOS). VIOS services were failed over to secondary as a workaround to restore service.  cSAP BW and BPC were unavailable between 11:30 - 14:00, another outage of 43 minutes was taken to fix a performance issue with the Enterprise portal.  This issue impacted user’s ability to perform planning and reporting activities in SAP.  The root cause was found to be a memory issue in a monitoring agent and IBM have provided a temporary fix which is being tested.",The actions are tracked in the action tab ,11/24/2022,11/24/2022,,,,Yes,Yes,Yes,Yes,Customer Tech issue,NA,"VIOS are virtual servers which virtualise the physical I/O cards such as ethernet, fibre cards etc.  

The initial cause of the SAP slowness was due to the shared ethernet adaptor switching continually from primary VIOS to secondary VIOS (flapping). A case was raised with IBM who advised that the Premium VIO agent for the ITM Tivoli monitoring had an issue which caused a memory leak in the VIO server.  This agent used for ITM monitoring – it was malfunctioning and utilising most of the memory.  On the day of the issue led to the ethernet cards flapping",NA,NA,NA,12/5/2022,December,2022,Sravan,Closed,,,2022,11,November,9/16/2025,1030,>60
11/18/2022,11/18/2022,88975453,11:30,14:00,02:30,Group Platforms,Enterprise Technology Platform,Group Platforms,Finance,,KI,Delay in Day -1 processing in SAP,TCS,SAP,,EDW Day-1,RC identified,63000,"From 18/11, there has been a considerable delay in the Day-1 processing in SAP ECC and BW systems. An emergency outage was agreed to restart the SAP systems, however the issue still exists. Our SAP teams and working with vendor SAP to find the root cause and resolution. ",The actions are tracked in the action tab ,11/24/2022,11/24/2022,,,,Yes,Yes,Yes,Yes,Customer Tech issue,NA,"VIOS are virtual servers which virtualise the physical I/O cards such as ethernet, fibre cards etc. 

The initial cause of the SAP slowness was due to the shared ethernet adaptor switching continually from primary VIOS to secondary VIOS (flapping). A case was raised with IBM to understand the actual cause.
 

The IBM advised that the Premium VIO agent for the ITM Tivoli monitoring had an issue which caused a memory leak in the VIO server.  This agent used for ITM monitoring – it was malfunctioning and utilising most of the memory.  On the day of the issue led to the ethernet cards flapping",NA,NA,NA,11/24/2022,November,2022,Saloni,Closed,,,2022,11,November,9/16/2025,1033,>60
11/18/2022,11/18/2022,88971977,10:25,14:10,03:45,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Orders being placed offline & Sterling COM & DOM Unavailable,TCS,Sterling Support,,"order amendments, refund processing, parcel scanning, paypal orders etc",RC unknown,62972,"Alerting indicated that some orders were placed in offline mode between 10:25 and 12:26. Colleagues (Ollerton, Contact Centre & Stores) were unable to access Sterling impacting order amendments, refund processing etc. In addition, parcel scanning  was impacted at stores. Further impact was outlined in the tech comms. Issues were identified in the primary Sterling database which was inaccessible. Services were failed over to new primary database and system resources were increased to bring up the Sterling API servers by 14:10, 18/11. Message reconciliation identified 468 missing orders in Sterling and details have been shared with Ops to communicate to customers if required. Services are back to usual across all impacted areas. A detailed PIR will be carried out.",All actions are being tracked in Problem Actions tab,11/18/2022,11/25/2022,,,,Yes,Yes,Yes,Yes,RC unknown,No,"Post server restarts 17th night/18th morning, 2 web servers were responsive but OMS was not running on them. Classes were not loaded properly & logs were not loading into the Splunk forwarders. To resolve this, servers were restarted (scaled up & down) but as you could not see via logs which exact servers were impacted you could not tell if the impacted servers had been restarted or not. Server restarts were carried out at queue manager level (18 out of 36 servers at a time) to catch the impacted servers but post restart more of the servers were impacted by the original issue.

This then caused a spike in process utilisation which was caused by a spike in application connections on the database, the memory then spiked and went high which it could not handle and caused it to go non-responsive/hung/slow state. Automatic database failover did not occur as there were still connections to the primary database. Data loss occurred during manual database failover because old primary database could not convert to standby and remained in open state . ",NA,RC unknown,,1/11/2023,January,2023,Kavitha,Closed,,,2022,11,November,9/16/2025,1033,>60
11/16/2022,11/16/2022,88965488,10:00,12:30,02:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Orders picked not moving to pack screen in SPPD application,TCS,BOSS Support,,ISF picking and packing,RC identified,62960,"Store colleagues were unable to pack items via SPPD between 10:00 and 12:30 . This impacted the packing and dispatch of some of the ISF (In Store Fulfilment) orders. A job responsible for processing the picking messages to Sterling was not processing the eligible picks due to its design and increase in partial picks at the stores. To permanently fix the issue, the job was modified to process the eligible picks more efficiently.",All actions are being tracked in Problem Actions tab,11/16/2022,11/17/2022,,,,"No, Yes",No,No,Yes,Customer Tech issue,NA,"The DRL job was set to pick all the orders with picking in progress status (Fully picked, partially picked that are nearing pick expiry, partially picked DNs). This is existing logic since SPPD launch, however, the issue occurred due to peak volume and increased partial picks in stores",NA,NA,NA,1/30/2023,January,2023,Sravan,Closed,,,2022,11,November,9/16/2025,1035,>60
11/15/2022,11/15/2022,88964193,09:58,11:10,01:12,GTS,Digital Workplace Services,All BU's,All BU's,,SI,Non Customer or unregistered devices unable to login to Microsoft applications,Microsoft,NA,,Microsoft O365 and Azure applications,RC identified,63029,"Colleagues were unable to access Microsoft applications between 9:58 and 10:40 from their own devices Laptops/Phones.  As part of housekeeping, new policy groups were added which triggered a bug and inadvertently changed the policy to default i.e. both MFA AND compliant devices rather than ‘either MFA OR Compliant Device’. As a workaround, the compliant device check was removed from the policy whilst the issue was resolved by Microsoft. The bug was at Microsoft’s end after a change they carried out on 14/1, this was reverted by them to fix the issue permanently.",All actions are being tracked in Problem Actions tab,11/15/2022,11/15/2022,,,,No,No,Yes,Yes,3rd party change,,"A bug in Microsoft end was identified after a change on 14/11 which was reverted to fix the issue permanently.

Microsoft advise their change was “There was a problem introduced in the latest build on the Grant control configuration pane.”",,,,1/14/2023,January,2023,Saloni,Closed,,,2022,11,November,9/16/2025,1036,>60
11/14/2022,11/14/2022,88963304,15:00,22:00,07:00,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Click and Collect orders are incorrectly showing as overdue after 7 days,TCS,Sterling Support,,Early return and refund  of C&C parcels,RC identified,62963,"The in-store click & collect orders were incorrectly showing as overdue after 7 days instead of the normal 10 days. Due to this issue, 2876 C&C parcels were returned and refunded early and ready to collect emails were showing incorrect collection dates. A temporary fix was implemented to correct the C&C orders on 14/11. The 7-day overdue date was introduced after a configuration change was deployed on 3/11 to fix a separate issue. A permanent fix was tested and deployed on 17/11.",The actions are tracked in the action tab ,11/14/2022,11/23/2022,,,,"No,yes",No,No,Yes,Unauthorised Change ,Yes,"1. Early overdue collection date was caused by toggle (DNCTA-2702) being switch off. This toggle changes the last collection day date to Del date + 7 in SYW 1, 2, 3 & Del date + 10 for SYW-4.
2.Incorrect last collection day dates stamped in Ready to Collect emails was caused by moving the custom XSL to OOB XSL and the attribute associated to toggle (DNCTA-2702) not being available in OOB XSL to change the last day collection day dates to the above del dates",NA,NA,EG-5419,11/29/2022,November,2022,Kavitha,Closed,,,2022,11,November,9/16/2025,1037,>60
11/13/2022,11/14/2022,88961520,22:48,0:39,01:51,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Carrier label print failures across Bradford C&H and Donington ,Sorted,NA,,Carrier Label Printng,RC identified,63015,Bradford C&H and Donington reported print carrier label failures across all pack lines between 22:48 and 00:39. This caused a capability loss of 19k singles resulting in 455 miss promised orders at Donington along with 82 ecom orders miss promised at Bradford.  Vendor Sorted identified an issue with their authentication server which was restarted to restore services.,All actions are being tracked in Problem Actions tab,11/15/2022,11/21/2022,,,,"No, Yes",No,Yes,Yes,3rd party issue,NA,"The root cause for this incident was failure in both instances of the Auth service.  The first instance failed at 21:08 UTC on November 6th. The second instance failed at 18:00 UTC on November 13th. In the absence of the Auth service, the SHIP platform was unable to execute internal calls and thus requests failed. This behaviour was not exposed to customers immediately after the second instance failed. Auth tokens are cached internally and so customer impact did not commence until the relevant caches started to expire at 23:42 UTC. 

Both Auth service instances failed by entering a “Crash loop back-off” state. This is a Kubernetes specific state where repeated restarts are automatically detected and increasing back-off time is applied to allow for detection and remediation. Whilst in this state, the instance will not handle requests, but it will not fail health checks (as it is “paused”). The root cause for the failed starts transient fault related to connecting to storage triggered by pods being restarted. Sorted have increased the number of pods to increase resiliency ",NA,NA,NA,1/4/2023,January,2023,Sravan,Closed,,,2022,11,November,9/16/2025,1038,>60
11/10/2022,11/10/2022,88953113,12:00,13:15,01:15,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Store colleagues unable to find parcels via Collect application.,TCS,"Sterling,  Middleware B2B",,Collection app,RC identified,63009,"Store colleagues were unable to identify customers parcels received into store via the Collection App on Honeywell devices. This impacted store operations as they were unable to locate parcels and  therefore mark it as ""Collected"". Message pile ups were observed after a change was deployed on 10/11 to migrate 2 APIs from AWS to Azure which caused the issue. This change was reverted by 13:15 to process the backlog messages.","All actions are closed, with most of them with Jira IDs",11/11/2022,11/11/2022,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,"After migrating the APIs from AWS to Azure V2  - where the Prod and DR configuration are active-active in V2, the DR cluster was not serving any traffic as it was not whitelisted.  Therefore the Apigee could not reach DR. As per design, Apigee will wait for 3000 ms and then connect to production where the response time is less than 50 ms - this is why the API response times increased  causing the issue.",CM - 8851,inadequate testing,NA,11/30/2022,November,2022,Gokul,Closed,,,2022,11,November,9/16/2025,1041,>60
11/8/2022,11/10/2022,88950014,16:30,5:30,37:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Some customers did not received carrier email notifications,TCS,Sterling,,Carrier notifications,RC identified,63005,"On 08/11, some customers had not receive the carrier email notifications for their orders. This impacted the orders dispatched by the fulfilment partners from midnight until 5am. A configuration issue in the recent change was fixed to restore services. ",The actions are tracked in the action tab ,11/14/2022,11/14/2022,,,,"No,yes",No,No,Yes,Unauthorised Change ,WO,"The Sterling/OMS daily shipment file process to Sorted was holding old configuration which contained dispatched orders from midnight to midnight which caused the issue. The correct configuration from 5am previous day to 5am current day was lost during a change deployed to add an additional carrier to this service.

Problem Resolution: The configuration of the Sterling shipment file was changed back to 5am previous day to 5am current day.",1257899,,6836,11/20/2022,November,2022,Saloni,Closed,,,2022,11,November,9/16/2025,1043,>60
11/8/2022,11/8/2022,88947892,07:25,7:56,00:31,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Carrier label print failures observed in Bradford (C&H and Foods) and Castle Donington,Sorted,NA,,Carrier Label Printng,RC identified,62943,"On 08/11, Bradford (C&H and Foods Ambient) and Donington reported print carrier label failures which impacted all pack lines between 7:25 and 7:56. The DNS network connectivity at Sorted was affected due to a failed change at their DNS provider BRANDIT, thus causing the issue.",https://mnscorp.sharepoint.com/:f:/r/sites/PIRDocuments/Shared%20Documents/88947892?csf=1&web=1&e=t3NwQ7,11/8/2022,11/11/2022,,,,Yes,No,Yes,Yes,3rd party change,NA,"The DNS network connectivity at Sorted was affected due to a failed change at their DNS provider BRANDIT, thus causing the issue.",NA,NA,NA,11/22/2022,November,2022,Gokul,Closed,,,2022,11,November,9/16/2025,1043,>60
11/8/2022,11/8/2022,88949337,07:00,12:30,05:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Store colleagues unable to print ISF labels via SPPD,TCS,BOSS Support,,SPPD,RC identified,62960,"On 08/11, store colleagues were unable to pack items via SPPD between 07:00 and 12:30 impacting the packing and despatch operations of the ISF (In Store Fulfilment) orders. The root cause was attributed to a change deployed on 08/11 which was reverted to restore services.",The actions are tracked in the action tab as part of 88965488,11/16/2022,11/17/2022,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,"The DRL job was set to pick all the orders with picking in progress status (Fully picked, partially picked that are nearing pick expiry, partially picked DNs). This is existing logic since SPPD launch, however, the issue occurred due to peak volume and increased partial picks in stores

Root cause of pick screen in SPPD not loading – Null pointer exception returned by API. This API was modified for recycle bag change and the same is being used while loading pick screen.
",CM - 8848,,,11/22/2022,November,2022,Sravan,Closed,,,2022,11,November,9/16/2025,1043,>60
11/3/2022,11/3/2022,88940631,09:27,11:30,02:03,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,"Delay in sending UPT ""units per tray"" Extract File to 27 foods suppliers",TCS,SCRD & Quantum,,Quantum,RC identified,62882,27 foods suppliers were sent incorrect order quantities for 102 chilled UPT products. This issue affected allocation of quantities for franchise/main chain/ROI stores which could potentially lead to waste.  The root cause was attributed to overnight allocation file processing issue.  A PIR has already been carried out. ,The next steps are in the problem actions tab,11/3/2022,11/3/2022,,,,"No,Yes",No,No,Yes,Data issue,No,"SCRD has been designed to run one job at a time. Due to peak volumes, yesterday’s Network extract job took additional time to complete and therefore the Constraint (UPT) extract job was unable to start on time.  The Quantum batch skipped the UPT extract file as it was not set to be a “mandatory” file",NA,NA,NA,11/8/2022,November,2022,Kavitha,Closed,,,2022,11,November,9/16/2025,1048,>60
11/2/2022,11/2/2022,88938344,07:30,10:00,02:30,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Issue in printing tickets for 116 Foods UPCs in ECS ,Customer,NA,,ECS,RC identified,62902,116 foods price labels for 3 for £12 promotion were printing as normal price tickets in ECS. A colleague had inadvertently ended the 3 for £12 offer in the system while amending promotions via ECS GUI.  Affected stores were advised to stop printing these tickets if not done already.  An emergency batch was triggered to reinitiate the promotion which activated at 11:30.  ,The next steps are in the problem actions tab,11/2/2022,11/2/2022,,,,No,No,Yes,,Human error - Business,NA,A colleague was trying to take a couple of UPC's out of the 3 for £12 deal and inadvertently ended the whole promotion.  Foods pricing & promotions have confirmed that colleagues were making changes for the past few weeks in ECS as there was an issue with the changes being done in SAP – which should be the normal way. This issue has now been fixed by ECS as of three weeks ago and Foods pricing and promotions will tell all relevant colleagues that they can now go back to the normal process of making any changes in SAP.,NA,NA,,11/20/2022,November,2022,Saloni,Closed,,,2022,11,November,9/16/2025,1049,>60
11/1/2022,11/1/2022,88933293,06:58,8:55,01:57,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,ECS application was inaccessible,ECS,NA,,ECS,RC identified,62872,"ECS application was inaccessible displaying an error on both Honeywells and Workstations between 06:58 and 08:55.  Store colleagues were unable to print shelf edge tickets causing a possible discrepancy between the price displayed and price on the tills.  Vendors ECS upgraded the heap size memory from 1gb to 2gb after to restore services. 
  ",The next steps are in the problem actions tab,11/1/2022,11/1/2022,,,,,No,Yes,,Customer Tech issue,NA,"It is believed that stores have carried out a lot more adhoc searches this morning than normal.  Until 09:10 today, our store colleagues searched/printed 30000 UPC’s as compared to 821 searched for and printed yesterday by 09:10. There was a Halloween reduced to clear activity today, plus today/tomorrow is also phase change.  The enlarge function on the ECS ticketing app remains and is a contributary factor to this incident as it makes two calls to the API per UPC.  These activities have caused a heap memory resource constraint on the ECS application.    ",NA,NA,NA,11/23/2022,November,2022,Saloni,Closed,,,2022,11,November,9/16/2025,1050,>60
10/29/2022,10/29/2022,88928761,09:35,12:24,02:49,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Castle Donington reported issues in Ecom order processing,TCS,"WCS, WMS",,"WCS, WMS",RC identified,62868,"On 29/10, Castle Donington reported issues in Ecom order processing between 09:35 and 12:24. The global C&C proposition was moved to 24hours. The root caused was attributed to an incorrect order delivery date owing to clock change activity which was corrected to resume order flow",All actions have been completed,10/30/2022,10/30/2022,,,,NA,NA,NA,Yes,Customer Tech issue,No,The root caused was attributed to an incorrect order delivery date owing to clock change activity which was corrected to resume order flow,NA,NA,NA,11/11/2022,November,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1053,>60
10/23/2022,10/23/2022,88914810,19:10,20:31,01:21,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI, Issues with Bradford C&H DC operations,KNAPP,NA,,Bradford C&H DC,RC identified,62853,"Orders for Bradford C&H DC were failing in Sterling as Kisoft WMS could not process any messages from WCS automation. This resulted in loss capability for the site between 19:10 and 20:31. In addition, Ecom proposition was pushed out and 95 orders were miss promised. Knapp identified multiple data export sessions in the database and hence restarted the database services and JBOSS messaging services to restore services.",All actions are being tracked in Problem Actions tab,10/23/2022,10/25/2022,,,,No,No,No,Yes,3rd party issue,No,To complete a specific report on “Returns” the managers / team leaders take extracts from KISOFT on a hourly basis. Filters are used to minimise data extract to specific hours. Knapp had a new step up team leader who ran the query without any filters and exported into EXCEL. This query ran for 2 hours and brought down JBOSS.,NA,NA,,3/28/2023,March,2023,Sravan,Closed,,,2022,10,October,9/16/2025,1059,>60
10/20/2022,10/20/2022,88907857,14:30,18:00,03:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Drop in Brands availability picture,TCS,Sterling,,Sterling,RC identified,62767,"Alerting identified a drop in the number of in stock and buyable products availability for Online Brands between 14:30 and 18:00 . Around 600 orders were impacted when compared to previous days resulting to a significant drop in sales and poor customer experience. To restore services, a fresh SOD (Start of Day) file was triggered with a full inventory picture from Brands WMS to OMS. As part of ongoing stock alignment activity between WMS and OMS, the inventory was inadvertently set to 0, thus causing the issue.",Actions are updated in problem actions tab,10/20/2022,10/25/2022,,,,Yes,Yes,Yes,Yes,Human error - Customer Tech,No,"As part of ongoing stock alignment activity between WMS and OMS, the inventory was inadvertently set to 0, thus causing the issue.",No,NA,,11/16/2022,November,2022,Kavitha,Closed,,,2022,10,October,9/16/2025,1062,>60
10/19/2022,10/19/2022,88904516,13:45,14:45,01:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI, Checkout issues on International Flagship sites,Global-E,NA,,Demandware,RC identified,62851,Alerting indicated that no orders were taken on the international flagship sites between 13:45 and 14:11. Vendor Global E confirmed an issue with their database servers which was restarted to restore services.,All the agreed actions are closed ,10/19/2022,10/25/2022,,,,Yes,Yes,No,Yes,3rd party issue,No,Global-E had an internal malfunction due to an unexpected error in one of the Microsoft - SQL DB clusters which caused an outage to Global-e checkout and admin. A full database recovery process was carried out to restore the service.,NA,NA,NA,11/29/2022,November,2022,Kavitha,Closed,,,2022,10,October,9/16/2025,1063,>60
10/17/2022,10/17/2022,88897530,04:00,9:09,05:09,Group Support,Finance,Commercial Trading,C&H Commercial Trading ,,SI,Delay to the GMOR critical batches,TCS,GMOR,,GMOR,RC identified,62835,"Overnight, alerting indicated that multiple jobs were failing in GMOR system. This delayed the availability of the GMOR BO reports, daily sales reports and sales dashboard in BEAM. In addition, there was a delay in updating the latest planned intake data in SSI reports. The issue was caused as one of the table did not get created in the database after the repartitioning and hence was re-created to recover the batches. ",Actiond are updated and closed in the problem actions tab,10/17/2022,10/18/2022,,,,Yes,Yes,Yes,No,Configuration issue,NA,"The repartition program started with a delay on 17/10 as its predecessor housekeeping job responsible for data backup overran on 16/10 due to huge volume of data. The overrunning was caused as the selective deletion (which deletes historic data) process did not function as expected. This is currently being investigated.

On 17/10, as the GMOR batch and the repartitioning were running simultaneously, it locked the system causing the GMOR jobs to fail and the repartition to go into hung state with a database error.

As the repartitioning went into hung state, one of the tables did not get created in the database causing the issue. ",NA,NA,SAP-GMORBW-00051,11/2/2022,November,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1065,>60
10/13/2022,10/13/2022,88888766,07:00,9:30,02:30,GTS,Enterprise Technology Platform,All BU's,All BU's,,KI,On prem Control M system was unavailable,TCS,Control M,,Control M,RC unknown,62755,The on-prem Control-M batch scheduler was unavailable between 7:00 and 9:30. There was no residual impact to any portfolio.  The issue was caused by a long running transaction on the Control M database server which was restarted to restore services. Vendor BMC are investigating further. ,All the actions are and closed in problem actions tab ,10/21/2022,10/21/2022,,,,Yes,Yes,Yes,No,RC unknown,,"Rootcause is undetermined as both BMC and Microsoft confirmed that there no issue in the database. There is no feasibility to configure alerting for long running sessions. Hence, SOP has been created documenting the restoration steps and updated in Control M SOM",,RC unknown,,11/15/2022,November,2022,Saloni ,Closed,,,2022,10,October,9/16/2025,1069,>60
10/13/2022,10/13/2022,88889338,10:30,12:36,02:06,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Relex foods service was inaccessible,Relex,NA,,Relex,RC identified,62756,"Relex service was unavailable between 10:30 and 12:36. Users were unable to access the system to perform foods forecasting, ordering and allocation. The outage was caused by a central issue at Relex which impacted other customers including Customer. Awaiting detailed root cause from Relex. ",Relex has implement improvements during their certificate renewal activity to avoid such issues in future,10/20/2022,10/20/2022,,,,No,No,Yes,Yes,3rd party issue,NA,The issue was caused due to a certificate issue in Relex web plan gateway. The certificate failure caused connectivity issues between customers and Relex user Interface which caused the outage.,NA,NA,NA,10/28/2022,October,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1069,>60
10/7/2022,10/8/2022,88876687,19:00,15:50,20:50,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Drop in mobile web orders,TCS,MCP,,MCP,RC identified,62718,"Customers reported that they were unable to log in to the website via mobile web. Due to this, registered customer were unable to place orders via mobile web causing a drop in orders. No impact to guest orders. As a work around, a change was implemented to bypass the problematic component and land directly on the mobile pages. The root cause was attributed to a change that was deployed on Friday 07/10.",All actions completed,10/9/2022,10/9/2022,,,,"No, Yes",No,Yes,Yes,Customer Tech change,CR,A change (CM-8631) was implemented in the mobile web architecture (MCP) to update ingress version (cloud-based load balancer) and related pipeline upgrades to support upcoming Infra deployments. This code had a syntax error which disrupted the mobile login journey., CM-8631,Lack of testing,,10/14/2022,October,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1075,>60
10/6/2022,10/6/2022,88872438,10:27,11:29,01:02,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Gap scan and counting processes issue,TCS,CSSM,,Gap Scaning,RC identified,62719,"A change went in to bring efficiencies to the Gap Scan and counting processes within stores. The change should have been to deploy the app, in dormant mode and make live on Saturday night/Sunday morning. Unfortunately, the change went live in all stores on Friday and caused confusion within Retail. Stores had to be communicated to, to start using the new app from Friday, 2 days earlier than expected",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,Human error - Customer Tech,NA,It has been agreed that this was not an actual issue but a communication gap. Hence no PIR conducted and no root cause to be chased.,NA,NA,NA,10/10/2022,October,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1076,>60
10/3/2022,10/3/2022,88861216,08:55,9:24,00:29,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Alerting indicated an order dip on UK & I.E. website,TCS,Ecomm platform support,,WCS ,RC identified,62818,Alerting indicated an order dip on UK & I.E. website between 08:55 and 09:24. This caused drop in orders when compared to the previous days. The issue was reported after diverting the website traffic to a different hall (Website traffic is split into two halls)  with a new release code was deployed on 03/10. The website traffic was diverted back to the previous hall to restore services. ,All actions  completed.  ,10/3/2022,10/3/2022,,,,Yes,Yes,Yes,Yes,Customer Tech issue,NA,"As part of a shopping release, a code was deployed to hall 2 (Website traffic is split into two halls) and was made customer facing while attempting to deploy the code to hall 1 (original hall). There was a significant drop in orders as one of the webservers in the hall 2 did not come up properly after the deployment and therefore, was unable to serve traffic. ",NA,NA,NA,10/14/2022,October,2022,Saloni,Closed,,,2022,10,October,9/16/2025,1079,>60
10/2/2022,10/3/2022,88861805,08:00,18:00,34:00,Group Platforms,Enterprise Technology Platform,Group Platforms,Finance,,SI,Issue with e-SAP operating from DR environment,TCS,Network,,eSAP,RC identified,62642,"During the e-SAP DR activity on 02/10 the system was unable to receive data from clouds system like POSDTA and GMOR.  Deliveries to stores are impacted due to understated stock positions, finance reports will have understated sales figures and GMOR report will up to date stock and sales figures due to this. A decision was made to allow the DC to use eSAP application to perform picking and dispatch operation for 03/10 to minimise the impact. A network route was missing in the eSAP DR environment and this was fixed at 18:00.  Batch catch-up is in-progress.",Actions have been updated in the problem actions tab,10/10/2022,10/10/2022,,,,NA,NA,NA,Yes,Customer Tech change,NA,"When eSAP application was failed over to Swindon for the DR exercise, the connectivity to the Azure cloud based applications such as POSDTA and GMOR was there but did not work due to Asymmetric routing.  This this means that out bound traffic was going via Swindon but returning inbound via Stockley.  If Stockley was actually isolated then the DR test would have worked without any further configuration needed to stop asymmetric routing.",NA,NA,NA,10/11/2022,October,2022,Rehan,Closed,,,2022,10,October,9/16/2025,1080,>60
9/30/2022,9/30/2022,88857202,16:30,17:42,01:12,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Customers are unable to redeem Loyalty Reward Vouchers online,TCS,Ecomm platform support,,Loyalty Rewards Services,RC unknown,62632,"Monitoring identified a dip in number of orders where loyalty rewards vouchers were used for payment.  Customers were unable to use the vouchers to pay for their orders. This impacted the transactions through PSG (legacy) route. The POG route (used for Customer colleague transactions) was working fine.  An issue with routing traffic from PSG to LRS (Loyalty Reward services) through Stockley Park was identified, therefore to mitigate impact,  services were failed over to the Swindon at 17:42","Open Actions:
1	Monitoring identified that the number of orders where Loyalty Reward Vouchers is used for payment is very low.  The issue started at 00:20 but was escalated to a sev 2 at 16:20 which is 16 hours later.  We need to address why it took this period of time to identify that this was a significant issue.  Could the potential impact have been better assessed?	Hisham Khalid		
2	LRS application is hosted in Azure and there was an issue in reaching the application in N. Europe (not an application issue).  It was not failing over to W. Europe through Stockley Park due to missing setup – this missing setup needs to be investigated.
	Hisham khalid		
3	What was the exact issue because of which we could not reach the application in North Europe – this is a Microsoft issue and we need to follow up on the cause.	Hisham Khalid		
4	The LRS application was not failing over to W. Europe through Stockley Park due to missing setup – this missing setup needs to be investigated.
	Hisham Khalid/George Loizou		
	",9/30/2022,9/30/2022,,,,"No,Yes",No,Yes,No,RC unknown,No," There were no issue identified in the investigation , Aged-out and reset were observed in the server logs during that issue, however  there was no connection breaks on Network layer. The only issue from network end was the health monitor mapping and it has been configured in both North and West Europe regions. Hence root cause inconclusive ",NA,RC unknown,,2/14/2023,February,2023,Kavitha,Closed,,,2022,9,September,9/16/2025,1082,>60
9/29/2022,9/30/2022,88850486,07:00,17:00,34:00,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI, Printer access issues on the Windows 10 workstations,TCS,SIT (Software  Implementation Team),,Store printers,RC unknown,62365,"Store colleagues were unable to access the printers on their Windows 10 workstations. This impacted the printing capability for all Win10 stores, however, 86% of  tickets were already printed for the day. A fix was deployed to the entire Win 10 store estate to map the local printers to the workstations. ",Actions updated in the Problem actions tab,9/29/2022,10/16/2022,,,,"No,NA",No,No,Yes,Customer Tech change,CR,"The root cause is unknown however the in-depth problem investigation has strongly indicated that CR169676 was linked to the cause. This objective of this change was that in preparation for peak, a prerequisite activity was carried out on 29/09 which deployed a script to activate local printer mapping to the Win 10 workstations and relieve the till distribution points in stores from carrying out print server tasks so that they can once again be used as tills.",169676,inadequate testing,NA,10/20/2022,October,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1083,>60
9/29/2022,9/29/2022,88851924,07:45,13:50,06:05,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,"Intermittent Errors on Homepage, DLP, SRP & PLP Pages",TCS,Ecomm platform support,,Customer .com website,RC identified,62363,"A number of errors were intermittently observed on the Homepage, Display Landing Pages (DLP), Search Results Pages (SRP), Product Listing Page (PLP).  This resulted in poor customer experience while browsing the website impacting 3% of the customer traffic.  The issue on the Homepage and DLP was caused by an incorrectly configured certificate during a certificate renewal and was corrected by 13:40.  The majority of customers were able to browse as they were protected by cache.  

A separate certificate was inadvertently deleted via an automated mechanism which resulted in customers experiencing intermittent errors on the SRP and PLPs. To mitigate the impact, the customer traffic was routed to a different Azure region. ",Actions have been updated in the problem actions tab,29/09/2022,10/6/2022,,,,Yes,No,No,Yes,Customer Tech change,CR,"Homepage/DLP 
Certificate that was added was a partial certificate (This was added in error) which only included the intermediate rather than the entire certificate chain. As a verified certificate chain did not exist the SSL handshake did not complete successfully and resulted in errors when Akamai attempted to connect to the origins.

SRP/PLP 
Automated certificate manager was attempting to renew an expired certificate. This did not work correctly as the Certificate Manager faced some compatibility issues following an Azure Kubernetes Service upgrade on the impacted cluster. This resulted in current certificate being deleted and new one not being issued which resulted in SSL handshake failures when Akamai attempted to connect to the impacted cluster’s origins. The cause of this problem was related to an AKS cluster upgrade in the previous month which resulted in incompatibilities with certain services. The incompatibilities were not immediately apparent for this specific scenario with the Cert Manager Service as no certificate had expired. When the certificate did expire, and Cert Manager was unable to successfully renew it the issue was brought to light. There was also a compatibility issue with an external DNS which is used to validate certificates by the issuer (In this case Let’s Encrypt). This compatibility issue also existed because of the AKS upgrade to the NEU cluster. WEU (Western Europe) cluster has not yet been upgraded and so did not experience any of these issues.",CM-8630,inadequate testing,NA,10/11/2022,October,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1083,>60
9/27/2022,9/28/2022,88846160,09:25,9:00,23:35,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Collection slots not showing intermittently on CFTO App,TCS,CFTO collection App,,CFTO collection App,RC identified,62630,"Some stores reported intermittent issues with CFTO app collection slots availability, orders placed on the website were unaffected. This was caused by the CFTO app not supporting compressed/zipped data.  An alternative compatible library was tested and successfully deployed into production to resolve the issue by 9am on 28/09. ",Actions in the next tab.,27/09/2022,28/09/2022,,,,Yes,Yes,Yes,Yes,Customer Tech issue,NA,Incompatible software library not supporting compressed data,NA,NA,CFTOOC - 272/273/274,11/22/2022,November,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1085,>60
9/27/2022,9/27/2022,88848166,19:15,20:20,01:05,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Issues with Bradford C&H DC operations,Knapp ,NA,,WMS dispatcher,RC identified,62361,Orders for Bradford C&H DC were failing in Sterling as Kisoft WMS could not process any messages from WCS automation. This impacted the DC operations capability of fulfilling retail or website orders between 19:15 and 20:20..  Vendor Knapp found that the JBOSS service which imports messages into WMS had stopped working. This service was restarted by Knapp to resume the message flow.,"1) We await the actual root cause from JBOSS
2) Closed down the ticket so that the root cause can be provided 24 hours after the closure Time
3) There should have been alerting for the fact that messages were not flowing – Knapp to advise if alerting is in place and if not what can be configured 
4) Customer Tech and Knapp – we need to review the process and improve it and align it between the two stakeholders
5) Site cannot be kept down to investigate the root cause and this is what happened in this instance.  We need to document and formalise the agreed recovery process between Customer Engineering/Customer Tech and Knapp – who will take the lead and take the decisions?
6) In this instance command centre came to us to ask about impact and recovery when in fact they need to go GXO directly. 
7) When we have any WMS down time, Sterling have to take Bradford out of fulfilment as there is no buffering/exception queue – there is a change due for release 61 (end of Oct) – update required.",27/09/2022,10/7/2022,,,,TBC,TBC,Yes,Yes,3rd party issue,NA,"Knapp have got some new IT engineers who have just ended their training.  When a single unit (plastic tote used to store goods) goes missing or needs to be investigated, Knapp IT engineers can enter the node ID and run a snapshot.  This snapshot looks across WMS, WCS and SRC and comes back with the status of the tote.  In this case instead of placing a filter for the particular tote ID, the engineer took a snapshot of the whole of WMS to include every single article and node ID.  This was a very large query and therefore JBOSS shut down as it could not process the amount of data that the snapshot produced.
Knapp have now decided that engineers can no longer run the entire snapshot and they can only run it by a unique ID.",NA,NA,NA,12/19/2022,December,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1085,>60
9/24/2022,9/30/2022,88886973,11:36,10:15,22:00,Group Support,Finance,Supply Chain,Foods Supply Chain,,SI,"Goods Receipt messages from Bedworth’s WMS system, Ultima, had not reached Customer/SAP",OpenText,NA,,SAP,RC identified,62644,"Goods Receipt messages from Bedworth’s WMS system, Ultima, had not reached Customer/SAP between 24th and 30th September 2022, this caused an impact to supplier payments and invoice processing for at least 54 frozen suppliers, including Bakkavor.
Finance also raised a risk due to the P/L impact to their Half year finance end process. The issue was caused due to incorrect data content sent in transaction messages from GXO's Ultima system that had failed to process through OpenText and thus Customer.  A fix has now been successfully deployed and all the backlogged messages have been processed at Customer with the corresponding Proof of deliveries also sent to suppliers for payments and invoicing. Customer Tech and Colleagues will work along with GXO/GIST to better alert and handle failures of this kind going forward.","The action are tracked under the incident 88838928
Actions are updated in problem actions tab",10/28/2022,10/28/2022,,,,"No, yes",No,No,Yes,3rd party change,Yes,A new SFTP server and application was configured using a different OpenText mailbox to send the delivery messages from Ultima to OpenText. The change in the route had an error in the name of the directory path.  The delivery messages started to get backlogged without processing to OpenText thus causing the issue – human error. ,NA,Human error,,11/9/2022,November,2022,Kavitha,Closed,,,2022,9,September,9/16/2025,1088,>60
9/24/2022,9/24/2022,88841443,09:25,10:59,01:34,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Issues in packing ISF orders via SPPD,TCS,BOSS Support,,SPPD,RC unknown,62348,"On 24/09, store colleagues were unable to pack items via SPPD between 09:25 and 10:59 . This impacted the packing and despatch operations of the ISF (In Store Fulfilment) orders. A job responsible for processing the picking messages to sterling had stopped responding which was restarted to restore services.","1.Configuration of an alert to proactively identify whenever the job stops responding in future.: Lokesh <<Initially, there was a miss from tech to configure an alert for this job. 
However, a sev1 alert has now been configured based on the count of messages picked up by the job to be sent to Sterling. For eg: If the message count is 0 between 7am to 7pm for 5 mins, the alert will get triggered. This will be monitored to confirm the effectiveness of the alert.>>
2.Why did the job stop responding?	Lokesh	
<<A catchup call is scheduled today with the tech teams (only) to perform detailed analysis. ETA: 29/09>>
3.The level of access has to be similar across the Dev and ops team. This is to ensure a seamless identification and resolution of the issue	Lokesh/Arthy: Completed.
4. Considering the SPPD application has been recently migrated to Azure V2, teams to create SOP document for new scenarios.	Lokesh/Arthy	ETA: 29/09",24/09/2022,9/28/2022,,,,"No, configured",No,Yes,No,RC unknown,NA,"On 24/09, the Demand release job responsible for sending the picking messages for the ISF (In Store Fulfilment) orders had stopped responding which impacted the message flow to Sterling. This caused a backlog of messages and hence the orders did not move from pick screen to pack screen in SPPD application. 
Team performed a detailed investigation along with the development team and could not find the reason why the job had stopped picking messages. Support also monitored the job performance and the issue did not re-occur.",NA,NA,NA,10/31/2022,October,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1088,>60
9/22/2022,9/23/2022,88838928,11:36,10:15,22:00,Foods,Food Supply Chain,"Store Operations, Supply Chain","Run the Store, Foods Supply Chain",,MI,Issue with frozen delivery messages interfacing into CSSM,OpenText,NA,,CSSM,RC identified,62644,CSSM did not receive systemic delivery messages for Bedworth frozen deliveries from 20/09.  This impacted the store stock position and resulted in over allocations for frozen deliveries across 437 stores for into store date 23/09 and 24/09. The root cause was attributed to a configuration change by vendor GXO that resulted in the message flow being interrupted. The back log of messages from 20/09 were manually processed to mitigate impact and a permanent fix has been applied.  Monitoring of further runs in progress.,"1	What went wrong with the change and were there any E2E checks after implementation of the change
2
	Was Customer made aware of the change? 

3	Was there any alerting in Ultima to notify message pile up from 20/09 before the issue was highlighted by Customer on 22/09?

Why was the connection issue between OpenText and Ultima was not highlighted before as it was ongoing from the morning? 

Why there was no alerting on 21/09 to notify that the are no frozen delivery messages for past 24 hours in CSSM ?
If there are any changes to the schedule for frozen deliveries, Foods teams need to advise the CSSM team so that they can amend their alerts accordingly
We had overallocation in ASO as the stock was understated in CSSM. Is there a way to highlight this in ASO/Quantum?
Was there a delay in stores comms because the entire impact was not realised in a timely manner?
",10/28/2022,10/28/2022,,,,Yes,Yes,No,Yes,3rd party change,Yes,A new SFTP server and application was configured using a different OpenText mailbox to send the delivery messages from Ultima to OpenText. The change in the route had an error in the name of the directory path.  The delivery messages started to get backlogged without processing to OpenText thus causing the issue – human error. ,NA,Human error,,11/17/2022,November,2022,Kavitha,Closed,,,2022,9,September,9/16/2025,1090,>60
9/21/2022,9/21/2022,88837724,13:58,20:01,06:03,GTS,Enterprise Technology Platform,"Customer Channels, Supply Chain",C&H and Intl Supply Chain,,KI,Issue with DTC services on Infoblox,Infoblox,NA,,CD labelprinting and .COM website API calls,RC identified,62646,"On 21/09, alerting indicated performance degradation to .COM external web calls and carrier label printing in Castle Donington and Thorncliffe between 13:58 and 20:01.  This was due to an intermittent restart of DNS traffic controller (DTC - monitors applications health) caused by an incorrect code provided by Vendor Infoblox.  This was corrected to restore the service.","Open
3	Investigate why it took four hours to resolve the issue, identify the challenges and suggest improvements;Network (Fastin);04th Oct, 2022
5	Are there any alerts configured for DTC service restart?;Network – Govidaraj;The priority of the email alerting has been increased to Sev1. The feasibility of configuring a pager duty alert is being explored. ;04th Oct, 2022
7	Why were the response failures in carrier gateway not addressed or highlighted earlier to relevant teams?; Carrier gateway/ Jeevan ;PagerDuty alerting is already configured, however on the day of the incident the failure didn't reach the threshold level for a critical alert to be triggered.The threshold and severity of the alert to be revisited; 30th, Sep 2022
6	What could have been done differently to identify and fix the issue sooner? 	Network	The impact due to the DTC restart was believed to be minimal. Hence there was a delay is actioning the alerts. Team have now understood the significance of the DTC service alerts and will ensure that alerts are actioned with priority going forward.;The threshold and severity of the alert to be revisited ;	30th, Sep 2022
8	Can any alerting be put in place if there is a failure in carrier gateway to notify Castle Donington DC?; 	Carrier gateway/ Jeevan;PagerDuty alerting is already configured, however on the day of the incident the failure didn't reach the threshold level for a critical alert to be triggered.
The threshold and severity of the alert to be revisited;	30th, Sep 2022
9	The list of applications which uses DTC services needs to be documented and they will be engaged to assess the impact incase of any issues with the services ;Network/Platform; Network team will liaise with platform team to document the list of applications which uses DTC services. Team have agreed in the PIR call to engage the relevant teams in case of any issues with DTC service.;30th, Sep 2022
10	Why did the change in non-prod impact production service?	;Infoblox/ Rajesh ; A case has been raised with Infoblox to understand why it impacted production services. Network is following up with Infoblox to understand the RC. Infoblox has advised that the similar issue has been observed for the customers and they are continuing to investigate.Rajesh to chase up Infoblox for the RCA.;No ETA as of now
11	Production change process to be followed for components which share both Production and Non Prod services;	Kamal and Rajesh;	No ETA as of now


Closed:
1	Was the automated change tested before deployment? 	Network	This is the only testing environment. So, we won’t be able to test the changes before deploying in Non-prod. 	
2	Do we have any change reference for this change that was deployed via automated pipeline?	Network	All automated changes go via portal request and no change management process involved 	
4	Can any measure be taken to prevent the impact to production for such globally managed services? If not, is it possible to split the service for prod and non-prod separately?	Network- Govidaraj	Based on the current system architecture, there is no feasibility to add any preventive measure in the system. Infoblox architecture needs to be revisited and will discussed with the relevant stakeholder which will be tracked under the back log - CNWK-801	",9/21/2022,9/23/2022,,,,Yes,Yes,No ,Yes,Unauthorised Change ,Yes,A change was deployed as part of testing in non-prod to the DTC which is the DNS traffic controller which monitors and load balances application health. The code given by vendor Infoblox had a syntax error causing both prod and non-prod services to restart.  A case has been raised with Infoblox to understand why it impacted production services.,NA,inadequate testing,,11/4/2022,November,2022,Kavitha,Closed,,,2022,9,September,9/16/2025,1091,>60
9/20/2022,9/20/2022,88835977,13:00,20:15,07:15,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Entire orders getting cancelled instead of the out of stock lines,TCS,Sterling ,,Sterling,RC identified,62377,"From 06/09, whole orders were getting cancelled in Sterling instead of cancelling the specific out of stock items. Due to this, 6684 orders were cancelled. The issue was caused by problematic code deployed in the Sterling release on 06/09. A code fix was deployed to resolve the issue.","Closed actions:
2) Agree threshold and set up alerts in EM for OMS Unreachable orders. 
3) Review WCS offline order alerts and update 
8) To resolve slowness in SIT testing environment, increase Infrastructure (number of pods) this will reduce offline orders 
9) Add to SIT testing phase validation of exception errors 

Why did it take 14 days for us to identify the issue? - Orders go offline daily and orders are cancelled daily due to stock not available.  There was no alert for stock not available. This is why all the thresholds are now being reviewed as below.
Two no stock available alerts have been added – Sev 2 300 lines Sev 1 600 lines (average cancellations in this area is 700 a week) - Completed
5) Root cause: The null pointer error was not mapped in Enterprise middleware (EM) and therefore EM was translating this as OMS unreachable – why was it not mapped?  What happened in testing?
6) Post Sterling Release have a warranty period by OMS Development Team 
7) As part of the above warranty period include validation and monitoring on exception errors""",27/09/2022,27/09/2022,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,"When a click & collect multi-line order is placed, the ATP (Inventory check) API response gives a null point error when the first line item of the order was out-of-stock. This null pointer error is not mapped in Enterprise middleware (EM) and therefore EM was translating this as OMS unreachable.  This caused WCS to place the order in offline mode.   When an offline order is processed in OMS, another inventory check is made resulting in the same condition failure and therefore OMS cancelling all the lines in the order. ",CM8452,inadequate testing,NA,10/31/2022,October,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1092,>60
9/19/2022,9/19/2022,88833884,00:15,0:52,00:37,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI, Spike in offline and failed card transactions for stores,Worldline,NA,,Contactless transactions,RC identified,,"Spikes were observed in failed card payment transactions between 10:03 and 10:10 on 15/09 . Contactless transactions and any payments over £150 would have been declined. 7 stores reported the issue to the desk. Worldline advised that their Acquirer hub was unreachable while preparing for a change which caused the issue.

On 19/09, similar spikes were observed in failed card payment transactions between 00:15 and 00:54. Only 24/7 stores would have been impacted and 1 store reported the issue to the desk. We are awaiting root cause details from Worldline.",Actions were updated in Problem actions tab,10/25/2022,10/25/2022,,,,"Yes,No",Yes,No,NA,3rd party issue,NA,"On 15th September 2022 Worldline's Advanced Infrastructure Services (AIS) deployed an automated change applied to the ahub.global.ingenico.com DNS record. AHUB is a shared service so this resulted in failures of payment platforms depending on it (including the AXIS platform for customers that rely on AHUB(authorization flow). The change was erroneously executed not in ""test"" mode but in “execute”, effectively creating a new DNS subzone for each tenant that held the name of the tenant(ahub.global.ingenico.com). As the tenant already had an Address record pointing to the same name (ahub.global.ingenico.com). The new subzone effectively masked the visibility of the original Address record.
19th RCA
On 19th September 2022, Worldline's Global Infra deployed a change where an erroneous modification was made resulting in incorrectly directed traffic. This impacted customers connecting to Axis from the Internet directly.",NA,NA,NA,11/7/2022,November,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1093,>60
9/16/2022,9/16/2022,88830426,02:30,4:00,01:30,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,"
Delay in supplier order generation for Relex products",Relex,NA,,Supplier orders,RC identified,62712,"On 16/09,  empty files were received from Relex due to a known issue which impacted the supplier order generation for Relex products. To mitigate the impact, supplier orders were generated using previous day’s 28 DOP and sent successfully by 8:00.",All actions completed,9/16/2022,09/28/222,,,,Yes,Yes,Yes, Yes,3rd party issue,Na,The peak file was generated on the wrong date as the support activity to correct this has not been made for this year. This is due to a gap between the RELEX support function and project function over the ownership of supporting Customer specific configuration.,NA,NA,NA,10/10/2022,October,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1096,>60
9/15/2022,9/15/2022,88828574,07:00,9:30,02:30,GTS,Enterprise Integration,Customer Channels,Platform & Store Ops ,,KI,No BP Sales received into CSSM,TCS,Integration Services,,CSSM,RC identified,62814,"On 15/09, alerting indicated that CSSM had not received any BP sales from 07:00. Store counting was blocked for 291 BP stores between 07:00 and 09:57 to prevent stock corruption. The Retail MQFTE agent unexpectedly went to an ""unresponsive"" state during the B2B cutover change which was restarted to restore services. ","1	Why did the agent not come up automatically	Aarthy	In progress.
ETA: needs time for investigations. It is an usual behaviour during a refresh activity that the agents go into hung state. Team will further check the logs and revert.
2	Health checks to be prioritised on the agents across the portfolios after a cluster refresh activity. 	Swathika/Aarthy	In progress.
Discussions are ongoing whether an automated or manual solution will be put in place.
3	Analyse the impact that might incur before the next migration activity is planned.	Swathika/Aarthy	In progress.
ETA: 21/09
4	Alerting mechanism to be revisited across the teams	Swathika/Aarthy	Completed.
On a regular business day, if this agent would go down, the team have a process in place to bring up the agent in 15 mins. If that does not work, based on the business impact, MIM would be initiated. ",9/15/2022,9/15/2022,,,,"Yes, no ",Yes,Yes,Yes,Customer Tech change,Yes,"As a part of International cutover under CR168341, while the B2B cluster refresh activity was performed, the Retail MQFTE agent went to a unresponsive state and did not come up automatically, causing the issue. 

The reason behind the agent not coming up automatically is currently being investigated.",CR168341,Insufficient testing,NA,10/21/2022,October,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1097,>60
9/13/2022,9/13/2022,88826158,11:45,13:09,01:24,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Issues in accessing Sparks hub,Apple ,Apple wallet,,Sparks hub,RC unknown,TBC,"On 13/09, alerting indicated an increase in CPU utilisation causing slowness in accessing the sparks hub between 11:45 and 13:09. This impacted the customer experience on the Sparks journey. This was caused due to a database query related to Apple wallet which was then blocked to restore the service.",Actions were updated in Problem actions tab,10/14/2022,10/14/2022,,,,"Yes, no ",Yes,Yes,NA,3rd party issue,No,"The issue believed to caused by either of the below scenario due to a fault at application end. 
1. More people might have be added causing a spike. 
2. Apple have change some logic increasing the traffic, but the funcationality is disabled. Hence to avoid the traffice we should remove the web service url ",NA,NA,NA,10/25/2022,October,2022,Kavitha,Closed,,,2022,9,September,9/16/2025,1099,>60
9/11/2022,9/13/2022,88822742,18:00,3:35,33:35,Group Platforms,Finance,Group Platforms,Finance,,KI,Issues in processing retail sales data in SAP,TCS,SAP BI,,"GMOR, SAP ECC",RC unknown,62714,"On 11/09, the billing extractor job failed in GMOR and cSAP BW system after the cSAP AIX patching on 11/09. This will delay the availability of EDW Day-1 reports and the retail sales feeds to BEAM. In addition, there will be a possible variance observed in RD vs BI recon. The root cause and resolution are currently unknown and is being investigated by vendor SAP.","Detailed understanding on the inconsistent QRFC T id: Murali/ Rhammya	ETA: TBC. 
Call out to be performed. Root cause investigation is in progress from SAP end.

SOP to identify the problematic id and perform the deletion: Murali <<Documenting is completed. Validation is in progress.>>

Vendor SAP engagement to be reviewed:  Remko/Krishna	",9/11/2022,NA,,,,Yes,Yes,NA,NA,RC unknown,NA,The root cause is an inconsistent transaction id for a record in SAP ECC system which caused the billing extractor jobs to fail in both cSAP BW and GMOR. The reason behind the inconsistent id is currently being investigated by vendor SAP.,NA,NA,NA,11/8/2022,November,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1101,>60
9/8/2022,9/10/2022,88821404,06:30,15:40,57:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Issues in the Bespoke School uniform microsite,Microsoft,NA,,Bespoke School uniform microsite,RC unknown,62308,"On 10/09, customers were able to order out of stock products visible on the PDP pages on the Bespoke School Uniform microsite after the upgrade on 08/09. 957 out of 1,375 orders taken on the bespoke school uniform microsite were out of stock products. Microsoft identified that two missing product-related properties impacted the product availability feature and these were added to fix the issue. Orders containing out-of-stock products will be managed with the customers.",Please refer actions mentioned in 88567503,9/15/2022,9/15/2022,,,,No,No,Yes,Yes,3rd party change,Yes,"As a part of upgrade activity in the Bespoke microsite, the out of stock feature was not functioning properly as two product properties were missing from the production batch job. Microsoft suspects that the commerce initialization (restart of the site inventory component) was not performed after the upgrade, which was unknown to Customer teams, thus causing the issue. ",NA,NA,NA,11/2/2022,November,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1104,>60
9/8/2022,9/9/2022,88817656,10:23,8:08,21:45,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Issues with Stores C&H SST application,TCS,Colleague Devices Efficiency,,SST,RC identified,62406,"On 08/09 at 10:23 store colleagues reported that they were experiencing issues with the C&H SST app issues on the Honeywells, with an error message ""the app keeps stopping.  De-catalogued upc's from the website but available in store were impacted.  A fix was implemented to restore services.",Add the null point checks to prevent the application crash for failed API calls. - Completed,9/14/2022,9/14/2022,,,,Yes,Yes,No,Yes,Customer Tech change,NA,"The issue had caused due to an existing bug in the SST application when the Bloomreach API will not send the UPC details to the SST application if an online stock for a particular UPC is 0. On the day before the incident, the business purged 10k+ products from the system and these UPCs were giving empty responses in Bloomreach API which crashed the SST application.",NA,NA,NA,9/14/2022,September,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1104,>60
9/8/2022,9/8/2022,CRQ167461,09:00,10:00,01:00,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Some stores blocked from counting,TCS,"CSSM, WMS",,CSSM,RC identified,62321,"On 08/09 as part of Foods stock loss programme, a planned changed migrated the Bradford to direct store dispatches in ASN route. The team observed message failures in CSSM due to a special character in the article description field in inbound xml and we reverted the change. To protect stock positions Bradford stores blocked from counting.","1)Were there any post-implementation checks in-place? - Yes, the issue was identified on the back of Post checks

2) How did the special character generate? - WMS xml generation process was earlier handling only set of special chars and hence resulting unhandled special chars reached CSSM. Now its amended to send only alpha numeric (no special chars) in article description field as its no where further referred in system processing

3) Was there any alerting for ASN failures with the error “XML Parsing”?There were NO alerts for this scenario earlier. However, the alerts are configured now. 

4) Were there any delays in the resolution after the issue was identified? As soon as the issue was identified, IT teams from CSSM, WMS, Middleware and project were focused on recovery and all messages were corrected in parallel for speedy resolution.",9/15/2022,9/15/2022,,,,"No, Yes",No,Yes,Yes,Customer Tech change,CR,ASNs carrying special characters (unhandled) in the article description field failed to process in CSSM through the WMS XML generation process. ,167461,inadequate testing,NA,10/3/2022,October,2022,Sravan,Closed,,,2022,9,September,9/16/2025,1104,>60
9/6/2022,9/6/2022,88813770,07:30,9:10,01:40,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Issue with ECS application,ECS,NA,,ECS,RC identified,62310,"On 06/09, Store colleagues were unable to perform foods price compliance check between 07:30 and 09:10. Impact was minimal as this activity can be carried out throughout the day.  A change deployed on 30/08 resulted in memory contention on the ECS application and hence the heap memory was upgraded from 1GB to 3GB to resolve the issue.",Actions updated in the Problem actions tab,9/12/2022,9/12/2022,,,,Yes,No,No,Yes,3rd party change,CR,"A change which occurred on 30/8 to deploy a magnifying glass, so that tickets can be previewed as a large image. It sends more calls to the ECS API and hence results in a lack of connections and subsequently heap memory thus causing the 504 error and resulting in the application tom cat instance (hosted by ECS) to stop responding.
06/09
The issue reported was that there was no ticket available to check. This issue occurred again due to lack of heap memory after the change on 30/08. The reason for the error was that the 504 errors reoccurred (gateway timeout) – and due to this lack of connectivity the job that sends sample compliance data to stores failed.",NA,Lack of heap memory,NA,10/11/2022,October,2022,Rehan,Closed,,,2022,9,September,9/16/2025,1106,>60
9/5/2022,9/5/2022,88887235,11:00,20:22,09:22,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Issues with Tasks,Microsoft,NA,,Tasks app,RC identified,62823,On 05/09 Retail Communications Team faced issues with publishing Tasks to stores impacting their ability to communication key weekly messages.,All actions are closed,9/15/2022,9/15/2022,,,,"No, Yes",No,No,Yes,3rd party issue,No,"The tasks app in teams is used to publish the activity list to the stores every Monday evening by 15:00.
At 11:00 on 05/09, an internal processing queue was piled up. The publishing list (task app) processing message was also posted in the same queue. Since the queue was piled up, there was a delay in processing resulting in a high latency. Hence the Publishing screen in the Teams Tasks app kept loading for a very long time.
The internal queue is used for multiple purposes and one of the major activities is to synchronize data with Teams platform. There was a spike of events from Teams platform which resulted in a backlog of queue messages.",NA,NA,NA,10/18/2022,October,2022,Kavitha,Closed,,,2022,9,September,9/16/2025,1107,>60
9/3/2022,9/3/2022,88810002,07:45,15:00,07:15,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Store colleagues are unable to print ISF labels,TCS,BOSS Support,,Label priniting for ISF orders,RC identified,62079,tore colleagues were unable to print carrier labels for In-Store Fulfilment orders on the SPPD printers between 07:45 and 15:00. This delayed the packing operations at the stores resulting in 3943 miss promised orders. The SPPD Print service certificate had expired and was renewed followed by a deployment across the store estate to restore services.,"Retail and Dot com to discuss on the app ownership and support wrap combined with adding more SMEs to reduce the reliance on Colin: Vijay/ Steve
<<The problem identification and certificate renewal to be owned by the product team. The testing and deployment bit will be owned by the SSI team. Agree to circulate the contacts of the SMEs, shift rota of the team. Emergency contact details during major incidents.>> ETA: to be confirmed tomorrow.

Details around the certificate renewal last year: Renjith	Completed. <<The certificate used previously was a self-signed one with 10 years validity. However, as Chrome and Edge do not support the earlier version, the current version of certificate was then issued by AD with a 1year validity in 2021. >>

SOP for certificate renewal. : Ram	 ETA: 7/09

Ownership of the certificate extension along with knowledge transfer: Vijay/Steve	ETA: to be confirmed tomorrow

Why was the certificate expiry missed this year?	Vijay/Steve	<<No alerts from the certificate team on the renewal. Action is to ensure the alert is setup by the certificate management team atleast a month in advance to cater any changes needed. As a backup application team will setup manual alerts based on the expiry of the certificate dates. This will be taken care in action no. 7>>

What technical complexities were faced by Ram and Phil during the certificate renewal and package building?	Renjith	Completed. 
<<There was no complexity faced by Phil/Deepak. The additional step of packaging and validation was done by Phil. There was an additional step of removing the existing certificate was identified and a step to check that is now implemented as part of the packaging.>>

Can the certificate renewal process be automated?
Work around the alerting mechanism to notify the self signed certificate expiry well before to avoid such issue?	Vijay/Steve	
<<AD team can help in confirming if there is a solution for auto- renewal. But the packaging and deployment process should still happen to make the end to end solution to work. Alert implementation is the other option with a window of 1 month in advance.>>
ETA: to be confirmed tomorrow.

Define the certificate in the central dashboard to alert the support team whilst the certificate is due to expire: Sravan: Completed

SOP to document the testing and package building process to be deployed across the stores.	Colin/Phil: We are in the process of creating a confluence page with all the details for this
ETA: 09/09

Workshop for knowledge transfer for modifying the script, package building and deployment : Colin & Phil	ETA: 14/09

Engaging SSI team during non business hours	Renjith	To be confirmed

Deployment stats to be shared to track 	SSI/ Shyam	To be confirmed",9/3/2022,9/3/2022,,,,No,No,No,Yes,Certificate issue,NA,The SPPD print service certificate was expired on 02/09 causing the issue,NA,NA,EG 5457,12/21/2022,December,2022,Saloni,Closed,,,2022,9,September,9/16/2025,1109,>60
9/2/2022,9/2/2022,88808426,08:45,10:24,01:39,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,MI,ECS application inaccessible,ECS,NA,,ECS,RC identified,62193,ECS application was inaccessible displaying an error on both Honeywells and Workstations between 08:37 and 10:24.  Store colleagues were unable to print shelf edge tickets causing a possible discrepancy between the price displayed and price on the tills.  Vendors ECS performed an API restart after which the problem persisted but from 10:24 the errors stopped and service recovered.  We await root cause and resolution details.,Actions updated in the Problem actions tab,9/2/2022,9/12/2022,,,,Yes,No,No,Yes,3rd party change,CR,The root cause has been identified to be a change which occurred on 30/8 to deploy a magnifying glass so that tickets can be previewed as a large image.  This change sends more calls to the ECS API and hence results in a lack of connections and subsequently heap memory thus causing the 504 error and resulting in the application tom cat instance (hosted by ECS) to stop responding – this is why users could not access the application.,NA,,NA,10/11/2022,October,2022,Rehan,Closed,,,2022,9,September,9/16/2025,1110,>60
9/1/2022,9/1/2022,88805924,09:30,10:41,01:11,GTS,Enterprise Integration,"Customer Channels, Supply Chain","Selling Experience, Foods Supply Chain",,KI,Message pileup observed in the IIB gateway,TCS,Integration Services,,"Pack orders using SPPD, ASO was in backup",RC identified,62081,"On 01/09, message pile ups were observed in the middleware IIB gateway between 09:30 and 10:41. Store colleagues were unable to pack items via SPPD and ASO was in backup allocation. A price/category activity carried out by a colleague resulted in an influx of messages to a wholesale price interface in SAP. The pile up was cleared by 10:41 to restore services.","1. Identify why the I036 jobs were running in the morning: Moorthy: Completed. Due to a price list upliftment activity by a finance colleague, the wholesale prices were calculated across all the departments which caused the some jobs to overrun until the morning
2. Improve in communication between SAP and middleware and other downstream systems during such scenarios.: Moorthy: Completed.<<Reiterate the team members to notify Middleware and the downstream systems overnight if there are any issues with this interface. Notify MIM as well.>>
3. Understand the business plan for making the changes for the next 2 weeks: Moorthy: Completed. <<Team will advise the colleagues to implement changes in a systemic manner thereby not causing the issue again>>
4. Can a separate channel be configured to process the I036 messages?	Mukundan/ Shayolin	: Completed
5. Solution to be identified to accommodate the message spike in batches to avoid message pile ups	Mukundan/ Shayolin: Completed	
6. Revisit the design of communication b/w onprem and cloud	Mukundan/ Shayolin	: Long term action
7. Do we have the right alerts from both ASO and SPPD for us to pick this issue pro-actively before GIST/Store raise it with us	Lakshman/ Hisham: Completed",1/9/2022,1/9/2022,,,,Yes,No,Yes,Yes,Customer tech issue,NA,Finance colleague had performed a price list upliftment activity across 82 C&H departments on 31/08 and hence the whole sale prices were calculated for all the departments resulting in influx of messages. This caused one of the wholesale price job to overrun until the morning of 01/09. The sudden spike in the inflow of messages caused pile ups in middleware,NA,NA,NA,15/09/2022,15/09/2022,#VALUE!,Saloni,Closed,,,2022,9,September,9/16/2025,1111,>60
8/31/2022,8/31/2022,88805217,12:00,13:39,01:39,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Sparks mobile application inaccessible in both android and iOS devices,TCS,Sparks Hub Product Support,,Sparks mobile apps,RC identified,,Sparks mobile application was unavailable on Android and iOS between 12:00 and 13:39. The issue was caused due to a change while adding the new charity service which was then reverted to restore the service.,Tech have agreed to perform thorugh testing on the Uis as well as raise a CR at the back of any charity changes. So they have organized a centralized change discussion call within Customer engagement where we discuss and get agreement from all the teams before going ahead with change,8/31/2022,8/31/2022,,,,No,No,No,Yes,Customer Tech change,Yes,"The root cause was a change to add new a charity in Sparks. However before the change was deployed in production it was identified that there a few minor corrections were needed such as removing optional attributes. After final modifications it was not possible to verify the changes as SIT2 was down. It was found that the attributes  were optional in charity API, however the aggregate user was expecting the removed attributes as mandatory, and hence it came in as null instead of a selected charity id thus causing the issue. On the web only the specific charity page was impacted however on the Mobile app the entire hub was not accessible. ",No CR raised,Lack of testing,NA,31/10/2022,31/10/2022,#VALUE!,Saloni,Closed,,,2022,8,August,9/16/2025,1112,>60
8/30/2022,8/30/2022,88801699,06:30,16:00,11:30,GTS,Enterprise Technology Platform,"Customer Channels, Store Operations, Commercial Trading","Service Experience, Run the store, Foods Commercial Trading, C&H Commercial Trading",,SI,Microsoft central issue impacting the deployment and recreation of new pods in AKS,Microsoft,NA,,"SSI, OFP, Scan and Shop, Digital Café, C&H returns etc",RC identified,62198,"From 09:18, proactive alerts indicated that pods were either restarting or newly deployed pods were going into a non-functional state. This impacted the availability of multiple applications - SSI, OFP, Scan and shop mobile, digital café, Pay With Me, C&H returns and order retrieval extract. Microsoft identified a bug in their software following an AKS software upgrade impacting multiple customers. A fix was applied to ensure new nodes download the bug free software image.  To mitigate impact, autoscaling was enabled for all portfolio applications to ensure any pods that are recreated place themselves on new nodes ensuring the service runs as normal.","Closed Comments: All actions are closed after the MS review meeting
1. If Microsoft place something their health portal, who in our support teams are responsible for monitoring the portal and advising the relevant stakeholders? There is potential for improvement with the alerting here. The individual product teams should get the mail alerts and advise accordingly.  CLOSED
2 Could Microsoft have alerted us nearer to 7am when their issue actually started – As this is a global issue Microsoft cannot cater to individual communication requirements.
3 The Microsoft MIM did not seem aware of the global issue that Microsoft were running – Fed back regularly.
4 Why at 14:00 did multiple pods for digital café? Why did the alerting not inform us of the issue rather than the stores? There are multiple pods and some go into an unresponsive state and not fully down hence no alert.
5 If this type of issue was to reoccur, we should place all application team on hypercare to ensure that any further impact can be proactively captured. ALL
6 Microsoft reviewing their release process to ensure that future changes are not deployed on their primary and DR environments simultaneously
7 How can we get a list of applications hosted in the various cloud types – Azure, PaaS, AKS etc. There is a backlog with the cloud team who are working on an app service inventory report. ",8/30/2022,8/30/2022,,,,Yes,Yes,No,Yes,3rd party change,NA,Microsoft identified a bug in their software following an AKS software upgrade impacting multiple global Microsoft customers.,NA,NA,NA,3/3/2023,March,2023,Sravan,Closed,,,2022,8,August,9/16/2025,1113,>60
8/23/2022,8/23/2022,88792689,09:15,17:30,08:15,Supply Chain,International Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,IFOS intermittently unavailable,Minster,NA,,IFOS,RC unknown,62182,IFOS application was intermittently unavailable between 9:15 and 17:30 due to a connection error. Bradford & Bedworth Foods DC's were unable complete their exports but the resulting impact was minimal.  The connectivity issue was resolved by isolating the problematic server from the digital perimeter. Root cause investigation underway.,"1. What was the root cause of the issue in detail?

2. Who is overall responsible for IFOS
Akshaya and Srivaths are responsible for IFO (new system to replace IFOS) only and therefore as they are part of the International Foods team they have been asked to pick up co-ordinate for IFOS.

3. Why was there a delay in resolving the issue?
George has advised that after a previous incident iT was agreed that the network team would pick up the next occurrence and start doing investigations/taking logs etc.  The network team did this on 23/08, the were working with users.  At 10:30 a H/O users confirmed the issue had recovered.  At 16:07 a Bradford user said they had issues from the morning and that’s when it became a sev 1 – hence there was no delay in investigations as the issue had looked to have self-healed.

4. We need to reiterate to the IFOS team that any IFOS issue needs to be owned by Ashwini and the TCS IFOS team – this is the case whether the issue is application, infrastructure or any other cause, the IFOS team will own and drive the issue?  It has been agreed that IFO team will do the driving and co-ordination going forward.

5. Please advise what the IFOS team did from 10am 23/08 with a timeline of their actions – as when we study the issue, no progress was made until MIM was engaged at 16:00. - Answered in number 3.

6. Please can you advise of the action plan with timelines to investigate and resolve the issue with web server 1400",9/1/2022,9/1/2022,,,,No,No,Yes,Yes,3rd party issue,NA,"Minster confirmed that the DCOM component which provides the service was in an error state. However, they cannot confirm why DCOM process went into error state.",NA,NA,NA,10/10/2022,October,2022,Rehan,Closed,,,2022,8,August,9/16/2025,1120,>60
8/19/2022,8/19/2022,88786139,03:27,6:26,02:59,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Delay to the overnight critical batches,TCS,Control M ,,Picking and packing operations,RC identified,62035,"An overnight job over run at Blue Yonder initially delayed the receiving of C&H D&F orders by 2 hours.  In addition, further job failures were observed in SAP impacting the order flow between 01:00 and 03:30.  This delayed the picking operations at Welham Green, Stoke, Swindon & West Thurrock and Foods Bradford DC's by 20 minutes.  The SAP Control M batch scheduler went into unresponsive state and was restarted to resume the order flow. The root cause is currently being investigated.","Closed actions: 
Improve the communication process between BY and Customer D&F and understand the reason behind the overrunning job:  Completed
SOP to manually process the orders and monitor the jobs:	Kevin: Completed
Analyse the jobs when there are multiple alerts received	SAP	partially completed. They have 800 jobs to analyse
Improvement in communication across the teams in MIM bridges	Kevin, Santhosh G: Completed	
Configuring an alert in WMS to identify if no orders have interfaced by 02:30	Sakthi	Completed
Fix the Foods NDC dashboard	Joey/ Nandha/Shankar	Joey will discuss with Nandha and expedite the recovery process : Completed

Open Actions
A case has been raised with BMC to understand the reason behind the blockage	Santhosh G : As per the Control M logs, the jobs were stuck in execution state, as the 
Control M ports were blocked which hindered the communication between application and Control M.
Alerting to be configured/ Checks to be put in place if the agent becomes unresponsive, not processing the jobs: 	Santhosh G: Completed
Feasibility to configure a data level alert at SAP to identify such issues proactively	Kevin : Completed

",8/19/2022,8/19/2022,,,,Yes,Yes,Yes,No,3rd party issue,NA,"The job overran at Blue Yonder as one of the insert statement  had issues with the column index due to data changes over a period of time

The SAP Control M agent went to unresponsive state which interrupted the communication between the application and Control M causing multiple SAP jobs to not start and overrun. The reason behind the unresponsive agent is currently being investigated by BMC",NA,NA,,10/29/2022,October,2022,Saloni,Closed,,,2022,8,August,9/16/2025,1124,>60
8/18/2022,8/18/2022,88785880,18:49,18:52,00:03,GTS,Enterprise Technology Platform,Customer Channels,Selling Experience,,KI,Issue with WCS primary database,TCS,Cloud Infra,,Customer .com website,RC identified,62044,"On 18/08, the WCS primary DB went into hung state at 18:49 and failed over to secondary DB. There was an order dip in the website and no orders were taken for 2 minutes (18:50 - 18:52). Splunk reports and dashboards were impacted as well. The mount point was full causing the database to go into unresponsive state.","1. Need to remove older backups from dotprdwcsl1db02(current standby) server, tomorrow (20th Aug) - DBA team: Completed
2. Enable housekeeping jobs in crontab to remove the trace files older than 3 days in both the servers- DBA team: Completed
3. Revisit alerting/monitoring setup in cloud Azure DB servers - 
4. Deadlock scenario troubleshooting - Sumitro Chakravarty:  Team says its an usual nature of the app to create deadlocks which are terminated in real time.
5. Raise SR with Oracle support to checking trace file sizing - DBA team: Completed - Oracle support requested application team to check the application design to avoid deadlocks as expected.",8/18/2022,10/21/2022,,,,"No, yes",No,Yes,Yes,Customer Tech issue,NA,"As per design, the WCS primary database was pointing to the backup mount point which breached its utilization capacity and hence the database went into hung state. This was caused as the trace file generation was too high in the backup mount point due to multiple deadlocks in the database.",NA,NA,NA,11/1/2022,November,2022,Saloni,Closed,,,2022,8,August,9/16/2025,1125,>60
8/17/2022,8/17/2022,88783038,03:00,16:00,13:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Azure databricks system was down,Microsoft,,,UK TMO marketing emails and BEAM reports,RC identified,62065,"Azure Databricks system was down
On 17/08, Azure Databricks system was down between 03:00 and 16:00, which impacted the preparations for Planned TMO (Tailor made offers) launch. The UK TMO marketing email sends was delayed by 4 hours, ROI TMO remains unaffected. In addition, there was a delay of 10 hours in refreshing 4 BEAM reports with latest data across C&H and Foods. Rebuilding of data tables was performed  to restore services. We await root cause details from Microsoft.","Additional monitoring for query performance degradations with more aggressive thresholds for earlier alerting and mitigation - Arup Neogy
Increase the frequency of load testing against our critical databases for expensive queries. - Arup Neogy",8/25/2022,8/25/2022,,,,"No, Yes",No,No,Yes,3rd party issue,,"Microsoft identified that a backend database used for Databricks workspace authentication experienced a sudden increase in CPU usage, leading to authentication failures. On further analysis, a specific database index as potentially being the most significant contributor to the CPU usage - investigation into why this had the effect it did continues.",,,,9/23/2022,September,2022,Sravan,Closed,,,2022,8,August,9/16/2025,1126,>60
8/17/2022,8/17/2022,88782583,06:00,10:00,04:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Bradford C&H replenishment not being communicated to WCS,Knapp ,NA,,Picking and packing operations,RC identified,62029,Bradford C&H replenishment was not being communicated to WCS.  Site were able to pick existing stock available but could not pick new replenishments to fulfil orders.  Approx 4k retail singles which were due on the early despatch vehicles were missed.  All other volumes were re-scheduled for later shipments at 12:30.  New Ecom volume was not fulfilled through Bradford until issue was resolved.  Knapp confirmed that the issue was related to a configuration error after WMS Upgrade.  By 10:00 the error was fixed and service restored,Actions updated in the Problem actions tab,8/17/2022,10/20/2022,,,,Yes,Yes,No,Yes,3rd party issue,CR,"After a deployment, all the components of the sequencing of the relevant steps were brought up together.  As the WMS server was now much faster, these steps ran a lot more quickly than the others hence, running out of sequence.  Further technical detail needs to be sought from Knapp.",NA,3rd party change,NA,10/20/2022,October,2022,Sravan,Closed,,,2022,8,August,9/16/2025,1126,>60
8/16/2022,8/16/2022,88784091,10:44,10:56,00:12,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Spike in card payment transaction failures between 10:44 and 10:56,Worldline,NA,,Card Transactions,RC identified,62042,Spikes were observed in failed card payment transactions at stores between 10:44 and 10:56. Worldline advised that the issue was potentially with the acquirer WorldPay. No stores reported the issue. Awaiting detailed RCA from Worldpay,"Open:
1. Can any alerting be configured if the number of declined transitions is increasing?;Bala;Email alerting has been configured for a sample store (0343) if there are continues declines.
2.Aged out alerts are configured which highlights if there are any timeouts in transactions between network and worldline. However further analysis needs to be done to understand where the timeout is happening?;Rajesh; 	
3.The root cause and recovery actions for the issue to be confirmed;Gary – Worldline;Bala and Suresh to follow up with Worldline",8/16/2022,10/25/2022,,,,"No,yes",No,No,Yes,3rd party change,Yes,"No action was taken to resolve the issue during period of spikes. The root cause was implementation of a technical change in combination with an undiscovered dormant bug in Ahub. Fixes have been applied for the bug, meaning a similar change implementation would not result in the same problem.",NA,Human error,,10/31/2022,October,2022,Kavitha,Closed,,,2022,8,August,9/16/2025,1127,>60
8/11/2022,8/11/2022,88775876,14:37,16:42,02:05,GTS,Enterprise Technology Platform,Customer Channels,Selling Experience,,SI,Sparks hub mobile app inaccessible on both Android and iOS,TCS,Akamai,,Sparks,RC identified,62046,Alerting indicated that both sparks hub and Scan & shop mobile applications were inaccessible on Android and iOS devices between 14:37 and 16:42.  The root cause has been attributed to an inadvertent error while configuring the IP addresses during a Akamai IP whitelisting activity which were then added to restore services.,"Closed actions
Setup alerting for high error rates in all loyalty API's consumed in mobile apps: Ryan-Mason: Open – ETA – 2nd September: Completed
Process of bringing up holding page on Apps should be quicker: Ryan-Mason: Open – ETA - 26th August: Completed
Open actions: 
Review the email distribution list used for communicating akamai changes and ensure relevant teams are included on the DL. This should be reviewed at portfolio level.: David Raine:Open – We can set ETA as 30th September and work on it. This involves stake holders from various portfolio’s so the ETA will change depending upon how they prioritise this work.
Automating the process of updating IP ranges at Akamai end. David to own 50% of automating the process rest 50% to be owned by teams from relevant portfolio: David Raine: Open –The automation framework can be provided by Akamai end by 30th September. But it depends on how other teams adopt to it.
Have a separate workshop with the teams to agree on the short-term solution and potentially get approximate timelines and come up with permanent solution: Hisham/ Aadil/ Divakar: Open – We can set ETA as 30th September and work on it -This involves stake holders from multiple portfolio’s the ETA will depend upon how they prioritise it.",8/11/2022,8/11/2022,,,,"No, configured",No,No,Yes,Human error - Customer Tech,NA,"As a part of routine exercise by Akamai to remove and add new IP addresses which involves several infrastructure and application teams to carry out the changes on Customer side, these changes were missed for Loyalty Services causing the issue.",NA,NA,NA,11/9/2022,November,2022,Saloni,Closed,,,2022,8,August,9/16/2025,1132,>60
8/11/2022,8/11/2022,88775619,12:56,16:07,03:11,GTS,Enterprise Technology Platform,Supply Chain,Foods Supply Chain,,KI,Issue with live allocation responses to Barnsley Depot.,TCS,Network,,ASO,RC identified,61649,"On 11/08, Barnsley depot reported issues with live allocation from ASO. The depot was on back-up allocation between 12:56 and 16:07.  A core switch issue at Barnsley was fixed to restore services. The root cause has been attributed to a power failure..","Configure the alerts for Core Switch Down - Completed
Why did the Power not re-routed, and the trip was not reset by the site after a power failure? - Advised DC to follow the SOP when they see a power issue",8/11/2022,8/11/2022,,,,"Yes,no",No,No,yes,Infrastructure issue / Hardware failure,NA,A core switch issue at Barnsley was down after the power failure. The switch was brought up and power was rerouted to fix the issue,NA,NA,NA,9/2/2022,September,2022,Sravan,Closed,,,2022,8,August,9/16/2025,1132,>60
8/10/2022,8/11/2022,88774563,15:45,4:10,12:25,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Issue in processing new ECom orders in Castle Donington WMS,TCS,"WMS, Middleware",,WMS Donington,RC identified,62129,Customer .com customer order flow to Donington was interrupted between 15:45 and 04:10. Around 3.5K orders with delivery date 11/08 were miss promised and the C&C preposition was moved to 24 hours. A restart of the WMS application and queue manager was performed to resume the order flow into WMS. The root cause has been attributed to a misconfiguration as a prerequisite for the tech stack migration.,"Open:

1. Configure an alerting for to notify the ‘Unable to get message – error code (2033)’ ;Dean- CD WMS; Inprogress
2.‘Zero KB’ file was created in previous instance on 29th of Jun and 9th Jan. Further analysis to be made to under the scenarios in which the file is getting created and is there any actual impact due to this?;WMS devops/ Varun; Inprogress
3.Create a SOP with detailed steps to recover the issue related to this specific error message – ‘2033’ ;Varun/ Anand ; Inprogress
4.How to restrict the connection between one source application and MQ manager in production stack or can any alerting be configured if the queue is picked by two daemons?;Murali/ Middleware;Inprogress


Closed: 
	
1. What is ‘Unable to get message – error code (2033)’ mean?;Dean- CD;WMS 	2033 error means no message available in the queue for picking by daemon. These error messages were thrown since the message was picked up HP stack daemon and message was not available for Exadata daemon pick up the message from queue.
2.We identified that there is data loss between Middleware and WMS after recon. What is the reason behind the data loss?;Anand - CD WMS;There was no data loss, some messages were transferred to HP stack by the daemon which was active after the dry run. 
3. Ensure that the message is not picked up by the daemon to dormant stack?;Anand- CD WMS ;The cut over plan has been updated such the daemon parameter gets changed so that messages are not sent the dormant Stack 
4.The MQ manager (P1RPDN01) and WMS application restart fixed the issue? Can MQ restart be performed without any outage to site?;Anand- CD WMS	The MQ ;manager cannot not be restarted separately which would incur data loss between WCS and WMS
5.Should we have decided to restart the MQ manager earlier during the incident?;Martin;The decision was made to restart after 03:00 AM after the trailer shipped to ensure that there is no impact to site. But if we had known the impact will be 3.5K a decision might have been taken to restart the MQ manager earlier. 
6.Zero KB files were getting generated whenever there this a ‘2033’ error. Why was it getting created? ;Dean;Since there no message available to pick from the queue, daemon created a 0KB file. 
7.Why did the WMS dashboard did not highlight the correct number of orders impacted?;	Anand/Dean;	The dashboard was giving the number of orders impacted based on the OF messages failed (Z type failure) in Exadata stack, the numbers were incorrect since some messages were in HP stack which not accounted. 
9.How do we ensure that the E2E recon is done without any deviation during the incident to quantify of the impact?;Sterling/WMS/Command centre; 	Splunk dashboard should be monitored to understand the exact impact. SOP needs to be updated to monitor the Splunk dashboard and investigate if there are any differences across the system.
10.Ensure that the all the recent changes are validated during any incidents before ruling out that it not the cause of the issue ;All ;Teams have agreed in the PIR call to validate all the recent changes. 
11.Any activity/changes related to tech stack needs to be communicated to the wider audience;Vanchee/Anand;Agreed to communicate to wider audience about the ongoing activities related to tech stack changes in the PIR call, 
12. Why was the change not part of the change list ? Priya/CM; Change was not part of the cab and was approved offline. Team has been advised to include all the changes in the list going forward",8/12/2022,8/12/2022,,,,"Yes,NA",Yes,No,Yes,Customer Tech change,Yes,"As a perquisite for CD (Castle Donington) tech stack go live on 24/08, a dry run was performed by oracle team to transfer the data from Exadata to HP stack by 14:30 on 10/08 under the CR# 165705. After the data transfer, both daemons which sends the data to Exadata (active production stack) and HP stack (dormant now) was active in CD WMS application. As the daemon configuration for inbound messages to HP stack was not removed while starting the application for collecting the logs for another open issue in new HP stack, some messages were picked up by the daemon, which sends the data to HP stack, and stored the messages in HP stack. Hence the messages were lost in the active Exadata stack . Once the MQ manager and WMS application restart was performed the connection between the HP stack daemon and CD WMS was stopped after which the messages were sent only to the Exadata stack.",165705,Human error,,10/18/2022,October,2022,Kavitha,Closed,,,2022,8,August,9/16/2025,1133,>60
8/10/2022,8/10/2022,88774345,16:50,17:55,01:05,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,,MI,Contactless transactions getting declined at stores,TCS,Network,,Payments,RC identified,62127,"Store colleagues reported issues with contactless transactions getting declined between 16:50 and 17:55. Transactions over 150£ were declined, however transactions using chip and pin were successful. A recent change in the Palo Alto firewall was reverted to restore services.","Open actions:
7) The Customer contact details in operations manual needs to be updated and share with Worldline to contact Customer in case of any card payment issues.  - The DL was updated and needs to be reviewed with Laura. 
8) The Run Book to be updated with the detailed operation procedures for all the card payment issue scenarios faced so far - NOC run book is completed and POS run book is in-progress
10) Run through the run book with all relevant stakeholders – MIM, SD, POS, NOC, Laura, Rosie - This action is dependant on action8
12 a) Feasibility to check whether we can communicate the notification thorough Honeywell device (Similar to RED alerts) - This action is dependent on Customer to provide us a tool to publish the notifications. No ETA at the moment.
12 c) Use store self-portal to publish any central store issues - This action is dependant on Customer to provide us a tool to publish the notifications. No ETA at the moment.

closed actions:

1) Configure the alerts in Palo Alto firewall to notify if there are any transactions being denied to Worldline from Customer network.
2) Feasibility to configure the alerting in Panorama (Firewall Management tool) if there are any wider address ranges are applied in the policies before it is pushed to the firewalls
3) Why did the network addresses within the policy got corrupted/ truncated?
4) Why was the change implemented during the store operating hours? And who approved the change?
5) Anything could have been done differently to restore the service quickly?
6) Create a SOP (Standard Operating Procedure) to handle such failures in future.
9) SOPs to be created to handle all network related incidents
11) MIM to update the Retail Service Connect channel in-case of any central issue with the stores 
12 a) Enable Push notification to tills to alert store colleagues/customers if there is a central issue with card payments - This requires a product change, which is yet to be discovered or added to the product backlog. Backlog has been created - RPOS-6712 
13) Alerting mechanism to be enabled from worldline end whenever there is a spike/drop in incoming transactions",8/10/2022,8/12/2022,,,,"No, Yes",No,No,Yes,Customer Tech change,CR,"As part of a change (CRQ 165993) to remove WPAD from the network which is a pre-requisite change for Zscaler rollout to Customer laptop users including store travellers, it was supposed to route vlan 105 traffic, (which is wired store traveller) to Zscaler and away from the digital perimeter. The change contained multiple network objects which had to be sent to all stores network and so executed using a CLI script as per normal process when applying this sort of change.

One of the network addresses within the policy applied, got corrupted/ truncated. It was supposed to only send traffic from mask subnet /24 through to Zscaler, but because of the corruption it sent /2 traffic (it truncated the 4 from the end). This change had multiple objects configured and added to the wider Store internet access policy group which also got deny policy, all this firewall policy rule numbers are above the Payment access rule which took the priority and denied the access to worldline destination address both Primary & Secondary.

A manual update to correct it back to /24 within the policy that restored the service.",165993,inadequate testing,RPOS-6712,12/16/2022,December,2022,Sravan,Closed,,,2022,8,August,9/16/2025,1133,>60
8/3/2022,8/3/2022,88763682,11:19,17:55,06:36,Commercial Trading,Foods Commercial Trading,Commercial Trading,Foods Commercial Trading ,,SI,Issues in Foods pricing and promotion flow,TCS,ECS,,ECS,RC identified,62024,"Investigations in the pricing and promotions end to end flow revealed a number of issues impacting store promotions.  This impacted 212 stores - 1200 unique UPC store combinations. This was caused by a missing code condition in the Retail (RET) layer, which was fixed and all affected promotions were successfully retriggered. A potential bug in SAP was identified which does not consistently process promotion activations and a plan has been devised to mitigate any issues with promotion activations in SAP as a workaround.  ",20/10: Recon between SAP and ECS is not feasible as ECS is taken care by third party. A recon between RET and SAP is tracked under EP-1076,8/3/2022,8/3/2022,,,,"No,yes",No,Yes,Yes,Design issue,No, The issue is within the RET layer (as part of the Retail/ECS flow). There was a missing code logic condition for Store Level TPR which has now been added to RET to permanently fix the issue going forward,NA,NA, EP-1076,10/20/2022,October,2022,Kavitha,Closed,,,2022,8,August,9/16/2025,1140,>60
7/31/2022,8/1/2022,88759635,10:30,6:00,19:30,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Store colleagues unable to complete labelling compliance checks,TCS,Enterprise Technology platform,,Bake Label,RC identified,61776,"On 31/07, Store colleagues were unable to carry out labelling compliance checks using the Bake label application. No impact to trading as stores were performing manual checks. An expired Azure secret key was renewed to restore services at 06:00, 01/08. ","Hold a workshop with the relevant stakeholder to go through a technical overview of Bake Label app and compliance app. Create a technical overview document including a support wrap highlighting SMEs and their 24*7 availability with contact details: James and Colin: James: Completed
ames to lead with teams and produce support wrap document

JH 03/08: From a compliance app perspective, support documentation that the service desk must use is available on Confluence here. FYI – given these apps that this team supports, such as this Labelling Compliance app, are not deemed business critical (and therefore incidents should not be classified as Sev-1’s as per #6 below), we support them in office hours only across UK & India time zones (i.e. 12x5).

@Wilson, Colin P. – please add any required details from a Bake Labels perspective.

Confirm the ownership at an app level of the sharepoint client secret key for Label compliance: James: Completed

List the apps (3) using a secret key and ensure app owners know when the keys will expire and ensure they are responsible: Colin and James
Labelling Compliance
Note: a new business process is being defined to manage labelling compliance in stores that will require a new iteration of the compliance app solution. It is envisaged this version will not require data from the Bake Labels app
Networked Temperature Compliance (not live to store estate, in pilot only)
JH 12/08: production keys expire on 31/7/2023 for #1 and 08/08/2023 for #2. As above, #1 will be remove when new process is released (likely Q3)

Configure alerting or monitoring tool to proactively identify when and for which app, the secret key is about to expire: James and COlin : 
Discussions completed on options to monitor, approach being agree with architecture community before implementation. Approach agreed with Retail Services team for updating for process #2 above

If the issue reoccurs, an IVR message is to be placed advising the stores to perform manual workaround on paper: Completed: Not to be treated as a MI

Further App development involving Sharepoint - approval process to include key authentication expiry alerts: JH 03/08: FYI – no use cases in the product backlog of the Compliance & Back Office team currently require data integration into a SharePoint list.",7/31/2022,7/31/2022,,,,No,No,No,Yes,Customer Tech issue,NA,"The sharepoint secret key which authenticates the Bake label application to send the compliance data to the Retail compliance sharepoint was expired on 31/07, causing the issue.",NA,NA,,10/31/2022,October,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1143,>60
7/30/2022,7/30/2022,88758611,06:30,12:30,06:00,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Issue with Ocado orders,TCS,ASO,,Ocado Orders,RC identified,61777,"On 30/07, alerting indicated that approximately 50% of Ocado orders were over allocated into Bradford and Milton Keynes Foods NDCs. This caused a delay to picking operations at both centers for Ocado orders. The incorrect orders were deallocated to resume operations. Provisional and final orders for 30/07 were aggregated in the database, in error, causing overallocation.","Open:
3.An alert to be configured if provisional order logic app is not running;Marcina;In Progress (ETA 22nd August, 2022)
5.Create a SOP to identify the over allocated quantity so that the information can be shared to WMS quickly ;Marcina;In Progress (ETA 16th August, 2022)
6.Create a SOP to deallocate the over allocated Ocado quantity in WMS;	Marcina/Sakthi;	In Progress (ETA 22nd August, 2022)

Closed:
1.Why was the provisional order for the previous order not processed?Marcina; The provisional order processing 	was held in error as part of the RU issues in SCRD the previous Friday and it continued to be in held status until the issue occured. 
2.Wyy was the provisional order logic app is not running ? ; Marcina ; It was held in error as part of the RU issues in SCRD the previous Friday and it continued to be in held status until the issue occured. 
4.The current overallocation alert for Ocado orders is a Sev 2 and this needs to be changed to a sev 1;	Marcina/Vinoth; Completed",7/30/2022,7/30/2022,,,,Yes,Yes,Yes,Yes,Human error - Customer Tech,No,"Provisional and final orders for 30/07 were aggregated in the database, in error, causing overallocation.",NA,NA,NA,9/6/2022,September,2022,Kavitha,Closed,,,2022,7,July,9/16/2025,1144,>60
7/30/2022,7/30/2022,88758807,09:37,10:57,01:20,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Customer.com customers unable to checkout on International websites,Global-e,NA,,International Website,RC identified,61923,"Alerting indicated that the order flow into the International website was interrupted between 09:37 and 10:57. Customers were unable to checkout and place orders impacting around 140 orders when compared to last week. Global E required an unplanned outage in order to fix a performance degradation on their database, thus causing the incident. Once this outage was completed, services were restored.","Was there any emergency communication received from Global-e on the unplanned outage? If not, how to improve this? No emergency communication recieved

Global-e actions  - Completed
 
Additional monitoring was added to identify potential malfunctioning in this component. 
 ",8/4/2022,8/4/2022,,,,Yes,Yes,No,Yes,3rd party issue,NA,Global-e had an internal malfunction in their main DB component which caused service degradation from 09:36 until 10:19 BST.,NA,NA,NA,9/2/2022,September,2022,Sravan,Closed,,,2022,7,July,9/16/2025,1144,>60
7/27/2022,7/29/2022,88751272,10:57,13:13,50:16,Group Platforms,Finance,Group Platforms,Finance,,KI,Slowness in SAP Create Promotion screen,TCS,SAP,,SAP price promtion screen,RC identified,61767,"On 25/07,  Foods promotion team reported slowness in the SAP create promotion screen. This impacted their ability to create 500 promotions planned for 28/07. A tactical fix was performed to recover services. The root cause was attributed to an existing data validation logic associated with the promotion screen change and a permanent fix will be planned in August.","With the data validation logic removed, are there any residual impact on the new promotions being created?: No impact: Closed
Details around the permanent fix: CRQ000000166402- To create an index for A155 Table” was deployed yesterday. P&P team validated the change, and they did not observe any slowness",7/29/2022,7/29/2022,,,,No,No,Yes,Yes,Customer Tech issue,NA,An existing data validation logic associated with the promotion screen change was causing the slowness.,NA,NA,,7/29/2022,July,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1147,>60
7/27/2022,7/27/2022,88754449,10:47,12:40,01:53,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,IFOS application inaccessible,TCS,IFOS,,IFOS,RC unknown,61766,"On 27/07, IFOS application was inaccessible between 10:57 and 12:40 due to a connection error. International franchise partners and depot users were unable to place and dispatch orders respectively however no deadlines were missed. The connectivity issue resolved without any intervention.",Closed:,7/27/2022,8/9/2022,,,,"No,yes",No,No,NA,RC unknown,No,"Infra teams investigated further and could not identify any issue during the issue window. Hence, the root cause for this issue unknown",NA,NA,NA,8/17/2022,August,2022,Kavitha,Closed,,,2022,7,July,9/16/2025,1147,>60
7/25/2022,7/26/2022,88751661,16:58,2:30,09:32,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Issue in Customer .com customer order flow to Donington,TCS,Sterling,,Sterling,RC identified,61746,"Customer .com customer order flow to Donington was interrupted between 16:58 and 18:51. Around 2169 orders with delivery date 26/07 were miss promised and C&C proposition was moved back. A fix was implemented to resume the order flow by 18:51 and the impacted orders were sent to Donington by 02:30, 26/07. The root cause was attributed to an implementation error during a planned change.",All actions closed,7/25/2022,7/25/2022,,,,Yes,Yes,No,Yes,Human error - Customer Tech,NA,"After a planned change to publish Bradford inventory to the website and remove Bradford from the sourcing logic in Sterling (OMS), a mistake was made in the implementation of the later, which caused the orders to go down the ISF queue rather than the Donington queue",NA,NA,NA,9/13/2022,September,2022,Sravan,Closed,,,2022,7,July,9/16/2025,1149,>60
7/22/2022,7/22/2022,88748807,23:30,3:59,04:29,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Delay in sending FCO orders to suppliers,TCS,ORCA,,Food to Order,RC identified,61902,"On 22/07, the FCO finalization failed as an existing order processed on 21st July was cancelled but not deleted from the ORCA tables. The problematic order was removed, and the orders were sent to the suppliers with a delay at 03:59",1.Why was the provisional order for the previous order not processed?Marcina; The provisional order processing ,7/27/2022,7/27/2022,,,,Yes,Yes,Yes,Yes,Customer Tech issue,NA,"An order received from Sterling was finalized on 21/07, but was cancelled on 22/07 and hence was not systemically removed from the ORCA table which caused the FCO finalization step to fail. ",NA,NA,NA,7/27/2022,July,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1152,>60
7/21/2022,7/21/2022,88745397,07:10,7:25,00:15,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,"Sorted Label printing issues across Donington, Thorncliffe and Bradford C&H DCs",Sorted,NA,,Carrier Label Printng,RC identified,61745,"Castle Donington, Thorncliffe and Bradford C&H DCs reported issues with Sorted carrier label printing between 05:50 and 07:25 on 21/07. This impacted dispatch operations for e-comm orders. This was caused due to an issue with Microsoft Azure SQL services across their Western EU region which was fixed and services came back to normal. Awaiting detailed root cause from Microsoft. ",2.Wyy was the provisional order logic app is not running ? ; Marcina ; It was held in error as part of the RU issues in SCRD the previous Friday and it continued to be in held status until the issue occured. ,,21/07/2022,,,,No,No,No,Yes,3rd party issue,NA,Microsoft had an outage in their Azure SQL server services which impacted their Devops services across the Western EU region,NA,NA,NA,1/8/2022,January,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1153,>60
7/21/2022,7/21/2022,88745387,02:30,4:45,02:15,GTS,Enterprise Technology Platform,GTS,Enterprise Technology Platform,,KI,Teams application inaccessible,Microsoft,Datacentre messaging,,Microsoft Teams,RC identified,61699,"Teams application was inaccessible between 02:30 and 04:45 on 21/07, both web and desktop version were affected. Microsoft identified a misconfiguration in their recent deployment which impacted multiple customers including Customer.  Microsoft restored services from 04:45. We await detailed resolution actions and root cause.",4.The current overallocation alert for Ocado orders is a Sev 2 and this needs to be changed to a sev 1;,7/27/2022,7/27/2022,,,,No,No,No,Yes,3rd party issue,NA,A code bug in the recent deployment at Microsoft ,NA,NA,NA,8/26/2022,August,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1153,>60
7/21/2022,7/21/2022,88746269,11:15,12:07,00:52,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,,SI, Multiple ROI stores hosted by Eircom were hard down,Vodafone,Network,,Stores Network,RC unknown,61742,"Proactive alerting indicated that 18 ROI (Republic of Ireland) stores hosted by Eircom were hard down between 11:15 and 12:07.  Affected stores traded offline, contactless payments were not available and chip and pin card transactions were successfully approved up to £150.  A routing issue was identified and a workaround was implemented to route the traffic to the secondary link. Vodafone continues to investigate the cause.","1. Need to inform stores proactively when the stores are hard down or any network issues
2. Engage Nick Baldwin straight away in case of multiple stores are hard down during business hours - Completed
3. Nick and Paul to check with Vodafone whether any logs/traces need to be captured to identify the root cause if the issue re-occurs - Completed",7/22/2022,7/27/2022,,,,Yes,Yes,Yes,Yes,3rd party issue,NA,"The root cause is believed to be a routing anomaly in the Vodafone core network. However, Vodafone could not investigate further because of inadequate logs.",NA,NA,NA,8/17/2022,August,2022,Sravan,Closed,,,2022,7,July,9/16/2025,1153,>60
7/20/2022,7/21/2022,88745216,15:15,15:49,24:34,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Non-order goodwill gift card issue: ,TCS,"Cloud Exponence, Integration",,Gift Card,RC identified,61904,Gift cards issued as goodwill gestures were intermittently duplicated.  The majority of the duplicates were successfully invalidated before being redeemed by a customer.  The duplicates were caused by a file lock issue in the SFTP b2b transfers after the V1 to V2 migration on 18/07.  The storage account service was upgraded from standard to premium to resolve the issue from 15:49 as recommended by Microsoft. The interfaces were monitored overnight and no further issues reported.  ,"Closed: 
1. Any alerting can be put in place in consumer system (Sterling- An alerting has been configured to alert if any non-order good will gift cards duplicates are received in Sterling 
2. Why did the issue impact only the dot com related files though the storage account was common- The storage account is not common to all portfolios. Each portfolio has a separate storage account. The storage account of Dotcom was affected since the transactions are high in dotcom storage account than other portfolio storage accounts- Complete""
3. Any alerting can be put in place in middleware systems 	-Cloud Exponence is working with Microsoft on the feasibility to configure alerts for the utilisation of storage account-; Cloud Exponence; Alerting is configured 
4. Implement a deduplication logic in the consumer system which processes the non-order good will gift cards (Sterling); Emma; A backlog will be raised for tracking; Cannot be implemented due to design constarint- closed ",7/20/2022,7/20/2022,,,,"No,yes",No,No,Yes,Customer Tech change,Yes,"The gift card files to be processed are placed in a SFTP folder on a VM disk and then a Rsync process (runs every 2 mins) moves the file from SFTP VM disk to a storage account. On the day of issue, multiple open file handles existed for dot com storage account, and this affected the Rsync process thus files were left on the source disk even after copying to storage account, these files were repeatedly picked up by the Rsync causing duplicates.

The below is the flow for processing the Non order gift card:
Anana->SFTP VM (Rsynch)> storage account > MFT agent> V1 queue manager> EM> Sterling

The issue was in the following – ‘SFTP VM (Rsynch)> storage account >’ part of the flow.",164395,inadequate testing,NA,9/22/2022,September,2022,Kavitha,Closed,,,2022,7,July,9/16/2025,1154,>60
7/20/2022,7/20/2022,88744174,02:25,7:38,05:13,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Delay to the Foods critical batches,TCS,Quantum,,SCL & FOAR,RC identified,61694,"Alerting indicated that one of the overnight Quantum job was overrunning due to a connectivity issue between the application and the database server. This delayed the supplier orders, picking operations at Bradford NDC and Quantum UI availability by 30-45 minutes.  The database and application services were restarted to recover the job. The detailed root cause is currently being investigated.","Closed actions:
Configure proper alerting for the problematic job: Shravan: Completed. 
Network team to check for any connection loss between 00:00 and 1:30 AM on 21/07: Rajesh/ Manikandan: Completed. No errors observed.
Unix team to raise a case with IBM and check for any abnormalities happened between 00:00 and 1:30 AM:  Sahish/ Haritha: Closed. No OS errors observed on 20/07. Unix team to monitor the utilization during tonight’s (21/07) run.
SEV1 case already raised with Oracle. Logs have been shared. Awaiting feedback	Dipti/ Oracle: Closed: No errors observed by Oracle
Increase the db parameter “process limit” improving performance of the job execution. 	: Dipti	: Completed. The process limit has been changed from 1250 to 1500
Recommendation to perform a full stack restart of the DB and app servers followed by AIX upgrade.: Sathish/ Haritha/ Dipti: Completed on 24/07

Hypercare plan for 21/07 batch. Support teams will be on a call starting 23:30 : Shravan, Sathish, Dipti, Manikandan: Completed and ongoing
Unix team to run Perf-PMR report if same issue occurs tonight. But it might cause some performance issue so must be taken carefully	Sathish/ Haritha: On hold.
E2E review of the alerting that runs between 23:30 and 02:00: Santhosh/Lakshman: Closed

Open actions: 
A long-term recommendation to update the JDBC drivers: Shravan: This needs extensive analysis and we will seek help from Phil to first analyse quantum E2E and then take a decision: The JDBC driver upgrade failed due to an existing issue which can be fixed by AIX hardware upgrade from P8 to P10 as in previous instances the upgrade from P6 to P8 had fixed the intermittent JDBC exceptions. This activity is planned before peak and will be tracked under a RISK. Meanwhile,the mitigation is the alerts and hyper care we have in place and immediately bounce the qlogic & openMQ services in Quantum if we hit a jdbc exception error.",8/7/2022,NA,,,,Yes,Yes,Yes,NO,Design issue,NA,The root cause is believed to be an exisiting JDBC error.,NA,NA,Risk 1738,13/09/2022,13/09/2022,#VALUE!,Saloni ,Closed,,,2022,7,July,9/16/2025,1154,>60
7/19/2022,7/20/2022,88743714,14:21,13:30,23:09,GTS,Enterprise Technology Platform,GTS,Enterprise Technology Platform,,KI,3 air conditioner units were not working in Machine Hall 3 in Stockley Park.,TCS,"Wintel, Unix, Linux and Network",,,RC identified,61763,"Due to the extreme temperature, three air conditioners failed in the machine halls in Stockley park on 19/07. To proactively prevent any impact, some physical non-prod servers were powered off and mobile air conditioners were arranged.  Servers were powered back on once the temperature reduced to normal.",4 additional portable aircon units will be arranged in SP - Completed,7/19/2022,7/19/2022,,,,No,No,Yes,No,Infrastructure issue / Hardware failure,NA,"Due to the extreme heatwave in UK, the air conditioning units struggled to operate and failed.",NA,NA,NA,8/18/2022,August,2022,Sravan,Closed,,,2022,7,July,9/16/2025,1155,>60
7/19/2022,7/19/2022,88743715,16:00,22:21,06:21,Group Platforms,HR,Group Platforms,HR,,SI, MyHR application inaccessible,Oracle,NA,,MyHR,RC identified,61695,"Colleagues Services reported that MyHR application was inaccessible impacting people management activities between 16:00 and 22:21. This delayed the feeds to downstream systems.  Due to the extreme heat, vendor Oracle identified an issue with their cooling infrastructure in their UK south data centre and this was powered off to prevent any hardware failures. Services were restored and MyHR remained stable from 22:21.",Oracle have identified solutions to handle such scenarios in a proper manner,19/07/2022,19/07/2022,,,,NA,NA,NA,Yes,3rd party issue,NA,"Due to extreme heat, Oracle identified an issue with their cooling infrastructure in their UK south data centre and this was powered off to prevent any hardware failures",NA,NA,NA,7/9/2022,July,2022,Saloni,Closed,,,2022,7,July,9/16/2025,1155,>60
7/8/2022,7/8/2022,88727706,01:30,7:14,05:44,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Delay to the critical Foods batches,TCS,Quantum,,SCL & FOAR,RC unknown,61698,Alerting indicated multiple Quantum jobs were overrunning due to database contention.  This delayed the picking operations at Bradford NDC.  The database stats were updated to recover the issue on 07/07 and on 08/07 the problematic job was skipped and the Quantum application and database were restarted to recover the service.  The detailed root cause is currently being investigated and hypercare remains in place.,"Closed actions: 
Adhoc run of the ITL job in prod: Bhuvana: Closed: It was planned between 15:00 and 16:00 on 08/07. However, the problematic job completed fine with no evidence of database contentions
Analyze the java logs to identify the spike in the number of sessions whilst the adhoc run: Bhuvana: Closed: No issues were identified as the job completed within its usual time.

Vendor Oracle analysed the logs and identified an issue with the I/O sessions of the problematic job. A recommendation to perform PGA upgrade was suggested to prevent the database contention: Dipti: Completed on 11/07

Reducing the number of threads from 80 to 40 to ensure less processes are executing at a time: Completed on 11/07

Execution of the problematic job after the PGA upgrade: Shravan: Completed: The job ran fine and no issues were identified.
Teams to ensure that MIM has been given a call out at the correct time: Bhuvana: Completed

A workaround to skip the ITL job is in place inorcer to save the SLA over the weekend if the issue occurs: Bhuvana: Completed

Plan a hypercare over the weekend: Santhosh/ Jeeva: Completed

PCM to improve their call out process: Suganya: Closed

Analyse the I/O sessions with the Oracle vendor: Dipti: Closed

Recommendation to perform table defragmentation for improving the job performance: Dipti: Completed

There was a delay in sending the Inbound Booking Orders (IBO) to the DCs which is usually sent between 08:00  and 08:30. 
FLow: SAP -> Q -> WMS: Bhuvana/ANgeline: Closed",8/7/2022,8/7/2022,,,,Yes,Yes,Yes,Yes,RC unknown,,"The Quantum ITL job caused database contention resulting to multiple jobs to overrun. The database contention is currently believed to be due to a SQL query of the ITL job producing an increased number of sessions in the database, however, there are insufficient logs to ascertain the cause. Hence, root cause inconclusive",,,,27/07/2022,27/07/2022,#VALUE!,Saloni ,Closed,,,2022,7,July,9/16/2025,1166,>60
7/7/2022,7/15/2022,88725550,07:40,14:26,212:29,Group Platforms,HR,Group Platforms,HR,,SI,Colleagues missing additional hours payments in July pay,SDworx,NA,,HR,RC identified,61778,"762 colleagues had incorrect additional hours for w/e 18/06 – (714 underpaid and 48 overpaid). Faster payments have been made for any underpayment of £50 or more (155 colleagues), and the remainder (559 less than £50) will be corrected for the next pay. Colleague Services are communicating to those affected and SD Worx are investigating the root cause.","To update the SOPs to detail the recovery steps including the reversal of the timesheet status for the timesheet records that have been exported but not updated to HRe Pay table. A Knowledge Transfer (KT) session was held with the support and the development team and scripts have been provided to revert back all timesheets to original state before the process can be kicked-off from scratch.

To regroup with subject matter expert of the following day that a timesheet issue has happened (after the recovery steps have been applied by the support team) to validate the recovery approach adopted. This has been included as a mandatory service improvement and will be followed going forward",19/07/2022,19/07/2022,,,,NA,NA,NA,NA,3rd party issue,,"On 20/06, the time export lost connection with the database and crashed eventually. As part of recovery, the timesheet export was retriggered which had not picked the up the timesheets which were already marked as completed prior to crash and caused some of the specific records were not in HRe pay table",,,,15/07/2022,15/07/2022,#VALUE!,Saloni,Closed,,,2022,7,July,9/16/2025,1167,>60
7/6/2022,7/6/2022,88724606 ,00:00,1:16,01:16,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Drop in orders on checkout on UK site,TCS,WCS,,WCS ,RC identified,61636,Alerting indicated a drop in orders on the UK Website. Customers would have experienced an error message whilst checking out. A change made on the UK checkout journey was reverted to restore services at 01:16. Root cause investigations underway.,"Closed:  
Make sure Dev’s and QA’s are aligned with the changes. No changes to be made post sign off. Take it as story/bug and retest it.
Include Throttle ON or OFF scenarios as part of regression.
Provide Hypercare support for features going live for first run.
 ",7/6/2022,7/6/2022,,,,Yes,Yes,NA,Yes,Customer Tech change,Yes,"In the lower/test environment, the functionality for ‘Stock as a Service for Wine’ had been tested and signed off with a ‘user block/throttle’ in place e.g. only limited users were routed via Stock as a Service.  The remaining users were routed through the normal stock checking route (WXS inventory grid).

Before going into Production, the ‘Stock as a Service for Wine’ functionality should have been re-tested without the user throttle in place, thus also testing the normal stock check route. However, this did not occur. When ‘Stock as a Service’ was switched on in Production, there was no user throttle and it exposed a problem with the ‘normal’ stock checking route (a null pointer exception was thrown) which had not manifested itself in lower environments   

The deep dive into the cause of the null pointer exception, determined that the cause of the exception was a local variable not successfully storing or passing back the inventory value obtained from the WXS inventory grid.  Instead it retained the null value with which it had been initialised.  ",CM-8190,NA,NA,7/20/2022,July,2022,Kavitha,Closed,,,2022,7,July,9/16/2025,1168,>60
7/1/2022,7/1/2022,88718810,11:24,12:09,00:45,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Staff customer orders failing with payment decline,Worldline,NA,,Payments,RC identified,61570,"On 1st July at 11:24, through alerting it was discovered that all payments going via the Ingenico route were being declined.  This impacted all staff customers as they are currently routed via Ingenico with the exception of orders placed via PayPal, Applepay, ClearPay).  Worldline advised that the incident has been resolved their side by 12:09 and we await root and cause and resolution details. ","1.Worldline to work on the change process traning.

 ",7/8/2022,7/8/2022,,,,Yes,Yes,Yes,Yes,3rd party issue,No,The issue was caused due to manual configuration change done at the worldpay end on acquirer level based on Saleforce request had blocked the transactions.The config changes was reverted to fix the issue to all the Merchants including Customer,NA ,NA,NA,7/8/2022,July,2022,Kavitha,Closed,,,2022,7,July,9/16/2025,1173,>60
6/29/2022,6/29/2022,88715510,11:19,14:00,02:41,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Sparks hub timeout errors for both ROI and UK desktop customers.,TCS,Loyalty team,,Sparks hub,RC identified,61498,"On 29th June, some customers in both ROI and UK were getting timeout errors while accessing Sparks hub from 11:19. The issue was caused due to a Loyalty Services (LS) change deployed in the morning. An underlying issue was identified in Sparks Hub affecting web and mobile customers who were logging in for the first time and the issue was fixed by 14:00. ",No open actions pending or agreed,6/29/2022,6/29/2022,,,,No,No,No,Yes,Customer Tech change,Yes,"The issue was caused due to a Loyalty Services (LS) change deployed in the morning. An underlying issue was identified in Sparks Hub affecting web and mobile customers who were logging in for the first time 
",CM8144,inadequate testing,NA,6/29/2022,June,2022,Kavitha,Closed,,,2022,6,June,9/16/2025,1175,>60
6/29/2022,6/29/2022,88715810,16:20,17:03,00:43,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Checkout issues on International Websites (Global-e),Global E,NA,,International website,RC unknown,61577,"On 29th June, alerting indicated that no orders were taken on the International website between 16:20 and 17:03. Global-e was engaged who confirmed the service restoration. We await further details on the fix and RCA from Global-e. ","Scaleup the RabbitMQ instances to be able to handle more concurrent incoming messages -Done.
- Adding additional monitoring scripts to allow quick identification and response -Done.",7/13/2022,7/13/2022,,,,Yes,Yes,No,NA,3rd party issue,No,"Unexpected volume of connection to RabbitMQ from API had caused the issue. However, they are continuing to investigate the reason for the increased number of connections.",NA,NA,NA,8/22/2022,August,2022,Kavitha,Closed,,,2022,6,June,9/16/2025,1175,>60
6/29/2022,6/29/2022,88714999,11:05,15:47,04:42,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,MI,WMS application inaccessible,TCS,"Database, WMS",,Castle Donington DB,RC identified,61574,"WMS application was unavailable between 11:05 and 15:47 impacting Donington, Ollerton and Thorncliffe. This resulted in five hours capability loss and customer failure rate was 1.73%.  As a part of legacy decommission, two DNS servers were shut down after which the database storage node went to hung status.   With the help of oracle vendor, database storage nodes were recovered and WMS the application was restored. ","
Open actions 
Investigate scripts to analyse hardcoding of DNS in every Unix and Linux server config in SP and SW. Completed and confirmed by Elizabeth Golder

Closed Actions:

Correct the above root cause statement

What impact analysis was carried out before powering off the DNS servers as part of the legacy decommission?  How can we avoid the situation again where DNS servers we rely on have been powered off.

Was there an unreasonable delay in recovering the issue?

What is the available DR resilience in our architecture for an issue like this and if there is resilience then why didn’t it work?

Were there any alerts configured for Cell (storage) nodes hung/inaccessible?

Can communication be improved during an incident?

Was this incident dependant on the DNS servers being shut down as the issue was reported 24 hours later?

Can we check if any traffic is still routing to a particular DNS once it has been turned off.

What is the difference between DR and PROD? Quick recovery seen in DR and not in Prod.

Note: Jason commented that the teams seemed to be digressing and carrying out actions without calling them out – which left stakeholders out of the loop.

Review configuration of infrastructure in SW to ensure it is not using SP DNS

Put a process in place to ensure comms are sent to all Infra and App teams on-premise when new DNS servers are commissioned, so they are re-configure their applications to use them""

What is the available DR resilience in our architecture for an issue like this and if there is resilience then why didn’t it work? - Issue sorted after migrations to exadata. ",6/29/2022,7/1/2022,,,,Yes,Yes,No,Yes,Customer Tech change,CR,"As a part of legacy decommission (CRQ163468), two DNS servers (MSHDCMNSUKP0103 & MSHDCMNSUKP0104) were powered off.  When DNS servers are not reachable during the validation phase of regular default maintenance tool called Exawatcher, the cell (storage) nodes will try to reach the DNS multiple times and will fail to establish a connection, this results in cell node going into hung status.",CRQ163468,DNS servers were powered off,NA,10/5/2022,October,2022,Rehan,Closed,,,2022,6,June,9/16/2025,1175,>60
6/29/2022,6/29/2022,88715027,06:00,21:15,15:15,GTS,Enterprise Technology Platform,All BU's,All BU's,,MI,"Issues with multiple applications in Foods, C&H, Retail, International and Customer Channels",Microsoft,NA,,Azure loadbalancer ,RC identified,61576,"From 6am, proactive alerts from multiple applications indicated an intermittent cloud connectivity issue. Microsoft confirmed that the cause of the issue was a physical networking device failure in their infrastructure, this was isolated to restore services at 21:15.
 
Impact Summary:
-International C&H and food ordering for franchise partners
-Foods front end applications experienced performance issues intermittently e.g.: ASO, OFP and SCRD.
-Customer Channels - Intermittent issues with the Ingenico staff discount, order flows in Bradford, adding card/submitting payment and balance enquiry calls and order payment authorisations 
-Retail - Digital café in four stores was offline for the day and intermittent issues with gift card transactions
-C&H Commercial Trading – Intermittently impacted purchase orders, markdown and planning activities.  ","1) Microsoft to confirm the actual root cause - Adrian (MS MIM) stated that the MS technical team is still investigating the root cause and need time to come back with a detailed statement.Adrian says that he will provide an update by end of day 06/07

2) The Microsoft comms timings for the issue are not matching our impact timings - Microsoft to confirm the timings of their incident.We started seeing errors in application logs from 03:42am (BST) on our side.

3) Following on from the root cause – how did Microsoft not identify the hardware fault sooner and did their alerting not indicate anything?  Why did recovery take so many hours?  Did/does Microsoft have resilience in place?	
	
4) Were there any applications not impacted by this incident?  What is the difference in the way the impacted applications are impacted compared to those that are not? - Microsoft advised on the day, if the azure load balancer is serving traffic to more than 16 physical VM nodes, these are the applications that are impacted.  However we saw impact to applications that are on three nodes – Microsoft to provide the detailed RCA as in action 1.

5) Is there anything else we could have improved upon to mitigate the impact ourselves?	- The failure is seen at the application end and not at the load balancer side.  Cloud Platform team to look at the level of alerting that is available and how it can be improved upon to capture the 504 error.
Update 06/07: Ragu advises that capturing particular 504 errors is not possible but they can capture the failure rate and  investigations as to what can be recorded continue. ETA: 08/07

6) Applications teams to individually look at the level of alerting that is available and how it can be improved upon to capture this type of problem.	All impacted application owners (SDM/TDM)	

Update on 6/07:

Business service monitoring – end to end touchpoints – have all the portfolios got the relevant dashboards available to monitor their services overall?  Is anything else required?

What are the error codes that the application teams need to consider and what are the thresholds that need to be set?

Additional action added (06/07)
All teams to look at if their applications to see if they have retry logic and report back if they don’t – some do and this mitigated their impact.Vidhya to provide a list of applications on V2

",6/29/2022,6/29/2022,,,,No,No,No,No,3rd party issue,NA,"Microsoft confirmed that the root cause of the issue has been found to a physical networking device in their infrastructure not working as expected.

Further details to be confirmed.",`NA,NA,,4/4/2023,April,2023,Sravan,Closed,,,2022,6,June,9/16/2025,1175,>60
6/27/2022,6/27/2022,88712563,21:20,21:50,00:30,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Issues with Contactless transactions.,Worldline,NA,,Card Transactions,RC identified,61494,Store contactless transactions were declined between 21:20 and 21:50. Transactions over £150 would have been declined. A global network outage at Worldline had caused the issue.  This also impacted 93 .com online staff customer orders as they are currently routed for payments via Worldline.  We await further details on the fix and RCA from Worldline. ,"1) Freeze network changes impacting Axis platform until end of summer. And introduce dual change approval process for mandatory changes impacting Axis platform - Completed
2) Full reassessment of the way of applying changes inside the network team. All the production changes will be executed by two engineers. A senior engineer will be always involved to assume the 4 eyes principal role - Completed
3) Fine tune technical monitoring by adding a monitoring probe to detect dynamically abnormal BGP_CEASE_PREFIX_LIMIT_EXCEEDED alert on PE routers Completed
4) Review router configuration and capabilities for possible rejection of unknown prefixes -Currently not possible, we will continue to work with provider - closed
5) Full review of Customer e-Commerce (Ogone platform) incident communication to align with current processes for Customer in-store ( Axis platform)"" - Not applicable to in-store. For eCOM this is being covered in the project space",7/2/2022,7/2/2022,,,,Yes,Yes,NA,Yes,3rd party issue,NA,Worldline confirmed that during  a network change to add two new routes in their global platform. a configuration error impacted the processing of payment transactions.,`NA,NA,,7/29/2022,July,2022,Sravan ,Closed,,,2022,6,June,9/16/2025,1177,>60
6/26/2022,6/26/2022,88710574,07:20,9:50,02:30,Group Platforms,Enterprise Technology Platform,Group Platforms,Finance,,KI,SAP PO system inaccessible.,TCS,"Unix, VMWare, SAP PO",,SAP PO,RC identified,61552,"Alerting identified that the SAP PO system was unavailable from 07:20, after the VMware host movement activity performed in the morning . Message pileups were observed impacting the trailer movement to the DCs. The change was reverted and the SAP cluster was restarted to restore services at 09:50. The impacted trailers were successfully dispatched without any delay, therefore, no impact was incurred due to this issue.","Closed actions: 
1. Post implementation checks to be performed after every deployment from both Infra and app end: Sathish/Jafar/ Deepa: Completed
2. Clear communication among the teams involved in a change: Closed: All the SDMs, TDMs and SMEs have to be aware of the change being implemented and the impact if it induces
3. Hypercare process to be in place after every change: Completed: SDMs have reiterated the fact that the teams need to have hypercare monitoring after every deployment to ensure  no impact to services",6/26/2022,6/26/2022,,,,NA,NA,NA,Yes,Customer Tech Change,Yes,This was caused due to CRQ163034 - Legacy VMware Host movement from old VCenter v6.0 to newly build VCenter v6.7. ,163034,Insufficient testing,,7/8/2022,July,2022,Saloni ,Closed,,,2022,6,June,9/16/2025,1178,>60
6/23/2022,6/23/2022,88707710,09:38,10:50,01:12,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,Intermittent spikes were observed due to failed and rejected card payments at stores between 9:38 and 10:50 on 23/06:  ,Worldline,NA,,Card Transactions,RC unknown,61466,Worldline advised that the issue was with the acquirer WorldPay. We await further details on the fix and root cause from WorldPay. Minimal impact but customers would have had to possibly retry transactions. No stores reported issues.,Awaiting RCA from Worldline,,,,,,Yes,Yes,No,No,3rd party issue,NA,"No action was taken to resolve the spikes. The issue was caused by VPN timeouts to acquirer Worldpay and there was no issue at Amex and Customer Bank. The root cause of the timeouts has not been determined. Worldpay were engaged in investigation on 25th July, but were unable to analyse timeouts. There was also no recurrence on sample timeouts reviewed by WL/WP. ",NA,NA,NA,10/26/2022,October,2022,Sravan,Closed,,,2022,6,June,9/16/2025,1181,>60
6/22/2022,6/22/2022,88705709,08:00,13:40,05:40,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Reduction in Relex foods products ranged to stores ,TCS,SRD,,"Supplier orders, RDC allocations",RC identified,61554,"There was a reduction in the number of Relex food products ranged to stores on 22/06. Ordering and allocation was not carried out for 277 Relex products.  Product location files were not sent to FOA (Foods ordering and allocation) from SRD (Space, Range and Display) as they were archived inadvertently. To mitigate the impact, suppliers were advised to use the 28 day order plan and a contingency plan was put in place for RDC allocations.","1 Why was the manual extract run in production during business hours - Reason has been discussed and closed in the PIR call
2 Why only 2 out 3 files were archived?  Reason has been discussed and closed in the PIR call
3  SOP needs to be created with detailed steps – SOP documented
4 The archival process and full data extract are being tested now- No , archival functionality is introduced for a valid purpose. So this is not possible.
5 Exploring the option for configuring an alert? Will be taken cared under backlog id – FOATR – 9559/10227/10215
6 Exploring the option of validating the data in case of any discrepancies? Will be taken cared under backlog id – FOATR – 9559/10227/10215
7 Can any dependencies be set to the job so that the job should start processing only if it receives all the 3 files? Reason has been discussed and closed in the PIR call
8 The process of calling up the SD for raising Sev-1s/Sev-2s to be re-iterated to business – Done
9 Why there was delaying in raising the sev 1 log – Team has been advised to raise tickets with agreed SLA",6/22/2022,6/24/2022,,,,"No,Yes",No,Yes,NA,Human error - Customer Tech,No,"On daily basis, Product location files are sent from SRD to FOA (Relex). Two of these files were archived in SRD in error before they were consumed by FOA.  Due to this, these two archived files were not processed and hence ranges were the dropped in FOA.   

A full extract job was triggered in SRD DS manually to share the data to EDW for investigating another issue. While triggering the job team had missed to change the archival location from Relex to EDW in the job. Hence the product location files in Relex path were archived and therefore those files were not to FOA",NA,NA,NA,7/27/2022,July,2022,Kavitha,Closed,,,2022,6,June,9/16/2025,1182,>60
6/21/2022,6/21/2022,88703869/88702099,11:25,12:54,01:29,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Issues with VOD(Vision on Demand) impacting Scan and shop & Digital Café,TCS,Cloud Exponence Support,,Scan and Shop and Digital café,RC identified,61534,Multiple stores reported intermittent connectivity issues with Digital Café and Scan & shop in Store and mobile applications on 20th (9:30 and 14:48) and 21st June (11:25 and 12:54) .  Problematic PODs were restarted to fix the issue on 20th and a functional VOD change which was deployed over the weekend was reverted to restore services for the issue on 21st. The overall root cause is believed to be due to memory errors on nodes hosting the VOD Pods. Vendors Flooid and Microsoft have been engaged for further investigation.,"Cloud Exponence to work with Microsoft to analyze the behavior of the nodes memory utilization before and after the app change was reverted: Ashok/ Microsoft: Completed: Tech teams and Microsoft discussed and could not identify anything that relates to the change causing the MI.
Is there any relation with the VOD deployment that went on 19th June? Venky/FLooid: Completed: There were no memory spikes, hence tech is unable to identify the changes after the deployment.
App team to try and replicate the issue in lower environment and work with Flooid to understand the changes before and the change was reverted: Venky: Closed: Unable to replicate the issue.
Check with Flooid for any issues at app end: Completed: The patch was applied on 11/07 and the change was implemented. Teams monitored and no issues were observed.
Next time we have any type of major incident relating to pods, is there anything we can do differently on the day of the incident to capture all relevant logs and any type of evidence so we can investigate root cause better.: Teams will perform detailed checks and fine tune where needed as a part of peak activities under a JIRA ref: RSS 2239",22/06/2022,5/7/2022,,,,Yes,No,No,NA,Customer Tech Change,Yes,"The overall root cause is believed to be a memory issues after the VoD deployment. However, due to insufficient logs, it could not be proved",CRQ#162197,NA,,13/07/2022,13/07/2022,#VALUE!,Saloni ,Closed,,,2022,6,June,9/16/2025,1183,>60
6/19/2022,6/22/2022,88700847,16:00,6:00,62:00,Group Platforms,Finance,Group Platforms,Finance,,KI,Stock position overstated in GMOR and SAP by £2.8 billion,TCS,SAP,,"GMOR,MP,SSI, EAH and BEAM",RC identified,61452,"On 19/06, C&H commercial planning colleagues highlighted stock position were overstated by £2.8 billion in the SAP reports. SSI and MP reports will not have the correct data impacting the intake and planning activities. Colleagues will be unable to report any stock information for trading etc. This was caused due to incorrect postings by the Finance Inventory control team on 19/06 which was corrected on 20/06. This got processed in the overnight GMOR batch and the stock position came back to normal. The data has been corrected in MP and SSI. The data will be corrected in EAH after weekend's batch on 27/06 ","Colleagues have devised a plan to perform post checks by seniors after any postings. In addition, SAP will perform checks before sending it down to downstream systems.",19/06/2022,19/06/2022,,,,No,NO,No,Yes,Human error - Business,No,The user has entered an article number instead of quantity. This was corrected to fix the issue,NA,NA,NA,7/7/2022,July,2022,Saloni,Closed,,,2022,6,June,9/16/2025,1185,>60
6/14/2022,6/14/2022,88695094,12:04,13:25,01:21,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,A spike observed in failed/offline card transactions,TCS,Network,,Payments,RC identified,61451,"Monitoring indicated a spike in the number of failed & offline card transactions between 12:04 and 13:25. Two stores reported impact during the incident window.? Worldline was engaged who could not identify any faults.? The root cause is believed to be antivirus updates sent to all stores during business day causing network congestion, however detailed investigations continue.  ","Open Actions:

8) Regroup to understand our Dashboards and what each element depicts - How can we better resolve “it’s your network issue” debates – Laura Whittle - Analyse the requirements for any new dashboard so it looks at the overall picture. 29/07 – Few queries about Grafana dashboard is still pending with Ingenico and being tracked in weekly payments call. 


Closed Actions:
1) SCCM Updates to stores will not be scheduled for trading hours going forward. This is confirmed as of yesterday – Phill Gooriah Completed
2) When is the next SCCM/CB package distribution and what deployment approach will be followed – staggered or big bang? Phil Gooriah & NOC - Before any next update, Phil to analyse and talk to the NOC team (inc. Nick) for the best approach) - NOC team will be engaged for furture deployments and the deployment will also be in phased manner to avoid traffic spike

3)Secondary IP address has been added back to the current release and will complete by 24th June – 01/07 status - completed 50 stores.  ETA 6h July - Completed

4) Review and increase the 1GB link on the link between UK-POP and Vodafone – Paul Beasley - This is being reviewed and we are looking to change this one day w/c 27/6 (increase to 10Gb) – Completed
5) Review Network dashboarding that would have shown a network spike during the incident to help improve our ability to identify the cause quicker – Paul Beasley – Completed
6) Build a solution for the patch updates so that we don’t overwrite a higher version of file/update. Remove the human element of remembering – Bala & Phil - Come up with a logic by which only the latest patches can be deployed – Shyam confirmed that this is achieved by maintaining a release xml already, so nothing further is needed - Completed 

7) Ensure the current patch deployment contains the latest version of config – Shyam – CLOSED - The latest version has been added to the current till release ready for testing and deployed starting next week (following a successful central deployment tonight). After the first pilot stores receiving this will be ensure this is checked

9) How do we get to root cause more effectively? Renjith/Laura/Bala - Review the end to end transaction path from card reader to Ingenico and be able to determine where the drop/timeout is.  Understand the hops that the transactions make and understand what data is available to review that would show us where the cause is. Backlog RPOS-6769 is created to track this action

10) Do we have a better breakdown on failures – Splunk didn’t detect anything, should it have done – Laura/Phil - Looking at Splunk it does confirm the same spike seen in Grafana during the same time even though it is only in 5 stores. There was confusion on the call around what Splunk was showing us. We need to regroup as with the Grafana dashboard to ensure we really know what it is telling us.  Analyse what the thresholds should be set at. Completed as per action 8

11) Do we know the profile and impact across stores. – We can pull this from e-Portal

Complete the till release and confirm that the secondary IP address is deployed everywhere - 29/07: The activity was completed as of the night of 06/July. All tills in estate are having both IPs now.

12) Understand and document the logic for the timeout on primary and secondary IP for the PED. It feels like due to network utilisation at the time that we were on the cusp of the timeout in some stores - Manuvel/ Richard Wright Wells / Kamal/Phil Gooriah/Bala S - Backlog RPOS-6768 is created to track the issue.

Review other tools/ monitoring that is available to review during an incident that could help get to cause
""",6/21/2022,6/21/2022,,,,Yes,Yes,Yes,Yes,Configuration issue,NA,"Most likely a combination of factors:
•	SCEP release during the trading day
•	High volume of transactions happening through card readers as it was peak hours
•	There is a 1GB Bandwidth Cap restriction on the links between UK-POP and Vodafone Data Centre- this would have meant the download plus the volume of traffic would have utilised this 1GB cap and so requests were timing out. Such that we saw the primary Axiss IP address attempted, timing out and failing to offline authorisation. The tills that did have the secondary ip address available tried this and failed if still no response. ",NA,NA,NA,10/27/2022,October,2022,Sravan,Closed,,,2022,6,June,9/16/2025,1190,>60
6/11/2022,6/11/2022,88690749,04:44,6:00,01:16,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Stores network and Fridge alarm monitoring down.,TCS,Network,,Stores Network,RC identified,61516,"A pro-active alert at 00:00 indicated a network issue at Swindon data centre. As a result city, FM reported issues with monitoring store fridge alarms for 72 stores and issues with wireless connectivity affecting honeywells. The secondary network links hosted from Swindon were shut down at 06:00 for the impacted stores to restore services. The root cause was due to a hardware failure in one of the core switches on Swindon.",,6/11/2022,6/11/2022,,,,Yes,Yes,Yes,Yes,Infrastructure issue / Hardware failure,NA, The secondary network links hosted from Swindon were shut down at 06:00 for the impacted stores to restore services. The root cause was due to a hardware failure in one of the core switches on Swindon.,NA,NA, NKB-350,5/7/2022,May,2022,Rehan,Closed,,,2022,6,June,9/16/2025,1193,>60
6/9/2022,6/9/2022,88688507,09:50,11:15,01:25,Group Platforms,HR,Group Platforms,HR,,SI,Customer Colleagues unable to access People System.,SDWorx,NA,,Peoplesystem,RC identified,61515,People System was inaccessible to stores and support centre colleagues between 07:40 and 11:15.  Colleagues were unable to access payslips and book holidays. SD Worx identified that a Customer IP address was blocked at their hosting partner Navisite and it was unblocked to fix the issue. Awaiting detailed RCA from SD Worx.,"4) Review and increase the 1GB link on the link between UK-POP and Vodafone – Paul Beasley - This is being reviewed and we are looking to change this one day w/c 27/6 (increase to 10Gb) – 29/07: Fiber patch cables procurement is in-progress. Once procurement is done, Vodafone will be engaged to increase the 1 GB cap on the link. Waiting on PO number to be released by TCS to allow the cabling work to start.",8/10/2022,8/10/2022,,,,"No,Yes",No,No,NA,3rd party issue,NA,"• There is a 1GB Bandwidth Cap restriction on the links between UK-POP and Vodafone Data Centre- this would have meant the download plus the volume of traffic would have utilised this 1GB cap and so requests were timing out. Such that we saw the primary Axiss IP address attempted, timing out and failing to offline authorisation. The tills that did have the secondary ip address available tried this and failed if still no response. ",NA,NA,NA,8/16/2022,August,2022,Sravan,Closed,,,2022,6,June,9/16/2025,1195,>60
6/6/2022,6/7/2022, ,13:24,8:22,18:58,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Teams task down issue across stores.,Microsoft,NA,,Microsoft Teams Task,RC identified,,"On 06/06, Retail Communications reported that they were unable to send out weekly communications via Tasks.  Microsoft engaged and identified long running queries causing CPU spikes, the CPU was uplifted resolving the issue.","2) When is the next SCCM/CB package distribution and what deployment approach will be followed – staggered or big bang? Phil Gooriah & NOC - Before any next update, Phil to analyse and talk to the NOC team (inc. Nick) for the best approach). Update pending.",6/6/2022,6/6/2022,"No,Yes",NO,No,No,No,no ,Yes,3rd party issue,No,• SCEP release during the trading day,NA,NA,NA,6/6/2022,June,2022,Kavitha,Closed,,,2022,6,June,9/16/2025,1198,>60
6/3/2022,6/4/2022,88681754,15:21,7:00,15:39,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Allocation discrepancy in Foods Customer orders,TCS,ASO,,FCO,RC identified,61504,"An allocation mismatch was identified for foods customer orders affecting orders for 4 new lines that were introduced as a part of Jubilee weekend. As a workaround, GIST manually allocated the impacted orders in order to mitigate the impact. It has been confirmed that a bug in the current allocation algorithm has caused the issue, tech are working on a permanent fix. No impact to the usual FTO orders",,6/4/2022,6/7/2022,,,,"No,Yes",No,Yes,Yes,Code/Product bug,No,"Normally all FTO lines are received and allocated once a day. But on the day of incident (2nd June into Depot for into Store on 3rd June) there were multiple receipts and multiple allocations for a product to a same depot. 

The allocation logic was not considering multiple deliveries for the same UPC by the same supplier into RDC. It was expected that all FTO qty will be delivered into depo once by supplier and multiple deliveries were not considered. As the logic did not consider this, all subsequent allocations incorrectly ignored stock already allocated, unless the store requested qty has been met by prior allocation. Due to this some stores were under allocated, and some stores were over allocated. ",NA,NA,NA,7/11/2022,July,2022,Kavitha,Closed,,,2022,6,June,9/16/2025,1201,>60
6/2/2022,6/2/2022,88680041,05:33,6:15,00:42,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Customer stores were not receiving incoming calls,Mitel,NA,,Stores Telephony,RC identified,61407,"On 03/06, Customer stores were unable to receive incoming calls between 05:33 and 08:10. City FM reported the issue as they were unable to alert the stores for any refrigeration issues. Mitel rebooted the IVR 1 within their infrastructure to restore services at 08:10. Further investigations underway to understand the root cause of the issue.","8) Regroup to understand our Dashboards and what each element depicts - How can we better resolve “it’s your network issue” debates – Laura Whittle - Analyse the requirements for any new dashboard so it looks at the overall picture. 29/07 – Few queries about Grafana dashboard is still pending with Ingenico and being tracked in weekly payments call. Network issues are still happening in odd cases, rootcause for which is yet to be identified.",6/3/2022,6/22/2022,,,,No,No,Yes,Yes,3rd party issue,NA,"A security certificate issue at Twilio end caused a network outage causing the incoming service to go offline. The IVR1 server at Mitel end handled stores incoming calls prior to Twilio functionality. Since the Twilio solution suffered an outage, the Mitel IVR1 server had to be restarted to restore services.",NA,NA,,7/29/2022,July,2022,Saloni ,Closed,,,2022,6,June,9/16/2025,1202,>60
5/31/2022,5/31/2022,88678062,07:30,16:15,08:45,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Bradford C&H DC unable to access KISOFT WMS application,Knapp ,NA,,Picking and packing operations,RC identified,61416,"At 13:05 on 31/05, the DC reported the issue. KNAPP identified that their database configuration was corrupted. The DB config was restored and gateway nodes were restarted to fix the issue. Site operations resumed from 16:15. No major impact incurred.",,6/15/2022,6/15/2022,,,,"No,no",No,No,NA,3rd party issue,NA,A script ran inadverentely which corrupted the WMS gateway configuration.,NA,NA,NA,6/16/2022,June,2022,Sravan,Closed,,,2022,5,May,9/16/2025,1204,>60
5/27/2022,5/27/2022,88672295,10:07,11:30,01:23,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Contact Centre - Agents experiencing 50% silent calls,Sabio,NA,,Silent calls,RC identified,61319,"From 10:07, Contact Centre & Colleague Services reported intermittent silent calls impacting their ability to take calls from colleagues and customers. Sabio identified an issue with one of their calling layers PCIPAL (3rd party solution) which was fixed by implementing a change at their end by 11:30. ",9) How do we get to root cause more effectively? Renjith/Laura/Bala - Review the end to end transaction path from card reader to Ingenico and be able to determine where the drop/timeout is.  Understand the hops that the transactions make and understand what data is available to review that would show us where the cause is. 01/07 – Update outstanding and end to end review to be arranged. Laura to talk to Emma Holmes.,6/10/2022,6/10/2022,,,,"No,no",No,no,NA,3rd party issue,NA,"PCI Pal(3rd Party) received an excessive volume of inbound call attempts from one customer. The rate of call attempts peaked at 375 attempts per second and hundreds of thousands of calls beyond normal operational levels. Whilst limits were in place to guard against events of this nature, the limits worked on a concurrent calls basis, which prevented the customer in question from exceeding 14,500 concurrent calls. Whilst calls beyond this limit were rejected.",NA,NA,NA,6/20/2022,June,2022,Sravan,Closed,,,2022,5,May,9/16/2025,1208,>60
5/27/2022,5/27/2022,88673253,09:30,10:05,00:35,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Castle Donington reported errors on the WCS(Warehouse Control System) Ecom batches,SSI Scheafer,NA,,WCS ,RC unknown,61221,Castle Donington reported errors on the WCS (Warehouse Control System) Ecom batches. SSI Schaefer applied a patch (to ignore the errors) followed by a restart of the Hanging and Pick services to fix the issue. This issue induced a capability loss of 27k singles and Click and collect proposition was moved by 24 hours. No impact to CFR.,12) Understand and document the logic for the timeout on primary and secondary IP for the PED. It feels like due to network utilisation at the time that we were on the cusp of the timeout in some stores - Manuvel/ Richard Wright Wells / Kamal/Phil Gooriah/Bala S - The PED has a time out of 5 seconds. Understand whether this timeout is too short if we have a congested network.  Does it need changing?,5/27/2022,5/30/2022,,,,"No,no",No,no,NA,Human error - DC Operations,NA,RCA unknown – system is not showing how systematically order/load unit could have been missed up. We suspect this is due to manual intervention by flow room which could have impacted this and its impossible for systematically for the conveyor to go backwards.,NA,NA,NA,6/30/2022,June,2022,Sravan,Closed,,,2022,5,May,9/16/2025,1208,>60
5/23/2022,5/23/2022,88667053,15:46,16:02,00:16,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Contactless transactions declined at stores,Worldline,POS Support,,Contactless transactions,RC identified,61317,Store colleagues reported issues with contactless transactions being declined between 15:46 and 16:02. Transactions using chip and pin were successful throughout the duration of the incident.  Transactions over £150 along with Customer charge cards and budget cards would have been declined. A network change at Worldline had caused the issue impacting multiple systems including their acquirer hub. The change was reverted to fix the issue.  ," Worldline to perform a DR test and provide the result that everything is working as expected – DR test performed Oct 21, next scheduled Test Q4 2022

· Ensuring that Customer is notified before changes are made to Acquirer HUB - Worldline – Refer to Remediation Plan RP16, RP40, RP41, RP4",5/23/2022,5/30/2022,,,,"No,yes",No,No,No,3rd party change,NA,"The issue was caused by implementation of a network change (GLINFRA-11860) where an update of one of the route maps was required in preparation for a new external service to be added in the future. During this implementation, a logic error was introduced, which impacted some other rules in the route map list. As a result, the acquirers were not reachable from AHUB.  
 A procedural weakness in the change caused the error to be introduced. A subsequent issue was identified is the weakness in the current canary deployment approach used to validate routing map changes. Canary deployment is the practice of making staged releases. A software update rolled out to a small part of the users first to test it and provide feedback.
 Worldlines take way from this issue is that the deployment in the other datacentres should executed at least a day later if no impact is seen after monitoring the initial deployment on any one of the datacentres. This incident has also highlighted that the change category and restrictions for this kind of network change need to be revaluated.",NA,NA,NA,7/1/2022,July,2022,Rehan,Closed,,,2022,5,May,9/16/2025,1212,>60
5/22/2022,5/22/2022,88665854,18:30,19:51,01:21,GTS,Enterprise Technology Platform,Supply Chain,Foods Supply Chain,,KI,Bradford MQFTE transfer failures ,TCS,Linux,,WMS,RC unknown,61402,"Multiple MQFTE transfer jobs failed as the file system was in a hung state. A restart of the Linux cluster server was performed which refreshed the MQFTE connections and restored services by 19:51. As an impact, WMS application was unavailable between 19:34 and 19:51.",Understand and document the logic for the timeout – Completed,6/6/2022,6/6/2022,,,,Yes,Yes,Yes,No,RC unknown,NA,RCA Unknown - Redhat confirmed that they could not identify the reason for filesystem corruption,NA,NA,NA,6/6/2022,June,2022,Sravan,Closed,,,2022,5,May,9/16/2025,1213,>60
5/21/2022,5/21/2022,CRQ158544,,,,Group Support,Finance,Customer Channels,Platform & Store Ops ,,KI,Empty pallets sent to stores,TCS,SAP,,SAP,RC identified,,"After the SAP database outage on 21/05, a huge inflow of messages caused one of the idoc tables to go into ""locked"" status delaying the processing of some EDN messages into GIST. This issue resulted in stock gain of 350k in stores from 22/05 to 24/05 and was later adjusted by ICT team. Store stock position was not impacted due to this issue as CSSM processed the pallets via the backup route.",,5/21/2022,5/30/2022,,,,"No,no",No,no ,Yes,Code/Product bug,Yes," After SAP outage on 21st May, there were larger number of Idocs processed that resulted in table (EDIDC) lock, and deliveries present in the EDNs failed to create Inbound delivery due to locking issue and it was sent on subsequent days automatically based on the delivery date of the orders. The current job was designed  to pick the orders based on the current day and hence the locked messages were not reprocessed on the same day of failure.",CRQ158544,NA,NA,5/30/2022,May,2022,Kavitha,Closed,,,2022,5,May,9/16/2025,1214,>60
5/19/2022,5/19/2022,88662091,13:09,23:30,10:21,Customer Channels,Service Experience, C&H and Intl,C&H and Intl Supply Chain,,KI,AMT connectivity issues at West Thurrock DC ,TCS,RIT,,AMT,RC identified,61403,"On 19/05, the DC reported intermittent HHT connectivity issues and system slowness from 13:09. High bandwidth utilisation was observed after refreshing the windows server update service  (WSUS) as part of the ongoing Endpoint Protection for Legacy Devices. As a workaround, Microsoft update traffic was blocked and internet access was enabled to restore connectivity from 23:30. 
 ",Manuel and Richard discussing the timeout.,5/19/2022,5/20/2022,,,,"No,yes",Yes,Yes,Yes,Customer Tech change,Yes,This issue is caused after a WSUS (package management system) upgrade performed today as part of the ‘Endpoint Protection for Legacy Devices in Stores not being updated’ issue from May 13. ,CR#160960,inadequate testing,NA,5/20/2022,May,2022,Kavitha,Closed,,,2022,5,May,9/16/2025,1216,>60
5/19/2022,5/19/2022,88661176,08:03,8:17,00:14,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Contactless transactions declined at stores,Worldline,POS Support,,Contactless transactions,RC identified,61205,"Store colleagues reported issues with contactless transactions, however, the offline transactions were working as expected. Worldline was engaged who identified a spike in the number of offline transactions between 08:03 and 08:17, which is being investigated. Due to this issue, Contactless transactions were declined. In addition, and any transactions over 150£  along with Customer charge cards and budget cards would have been declined. Detailed root cause investigations underway.",All actions are now being tracked under the Customer Remediation plan and can be closed off this tracker.,5/19/2022,5/26/2022,,,,"No,yes",NA,NA,NA,3rd party issue,NA,"The incident was triggered by the DCI application (settlement generation) in (version 1.2.0-6) not functioning as expected. During the processing of payment transactions this had generated an abnormal increase of “exclusive lock”. In parallel these exclusive locks increased the number of “Not granted locks” causing the block in database until the manual command to clear the block was issued.  
 ",NA,NA,NA,9/28/2022,September,2022,Rehan,Closed,,,2022,5,May,9/16/2025,1216,>60
5/17/2022,5/17/2022,88658429,07:00,13:20,06:20,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI, ISF picking & packing issues,TCS,BOSS Support,,ISF picking and packing,RC identified,61084,"After migrating the picking service for ISF (In Store Fulfilment) from AWS to Azure V2, store colleagues reported various issues such as being unable to pick items via Honeywell devices, unable to nil pick items and orders were not moving from pick to pack screen in SPPD from 07:00. Picking services were moved back to AWS to restore the services by 12:45. By 13:20, all the orders from picking were moved to packing screen within SPPD successfully",Closed Actions:,5/17/2022,5/17/2022,,,,No,No,No,Yes,Customer Tech change,CR,"After the SPPD V2 Migration Phase 2 Go Live, Picking and Packing of SPPD application is managed by two different teams. Due to lack of proper testing the oder which were  picking in the honneywell app was not packed in SPPD side.",CM7931,,,27/05/2022,27/05/2022,#VALUE!,Saloni,Closed,,,2022,5,May,9/16/2025,1218,>60
5/14/2022,5/15/2022,88654591,13:30,1:00,11:30,Digital & Data,Data,Digital & Data,Digital & Data,,KI,Power BI reports using EAH data source are failing intermittently,TCS,EAH ,,Power Bi,RC identified,61094,"On 13/05, tech identified an issue with the Power BI Impala connectivity. A critical case has been raised with Microsoft and Cloudera to investigate the root cause. As a workaround, team had pointed the EAH connection to use the service/generic user account instead of the local account which allowed the reports to get refreshed as expected from 01:00, 15/05",3)Secondary IP address has been added back to the current release and will complete by 24th June – 01/07 status - completed 50 stores.  ETA 6h July - Completed,5/29/2022,5/29/2022,,,,Yes,Yes,Yes,,Customer Tech issue,NA,"The SocketTimeout is the number of seconds that the TCP socket waits for a response from the server before timing out the request. In this case, the socket timeout was not set-up which resulted the issue. As recommended by Microsoft, socket timeout was set to Zero which fixed the issue.",Configuration issue,NA,NA,6/20/2022,June,2022,Sravan,Closed,,,2022,5,May,9/16/2025,1221,>60
5/12/2022,5/12/2022,88652087,01:20,9:53,08:33,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Incorrect allocations observed in Foods critical batches,TCS,"CSSM, Datastage and Quantum DevOps",,"FFO, Foods Bradford and MK NDC allocation",RC unknown,61183,"Alerting indicated a NDC variance threshold breach for Bradford and Milton Keynes foods DC’s causing overallocation of orders. In addition, incorrect Foods Final orders were sent to  suppliers. Foods Suppliers were advised to use the 28-day order plan.  As the NDC orders were overallocated, backup allocation was applied which significantly delayed the picking operations at Bradford and Milton Keynes DC.

Tech identified that there were 2 EOD (End of Day) inventory files from CSSM for 10/05 and 11/05. The issue was caused by the 10th May EOD file not being systemically removed from the source (CSSM) which resulted in datastage picking up the incorrect file of 10/05, resulting to the issue.  A detailed PIR will be held",1) SCCM Updates to stores will not be scheduled for trading hours going forward. This is confirmed as of yesterday – Phill Gooriah Completed,12/5/2022,16/05/2022,,,,Yes,Yes,Yes,Yes,RC unknown,NA,"Due to an issue with the CSSM file archival process, two EOD inventory files of 10/05 and 11/05 were sent to DS which picked up the previous day’s file instead of 11th May and processed into Quantum. The issue occurred as Quantum used the older inventory file which was not having the latest data. ",NA,NA,NA,7/7/2022,July,2022,Saloni,Closed,,,2022,5,May,9/16/2025,1223,>60
5/10/2022,6/1/2022,88648696,10:22,16:00,533:38,Group Platforms,HR,Group Platforms,HR,,SI,Incorrect tax codes in the May Payroll resulting in potential over and underpayments,SDWorx,Payroll Operations Support,,Payroll,RC identified,61095,It was found that a number of colleagues had incorrect tax codes in the May Payroll resulting in potential over and underpayments. SD Worx provided details of impacted colleagues and those with an underpayment of £50 or more had a same day bank transfer to mitigate. 1609 colleagues have overpaid (1224 over £50) and 922 have underpaid. Impacted colleagues are being communicated to by Colleague Services whilst SD Worx investigate why HMRC have duplicate colleague records. Currently the teams are working on ensuring the June payroll is correct. Root cause is being investigated.,,8/6/2022,15/06/2022,,,,No,No,Yes,No,3rd party issue,NA,"On 29/04, Customer sent a HMRC file with higher number of records than usual containing an updated tax code which  got overridden with the existing tax codes of the employees. This activity should be carried out only for new starters having an emergency tax code. However, when the correct file was sent by Customer, the Payroll operations team missed it and loaded the incorrect one causing the issue.",NA,NA,,13/07/2022,13/07/2022,#VALUE!,Saloni ,Closed,,,2022,5,May,9/16/2025,1225,>60
5/6/2022,5/6/2022,88643533,05:00,9:44,04:44,GTS,Enterprise Technology Platform,Customer Channels,Service Experience,,MI,Sterling OMS Application Unavailable,Micorosft,Sterling Support,,Sterling OMS,RC unknown,61060,"After the planned Sterling AKS Cluster upgrade, the Customer.com order management platform was unavailable from 05:00.
Orders were taken in offline mode. Customer order history, parcel scanning in-store and contact centre access to customer order details were impacted. Tech teams identified a configuration change which is believed to have caused the issue. This configuration was amended to restore services at 09:44. ",5) Review Network dashboarding that would have shown a network spike during the incident to help improve our ability to identify the cause quicker – Paul Beasley – Completed,5/10/2022,5/10/2022,,,,Yes,Yes,NA,Yes,3rd party change,Yes,"After the AKS cluster upgrade, the IP address for the load balancer (ingress controller) had changed. Microsoft confirmed their was a request sitting on the load balancer to change the IP address. During the upgrade the request was picked up and applied. Unable to confirm when this was request was exactly raised, the data retention period here is 90 days and this request was their for the complete 90 days. ",CM-7880,NA,NA,6/28/2022,June,2022,Kavitha ,Closed,,,2022,5,May,9/16/2025,1229,>60
5/5/2022,5/5/2022,88641771,05:20,6:00,00:40,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Store colleagues unable to perform price reduction using DEF,Microsoft,NA,,DEF,RC identified,61187,"On 05/05, tech confirmed that a SQL database issue at Microsoft had caused the issue. To mitigate the impact, the tech teams created a new database using the latest backup to resolve the issue at 06:00. Stores confirmed service restoration.","6) Build a solution for the patch updates so that we don’t overwrite a higher version of file/update. Remove the human element of remembering – Bala & Phil - Come up with a logic by which only the latest patches can be deployed – Shyam confirmed that this is achieved by maintaining a release xml already, so nothing further is needed - Completed",5/5/2022,5/10/2022,,,,Yes,No,Yes,No,3rd party issue,,The  Microsoft SQL authentication account encountered an error while accessing the database as there was mismatch in the account id.,,,,5/10/2022,May,2022,Saloni,Closed,,,2022,5,May,9/16/2025,1230,>60
5/4/2022,5/4/2022,88641810,16:05,18:26,02:21,Group Platforms,Finance,Group Platforms,Finance,,KI,Stock-Hub HANA System unavailable,TCS,Linux Support,,D&F,RC identified,,"On 04/05, the Stock Hub (SAP HANA) primary database node system went down due to a hardware failure. To mitigate the impact, tech teams failed over the services to secondary node which made the system available from 18:26. Vendor Bluechip replaced the faulty hardware to bring up the primary node at 23:17. The services were failed back on 05/05 and remained stable.",No further actions agreed ,5/4/2022,5/4/2022,Yes,Yes,Yes,Yes,Yes,No,NA,3rd party issue,No,Vendor Bluechip replaced the faulty hardware to bring up the primary node at 23:17. ,NA,NA,NA,5/4/2022,May,2022,Kavitha ,Closed,,,2022,5,May,9/16/2025,1231,>60
4/28/2022,4/28/2022,88633663,12:15,14:09,01:54,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Till card transactions going through slowly,Worldline,NA,,Store tills,RC identified,,"From 12:15, stores reported that till card transactions were going through much slower than normal.  Worldline advised that they restarted their AXIS servers to fix the issue by 14:10. 283K transactions were impacted by latency during the period.  A detailed PIR will be arranged with worldline.","1.	Start an analysis on improvements to prevent slow processing of messages in the system as a whole – also ensure that all timeouts on the Worldline side are appropriately set so that failed transactions are “moved aside” in a timely manner to allow other transactions to flow.  This actions deals with the root cause and is being addressed systemically by Worldline – RISK – additional monitoring will help us catch the issue but not prevent it but then issue can happen with other acquirers.
In progress remediation Plan –  review Target date 29/07/2022 - closed off our tracker
2.	Ascertain exactly how and if the Amex change caused the slowdown in authorisations– as the timings are inconsistent. Were there any elements of the testing plan from Amex and Worldline side thorough or were there any gaps? Answer pending – 4th June 
In progress remediation Plan – review Target date 29/07/2022 Closed off our tracker

7) Ensure the current patch deployment contains the latest version of config – Shyam – CLOSED - The latest version has been added to the current till release ready for testing and deployed starting next week (following a successful central deployment tonight). After the first pilot stores receiving this will be ensure this is checked",5/6/2022,5/9/2022,,,,Yes,No,Ni,Yes,3rd party issue,NA,"The incident was caused by a capacity issue on the AMEX_APAC authorisation link, the limit of authorised sessions had been reached. A change on the Amex side, caused Amex authorisations going to Amex hosts to pile up on servers for a long time which led to slower processing times for all other providers.  ",NA,NA,NA,9/27/2022,September,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1237,>60
4/24/2022,4/25/2022,88627147,07:00,13:03,30:03,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Delay in the transfer of totes via the transfer belts at Bradford C&H DC,Knapp ,NA,,Transfer of totes,RC identified,,"From 7am on 24/04, Bradford C&H DC users have been experiencing delays in transferring totes via the transfer belts. This issue is causing a congestion throughout the picking automation area. Vendor Knapp have been engaged and are currently investigating. To mitigate the impact, there has been a reduced cap on the retail picking volume from 19/04.
Impact: 7 shipments consisting of 57.1k singles have been cancelled. Capability loss of 99k singles.",,4/24/2022,4/25/2022,,,,"No, configured",No,Yes,Yes,3rd party issue,,"When the deployment of the  Knapp – Secondary SRC server fixes & Full Fail over test occurred on 23/04 under CR159639 , there was an Oracle exception error, which kept on writing the error every 15ms and went up to 30 mb of text data. This failed to process the actual messages, causing message pileup on the database resulting in the delay of GU transfer belts.",,,,4/26/2022,April,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1241,>60
4/24/2022,4/24/2022,88627927,12:08,13:54,01:46,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Carrier label printing failures at Castle Donington and Thorncliffe,Sorted/ TCS,Mule/ Middleware/Sorted,,Label printing,RC identified,61123,"On 24/04, at 12:08, Donington and Thorncliffe team reported issues while printing carrier labels for the orders that process through ""Sorted"". Tech team found the API calls between Carrier Gateway and Mule were failing causing the issue. From 12:38, the services restored however this was not communicated to operations, therefore the operations resumed at 13:54."," Sorted support and SRE teams have set up a new monitor / alert around the specific issue seen on Sunday 24th April. The monitor alerts where the ""Allocate Shipment with Service Group Response Time"" average is 1500ms over 5 minutes. We saw this monitor trigger today, as a result of the unrelated issue (37531). The Sorted support team were successfully paged via Opsgenie.",4/24/2022,4/24/2022,,,,"No,Yes",No,No,Yes,Customer Tech issue,NA,"Fault with the Azure Service Bus managed service.From  logs we could see that during this time, OS updates were performed at our end and when this happens some containers move and servers reboot",NA,NA,NA,7/19/2022,July,2022,Rehan ,Closed,,,2022,4,April,9/16/2025,1241,>60
4/24/2022,4/24/2022,88626161,08:00,10:10,02:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Web Chat Unavailable,Sabio,NA,,Webchat,RC identified,,"On 22/04, Contact Center colleagues reported that the Webchat service was unavailable across all the Customer website journeys from 14:55. Sabio was engaged and advised that the issue was caused due to a failure on one of the their hosts responsible for offering chats. Awaiting detailed RCA from Sabio.",,4/24/2022,4/27/2022,,,,No,No,No,No,3rd party issue,,A backend server application at Sabio had an issue and failed over resulting in the proxy losing connection which caused high memory utlization errors on one of the hosts resposible for offering chats.,,,,5/5/2022,May,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1241,>60
4/22/2022,4/29/2022,88625502,10:13,16:15:00,7 days 6 hours 2 mins,Group Platforms,HR,Group Platforms,HR,,SI,40 colleagues not paid during the weekly payroll,SD Worx,NA,,Payroll,RC identified,61194,Colleague Services raised that 40 colleagues were not paid during the weekly payroll. SD Worx investigated and found 39 of the 40 colleagues required a pay adjustment and will receive a 68% pay advance this week. Root cause is currently unknown and is being investigated by SD Worx. ,Complete the till release and confirm that the secondary IP address is deployed everywhere - 29/07: The activity was completed as of the night of 06/July. All tills in estate are having both IPs now.,2/22/2022,2/22/2022,,,,No,No,No,Yes,3rd party issue,,"This was caused due to a process error where Impacted colleagues were transferred to monthly pay group, but already had a fixed term contract end date when they were on weekly pay group.",,,,7/29/2022,July,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1243,>60
4/21/2022,4/22/2022,88624734,17:30,Severity downgraded,NA,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,No orders and allocations generated for 43 Relex products,Relex,NA,,store allocation and supplier orders,RC identified,61122,"On 21/04, an issue was raised regarding supplier orders and store allocations not generated for 43 products served by Relex via Clonshaugh depot from 19/04. A missing transformation logic to process Clonshaugh depot related stock to the Relex application had caused the issue. A fix was deployed in the integration layer to allow the file transfer to Relex.  Since only 15 out of 43 products across 30 stores were impacted, a decision was taken to manage the issue within FOA programme.","11) Do we know the profile and impact across stores. – We can pull this from e-Portal CLOSED""",4/21/2022,4/21/2022,,,,No,No,No,NA,3rd party issue,,"The issue was caused due to a design and requirement miss from Accenture/Relex - As Clonshaugh was out of scope of the Pilot, so it was never configured or included in the scope. ",,,,4/26/2022,April,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1244,>60
4/21/2022,4/21/2022,88624000,13:34,14:40,01:06,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Store tills requesting for Authorisation code for card transaction,Worldline,NA,,Store tills,RC identified,61020,Multiple stores reported that store tills were requesting the authorisation code for card transactions from 13:34. The OLA code was provided to stores to trade offline to mitigate the impact. Worldline advised that there was an issue with their HSM (hardware security module) and a change was deployed to fix the issue after which online card transactions were stable from 14:40. Around 17.5k transactions were processed offline with an OLA code due to this issue. Awaiting detailed RCA.,"1.	Reset counters on Payshield HSM appliances (12 devices) in DC1 and DC2 JIRA ticket ref: SHASE-5186: Completed on 22/04

2.	Replay the impacted settlements 108k: Completed on 29/04

3.	Intensive monitoring of: - high CPU utilization on Payshield HSMs - increase of E2E errors on AXIS or SecServ level (before the end of migration to Payshield 10K): due on 02/05 – Completed

4.	Migrate the traffic from the legacy PayShield 9K to Payshield 10K in AIS environment.: COMPLETE 21/6

5.	Adhoc action, internal to Worldine: Analyse and qualify the settlement defects.: Due on 06/05 – complete – id’d what was stopping the transactions and the issue has been mitigated. – work around not a risk – complete

6.	R&D to address the escalated defects and provide fixes (MAP core/plugins) for deployment in production. Provide the schedule with fixed versions.: Due on 20/05 – pulling together all potential/needed dev work that supports defects/issues – will be covered by SO’s under a SIP with Nick/Gary – all COMPLETE 21/6",4/21/2022,4/28/2022,,,,No,No,Yes,Yes,3rd party issue,NA,"Worldline’s Payshield HSMs (hardware security module) transactions record capacity reached its threshold as it is running on 32Bit OS configuration which limits the number of transactions possible to record on a single counter. They have a known workaround to reset the counters on the HSMs twice a year. Due to the Covid situation, this was last done in the beginning of 2021 causing the HSM failure.",NA,NA,NA,6/21/2022,June,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1244,>60
4/19/2022,TBC,TBC,TBC,TBC,TBC,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Some store colleagues can access Teams from the previous user intermittently,Microsoft,NA,,Powerapps,RC identified,,Microsoft believe the issue is a feature within PowerApps but not a bug.  Further meetings will continue today and the service owner will ensure traction and progress ,,,,,,,"No, NA",No,NA,NA,3rd party issue,NA,Microsoft believe the issue is a feature within PowerApps but not a bug.  Further meetings will continue today and the service owner will ensure traction and progress ,NA,NA,NA,4/19/2022,April,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1246,>60
4/19/2022,4/19/2022,88620982,12:30,14:45,02:15,Commercial Trading,Foods Commercial Trading,Commercial Trading,Foods Commercial Trading ,,KI,OFP Foods users are unable to perform PO Amendment and Approvals,SAP ,NA,,One Customer Foods Platform,RC identified,59736,"On 19/04,  at 11:30, alerts were received highlighting a connectivity issue between OFP (One Customer Foods Platform) and SAP CPI (Cloud platform layer). From 12:30 the OFP users experienced issues when amending/approving the purchase orders. The SAP trust centre website confirmed that a disruption in their Netherland region impacted the connectivity which was restored at 14:45.",,4/19/2022,5/5/2022,,,,Yes,Yes,No,TBC,3rd party issue,,"A connectivity issue between SAP and OFP due to high amount of traffic on the landscape had caused the issue.
",,,,5/5/2022,May,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1246,>60
4/14/2022,4/19/2022,88610158,11:30,15:15,120:45,GTS,Enterprise Integration,Customer Channels,Platform & Store Ops ,,SI,Some price changes not flowing ES to ECS(Food Ticketing),TCS,Enterprise Services,,ECS,RC unknown,61016,"On 08/04, an issue was raised regarding price updates reaching the ECS application. Tech teams found that some price updates were not flowing into ECS due to a possible issue with the Mongo layer of ES. On 11/04, 468 unique foods product shelf price changes (across various Store Group combinations) were intermittently not reflecting in ECS from 23rd March. Therefore, for a period of time, the shelf pricing may have been inaccurate.  All backlogged price changes have been cleared on 14/04.   The tech teams are continuing to test the bypassing of Mongo watcher  and will aim to deploy the changes on 19/04. Meanwhile the teams will continue to perform the workaround of running the script to process the price updates from ES to ECS twice every day, until the change is deployed. The tech teams are working with vendor Mongo to identify the root cause of this issue.","Open actions:
Follow up with Mongo to ascertain the results of their investigations.: Rosmi/ Hisham/ ES teams: 
<<Tech liaised with Mongo and confirmed there are no mechanisms in place to compare data between two Kafka topics for unique combinations.  Finally came across few scenarios where events went missing.  Mongo recommended to have a further deep dive on this and has asked for a meeting along with team. In addition, the cluster storage size was increased to increase the provisioned IOPS to 2300IOPS on 27/04. However, team is working on options to compare topics. Team is doing analysis to agree on the right oplog size to better accommodate the workload and to provide a bigger replication window.>>

Closed actions: 
Walk through the general incident process to include: how to raise an issue, escalate an issue, communications etc.: Joey and Rosie - Completed
ECS Ownership gap to be discussed and address – formalise the ownership and document: Vijay, Mark Raper, Rosie: Completed
When stores report issues in ECS using their store-based app, how do these issues get reported into Customer Tech in a timely manner at the correct priority? Rosie, Jon Philbey: Completed
Is it possible to implement a systemic reconciliation between SAP and ECS to ensure required price changes have been delivered: Vel & Chris Thomas: Completed",14/04/2022,NA,,,,"No,yes",No ,Yes,No,RC unknown,NA,Possibly related to an issue with the ES Mongo watcher. The root cause is being investigated by vendors Mongo and our ES tech teams.,NA,NA,NA,25/09/2022,25/09/2022,#VALUE!,Saloni,Closed,,,2022,4,April,9/16/2025,1251,>60
4/13/2022,4/13/2022,88613222 ,06:23,10:36,04:13,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Stores reported that they were unable to connect to store admin printers from their workstations,TCS,POS,,Admin printer on Workstations,RC identified,61125,"From the morning of 13/04, stores reported that they were unable to see their admin printer from the store workstation. On 12/04 an ongoing change was deployed for 500 store servers to address spacing issues. Tech teams identified that 228 out of the 500 store servers were impacted by the issue after the change and therefore restarted the print spooler services and rebooted some store servers to restore services for the stores.  The ongoing change has been cancelled until we have a full understanding on the root cause of the issue.","Team have restarted the print spooler service for the impacted stores.: Completed
Check the server status and bring back online.: Completed
A manual cold boot will be performed for the affected physical stores.: Completed
A script implemented to identify if there are any further affected stores having the issue.: Completed
A script implemented to restart the service in multiple stores: Completed
Need to ensure post check on store servers whenever the change required for reboot on the server.: Completed for one server case, we have Ping view tools to check the server online status",4/13/2022,4/13/2022,,,,"No, Yes",No ,No ,NA,Human error - Customer Tech,NA,"After the change, the post reboot health check of the servers were not performed in error which caused only 38 servers to go offline. ",NA,NA,,19/05/2022,19/05/2022,#VALUE!,Saloni,Closed,,,2022,4,April,9/16/2025,1252,>60
4/12/2022,TBC,TBC,09:30,TBC,TBC,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,Six piloted stores running DN 1.1 software with NCR hardware on card only SCOT tills reporting latency ,DN,NA,,SCOT tills,RC identified,NA,"There are seven different issues that either happen in all stores, some or even one store.  These issues collectively result in longer transaction times in store.  A comprehensive action plan was created to  ensure that the above issues are confirmed, quantified and resolved in a timely manner.  The action plan has now been handed back to the project team by MIM to progress.
 ",NA,NA,NA,,,,NA,NA,NA,NA,3rd party issue,NA,An action plan has been devised and agreed to be managed by the project team,NA,NA,NA,4/22/2022,April,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1253,>60
4/12/2022,4/12/2022,88613093,17:20,18:45,01:25,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,"Carrier labels issue affecting Donington, Bradford and Thorncliffe ",Sorted,NA,, CD label printing ,RC identified,61116,"On 12/04, Donington packing team reported issues while printing carrier labels from 17:20. Tech team removed Hermes from the service groups at 18:30 based on previous experience. Site were operational with the other remaining carriers excluding Hermes Evri from 18:45. Vendor Sorted advised that they had carried out a database restore and that had caused the issue.  At 19:45, Tech re-added Hermes Evri and restored full service.  An initial PIR has been held where Customer Tech have raised concerns, a follow up meeting is being held on Tuesday 19th April. ","•	End to end review of the Database Restore process and CAB process – COMPLETED
•	Is the Sorted production Hermes DB sizing actually correct on the Sorted end so the data base does not have to rely on the elastic pool? Reviewed and corrected.  COMPLETED
•	Azure SQL Elastic Pool Size monitoring and alerting to be reviewed - COMPLETED
•	Update Watchdog alert to route to Opsgenie – COMPLETED
•	If Hermes (or any carrier) fails, then can we trigger an alert and reallocated to the next best option in the service group to keep operations running – carry out an overall review and advise Customer what is possible. Martin said no to this after the PIR for the issue which occurred on 04/04 - COMPLETED",4/12/2022,4/19/2022,,,,"No, Yes",No ,No ,NA,3rd party issue,NA,"A manual database restore was undertaken on Sorted’s production environment whilst investigating an unrelated issue. The change was not carried out via a formal CAB approval – this was human error. This manual restore placed additional load on the database, subsequently the elastic pool reached its storage limit and label failures were seen. A watchdog alert was triggered at 18:08 however, at the time it was only in a testing phase for this service and as a result did not trigger an Opsgenie page to the Sorted 24/7 Support team.",NA,NA,NA,7/19/2022,July,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1253,>60
4/10/2022,4/10/2022,88610764,05:00,17:00,12:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,3358 .com customer orders cancelled ,TCS,Sterling,,Castle Donington orders,RC identified,60777,"On 10/04,  Donington highlighted that a number of allocation failures within WMS were higher than usual. Tech teams identified that the failures were due to stock mismatch between Sterling and WMS. The stock positions were incorrect between 05:00 and 17:00 on 10/04 resulting in increased CFR and 2164 customer orders placed against inaccurate stock positions. To fix the issue, support triggered a new start of the day (SOD) inventory file on 10/04.  The mismatch was caused by the Inventory Adjustment job (IAA) for 09/04 being held to fix a known issue in Sterling inventory processing and the job not being released later in error. This led to Saturday’s (09/04) EDC IAA backlog being processed on Sunday (10/04) and thus corrupting the inventory picture. Residual orders completed processing by 13/04.","Sterling Support to  alerts for the following: 
To check every hour if job is on hold - Complete 
Alert when EDC have more than 100 allocation failures- Complete
Liaise with product fix the issue with automatic hold and release of IAA job to avoid manual intervention- Project team has cofirmed that this functionality is going to moved out of OMS to central stock hub services before peak hence no further actions pending- Complete",4/11/2022,4/11/2022,,,,No. Yes,No ,Yes,NA ,Human error - Customer Tech,No,The mismatch was caused by the Inventory Adjustment job (IAA) for 09/04 being held to fix a known issue in Sterling inventory processing and the job not being released later in error. ,NA,NA,NA,4/25/2022,April,2022,Kavitha,Closed,,,2022,4,April,9/16/2025,1255,>60
4/8/2022,4/8/2022,88607426,13:40,15:04,01:24,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,Stoke DC network down,TCS,Network,,Stoke DC network,RC unknown,60685,"On Friday 8th April, Stoke DC colleagues advised that had lost network connectivity affecting telephony and handheld terminals between 13:40 and 15:50.  The secondary core switch was powered down which allowed the primary core switch to recover.  Network teams found an issue with a particular access switch where a possible loop was present on a port.  The port was disabled mitigating the issue and the secondary core switch was powered back up.  ",There is a plan to replace all the devices as a part of LAN refresh project which is being tracked under  Risk ID-1091,4/8/2022,4/10/2022,,,,"Yes,No",Yes,NA,NA,RC unknown,NA,Multiple flaps on one of the switch caused the loop. Tech believes the issue was caused as the problematic switch is running on old IOS software and bugs. Further analysis is not possible due to insufficient logs.,NA,NA,1091,4/16/2022,April,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1257,>60
4/7/2022,4/8/2022,88606684,13:00,20:38,31:38,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,25 WHSmith stores blocked from counting as no sales were flowing into CSSM,WHsmith, Integration services,,CSSM,RC identified,60686,"On Friday 8th April Tech teams identified through proactive reconciliations, that the actual WHSmith sales were not matching what CSSM was reflecting and therefore 25 stores were blocked from counting to prevent stock level corruption.  A pileup of messages was found in the middleware layer and retriggered.  The 25 stores were unblocked once all of the messages were processed into CSSM.",BSP to ensure that Tech received proper outage notification from Whsmith hereafter.: Completed,4/8/2022,5/3/2022,,,,"Yes,No",Yes,NA,NA ,3rd party issue,NA,"An outage at BT end on 07/04 caused the stores to go offline and hence no sales were polled for those sites. In addition, the batch scheduler at WHsmith was halted during the same timeframe so even if there were any sales sent, they did not reach Customer. ",NA,NA,NA,5/5/2022,May,2022,Saloni,Closed,,,2022,4,April,9/16/2025,1258,>60
4/7/2022,4/8/2022,88605517,10:30,6:12,19:42,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,249 ambient products not present in SCRD,TCS,Food OAR,,Food OAR,RC identified,60782,"On 7th April, 249 ambient products were not ordered/allocated as they were inadvertently deleted from SCRD in error during BAU activities the previous day. For these products, a day’s deliveries were missed as they had failed allocation and for bulk orders into RDC’s, there was potentially a two day miss of delivery due to lead times.  The deleted products were restored and processed into Quantum.","SOP to be created and peer reviewed - complete on 02/05/2022
Number of products that are being deleted as part of this clean up acitivity needs to be reduced to 10 - Complete",4/8/2022,4/8/2022,,,,"No, Yes",No ,Yes,NA ,Human error - Customer Tech,No,"It has been found that during an ongoing work order to delete SR6 (SR6 - product linking to the store) for 249 ambient products, the actual products had been inadvertently deleted from SCRD. ",NA,NA,NA,5/2/2022,May,2022,Kavitha,Closed,,,2022,4,April,9/16/2025,1258,>60
4/4/2022,4/4/2022,88599352,01:24,5:00,03:36,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Issues with carrier label printing in Castle Donington and Bradford C&H DCs,Sorted,NA,,Label printing,RC unknown,60746,Castle Donington and Bradford C&H DCs reported issues while printing labels for direct orders from 01:24. Vendor Sorted identified an issue with the Hermes services. The tech teams then removed the Hermes services from the carrier group to restore services at 05:00. Impact to Bradford is minimal and impact to the packing operations at Donington is currently being assessed. We are working with Sorted to understand the root cause and agree on how and when to reimplement Hermes as a carrier.," Open actions:
3.	If Hermes Evri could not allocate then could the system choose the next option allowing us to continue to operate. (i.e., DPD or RM)?  - The process flow is to get a quote, which uses the carrier service groups, and to then allocate to the desired service.  By the time the allocate call is being made the option of using another carrier in the service group has passed, and it is in the allocate stage that the failures occurred.  Consequently at this time it is not possible to have the allocate function use another service in the service group if allocation fails. CLOSED

1.	What is the root cause and if cause is not determined then please share a plan of action?  - Tim Cox - Completed
2.	To prevent this issue happening again, a check is being implemented to ensure that the production table has content or it will revert back to the previous version of the table. - Tim Cox - Completed

The process flow is to get a quote, which uses the carrier service groups, and to then allocate to the desired service.  By the time the allocate call is being made the option of using another carrier in the service group has passed, and it is in the allocate stage that the failures occurred.
Consequently at this time it is not possible to have the allocate function use another service in the service group if allocation fails. Closed
4.	We removed Royal Mail from the service group to see if it made any difference.  Can we generate a script or even check the dashboard which pinpoints whether it’s all services not working or only one? - Tim Cox – ongoing – RISK until implemented.
At the moment it is not possible to build a dashboard and alerting that would show where a specific carrier is failing in allocation either through drop in allocated volume or from the logs. Closed
5.	Does Customer want Sorted to implement a process where a carrier is automatically removed systemically? Jason/Colin/Martin – definitely NO COMPLETED
6.	How can Sorted tell Customer about issues within Sorted proactively? Gemma Hanson - Completed
7.	It seemed to take a while to re-instate the Hermes Evri service and understand the problem with the routing table.  Do we have the appropriate resources overnight to support? - Sorted have training in place. COMPLETED",4/6/2022,4/19/2022,,,,"No, Yes",No,Yes,No,3rd party issue,NA,"Sorted engineering team have looked at all aspects of the journey/logs etc and cannot find why the table was written with no content.  The temp table had data, but the live table didn’t.  Sorted have advised that they could not find any logs to identify the cause of the issue – and cannot investigate further. ",NA,NA,NA,10/5/2022,October,2022,Rehan,Closed,,,2022,4,April,9/16/2025,1261,>60
4/3/2022,4/3/2022,88599017,03:26,17:28,14:02,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Potential threat to MP Weekly batch,TCS,SAP,,GMOR,RC unknown,60751,On 03/04 the Data Transfer Process (DTP) step in a job for the GMOR weekly MP interface flow was stuck. A very high OSS note was raised with SAP vendor however it was the Tech team who applied different combinations of database hints to the process chain and re-ran the problematic DTPs to successful completion.,SAP fix has been deployed,5/25/2022,5/25/2022,,,,Yes,Yes,No,Yes,RC unknown,NA,RCA inconclusive,NA,NA,NA,5/25/2022,May,2022,Sravan,Closed,,,2022,4,April,9/16/2025,1262,>60
4/1/2022,4/1/2022,88595982,00:18,4:11,03:53,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Overnight Quantum batches running with a delay,TCS,Quantum DevOps,,Quantum,RC unknown,60647,"On 01/04 two jobs in the overnight Quantum batch failed and therefore delayed Foods Final orders, NDC allocations.  The availability of Quantum UI was delayed by approximately 2 hours and the impacted batches completed within their respective SLAs.",NA,4/27/2022,4/27/2022,,,,Yes,Yes,Yes,NA,Customer Tech change,NA,Root cause is unidentified,NA,NA,NA,4/27/2022,April,2022,Sravan,Closed,,,2022,4,April,9/16/2025,1264,>60
3/31/2022,4/1/2022,88595280,13:20,11:30,21:50,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Issue with TMS schedule reports ,Microlise,NA,,TMS,RC identified,60748,"On 31/03 TMS schedule (Journey summary report) messages were not reaching Microlise (3rd Party) due to a FTP connectivity issue at their end.  As a result of this, stores were not able to see their incoming deliveries.  Microlise fixed the FTP connection issue by 11:30, 01/04.","An alert to be introduced in middleware incase of a FTP failure - Completed
",4/12/2022,4/12/2022,,,,"No, yes",No,No,Yes,3rd party change,CR,"As part of FTP change, Microlise team had carried out the implementation steps in out of sequence which resulted the issue",NA,NA,NA,4/4/2022,April,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1265,>60
3/30/2022,3/31/2022,88593788,22:00,11:15,13:15,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,C&H DCs unable to receive returns,TCS,Honeywell,,RTW(Return to Warehouse),RC identified,60747,"On 30/3 C&H DCs reported that they were not able to receive the returns form stores.  Approx. 80k singles need to be systemically received at DC's.  As part of the KMR (keep, move, or return) URRN (Unique Return Reference Number) solution introduced on 27/03, the URRN quantity needs to be “zero” in the CSSM database and RTW app.   The URRNs printed on labels generated using the RTW app were reflecting the actual quantity causing a mismatch which was preventing the DCs from receiving the returns. A fix was deployed to the RTW app on store Honeywells to resolve the issue.","All teams relating to the project need to illustrate all possible scenarios and match the testing as it would work in “real life” – especially relating to DC tasks such as label scanning - Nirmal, Shankar and Dileep - Completed

All testing scenarios need to be clarified theoretically and systemically prior to testing - Nirmal, Shankar and Dileep - Completed

SDMs/TDMs to communicate this testing process to the C&H portfolio and pass on to other portfolios - Nirmal, Shankar and Dileep -Completed

Before any change comes to CAB, when teams meet internally, all steps should be analysed step by step by demonstrations and then internally approved prior to the wider CAB -Nirmal, Shankar and Dileep

Alerting discussed but not possible due to volume of trailers and number of receiving issues daily – Completed

SAP/EDW cleanup must be done and a meeting to plan this will be held on 4th April - MIM",3/30/2022,3/31/2022,,,,No,No,No,Yes,Customer Tech change,CR,"RTW application generates the URRN barcode with quantity.  The CSSM application sends data to WMS regarding the shipment but without quantity.  In the DC, when they scan the barcode, the system compares the barcode with the data in WMS (no quantity) to the physical label which has the quantity hence causing a mismatch and the DC cannot systemically receive.",158876,NA,NA,4/27/2022,April,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1266,>60
3/27/2022,3/27/2022,88588119,04:30,11:16,06:46,Customer Channels,Service Experience,Customer Channels,Service Experience,,KI,430 Bradford NDC wine orders cancelled in OMS (Order Management System),TCS,Sterling,,OMS,RC identified,60635,"On 27/03, tech teams observed that 430 Bradford NDC wine orders were cancelled in OMS (Order Management System). The calculated ship by date which fell in the middle of clock change and were errored out by WMS as they did not receive the DRL (Demand Release) messages.","1.	Review the sev 2 comms process considering the number of orders cancelled – Dot Com Service Owners 
2.	Sterling and WMS leads to talk to their teams to ensure that full clarity, problem statement and impact is stated in all incident mails so that they can be dealt with as priority.  In parallel to a mail a Remedy incident should also be raised – Rajeev & Sakthi - Completed
3.	If Sterling does not receive the DRL messages or WMS is not sending the DRL, there must be sev2 alerting so that we can catch the issue in a timelier manner – Rajeev & Sakthi - Completed
4.	WMS and Sterling support to work out a way of stopping the automated cancellation and hold the orders until a solution can be found. – Jackson, Rajeev & Sakthi - Closed
5.	Change the shipment date and add 1 hour 5 minutes if it falls between 1am and 2am across all dates - Sakthi - JIRA backlog has been raised to cover this action",3/27/2022,3/31/2022,,,,No,No,No,,Customer Tech issue,NA,The Sterling application did not receive the DRL (demand release message) from WMS as the calculated ship by date which fell in the middle of clock change. As per process Sterling automatically cancels the order  if it does not receive a DRL. ,NA,NA,NA,7/4/2022,July,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1269,>60
3/24/2022,3/25/2022,88586364,22:00,12:00,14:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,KI,Intermittent errors when activating Sparks offers,TIBCO,NA,,Sparks,RC identified,60633,"On 24/03, alerting indicated an increased number of intermittent errors on the activation offer stage of the Sparks journey on Mobile Apps and Customer.com website. Vendor Tibco identified an overrunning job which performs housekeeping on their database was left running as a result of a human error. The job was manually turned off and services were restored at 12:00. ",NA,3/25/2022,5/16/2022,,,,"No, Yes",No,No,Yes,3rd party issue,NA,Human error,NA,NA,NA,5/16/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1272,>60
3/21/2022,3/21/2022,88577098,08:00,21:22,13:22,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,SI,Wireless Network connectivity issues at Ollerton DC,TCS,Network,,WiFi,RC unknown,60702,"At 7am, Ollerton colleagues reported that some devices like the AMTs, HHTs and laptops were not connecting to the wireless network due to connectivity issues.   DC picking and packing operations were impacted during the issue but they confirmed that as service had been restored by midnight, all work had been caught up.  After multiple recovery actions were taken to no avail, the core switch for the wired and wireless controller was restarted to restore connectivity on all the HHT terminals from 21:22.   The root cause is being investigated by network teams supported by vendors Juniper and Extreme.","1.	What was the actual root cause of the issue?  If it was something to do with the core switch then why was it mainly impacting half of zone 3? – Kamal/Paul
2.	Juniper to advise why the entire stack became corrupt after the entire stack was restarted - Kamal/Paul
3.	Depending on what was wrong with the core switch – what level of alerting can we put in place to save time and actual ascertain the problem in a timelier fashion next time - Kamal/Paul
4.	The DC was very willing to have outages during our MIM call to ensure we recover quickly, why wasn’t the core switch restarted sooner. – Closed
5.	NOC team need to take an action to change their SOP to ensure vendors are engaged after one hour of the start of the major incident. - Kamal/Paul (NOC) - Closed
6.	Review with Ollerton Operations & Castle Donington IT Support what exactly is a power check and what tasks it involves – Jason/Colin",3/21/2022,5/5/2022,,,,No,No,No,,Infrastructure issue / Hardware failure,NA,It believed to be - The Site Core switch could have experienced Split Brian issue due to Access Switch reboot on non-supported firmware. Juniper confirmed that NO further cause can be identified,NA,NA,NA,5/5/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1275,>60
3/21/2022,3/21/2022,88892347,11:30,17:30,06:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Marketing mail sends were failing for BT mailboxes.,BT,NA,,Marketing emails,RC identified,62753,10% of the marketing mails were failing when being sent to customers using BT mailboxes.  The issue was reported to SalesForce Marketing Cloud (SFMC) as they provide the platform utilised for mail campaigns.  BT team confirmed that there was an IP address blocked due to unusual traffic which was then unblocked to restore service.,BT to remove Customer Ips from Blacklist Completed,3/21/2022,3/21/2022,,,,Yes,No,No,Yes,3rd party issue,NA,BT blocked Customer IP address  due to unusual traffic ,NA,NA,NA,8/2/2022,August,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1275,>60
3/21/2022,3/21/2022,88579895,19:00,19:59,00:59,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Store pin pad terminals going offline,Worldline,NA,,Pin Pads,RC identified,,"From 19:00 to 19:59, multiple stores reported that card pin pad terminals were going offline and tills were requesting the OLA code.  The issue was caused as Customer IPs were blocked due to a DDoS attack on Worldline environment. During such attacks, Worldline diverts the customer traffic via their scrubbing centre which is supported by a third party ""Lumen"".  Further meetings were held with Worldline and Lumen to agree on the mitigating actions.",Please refer actions mentioned in 88567503,3/21/2022,3/21/2022,,,,"No, Yes ",No,NA,Yes,3rd party issue,NA,Ingenico confirmed that the issue was caused by a DDOS attack impacting 4-5 of their customers inc. Customer.  During this attack they divert customer traffic to their 3rd party “scrubbing centre” which is managed by Lumen. The Customer IP address were blocking by Lumen whenever Worldline diverts traffic to the scrubbing centre.,NA,NA,NA,7/3/2022,July,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1275,>60
3/18/2022,3/18/2022,88576860,16:15,16:35,00:20,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,COSTA orders not received by Suppliers,OpenText,NA,,COSTA & Food Final Orders,RC identified,60646,"On 18/03, 3 foods suppliers reported that they did not receive Costa Orders via OpenText EDI. Suppliers were sent manual order extracts in order to mitigate the picking and COSTA store impact.  The issue recovered itself and vendors True Commerce and OpenText advised that nothing was done at their end to resolve the issue.    
 
On 19 and 20/3, a few suppliers did not receive their Customer Final Orders (FFO) via OpenText EDI and were advised to use their 28 day order plan to mitigate the impact.  A setup issue at OpenText end caused the messages to become stuck in their queue fo the issue on 19/03, the cause for the issue on 20/03 is being investigated.  ","•	Find out if anything was done on 18th to fix the issue – get a timeline of the actions taken by OpenText – No major update from OT. 
•	For the issue on 19/20th Open Text noticed that most of the suppliers who were missing data were using “*LIAISON/NUBRIDGES” (3rd party) to receive their data Joey and PO to escalate to liaison to see if the files could be tracked on their end – Customer Foods Supplier Compliance have agreed with Suppliers that their intermediary EDI vendors will be involved in Technical investigations. - Complete 
•	For FFO we have a 28 day plan as a backup, what is the backup solution for Costa orders? – Lakshman/Santhosh - Completed
•	OpenText did not want to raise a sev 1 and their engagement was slow – OT client manager has confirmed that we can indeed raise a P1 when needed and it should not have been pushed back when we tried in this incident and for immediate response we should raise sev1 over a call- complete
•	OpenText escalation details to be updated  in the Contact Repository – Complete
•	Whenever there are EDI/Opentext issues there should be a playbook of steps to follow and teams to engage – this should be reiterated to relevant teams – complete
•	As we are sharing a common bridge with other clients for orders, what can Customer do to separate itself and have our own dedicated EDI bridge – can this be feasible?  - A dedicated bridge is being investigated by Architects and feasibility/benefit is being looked at by HoTs under a back log ID. 
•	The FFO voicemail was not recorded for around a month however PCM did not highlight this.  The issue was fixed by Mitel.  Why did PCM not highlight this and what did Mitel do to fix it? – Sravan/Suganya- Complete",3/19/2022,3/24/2022,,,,"No,NA",No,No,NA,3rd party issue,No,"
18 and 19/03 - This issue was caused by an incorrect setup for a different client of OpenText (not Marks & Spencer) also utilising a shared bridge.  A hung file for that client caused a blockage and so all files after that could not process and the entire MSOTXICS bridge got backed up which caused the missing files for MKSPN. The set up for that client was corrected and then the files began to flow successfully.
 
20/03 - We reported 700+ files missing. OpenText support noticed that most of the suppliers who were missing data were using “*LIAISON/NUBRIDGES” (3rd party) to receive their data – action assigned for Joey/Nico to speak with Liaison",NA,NA,NA,8/1/2022,August,2022,Kavitha,Closed,,,2022,3,March,9/16/2025,1278,>60
3/18/2022,3/18/2022,88575834,07:36,10:46,03:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Some Mother's Day Flower Products Not Appearing on PLP,Bloomreach ,NA,,PLP,RC identified,60705,"From 07:36 reports received from Command Centre advised that the number of products appearing on the Mother’s Day Flowers & Plants PLP (Product Listing Page) was significantly less than expected. Around 30 products were showing instead of the expected 70+.  To mitigate the a redirect was setup to point the to the ""All"" Flowers PLP. The Mother’s Day email to 2 million customers resumed at around 11am after it was suspended earlier. We await an update from Bloomreach regarding their investigation.",The permanent bug fix was deployed on April 13th to mitigate this issue.,5/6/2022,5/6/2022,,,,NA,NA,NA,Yes,3rd party issue,NA,"As part of a new feature release slotted products as well as the boosted products were supposed to be added to a parameter in the backend. There was an edge case scenario withbulk clear where this did not work. Engineering migrated all existing rules which used bulk clearand had slotted operations, so that slotting of products was respected.",NA,NA,NA,5/6/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1278,>60
3/18/2022,3/18/2022,88575818,08:39,11:00,02:21,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Card payments being declined on tills,Worldline,NA,,Store Tills,RC identified,60703,"From 08:39 stores reported that card payments were being declined at tills and the OLA code was provided to stores. At 11am Ingenico advised that to restore service they “switched” over from one database in their environment, to another.  More detail has been requested from Ingenico on the root cause and recovery actions.  We have advised Ingenico not to bring back their “affected” database until they can give us full confidence that they know the cause and have implemented any fix.  A detailed PIR will be arranged.","1.	Ascertain the root cause from Worldline – Completed
2.	The IVR on the day could not be set as a port was stuck in a ringing state and not picking up the call.  Can any alerts be set up on the Mitel side to advise us proactively of this issue? - Completed
3.	On the day of the issue the 185999 number started announcing “there is a technical fault” but calls still went through – could this have been due to load?  Mitel to investigate root cause. - Mitel - RC unknown
4.	Whilst the team struggled with the IVR we should have a process in place where this is tested periodically. It’s not something that we use every day, but when we need it, it needs to be instant. To have it not working lost us an hour in getting the OLA code to the stores – Merlin & Mitel
5.	We missed sending updates to franchises because we were unaware that they don’t use Teams and that was the only comms that was sent to stores – Service Owners to be made aware of the best process to quickly inform franchises of major incidents - Laura Whittle
6.	In delaying the OLA code on the IVR we caused an impact to the Worldpay call centre as they received calls from our stores – who can amend our SOPs to ensure Worldpay are made aware quickly?  Also verbally communicate this to the relevant team who will call WorldPay. - Laura Whittle
7.	The DL to which the proactive Worldline comms goes to needs to be reviewed and amended. This is already outstanding from a previous Worldline PIR - Laura Whittle/Gary Allenby
8.	MIM to ensure all actions in the worldline internal PIR document have been completed - Sravan",3/18/2022,3/25/2022,,,,No,No,No,No,3rd party issue,NA,High CPU utilisation caused by database long running query on the Worldline infrastructure,NA,NA,NA,5/5/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1278,>60
3/17/2022,3/17/2022,88574719,10:40,22:00,11:20,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Store colleagues facing intermittent errors whilst packing,Royal Mail,NA,,Store-Pick-Pack-Despatch,RC unknown,60632,"On 17/03, stores colleagues encountered intermittent errors in SPPD whilst packing parcels for delivery by Royal Mail.  Royal Mail confirmed that the issue was with their API’s which are hosted at their Data Centre’s.  The issue was potentially caused by a change and Royal Mail is working with their supplier for a fix and a feasible workaround to mitigate the impact. Store colleagues managed to pack majority of the Royal Mail parcels for 17/03.","BOSS support followed with Royal mail to understand why they change was made aware to Customer however there no response from Royal mail for last six months. Since the issue did not reoccur the problem record is being closed 
",3/17/2022,3/17/2022,,,,"No,NA",No,NA,NA,3rd party issue,NA,Royal Mail confirmed that the issue was with their API’s which are hosted at their Data Centre’s after a change being deployed at their end.The change was reverted at their to fix issue. However the change was not made aware to Customer by Royal mail,NA,NA,NA,8/2/2022,August,2022,Rehan,Closed,,,2022,3,March,9/16/2025,1279,>60
3/15/2022,3/16/2022,88572137,19:42,0:32,04:50,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Slowness observed in C&H Bradford automation systems,Knapp ,NA,,Automation system,RC identified,60707,"Bradford C&H DC reported slowness with their automation systems from 18:00 and the DC was at a stand-still later that evening. The issue was preventing the automation from processing the totes, causing them to get backed up. KNAPP developers identified that the issue was caused by a change that was deployed on 14/03 and reverted it to fix the issue by 00:32. Retail & Direct stores delivery failed for 1,616 singles. 1.9K full case singles for Donington failed and 7 Ecom orders were miss promised.  ",NA,3/16/2022,5/5/2022,,,,NA,NA,NA,Yes,3rd party change,CR,The code change in the OSR had caused the slowness in automation system,NA,NA,NA,5/5/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1281,>60
3/12/2022,3/12/2022,88567503,18:07,18:42,00:35,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,Card payments were failing on Store tills without OLA code,Worldline,NA,,Store Tills,RC identified,60410,Multiple stores reported that card payments were failing and asked for OLA code.  Ingenico confirmed that the issue was caused by a network issue at their end which was later fixed. Card payments were failing on store tills between 18:07 and 18:42 due to this issue. We are following up with Ingenico for a detailed RCA and PIR. ,"1	Suresh Ramamoorthy our Retail SDM called the Worldline SD from 18:57 to 19:17 and he got cut off at 19:17 – we need to forward Suresh’s number and the number dialled to reach the World Line SD.	
Fiona Durham and Gary Allenby	Why was Suresh on hold for 20 minutes?  Worldline seem to have no record of the call.
2	Suresh’s first call dropped at 19:17	
Fiona Durham and Gary Allenby	Agent profile was not routing properly and therefore the call dropped.  The config issues have now been resolved – Issue Closed.
3	During the incident POS support had issues engaging Worldline – the process needs to be reviewed and reiterated.	Merlin Elizabeth/Laura Whittle	As per the SD document, the Service desk should have engaged Worldline however the Customer SD is unaware of the ownership of this task. 
4	How can the proactive comms to advise Customer Tech of any incidents be improved by Worldline?	Fiona Durham and Gary Allenby	Gary advises that we should have received a notification from the French division and the reason why we did not should be provided in the RCA due EOD Friday 18/03.
5	Review the operational manual from Worldline to see the delivery list of who in Customer Tech should receive proactive comms from Worldline.	Laura Whittle/Andy Bull/Fiona Durham and Gary Allenby	Also, how can we review the DL’s periodically to ensure they are up to date? Add to Worldline service review meeting agenda.
6	Is any alerting possible at the Customer Tech end to scan for Worldline type issues or where till is requesting OLA code (continuous failing till transaction)?	Laura/Sandra/Seeva	Look at possibilities for central alerting and add to dashboards
7	Is there any centralised way that stores can obtain a fresh OLA code without having to go via Service Desk	Renjith Nair	We are carrying out a change to lift the floor limit to £150 without tills requesting for a OLA code.  This due to be rolled out from 21/3

For transactions over £150 – Customer Tech will, in future be able to use the self-help portal to advertise the OLA code.
8	An incorrect OLA code generates more work as the transactions require manual reconciliation.  This should be noted as an item in the SOPs for support teams to remember when the OLA code is used.	Laura Whittle/Suresh Ramamoorthy	Further discussion required.",3/12/2022,3/18/2022,,,,"No, Yes to be configured",No,Yes,Yes,3rd party issue,NA,Ingenico confirmed that the issue was caused by a DDOS attack impacting 4-5 of their customers inc. Customer.  During this attack they divert customer traffic to their 3rd party “scrubbing centre” which is managed by Lumen. The Customer IP address were blocking by Lumen whenever Worldline diverts traffic to the scrubbing centre.,NA,NA,NA,7/6/2022,July,2022,Rehan,Closed,,,2022,3,March,9/16/2025,1284,>60
3/11/2022,3/11/2022,88564542,04:00,5:46,01:46,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Stoke DC unable to process majority of the overnight D&F orders,TCS,WMS,,WMS,RC identified,60509,"On 11/03, the overnight D&F orders for Stoke DC were not allocated due to a configuration issue within WMS.  A system profile had been changed after a deployment which did not align with what should have been in Stoke. Tech teams manually corrected the configuration to allocate the orders by 05:46. This issue slightly delayed the picking operations at the DC.",NA,3/11/2022,3/11/2022,,,,No,No,NA,Yes,Customer Tech change,CR,"As part of change for Inbound split orders, system profile entries were modified which did not align with existing merge rules within Stoke causing incorrect order type to be populated as  “RETAIL” and “NON RETAIL”, instead of “NULL”. This caused the orders not to be allocated",156893,,,3/14/2022,March,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1285,>60
3/10/2022,3/10/2022,88563633,10:39,14:03,03:24,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI,Intermittent PayPal Issues during order checkout,PayPal,NA,,Customer .com website,RC identified,60510,"On 10/03 from 10:39, through proactive alerting, the Platform team identified that the number of errors during checkout with PayPal as payment method was unusually high. A global system issue at PayPal had affected the payment related API calls, which was fixed by 14:03. Detailed root cause analysis to be shared by PayPal.",Bhuvana advised we will not get any specific RCA document from Paypal,3/10/2022,5/30/2022,,,,Yes,Yes,TBC,,3rd party issue,NA,A global issue at Paypal end,NA,NA,NA,5/30/2022,May,2022,Sravan ,Closed,,,2022,3,March,9/16/2025,1286,>60
3/9/2022,3/12/2022,88562417,07:00,3:52,63:52,Group Platforms,Finance,Group Platforms,Finance,,SI,POS sales for 09/03 understated in SAP,TCS,SAP,,POSDTA,RC unknown,60505,"The POS sales for 09/03 was understated by £21 million in SAP when compared to Retail Dashboard. The EDW Day-1 reports for 09/03 and 10/03 was impacted as POS sales data for 09/03 (UK, Ireland stores, UK franchises, International Czech and Greece) were not processed out of SAP. The GMOR stock positions for 09/03 and 10/03 were overstated as well. Investigations revealed the process which fetches the sales data from POSDTA and sends it to SAP ECC was failing with a SQL error dump. In addition, data inconsistency in the aggregation table was observed which caused further issues. Tech teams resolved the dump issue and performed a work around to overcome the table inconsistencies, thereby reducing the variance to permissible values.","1	SAP vendor is analysing traces and root cause is 
	Remko	Root cause sla is a month and has been flagged to Ben and Gopi – they are to speak to SAP - Completed, RCA inconclusive
2	Service design – how is this service covered with sla’s, MIM, change, problem process – where is this document? PSDM transition to POSDTA 	Gopi	
3	Update the AIM with more detail on impact of POSDTA had an outage as no data was coming out - Completed	
4	Challenges in getting SAP onto the call – no operator available until start of the next day, even though escalation architect joined – they are just co-ordinators.  Consider raising a formal risk on the lack of traction.	- Completed
5	What kind of alerting can be put in place to identify that data is not flowing out of POSDTA – there was a delay in us realising this. Remko – no immediate alerting available at present.
",3/12/2022,44791,,,,Yes,Yes,Yes,TBC,RC unknown,NA,SAP recommended to upgrade the POSDTA HANA upgrade,NA,NA,NA,44791,August,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1287,>60
3/7/2022,3/8/2022,88559461,19:00,0:30,05:30,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Some .com customers received multiple order confirmation texts,Sorted,NA,,Royal Mail text message,RC identified,60405,"Some .com customers received multiple order confirmation and tracking reference text messages from Royal Mail meant for other customers. Vendor Sorted was engaged who identified a bug in the manifest creation in their upgraded version of the application which was deployed on 07/02. To mitigate the impact, Royal Mail was removed from the service group at 00:30, 08/03 - other carriers such as Hermes were available to deliver our customer’s orders. Sorted confirmed around 12K customers had not received any text messages, however, they received an order confirmation and tracking message via email. Around 450 customers received multiple text messages. Customer Legal agreed that there was no material GDPR risk. Sorted applied the fix in their environment and Royal Mail was triggered as a carrier at our end by 14:00, 10/03.","1	The parallel process of the manifest creation and then placing it in cache meant that the test case scenario could not identify the issue.  Further test scenarios will be in built to include further permutations of test data/scenarios.	Sorted Engineering	Implemented and closed
2	QA testing approach will be addressed so that functional testing under “load” is included  - the test load should replicate real life size of the manifests	Sorted QA Engineering	Implemented and closed
3	Is there anyway Tech teams at Customer can identify this type of issue rather than reply on end customers reporting the issue? 	All/Jason Elliott/Colin Haworth	In this case the SMS texts are sent by the carrier Royal Mail and we do not manage their systems.  Can we ask Royal Mail if they can implement any data checks and contact Customer if they see abnormal behaviours and agree who they would contact in Customer Tech.
4	MIM/Support were not engaged in a timely manner.	Jason/Colin	Project Managers in Tech have been appraised in the CMI meetings of the correct process of reporting incidents and engaging the correct teams including MIM/SO/SDM.
5	The DPO and GSOC were engaged in a delayed manner	MIM/Nik/Simon Harris	The MIM team has been informed that they need to inform InfoSec and GSOC if they have suspicions that the issue could be GDPR related and is a security issue. Talk to teams in the 0830 and 16:00 Ops meetings.
InfoSec will carry out further awareness presentations to refresh Tech teams regarding the process of engaging GSOC/InfoSec",3/8/2022,3/17/2022,,,,"No,NA",No,Yes,Yes,Code/Product bug,TBC,"The Sorted code that generated the manifest file had an error in the processing logic. The last mobile phone number seen when pre-processing the orders to be included in the manifest, was copied in for all orders when the manifest file was written, overwriting the correct mobile number for the customer.",,,,5/5/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1289,>60
3/4/2022,3/4/2022,88553279,09:08,12:10,03:02,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Some T65 furniture products are not showing on the website,TCS,Bloomreach Support,,PLP,RC identified,60408,.com colleagues identified that T65 furniture products were not showing on the website. This affected potential loss of sales and could have driven more calls into Contact Centres.  Around 60 lines were impacted.  A Search Control Layer (SCL) deployment caused issues with the Bloomreach feed.  The deployment was reverted and at 11:34 it was confirmed that all of the impacted products were sent to Bloomreach.  The FIND team are analysing the alerting around missing products in the Bloomreach feed.  A detailed review on the deployment will also be carried out.,"FIND team is working on below alerting and planning to pick this in next sprint 5(16/03/22 - 5/04/22):
1) Alert monitoring to be introduced at BAU level during full feed creation process - Completed
2) Validate the exception count for full feed process - Backlog 2806 has been created
3) Validate the completion of Full Feed process - Completed",3/4/2022,3/9/2022,,,,No,No,No,NA,Customer Tech change,CR,Code Issue - One of the functionality for colour swatches is to map the images from FE for a product image to change. The structure of T65 is a little different due to colour info not coming from same source and hence failing.,CM-7588,TBC,NA,5/5/2022,May,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1292,>60
3/2/2022,3/3/2022,88551926,12:30,3:00,14:30,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,T65 Furniture products showing incorrect lead-times,TCS,Enterprise Middleware,,MLT,RC identified,60261,"At 12:30 on 02/03, the furniture department highlighted that incorrect lead-times were showing on the Website for some T65 furniture products. This had potential loss of sales/revenue and to mitigate the customer impact and some of the products were removed from the site. The MLT (manufacturing lead times) updates were piled up in an Azure V1 queue.  After the migration of PIM EM (Enterprise Middleware) flows from Azure V1 to V2, the repointing of this flow was unfortunately missed. As a work around the files were copied over from the V1 to V2 file share for PIM to process. A subsequent change was made to point this flow to the correct PIM file share in V2 - full BAU was achieved by 03:00 on 03/03.",NA,3/2/2022,3/2/2022,,,,No,No,No,Yes,Customer Tech change,CR,There was migration of PIM EM (Enterprise Middleware) flows from Azure V1 to V2.  However the flow containing MLT was missed from the migration which resulted in   the MLT flow remaining on Azure V1,TBC,Incorrect deployment plan,NA,3/2/2022,March,2022,Sravan,Closed,,,2022,3,March,9/16/2025,1294,>60
2/25/2022,2/25/2022,88542263,07:02,9:30,02:28,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Milton Keynes DC received incorrect allocation volumes,Customer,Supply chain management ,,ASO,RC identified,TBC,"Milton Keynes DC received incorrect allocation volumes - At 07:02, tech teams proactively reported that the Milton Keynes (MK) DC had received incorrect allocation volumes. The issue was caused by the upload of incorrect data in SCRD (Supply Chain Reference Data) by Supply Chain colleagues. The tech teams de-allocated the allocations that MK had received overnight and re-triggered the backup allocations from ASO. The allocations successfully processed into the WMS system for Milton Keynes by 09:30",This can be closed as the error was carried oyt by Customer supply chain colleagues,2/25/2022,2/25/2022,,,,Yes,Yes ,No,NA,Human error - Business,NA,The issue was caused by the upload of incorrect data in SCRD (Supply Chain Reference Data) by Supply Chain colleagues,NA,NA,NA,5/5/2022,May,2022,Vinay,Closed,,,2022,2,February,9/16/2025,1299,>60
2/24/2022,2/24/2022,88540640,14:25,15:10,00:45,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,SI,C&H and Supply chain Colleagues were unable to access some files,TCS,Storage ,,C&H colleages,RC identified,60252,"Colleagues from C&H commercial and supply chain reported that changes to files or newly created files were missing in one of their shared drives. Investigations revealed that as part of the Hitachi to pure storage migration, tech teams copied data from the Hitachi legacy storage to the new pure storage at 22:00 on 23/02. The data migration from old to new will not be reattempted until the process has been verified with the vendor pure storage. As a work around, colleagues were provided with the old Hitachi storage path from where they can take a copy of their missing files. ","1. No further updates on from Pure Vendor on RCA. Microsoft has advised to upgraded Vmware tool, SSCI controller and increase the CPU from 8 to 12- These recommedation are incorporated on 30/05/2022- Complete.   
2. Team to monitor the further pending migration for stability without reoccurnace of the issue - ETA TBC ",2/24/2022,2/24/2022,,,,No,No,Yes,NA,Customer Tech change,Yes,"During the migration/copy process from Hitachi to Pure - the colleagues who had files open at the time would have lost these files as they would not have migrated. Also during the same period there was a MQFTE script also transferring files to a target location and these would also have been lost.
The copy of data from Hitachi completed at 10pm which means any files created, saved, changed from 10pm to 3am would not have been migrated to the PURE location",155064,Incorrect planning & testing ,NA,8/16/2022,August,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1300,>60
2/23/2022,2/23/2022,88539714,10:00,14:10,04:10,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,MI,Operations at the Bradford C&H impacted,Knapp ,NA,,Bradford C&H DC,RC identified,60162,"At 10:00, warehouse colleagues at the Bradford C&H DC reported that they were getting the error message “No unit found” upon scanning the order totes, indicating that WCS messages were not being sent to WMS. KNAPP tech teams diagnosed that one of the event handlers responsible for processing messages into WCS was down. The event handler was restarted at 11:00 and the site resumed their operations at 12:05. Further investigations from KNAPP revealed that the event handler had gone down due to a known bug within the WCS Oracle DB. The site was then advised to stop their operations whilst the KNAPP developers applied the workaround for this issue. The workaround was completed by 13:23 and the site resumed operations by 14:10. This issue resulted in a retail capability loss of 75k. The permanent resolution for the bug is via a DB upgrade and replacement of the Knapp WCS server, both planned within a couple of weeks."," Oracle Database to upgraded  from v12 to v19 - Completed
How do we improve the communication to stakeholders? _ Completed
This issue was placed in the weekly report at the last minute however MIM was not engaged which then results in questions from senior stakeholders.  We need to agree in what circumstances MIM are to be engaged for Knapp issues._ Completed ",2/23/2022,2/23/2022,,,,NO,No,NA,NA,Code/Product bug,NA,The communication between WCS and SRC (Order picking application managed by Knapp) was broken because the event handler had gone down due to an known Oracle Bug (ORA03114). Knapp had restarted the event handler to fix the issue. ,NA,NA,NA,4/27/2022,April,2022,Sravan,Closed,,,2022,2,February,9/16/2025,1301,>60
2/22/2022,2/22/2022,88537505,08:05,17:37,09:32,Commercial Trading,C&H Commercial Trading,Commercial Trading,C&H Commercial Trading ,,MI,PLM unavailable,PTC,NA,,PLM,RC identified,60255,The PLM (Product lifecycle Management) application was unavailable from 08:30 resulting in colleagues unable to carry out their development work on C&H products.  Vendor PTC advised that a few table indexes were dropped inadvertently whilst they were increasing the number of columns in their database tables.  PTC identified the missing indexes and recreated them in Prod to make PLM available by 17:22.  We continue to work with PTC to identify why their implementation did not go as planned and their post implementation checks need to be reviewed.,"Short Term

1. The Add Columns procedure has been thoroughly reviewed to strengthen it through a dedicated script. - Completed

2. The PTC team will export Production indexes periodically to keep track of it and be able to import indexes if any are getting dropped accidentally

3. Improve Customer internal validation & pro-active detection of issues

4. Add a step to compare the number of indexes before and after running the Add column command to determine if any indexes have been dropped.

5. Create a procedure to restore indexes and train the team accordingly.

6. After each maintenance activity, validation testing should include a dedicated performance test (line sheet display, copy product).

Long Term

1. Correct FlexPLM Product issue; timing and application of this fix is currently being determined.

2. Work jointly with R&D to improve Add Columns logic and provide proper alerts and failure messages if indexes are dropped during any maintenance activity",2/22/2022,2/23/2022,,,,"No,Yes",No,NA,NA,3rd party change,NA,"The cause of the performance issues was related to a software issue. The database table unexpectantly locked during the execution of a pre-planned maintenance task (Add Column), which resulted in some indexes not being correctly transferred. The indexes being dropped during this maintenance event resulted in performance issues.",155998 ,Code/Product bug,NA,2/23/2022,February,2022,Vinay,Closed,,,2022,2,February,9/16/2025,1302,>60
2/21/2022,2/21/2022,88536325,15:08,17:11,02:03,GTS,Enterprise Technology Platform,Customer Channels,Selling Experience,,SI,Customer Orders were going to Offline payment processing,TCS,Cloud Network,,WCS ,RC identified,60161,"At 15:08, through proactive alerting, platform tech teams identified that the majority of online customers orders were going to offline payment processing. ?The issue was inadvertently caused by a change in network setting which was done whilst attempting to fix a separate issue. The network settings were reverted and services were restored at 17:11. In total there were?7K orders in the backlog and these were cleared at 18:00 in WCS. ",NA,2/21/2022,2/21/2022,,,,Yes,Yes,NA,Yes,Unauthorised Change ,NA,"As part of the troubleshooting intended to fix the control-M server connectivity to on-prem Active Directory, the remote gateway option in the peering was updated to enable the connectivity. this resulted in asymmetric routing impacting the Dotcom cloud V2-PCI traffic flows to on-prem network. The remote gateway option was reverted to fix the traffic flows between Dotcom cloud V2 PCI and on-prem networks.",NA,NA,NA,2/21/2022,February,2022,Vinay,Closed,,,2022,2,February,9/16/2025,1303,>60
2/18/2022,2/18/2022,88531699,10:48,11:45,00:57,GTS,Enterprise Technology Platform, C&H and Intl,C&H and Intl Supply Chain,,KI,Multiple network devices unavailable in Swindon DC,NA,DC Network,,Swindon DC,RC unknown,61181,"From 10:49, the Network team observed alerts indicating that multiple network devices at the Swindon DC were not available.  Swindon DC’s Customer network and telephone lines were unavailable impacting DC operations including picking, packing, receiving, and despatching, all affected tasks were caught up quickly after recovery.  The Network team attempted multiple times to contact the DC but were unable to as all phone lines were unavailable.  By 11:45 network devices self-recovered.  Root cause is being investigated via a sev 1 log with Vodafone.",Vodafone has advised that they could not find any issue at their end and our network team suspects that this could have been caused due to Eugine storm on 18/02,2/18/2022,2/18/2022,Yes,No,,Yes,NA,NA,NA,3rd party issue,No,"There was WAN link failure on the Vodafone end supported by BT. There are two link, both were impacted at the same time, one still remains an issue and a config change was performed to fix the issue ",NA,NA,NA,2/27/2022,February,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1306,>60
2/15/2022,2/16/2022,88517212,06:43,13:45,31:02,GTS,Enterprise Technology Platform,All BU's,All BU's,,MI,Multiple applications experiencing performance issues,Vodafone,NA,,Network,RC identified,60159,"Store colleagues reported CSSM application latency and performance issues from around 06:40 on 15/02 with CSSM counting/scanning.  By 08:30, the performance had gone back to normal and the root cause was attributed to an issue with Vodafone’s DWDM link at Brentford. On 16/02 stores again reported issues with CSSM and a decision was taken to disable the Brentford link.  This was done by 10:55 however no improvement was seen and by 11:30 the Brentford link was re-enabled.  It was at this point that stores reported issues with tills freezing when carrying out refunds for .com purchases and when looking up click and collect orders. Tech teams could also see messages piling up in middleware impacting applications such as ASO, WMS, WLM, POS and Retail Dashboard.  Foods store counting was blocked from 13:00 to 16:45. Vodafone investigations revealed errors on their Hillingdon link and the impact services were recovered by isolating the Hillingdon link. On 17/02 , Vodafone found issues on a 5200 switch at Hillingdon. This is also believed to cause intermittent performance issues in few applications since 06th February. Vodafone had advised that Hillingdon hospital will allow their engineers to recover the switch only after storms have abated, due to health & safety reasons.  We will agree a date and time for the Hillingdon link to be made available after works have completed with full hyper care in place.","Vodafone stated different issues with the DWDM link, issues in Brentford and Hillingdon sites with attenuators and GMUX cards – is there any way better fault finding could have been carried out to ascertain the issues in a more timely manner?	Vodafone	Rich from VF advised that improved fault finding and alerting is not possible as this kit is end of life and we need to move to the DEA – CLOSED

Plan to move to DEA links	Paul Beasley	The replication for VM Ware part will be migrated on 09/03 at 17:00 via a focus change. implemented on 20th April under a CR - CLOSED

The impact assessment was not comprehensive and attained in a timely manner – can we create a list of all the services that rely on replication through the two DWDM links and the new DEA – this can be used for the impact assessment if there is an issue with the DWDM links in future before we move to the new solution- DEA is direct Ethernet access - we have apps running with replication between SP and SW and this replication traffic goes via DEA - VMWare Team will have the list of applications - Dharma.	

Look at event correlation on the VM Ware level to try and identify wider issues – not limited to one VM.	Dharma/Elizabeth	Alerting to be changed to notify of simultaneous multiple alerts, raised as a sev 1.

Reiterate to support teams if they are looking at something that is ongoing on a daily basis – they need to call it out to notify and request support.	Rehan	Communicate to SDM’s on the 08:30 - CLOSED
6	The communication to our stake holders was not as effective and detailed as it could have been – where and how can we improve the communication?	MIM (Rehan) and SO’s 	Simplify the level of communication and relate more to the business context and impact due to the issue. Teams updated and CLOSED 
7	When do we engage the Sev 0 process?
What is the criteria to engage the business continuity team?	MIM	A learning to note if more than one portfolio is affected with significant business impacting issues – consider engage the sev 0 process. -Closed
8	Run a mock sev 0 incident once a quarter.	ALL	Note to all. CLOSED",2/15/2022,2/15/2022,,,,"No,Yes",No,Yes,Yes,3rd party issue,No,The attenuator on the primary and secondary DWDM links were faulty causing signaling issue and the attenuators needed replacing. The GMUX card in the Hillingdon Vodafone site also needed replacing.,NA,NA,NA,10/18/2022,October,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1309,>60
2/14/2022,2/16/2022,88528387,12:36,9:48,45:12,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Digital Receipts not sent to customers in a timely manner,Yucudo,NA,,Digital Receipts service,RC identified,60256,"Customers who had subscribed to the Digital Receipts services were not getting their e-receipts via e-mail from 14/02. The issue was caused at the back of manual error by Yocuda: our 3rd party digital receipts mail supplier, while doing a config change on 14/02. Yocuda fixed the issue after which Customer Tech teams reprocessed the failed messages by 12:25, 16/02. Due to this issue, there was a delay in sending 48.7k digital receipts to the customers.",No actions pending ,2/16/2022,2/16/2022,,,,No,No,NA,NA,3rd party change,NA,Issue was caused by Yocuda revoking our production API key without creating new one first,NA,Human error,NA,2/16/2022,February,2022,Kavitha/Sravan,Closed,,,2022,2,February,9/16/2025,1310,>60
2/10/2022,2/10/2022,TBC,NA,NA,NA,GTS,Enterprise Technology Platform,GTS,Enterprise Technology Platform,,KI,Calendar entries not synched between delegate and VIP,Microsoft,NA,,Outlook 365,RC identified,,Microsoft have acknowledged that they had a central service outage that impacted a random set of their users and it was resolved later in the day today by them.  Scope of impact: Any users who are hosted on the affected infrastructure attempting to modify or delete meetings via delegate permissions were impacted. Our Tech teams refreshed the impacted user profiles to resolve the calendar synch issue.,NA,2/10/2022,2/10/2022,,,,No,No,Yes,,3rd party issue,NA,Central outage at the Microsoft end had caused the issue,NA,NA,NA,2/10/2022,February,2022,Rehan,Closed,,,2022,2,February,9/16/2025,1314,>60
2/10/2022,2/10/2022,88516137,00:30,6:01,05:31,Foods,Food Supply Chain,Foods,Foods Supply Chain,,KI,Delay to the Foods Bradford NDC allocation,TCS,SAP,,Picking and packing operations,RC identified,59415," As part of the COSTA Project, the STG for the 3 stores was first introduced on 20/01 and interfaced into Q successfully. However, from 22/01, the mapping of the three stores to the new COSTA STG was not sent to Quantum. On 09/02, the mapping was re-triggered from SAP and interfaced into Quantum and the job reattempted to add the mapping back to the Quantum tables causing the job to fail with a unique constraint error. As a workaround, the offending mapping was removed and the job was re-run to completion. Bradford NDC allocation completed at 06:01.",The permanent fix has been deployed on 23/02,12/22/2021,12/22/2021,,,,Yes,Yes,Yes,Yes,Data issue,No, The Q job failed with an unique constraint error due to an existing STG (store trade group) as it had no store associated to it.,NA,NA,NA,2/23/2022,February,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1314,>60
2/9/2022,2/10/2022,88514749,11:49,16:00,28:11,GTS,Enterprise Technology Platform,Customer Channels,Platform & Store Ops ,,MI,RCS colleagues unable to connect to Salina network,TCS,Network,,Contact center ,RC identified,60108,"At 11:45, the majority of the 40 users who take customer calls from Chester Complex Contact Centre lost their connectivity to the Salina wireless network and could not reconnect.  To sustain operations, 25 out of the 40 colleagues were asked to take customer calls from home on 09/02 and 10/02. Approximately 15 colleagues working from office were connected via a LAN with a small number remaining on SALINA. Since, the impacted colleagues had connected from home, there was no impact to operations.  Our investigations on 10/2 found a small number of ""cosmetic errors"" on wireless controller 2 - these were not service impacting errors but even so a decision was made to move all 21 access points in RCS Chester over to wireless controller 1.  Once this was done, the laptops that could not connect to SALINA immediately connected.  Root cause investigations under way.","1 WLC2 needs a restart and this planned for w/c 21/2. Once WLC2 Paul Beasley
has been restarted the AP’s for Chester will be shared between WLC1 and WLC2. This work will be done via two separate changes. - Completed

2 CISCO advised that we need to upgrade our firmware on WLC2 Paul Beasley We cannot upgrade the firmware as we run a mixture of AP’s and the old er AP’s are not supported by the new firmware. This is needs to be raised as a risk - The APs were moved to Chester.

3 There are limited LAN cables in Chester, 2 per a bank of 8. For a contact centre, to prevent multiple users going home to work, it would be prudent to add more LAN ports in the office. Steve Siddall/Scott Garner/Andy Berriman/Paul Beasley - Deferred due to budget constraints

4 Is there any form of alerting that can be created to pick up on this type of issue? Paul/Mustafa/Jana/Derek - Completed

5 None of the Operational team led by Alistair Brass were getting any updates – apart from the updates coming from Dan and Michael on our MIM call. Jay Mackenzie Revisit the DL’s and review the operational communication process from the side of the Service Ow",44607,44607,,,,NO,NO,No,NO,Unauthorised Change ,NA,"Whilst investigating the sporadic drops which started on Friday 4th February, some settings were changed – “session timeout setting” and a “DHCP address required” to connect to the network – this settings change caused a blip (as it refreshed the connection to SALINA) which was enough to disconnect most call agents.

 For the longer running issue, logs are not providing information.  Other sites were running on WLC 2 such as SP and Preston Brook. ",NA,NA,NA,5/12/2022,May,2022,Sravan,Closed,,,2022,2,February,9/16/2025,1315,>60
2/9/2022,2/10/2022,88515890,21:22,6:20,08:58,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,MI,Sparks Unavailable Online,TIBCO,NA,,Sparks,RC identified,60107,"Sparks was unavailable on the website & mobile apps from 21:22 impacting customers' ability to activate and redeem offers. Holding pages were applied for Sparks on the website & mobile apps at 22:14. TIBCO confirmed that there was a central issue with Sypnotek -  the supplier who manages TIBCO's Datacentre which was fixed by 05:58, 10/02. After performing the health checks, the tech teams removed the holding pages by 06:20.",The contract has been renewed,2/10/2022,2/10/2022,,,,Yes,Yes,Yes,Yes,3rd party issue,NA,"Lumin the company who provides the connectivity to TIBCO's data centre in Frankfurt stopped the connectivity because they did not receive payment for Tech Data
",NA,NA,NA,15/02/2022,15/02/2022,#VALUE!,Saloni,Closed,,,2022,2,February,9/16/2025,1315,>60
2/8/2022,2/8/2022,88512688,04:21,10:32,06:11,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,KI, Bloomreach Feed Index Failing,Bloom Reach,NA,,PLP and  SRP,RC identified,60110,"On 08/02, alerting and monitoring indicated that the daily full feed indexing at Bloomreach (Search service provider) which displays products on PLPs (Product Listing Pages) & SRPs (Search Results Pages) did not complete within the allotted time window. A central issue at Bloomreach had caused the problem which was later resolved. The indexing capability was restored by 10:32 and the full feeds were reprocessed by 11:20. The 15 missing products were also visible on the PLPs and SRPs by 11:30.","1.Engineering decommissioned all VPC nodes so the internal API would work again. Engineering
later recreated VPC nodes with public IPs to fully recover the system.
MITIGATION PLAN/FURTHER ACTIONS
2.Instituted additional robust monitoring checks on system health and connectivity during
VPC migration
3.Engineering and QA team will review and  institute comprehensice testing plan ",2/14/2022,2/14/2022,,,,Yes,Yes,NA,Yes,3rd party issue,NA,"Bloomreach feed data indexer uses a Cassandra Database to process data. The Cassandra
version is a special version customized by Bloomreach. Engineering is migrating it to a newer
infrastructure (VPC) to get better tech support and performance.
Migration of internal REST API component could not connect to the nodes on VPC. This is due
to the special logic in the customized code, which is looking for public IPs of Cassandra nodes.
Since VPC nodes were created with private IPs, they could not be found.
Unfortunately this was not caught during testing as the issue only started when the total number
of VPC nodes went above a certain threshold",NA,NA,NA,2/14/2022,February,2022,Vinay/Kavitha,Closed,,,2022,2,February,9/16/2025,1316,>60
2/4/2022,2/4/2022,88507651,09:00,15:00,06:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Some of the customers have not been receiving the order confirmation emails.,TCS,.com,,Customer experience,RC identified,59913,"Tech teams identified that some customers who had placed flowers orders with add-on products did not receive order confirmation emails.  This affected 11.8K customers since 17/01.  A new order confirmation email template went live on 17th Jan for 20% of the customers and the remainder on 26th . On 04/02,  a code bug in our custom code was identified which was causing the order confirmation emails not to be sent to these customers who had placed flowers orders with add-on items. Tech Teams applied a hot fix and restored the services at 15:00. ",Dev teams rewrote the code and deployed it to fix the issue,2/4/2022,2/4/2022,,,,No,No,No,Yes,Code/Product bug,NA,A code bug in the custom code  of the order confirmation template that went live on 17/01.,NA,NA,NA,15/02/2022,15/02/2022,#VALUE!,Saloni,Closed,,,2022,2,February,9/16/2025,1320,>60
2/4/2022,2/4/2022,88506439,06:33,8:25,01:52,Customer Channels,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,KI,RFID count issue,TCS,CSSM,,CSSM,RC identified,59914,"On 04/02,6 Stores reported issues with RFID as the messages were piled up in MQ due to a blocking session. Services were restored after the blocking session was terminated by 08:05. ","Analyse memory and CPU utilization of LCS azure DB and configured the alerts for memory percentage, maximum app CPU percentage and data IO percentage values in azure portal to notify us if the same kind of issue happens in future.",2/15/2022,2/15/2022,,,,"No, Yes its configured",No,No,NA,Application performance,NA,"Upon analysing the SQL DB query, we found that one recall count session was blocking other RFID count processing which leads to RFID count message queue pileup, that’s the reason store user was unable to progress on the RFID counting in the app. ",NA,NA,NA,2/15/2022,February,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1320,>60
2/4/2022,2/4/2022,88507618,13:12,15:10,01:58,Group Support,Finance,Commercial Trading,Foods Commercial Trading ,,SI,"
Digital POA features not available on One Foods Platform",SAP,NA,,One Customer Foods Platform,RC identified,59736,All OFP foods users were unable to perform PO amendments and approvals due to an intermittent connectivity issue. This issue occurred on Monday 31/01 and Friday 04/02 and root cause is being investigated by SAP vendor as they confirmed that they had an issue in their EU region and as a result they had to restart services.  Further details pending from SAP vendor,"""1. How can SAP vendor make us aware of forthcoming deployments even if global?
Sarthak will ask what is possible from  SAP vendor.  - Completed
 Kishan, Gopi
2.SAP vendor advise that during test and validation activities, such a concurrency issue was not observed.  How can we ensure that testing is done so that we know all is well before and after the deployment in their environment?  How do we know that their test plan is through enough to cover all possible risks?- Completed- Kishan, Gopi
3.If such an issue was to reoccur, what alerting can be implemented at SAP’s side so that they can make Customer proactively aware of any potential issues in a timely manner?- Completed - Kishan, Gopi
4.SAP are looking into a fix coming directly from the open-source community via a new library version.  What is the ETA for this and who can keep us updated?  Are there any risks to us whilst we wait for a permanent fix? - Completed - Kishan, Gopi
5.How will SAP ensure that their service can stay up in the event of a recurrence, is there any resiliency or can some be implemented? What is the agreed availability SLA % for SAP CPI?- Inprogess - Kishan, Gopi
6.We performed a manual disconnect and reconnect – is this a recommended action for next time if the issue recurs?  Are there any other actions that SAP can recommend that Customer Tech can do on our side?-Inprogess - Kishan, Gopi
7.The issue on 4th February was due to a different root cause. The detailed root cause for the regional issue is yet to formally confirmed.
How can we protect our environment from a recurrence?  Is there any resilience available? - Inprogess - Kishan, Gopi
8.For OFP, is there anything Tech teams can do in terms of resilience and design to protect ourselves from a vendor related issue? -Complete considering that SAP master data is the source for the data. - Completed
9.Alerting for OFP availability is already in place as sev 2 logs.  Raising this to a sev 1 will depend on the agreed availability for OFP.- alerting severity and outage window has been agreed -Completd
10.Ensure that there is alerting configured for connectivity between SAP CPI and Cloud Connector for proactive visibility of any issues.  Create a SOP that instructs engineers to check the cloud connector in PROD and non-prod in the case of recurrence – the same cloud connector is used in both and so if there is an issue in non-prod then it will also be there in PROD, this is a good, quick, sensor check. - Completed",2/4/2022,2/11/2022,,,,Yes,NA,NA,NA,3rd party issue,NA,"Issue on 31/01:
A regular platform update was carried out by SAP vendor in their SaS cloud system.  This deployment was rolled to all tenants hosted on that specific Cloud system.  The service update succeeded according to SAP. When run by users, a hidden concurrency issue occurred, which led to creation and accumulation of orphan database connections, this resulted in the congestion of the connection pool leading to requests processing issues.
SAP vendor advised that during test and validation activities, such a concurrency issue was not observed

The issue has been identified as a bug in the open-source dependency library, used for database communication

Issue on 04/02
SAP advised: there was a general disruption at their side on the EU20 region regarding connectivity and notification agent. They restarted and the connection was restored – see action 7.
Update for issue on 19/04:
Root Cause Analysis:During routine service updates that were expected to be non-disruptive, the Connectivity service exceeded its internal rate limit for authorizations on the platform, which prevented customers from establishing new connections.Preventive Measures:The internal rate limit that impacted service has been increased as a short-term measure.The operation that caused the Connectivity service to exceed the internal rate limit will be reviewed and improved.
",NA,NA,NA,9/7/2022,September,2022,Sravan,Closed,,,2022,2,February,9/16/2025,1320,>60
2/3/2022,2/3/2022,88507445,TBC,TBC,,Commercial Trading,Foods Commercial Trading,Commercial Trading,Foods Commercial Trading ,,KI,SRD users were unable to make approvals ,Blue Yonder,NA,,SRD,RC identified,TBC,"On 03/02, SRD (space range and display) users were unable to make approvals as the figures were inaccurate.  This occurred due to a change carried out by vendor BY.","1.Alerting has been put in place if there is a change in regional settings by any means. 
2. The Issue with logon script has been fixed and the monthly maintenance acitivties were monitored and confirmed that issue didn't reoccur",2/3/2022,3/8/2022,,,,NA,NA,NA,NA,3rd party change,Yes,"""The cause of the issue is related to regional settings change in the batch servers."" as part of regular maintenance activities, BY inadvertently changes the regional settings which caused the issue. The issue was caused due to logon script which was running when there is a server reboot",NA,NA,,3/8/2022,March,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1321,>60
2/2/2022,2/4/2022,TBC,12:38,13:37,00:59,Commercial Trading,Foods Commercial Trading,Commercial Trading,Foods Commercial Trading ,,KI,Users were unable to create new foods UPC's using the FIND application,TraceOne,NA,,Find,RC identified,,From 2nd to 4th February users were unable to create new foods UPC's using the FIND application.  This was due to a code deployment at vendor TraceOne who own FIND.  The code caused a validation conflict in our integration.  The deployment was reverted to recover services.,NA,2/4/2022,2/14/2022,,,,NA,NA,NA,NA,3rd party change,Yes,The introduction of a new ‘Pack Copy Language’ into FIND (French) that added a new logic force inserted compiled data into the description section. This then caused validation conflict against pre-existing logic resulting in System Timeout Error been seen by colleagues.,NA,NA,NA,2/14/2022,February,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1322,>60
2/2/2022,2/2/2022,88503069,08:21,9:15,00:54,Group Platforms,Corporate,Group Platforms,Corporate,,SI,Staff discount not applying,TCS,Corporate Services,,Staff discount elibility services ,RC identified,59907,"From 05:11 on 02/02 store colleagues and .com teams reported that the staff discount was not working at tills or online.  Investigations found that a deployment was carried out to deploy the “staff discount eligibility service”.  This was reverted and checks were successful after the delta was replicated to all tills. Root cause investigations have found that as part of the deployment, traffic was inadvertently rerouted to a new service which did not have the correct end points, the traffic had nowhere to go and hence staff discount process was not completed. ","1.As the URL of the new service was very similar to the existing URL the ingress sent customer traffic to the new service – to avoid this, an entirely different URL will be used in future – A confluence will be written and published to the engineering community. Mohan to publish to the appropriate DL’s.	Linsley Green/Gopal Rangarajan	- COnfluence page created - Complete
2.An alert for the staff discount service not completing its journey in either .com or POS needs to be configured.Product backlog item number required for closure - Inprogress
3.Can we set up a heartbeat to monitor all critical services and end points – cloud frameworks and the team discussed on “hearbeat” alternatives and have selected one that is agreed to be robust to be implemented in sprint 3 -(BLP-1139) - Complete
4.The product team believed that this change would be a four wall change – Mohan has confirmed that a change should still be raised with product owner approval.  This communication (Flyer) has been sent by Mohan – this flyer will be refreshed and sent out regularly.	-Completed
5.Post deployment validation in Staging (engineering test platform) and production – a process change has been made now that even if a new module is added to any given service, the existing module will be tested before and after the change.	Linsley Green/Gopal- Completed
6.Laura to check with Sandra Kallay if we can check whether staff discount transactions are processing correctly in Beanstore	Laura Whittle	Product backlog item number required for closure- Inprogress
7.A weekly verification of changes is being carried out by Corporate service owners George and HOT – Sharon. 	George Loizou and Sharon Peters	Completed",2/2/2022,2/4/2022,,,,"No, Yes to be configured",No,No,NA,Unauthorised Change ,NA,"A change was deployed from around 5am on 02/02/2022 which was intended to be a prerequisite for the new discount eligibility service which was supposed to go live at the end of Q4 or beginning of Q1. During the deployment the URL of the new and existing service were identical for the first part of the URL and as a result of this the ingress sent traffic from customers into the new service rather than the existing one.  As the traffic was rerouted to new service which did not have the correct end points, the traffic had no where to go hence looked like the service was down.",NA,NA,NA,2/10/2022,February,2022,Rehan,Closed,,,2022,2,February,9/16/2025,1322,>60
2/1/2022,2/1/2022,88502158,15:19,20:30,05:11,Group Platforms,Finance,Group Platforms,Finance,,SI,GMOR Stock position incorrect,TCS,SAP,,SAP,RC identified,59909,Various departments reported that their stock positions for 01/02 were either over inflated or under stated.  Investigations found that the GMOR stock position as of overnight 31/01 was incorrect. The BEAM daily dashboard and GMOR BO reports were impacted. Tech teams corrected the GMOR Stock position and the overnight GMOR batches were run and BO report refresh completed successfully. C&H colleagues carried out validations successfully.  Root cause investigations found that the start point for the overnight GMOR run was inadvertently set at 08/01 rather than 31/01 as a human error thus resulting in the issue.  Preventative measures have been implemented.   ,"SAP senior management have reviewed user access and have locked out more than 90% of Dev team members from SAP systems and only handful of experienced people are left who can access production.- Complete
Offer display only access for people who need this-configured for ECC and GMOR .- complete
Put in a policy that users require a firefighter ID if maintenance needs to be done – this ID allows the logging of any changes in a more detailed manner.  Firefighter ID requires specific time restrained approval. For example can be provided for a hour, day, week etc.- complete
System date level validation can be configured – comparing the start value of  day-1 and day-2 against the system.- Back log had been raised and it will be tracked under the same - complete",2/3/2022,2/3/2022,,,,"No,No",No,No,NA,Human error - Customer Tech,NA,"""A SAP project team member was checking job variant settings, production volume and runtimes in SAP GMOR to collect data for further development and testing purpose. It has not been confirmed how, but the start point for the overnight GMOR job run was inadvertently set at 08/01 rather than 31/01 thus resulting in the issue.   """,NA,NA,NA,8/9/2022,August,2022,Kavitha,Closed,,,2022,2,February,9/16/2025,1323,>60
2/1/2022,2/1/2022,88501434,10:45,12:07,01:22,GTS,Enterprise Technology Platform,"Supply Chain, Customer Channels","C&H and Intl Supply Chain, Selling Experience",,SI,Control M jobs not starting,Vodafone,Network,,Control M,RC identified,59912,"At 10:45, it was noted that multiple Control M jobs had not started.  The issue occurred as the Control-M server had lost connectivity to its DB server and therefore had automatically failed over services to Swindon node.  This impacted the Dot Com to Donington (DN & OF) messages. Manifest printing issues were reported by Welham and Westfield and clustering issues were reported by Welham and  Stoke. Control M Tech teams failed back the services to Stockley and restored the service by 12:07. The impacted DCs confirmed service restoration.  Vodafone advised that within their relay stations in Brentford they had a signal issue and this is being further investigated.","1.Vodafone require a two hour window within which they will need a 15 minute outage to install the attenuator (signal booster)
Paul, Santhosh and Mohan to liaise to agree the best time for the outage. -Closed
2.Why didn’t the service work from Swindon – the SQL DB failed over successfully to Swindon but the windows cluster tried to fail over to Swindon but couldn’t (most probably due to the continuing network signal issue) and therefore went into a failed state
This is the entirety of the findings and no more can be found hence action being closed.- Closed
3.The number of retries for the Windows cluster to failover to Swindon is set at one – this needs to be reviewed if it can be increased to more than once in six hours.  Other clusters also need to be reviewed for number of retries across the estate. Still being reviewed- Inprogress. 
4.Santhosh working on increasing the severity of SQL alert & introduce Sev1 alert for Control M team- Inprogress. 
They are also looking at options for setting up alerts from ITM which alerts if any job is not running for more than 15 mins on Control M.  Santhosh to also look to see if alerts can be set up for any errors on Control M rather than just for jobs not starting.- Complete 
5.If this issue was due to the signaling being weak on the DWDM, then why weren’t other services impacted?  -Control M, SCOM/SCORCH and Lenel databases have cross-site SP-SW clustering setup. Solar Winds has same site clustering setup. Control M and SCOM/SCORCH databases have witness configuration in Waterside, Lenel doesn’t have a witness in WS, team is investigating if there were any issues between SP and WS. Inprogress. ",2/9/2022,2/9/2022,,,,"Yes, no",No,Yes,Yes,3rd party issue,NA, Vodafone advised that within their relay stations in Brentford they had a signal issue.  Tech teams believe the Control M database failed over from Stockley to Swindon because the DWDM link to Stockley was not working as expected.  Vodafone stated that they need to place an attenuator in the line to help the improve the signal which is being planned.,NA,NA,NA,9/29/2022,September,2022,Rehan,Closed,,,2022,2,February,9/16/2025,1323,>60
1/30/2022,1/30/2022,88497656,03:05,15:39,12:34,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Issue with retail orders after C&H Bradford DR activity,Knapp ,Knapp,,Bradford Sorters,RC identified,59743,"After the DR activity, Bradford C&H DC could not process the retails orders due to a communication issue between WCS (Warehouse Control System) and SRC (Stock Retrieval Control). Knapp engineers identified that SRC was not able to access the VIP and fixed the issue following which the Order timer was run and orders started to flow in to SRC, however the infeed to the sorter was very slow. All users were asked to logout and a restart was done. The site was back to normal at 14:30. Due to this issue 9 Shipments and 60K singles were not sent. Full RCA awaited from Knapp.",NA,2/2/2022,2/2/2022,No,NA,Yes,NA,NA,NA,NA,3rd party change,Yes,The WCS used a wrong IP to connect to the SRC-DB the Cluster IP wasn’t reachable so now the IP from SRC-DB2 is used establish the connection. The SRC-DB servers had to be powered down and up again to fix a hardware issue and still appears to be slow. Knapp advised that the switchover tool was not working properly. This was fixed and the cluster was switched. But the other node was not able to carry the load. This was actually already detected some time ago and subject to be fixed but unfortunately  the BSC technician was not aware due to  a communication gap within Knapp,154826,NA,N/A,2/2/2022,February,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1325,>60
1/28/2022,1/28/2022,88495578,10:58,14:10,03:12,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Parcels booked in today are not showing in Collection app via Honeywell devices,TCS,Sterling support,,Collection app,RC identified,59721,"Multiple stores reported via Service Desk & Yammer advising that parcels  booked in today were not showing on the Collection App via Honeywell devices however, the parcels were showing via desktops on .com+.  As a work around parcels could be marked as collected via .com+ but this was slower and not all stores have easy access to a desktop. The server in Sterling which is responsible of pushing ready to collect messages to the Collection App was unavailable for use after the Sterling release.  67K of ready to collect messages were piled up in Sterling.  To restore service, the server config was restored and the messages were pushed to the Collection App.  By 14:10 the back log of messages were processed and service restored. Root cause being investigated.","1. Correcting configuration file which is responsible for spinning up the Pods- Complete. 
2. Configure a threshold alert for the piled up messages in Sterling  - Complete.
3. Create a Dashboard with number of Pods currently running for all the servers in sterling and generate an alert incase of issues - Complete",1/28/2022,1/28/2022,No,Yes,Yes,"No, Yes",No,Yes,Yes,Customer Tech change,Yes,There were no KUBE CTL pods spinned up in sterling agent servers which is responsible to send the ready to collect message to collection app. This Pods were not spinned as the configuration file which is responsible for spinning up the Pods had a incorrect value due to a manual intervention. ,CM7332,Human Error,N/A,1/28/2022,January,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1327,>60
1/19/2022,1/21/2022,88480776,14:00,17:15,51:15,Group Platforms,Corporate,Group Platforms,Corporate,,SI,Colleagues were over and under paid,SD Worx,SD Worx,,Payroll,RC identified,59738,"Weekly Timesheets approved on Monday 17th January for any additional hours failed within our payroll provider (SD Worx) resulting in 2413 colleagues not receiving their additional hours due to be paid on Friday 21st January. Same-day payments have been sent to those colleagues who were under paid by £50 or more on Friday 21st January (1338 colleagues), those underpaid by 50 or less will be corrected in this week’s pay run. Additionally 559 colleagues who have been overpaid, those funds will be claimed back in this week’s pay run. All impacted colleagues and their line managers are in the process of being notified by Colleague Services. SD Worx continue to investigate the root cause.","To ensure human errors are minimized, the 
CORE Operations team has reviewed their 
existing manual checks and an additional 
resource has been allocated to check the plugin 
completion message for timesheet file loads. In 
case issues/anomalies are identified, the 
Support team assistance will be solicited to 
investigate and remediate.",1/21/2022,1/28/2022,"No, no",N/A,N/A,Yes,NA,NA,Yes,Customer Tech change,Yes," A process was blocking plugin to process as expected. As a workaround, the CORE team restarted the workflow services but the new instance of the plugin was not triggered as the previous instance was still rolling back. After the rollback was completed, all records were set back as unprocessed, hence the monitoring table didn’t have a record for 17/01.  During the regular monitoring, the team assumed that the completed message was for the current date while it was for the previous successful run date of 16/01",154826,Communication gap,NA,2/2/2022,February,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1336,>60
1/14/2022,1/14/2022,88475831,20:16,20:41,00:25,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,"International Flagship -checkout not working,  no orders taken ",Global-E,Global-E,,International Website,RC identified,59577,"Through alerting, the International Support team identified that there was a problem with checkout on International sites which resulted in no orders being taken during the impacted period.","Global-E have completed an immediate “lesson learned” process following this incident. The main consultations are as follows: 
• They will reproduce the DB instance failure and its outcome in a test environment to examine better options to recover from this failure in the future.",1/14/2022,1/18/2022,"Yes, no",N/A,N/A,Yes,"No, Yes",Yes,Yes,Customer Tech change,Yes,"On December 14-1-2022 Global-E had failure hardware on their main Database instance which was caused due to an issue on the AWS side [CASE 9486166141], this led to the DB server crashing unexpectedly and the unavailability of the Global-e checkout.",CM7332,Human Error,N/A,1/28/2022,January,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1341,>60
1/13/2022,1/13/2022,TBC,NA,NA,NA,Group Platforms,Corporate,Group Platforms,Corporate,,KI,The Corporate website inaccessible for a short period of time on 13th January,Group tree,Corporate website,,Corporate ,RC unknown,TBC,The Corporate website inaccessible for a short period of time on 13th January,NA,NA,NA,,,,NA,NA,NA,NA,3rd party issue,NA,NA,NA,NA,NA,1/13/2022,January,2022,Rehan,Closed,,,2022,1,January,9/16/2025,1342,>60
1/13/2022,1/13/2022,88473020,08:41,9:42,01:01,Commercial Trading,C&H Commercial Trading ,Commercial Trading,C&H Commercial Trading ,,KI,SSI application unavailability,SSI,Cloud platforms,,SSI,RC identified,TBC,"On 13th January at 08:41, an alert indicated an increase in the thread counts from one of the SSI (Sales and Stock Intake) pods which impacted the application performance. Tech support performed a clean restart of all the application and ignite pods to restore services at 09:42. 
",The ignite node went down due to which cache of the Ignite pods was inaccessible. There are no logs available to investigate further. Team will work on enabling the logs as parts of their  'Tech Debt reduction' in Q4.,1/17/2022,1/19/2022,Yes,Yes,NA,NA,"Yes, no",N/A,Yes,3rd party issue,N/A,"The cache of the Ignite pods had become inaccessible which caused the user requests to pile up on the application pods, result in high CPU usage and thereby become unresponsive. The team had received an alert for high CPU utilization and restarted the pods (both application and ignite) to fix the issue",N/A,N/A,N/A,1/18/2022,January,2022,Kavitha/Vinay,Closed,,,2022,1,January,9/16/2025,1342,>60
1/13/2022,1/13/2022,88473764,14:16,15:25,01:09,Commercial Trading,C&H Commercial Trading,Commercial Trading,C&H Commercial Trading ,,SI,Customers received confirmation emails for old bespoke school uniform orders,VisionNet,Bespoke School Uniform,,Bespoke School uniform microsite,RC identified,59716,Some customers reported that they received order confirmation emails for old bespoke school uniform orders. It was found that these emails were triggered from the test (UAT) system hosted by vendor VisionNet and the mails triggered were for old orders that customers had previously placed. The issue was caused by some UAT testing carried out by VisionNet. They normally carry out testing on a day to day basis but the email confirmation job always remains disabled. VisionNet was investigating a version conflict in the test environment and orders could not be placed in testing. They resolved this issue and enabled the web job on 13/01 which sends out order email confirmations resulting in order confirmations for all orders that have been used for testing including actual and test customers in the past. 3015 customers were affected by the issue.  Their details were extracted and are being communicated to with an apology by the bespoke School Wear contact centre.  Customers affected were not charged in any way.  Information Security are aware of this issue and are assessing the GDPR/PCI DSS risk once confirmation of the scale of the issue and data types has been received.,"Being handled by Cyber Security and Halford internally
Delete all the production  data in the lower region- complete. ",1/13/2022,1/14/2022,"No, no",NA,NA,"No, NA",No,No,No,Unauthorised Change ,NA,"This issue has been caused by the fact that some UAT testing was carried out by our suppliers VisionNet. They normally carry out testing on a day to day basis but the email confirmation job always remains disabled.  

The issue they were working on today was that they had a version conflict in the test environment and orders could not be placed in testing. They resolved this issue and enabled the web job this morning which sends out order email confirmations. The UAT environment contains actual customer details as well as dummy customer details. When the email confirmation job was enabled this morning it sent out order confirmations for all orders that have been used for testing including actual and test customers in the past.
",NA,NA,Na,2/3/2022,February,2022,Rehan,Closed,,,2022,1,January,9/16/2025,1342,>60
1/11/2022,2/12/2022,88469962,10:34,10:45,23:49,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Collect app not working via Honeywells,TCS,Collection app,,Collection app,RC identified,59714,"Store colleagues were intermittently encountering ‘Service Unavailable’ errors on their Honeywells upon trying to mark orders as collected on the collection app. Comms were sent instructing store colleagues to use .com+ on the store workstations to mark orders as collected, as a workaround. Investigations revealed errors within the app indicating a mismatch of data between the number of customers and shipments within the collection database tables. However, when the records were analysed manually, the number of customers & shipments matched. There was a loss of connectivity with the primary Mongo DB which resulted in a loss of synchronisation between the nodes and a subsequent regular purge job ran which led to the issue.","1. What caused the loss of connectivity issue ith the primary Mongo DB which resulted in a loss of synchronisation - AWS team suspects it should be network blip however they could not identify the reason for connectivity issue - Complete
2. Configure an alerting  for the purge job when it runs/not runs- Inprogress. 
3. Manual monitor put in place to monitor the  purge job compeletion - complete 
4. In terms of DB syncing, there is no way to monitor this with the version of Mongo, So either utility has to be built to be able to monitor it or migrate the DB over to next mongo version mongo atlas. Dev Team would prefer to spend the effort migrating DB to Mongo Atlas and PO is reviewing possibility for this. - In progress",1/11/2022,1/11/2022,"No,Yes",No ,NA,"No, Yes",No,No,No,Customer Tech issue,NA,There was a loss of connectivity with the primary Mongo DB which resulted in a loss of synchronisation between the nodes and a subsequent regular purge job ran which led to the issue.,NA,NA,NA,2/3/2022,February,2022,Rehan,Closed,,,2022,1,January,9/16/2025,1344,>60
1/7/2022,1/7/2022,88464206,08:00,9:05,01:05,Group Platforms,HR,Group Platforms,HR,,SI,My HR not working via Teams,TCS,My HR Devops,,MyHR,RC identified,59684,"Users reported that MY HR was not accessible intermittently via Teams.  A work around was available as the direct URL was working and a comms was sent to users to this effect.  A feature change was carried out to implement single sign on for My HR on Wednesday 5th January at 18:00.  This change was implemented to allow the MY HR chat bot to work through single sign on (SSO) in Teams and have the users profile details available when using the app.  The My HR chat bot is still in development and has not yet gone live. 
 
The Workplace Tech team replicated the issue in the lower environment and identified that there was an issue with the configuration of the SSO and as such a decision was made to revert the change and go to My HR Teams version 1.0.7 which disabled the SSO.  Some users received the update and confirmed My HR to be accessible via Teams.
","Saran is continuing testing to ensure a fix can be developed asap and rolled out to a limited group of users after a change has been raised.  _complete  on 09/02/2022

Project team needs to understand the Customer change process to prevent this occurring again. They need to ensure they hold discussions on a regular basis prior to implementation to ensure everyone is on the same page with regards to understanding what is being implemented - Feddin/Kirsten

Review implementation plan and communication process to come up with an improved plan for the digital assistant pilot - Feddin/Kirsten

Change Management to contact all the resources related to this change and walk through the change process - Mohan





",1/7/2022,1/17/2022,"No, no",No,Yes,Yes,"No, no",Yes,Yes,Unauthorised Change ,Yes,"A change was implemented to allow the MY HR chat bot to work through single sign on (SSO) in Teams and have the users profile details available when using the app.  There was an issue with the configuration of the SSO which resulted in My HR being inaccessible via Teams.  Further investigations are continuing.
As part of the change, the SSO version should have been released to two users who were then to be contacted for regression testing in Production.  Instead the change was rolled out to all users.  There was a distinct lack of co-ordination (walk through) between the various resources related to this change.
",NA,Human error,NA,1/17/2022,January,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1348,>60
1/7/2022,1/7/2022,88463754,01:45,6:06,04:21,GTS,Enterprise Integration,Supply Chain,Foods Supply Chain,,SI,Delay to the Foods Bradford NDC allocation,TCS,Integration,,Quantum,RC identified,59702,"The Quantum job responsible for loading the product location data to the Q core table failed as the product location file was corrupted.  Tech teams used the previous day's product location file and re-ran the failed job to successful completion. 95% of Bradford orders interfaced into WMS by the 06:00 SLA and remaining orders interfaced by 6:06.  The corruption occurred as an alert was being introduced in case a corrupt network schedule file was received into DataStage. As a part of this change, the category field length was set as 2 instead of 4 causing the file length to be shortened in the DS output file, therefore the job responsible for loading the product location failed.","The issue was fixed by updating the correct column length the day after the issue - Closed.
Validation and testing process has been reviewed and has been internally communicated - closed

Ratnam is creating one pager Confluence page to ensure the validation process is recorded and centrally stores - Complete ",1/7/2022,1/18/2022,Yes,Yes,Yes,Yes,Yes,Yes,Yes,Customer Tech change,Yes,"In the past, a network schedule file would be received from the source, if the file was corrupt it would still get processed as the systemic job truncated and continued with the job and the appropriate data would not be passed on to the downstream systems.  To avoid this the teams decided to ensure by systemic change that the DS job should fail at source and the field length change was deployed, however the field length should have been 4 but was set as 2 in the Network Schedule File due to human error.",153552,Incorrect deployment plan ,N/A,2/2/2022,February,2022,Kavitha,Closed,,,2022,1,January,9/16/2025,1348,>60
12/22/2021,12/22/2021,88443784,08:40,16:00,07:20,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,"Stores reporting intermittent issues with CFTO Collection, Scan & Shop and Pay With Me",TCS,Colleague Devices Efficiency,,QR code for CFTO collection.  Scan & Shop and Pay with Me,RC identified,TBC,"Stores colleagues were encountering ‘unable to create basket’ error after scanning the customer QR code on the CFTO collection app. Tech teams observed high CPU utilisation due to a surge of 13000 sessions on the shared VOD (Vison on demand) database, this reduced without intervention at 09:15.  The issue reoccurred at 10:00 until 10:15 when stores again reported issues and we observed spikes with the CPU.

 At around 14:20, further pod restarts were observed and Tech colleagues helping in stores this week also reported further issues.  At 14:40 and 16:00 we implemented the holding page for iOS and Android Scan and Shop Mobile respectively and placed a message on the CFTO collection app to use a belted or SCOT till if issues are experienced.  At midnight the number of core CPU’s on the VOD DB server were increased from 16 to 64.

Since the afternoon of 22/12, pperformance and CPU levels for CFTO collection QR creation and Scan & Shop (Stores) within the shared VOD database remains positive and stable.  The Scan & Shop Mobile holding page was removed on Monday 27/12 and teams monitored the performance and could not find any issues.  Detailed root cause investigations with our teams and vendor Flooid continue.","Caching – In prod we saw a greater level of SQL queries interrogating the DB compared to the same load when we ran the queries in the lower environment - Jenny Mueller, Emma Holmes, Sneha Das. Update: Scan and Shop Mobile, Scan and Shop, Pay with Me, Digital Café and CFTO team to advise what retry logic they are using in Prod and what retry logic was used in the testing profile - Only Scan & Shop mobile had high CPU and only that application had a retry loginc in prod and that was removed.  Now in Oct 2022 the prod and non-prod environments are the same for the retry logic - Closed

In the lower environment this issue was not observed when we tested through load – can the load testing be improved?  Is the correct profile of the test load matching what happens in Prod? For peak 2022 all client apps have been asked to revise their profiles so that the prescribed load can be matched in testing.  Closed

Was the load for peak tested with a realistic peak profile? What should the growth plan have looked like during peak for the CFTO App, Scan & Shop Mobile, Pay with Me? Did we have visibility of the correct growth profile? - Peak 2022 the load is now matched to prod - revised profiles.

Command and control for comms being reviewed under the Peak PIR - Closed

We await further investigation results from Flooid – they believe that there is room from improvement in the VOD applications and they have advised that the latest updates will address some of the gaps - Oct 2022 VOD performance was improved after the incident - the caching mechanism was improved as Flooid provided a release.",12/22/2021,12/22/2021,,,,Yes,No,No,Yes,Customer Tech issue,No,"There are three aspects to the root cause of this issue:

1)	Increased retry logic in production for scan & shop mobile
2)	Caching – In prod we saw a greater level of SQL query interrogating the DB compared to the same load when we ran the queries in the lower environment.
3)	Items 1 and 2 led to CPU spikes and resulting in the pods restarting causing the intermittent issues.  ",NA,NA,NA,10/4/2022,October,2022,Rehan,Closed,,,2021,12,December,9/16/2025,1364,>60
12/22/2021,12/22/2021,88445029,12:05,12:32,00:27,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,"WMS inaccessible on workstations and packing benches at Donnington, Ollerton and Thorncliffe",TCS,WAS admin,,WMS,RC identified,59413,"WMS was inaccessible on workstations and packing benches at Donnington, Ollerton and Thorncliffe between 12:05 to 12:30. The root cause was attributed to the WAS JVM upgrade to address the Log4j vulnerability. Support had to revert this as the services did not come up properly after the upgrade. The impact was minimal considering the timely resolution and the picking operations was working with their HHT's during the above times. ","Further testing needs to be carried out in the AIX environment to ascertain why this patch caused issues in this instance.  Compatibility with New relic Jar with WAS 7 needs to be reassessed before we go again - Ravi/Murali Irrinki

The log4j patch needs to be reapplied to address the vulnerability and this needs to be planned and a date agreed - Elizabeth, Ravi, Colin & OP

",12/22/2021,12/22/2021,12/22/2021,,,"No, NA",No,No,No,Customer Tech change,Yes,The root cause was attributed to the WAS JVM upgrade to address the Log4j vulnerability. Support had to revert this as the services did not come up properly after the upgrade,CRQ153225,Not tested in AIX layer,NA,7/18/2022,July,2022,Kavitha,Closed,,,2021,12,December,9/16/2025,1364,>60
12/21/2021,12/22/2021,88443881,22:51,6:09,07:18,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Delay to the Foods Bradford NDC allocation,TCS,Quantum DevOps,,Picking and packing operations,RC identified,59415,"At 22:51 the job responsible for importing data into the Quantum main tables failed with an unique constraint error due to an existing STG (store trade group) as it had no store associated to it.  As a workaround, support removed the existing STG which then allowed the job to process with a new STG at 23:57.  This caused a one hour delay on the critical batches resulting in Bradford NDC allocation completing 6:09, 22/12.  95% of the orders were sent to the Bradford DC within the SLA of 06:00. The permanent fix and root cause analysis are being worked upon by the Devops team. ",The permanent fix has been deployed on 23/02,12/22/2021,12/22/2021,Yes,Yes,Yes,Yes,Yes,Yes,Yes,Data issue,NA, The Q job failed with an unique constraint error due to an existing STG (store trade group) as it had no store associated to it.,NA,NA,NA,2/23/2022,February,2022,Kavitha,Closed,,,2021,12,December,9/16/2025,1365,>60
12/17/2021,12/19/2021,88434230,10:00,5:20:00,19:20,Group Platforms, Finance,Group Platforms,Finance,,SI,EDW Day - 1 reports delayed,TCS,SAP,,EDW Day-1,RC identified,59323,"Tech support teams noticed that the 16/12 and 17/12 Day-1 flow was delayed. On 17/12 they found that a VAT job had been initiated which caused systemic contention resulting in a significant delay. The contention was caused because the VAT job and the day-1 job interrogates the same table. The job was terminated at 09:30 on 17/12 and our teams reran the Day-1 job at 09:40. The day-1 job again was running slowly and we found that the VAT job had again been restarted. Support teams stopped the VAT jobs at 14:55 and the Day-1 job was rerun from 15:05. SME’s created a full plan for the day -1 batch for this weekend, along with the golden reports batches as attached. The day -1 flow for 17/12,18/12 and 19/12 completed successfully at 07:16, 12:04 and 08:58 respectively. The Golden Reports were available to users since 16:16, 19/12","A decision is to be made with the Finance business if a BW report can replace the actual VAT job, as a workaround. If not, then what are the other options to be undertaken to prevent the job to overrun in future: Not feasible. However, meeting were held to ensure the simultaneous runs are not happening in future.",12/17/2021,12/19/2021,12/21/2021,TBC,TBC,Yes,Yes,No,TBC,Human error - Business,,The adhoc run of the VAT job by a finance user contended with SAP GL extractor job and caused it to overrun.,,,,5/5/2022,May,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1369,>60
12/14/2021,12/15/2021,88432137,14:00,19:45,29:45,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Customer.com - Interruption in Cancellation Emails going to Customers,TCS,Sterling and EM product team,,Customer .com website,RC identified,59146,"Customers and colleagues reported that they were not receiving cancellation emails. Investigations revealed that the emails were getting piled up in the SalesForce queue. The job responsible for processing the emails was stopped and restarted by SalesForce which cleared the backlog. However, out of 39.2K emails in backlog, 14K processed fine and 25.5K failed.  The failure is believed to be due to the 72-hour expiry time set for these emails. SalesForce recovered these failed mails on 15/12 and these got delivered customers successfully.  A count mismatch was identified for 40k partial orders cancellation emails which was recovered by Salesforce to be delivered to the customers successfully.","The email alerting was already in place, however, was not configured to trigger to the correct recipient. The alert ha now been coverted into a Remedy ticket which will be triggered key SMEs and EM.",12/15/2021,12/23/2021,12/23/2021,12/24/2021,12/24/2021,"No, configured",No,No,Yes,Design issue,NA,"The job that sends the cancellations emails failed as it received six payment types for an order whereas, the cancellation email template can handle only one payment type. On the other hand, Sterling is designed to send only one payment type, the reason behind sending six on this occassion is currenly being tracked under a backlog",NA,NA,NA,1/13/2022,January,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1372,>60
12/14/2021,12/15/2021,88431495,14:50,0:00,07:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,ADHOC,Some stores unable to print ISF labels,TCS,SIT (Software  Implementation Team),,Store-Pick-Pack-Despatch,RC identified,59240,"SPPD is the Store-Pick-Pack-Despatch Application which our store colleagues use to Pick and Pack the In-Store Fulfilment orders. A number of store colleagues reported an error message presented on their screen when attempting to print labels ready for dispatch “Unable to connect to Printer Service”.
 
In the process of fixing an issue with My HR , My schedule and another application which are hosted on Windows 10, the root certificate for SPPD printing was removed inadvertently. The root certificate was redeployed via a script to all Windows ten workstations whether they are attached to a SPPD printer or not.  We have called several large stores with SPPD printers such as Liverpool and Cheshunt and they have confirmed they are not impacted. 15 stores have reported the issue and Impacted stores confirmed that they are able to print the ISF labels successfully.",Rosie and Mohan to discuss with Phil G tocheck on wat basis this change was performed and will also enforce that any fixes (even the critical ones) needs to be approved by the service owners and Change Management: Closed,12/14/2021,12/14/2021,12/14/2021,12/14/2021,12/14/2021,No,No,No,Yes,Human error - Customer Tech,Change ,"Manual error - In the process of resolving an issue with three applications, inadvertantly, the certificate for the SPPD printing service was removed in error",Inc: 88429281,,,12/17/2021,December,2021,Saloni,Closed,,,2021,12,December,9/16/2025,1372,>60
12/10/2021,12/10/2021,88424917,03:00,10:04,07:04,Group Support, Finance,Supply Chain,"C&H and Intl Supply Chain, Foods Supply Chain",,MI,Order processing slowness at SAP delayed Foods NDC Allocations and D&F orders,TCS,SAP,,WMS,RC identified,59239,"The SAP job responsible for processing the orders of the critical batches: D&F orders, Foods Bradford and Milton Keynes NDC orders was taking longer than usual. Investigations revealed that the job was not processing the entire volume of orders picked in a single run and those were getting processed only in the consecutive runs, which caused the delay. A very high severity case was raised with vendor SAP who identified that the orders were not processing as expected as the number range had reached its highest limit in SAP. A solution has been identified to add a new number range in SAP which will allow the orders to get processed as expected.
 
Foods Bradford and Milton Keynes allocation SLA of 06:00 has been missed. There was a delay in sending the D&F orders to Welham Green, West Thurrock and Swindon DCs, 95% of the orders were sent to the DC before the SLA of 05:00.","Teams are checking all number ranges for all type of idocs to ensure they are part of the alerting for the appropriate utilisation thresholds - Completed

Teams have checked all number ranges for their individual current utilisations and have only found one which was more than 90% used (but had still not reached its alerting threshold) - Closed

Discussions are being held with SAP regarding the “turning off” of the wrap around feature and what the pros and cons of such an action could be - Completed: If we switch off wrapping, then the number range will run out but we will avoid slow downs",12/10/2021,12/20/2021,,,,Yes,Yes,No,Yes,3rd party issue,,"During the processing of the idocs within SAP, once the system reached the end of the STO number range, the “wrap around” feature - which is designed to allow the system to go back to the beginning of the number range – causing the jobs to run very slowly as the system had to find unutilised numbers within the fragmented number range to process the idocs.

Some context:
It is normal for a number range to be fragmented and therefore not all numbers were being used for the processing of idocs in sequential manner – this is due to the use of parallel processing for each job. ",,,,5/5/2022,May,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1376,>60
12/8/2021,12/8/2021,88422146,14:30,18:15,03:45,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Ollerton Network Issue,TCS,Enterprise Technology platform,,Picking and packing operations,RC identified,TBC,"At 14:39, there was a significant spike in outbound traffic on Ollerton's primary WAN router. This impacted normal DC operations such as picking/pack/receiving and processing of refunds. Stores and end customers were not affected. The team identified a network loop between two ports in a stack that were causing a “network storm”, generating a lot of traffic. This looping started at 14:39 and the port was disabled at 17:51. Ollerton was back to BAU from 18:15 after a full restart of their devices. More detailed root cause investigations underway.","1.How can loops be prevented? If a cable was bridging two switches in a DC, the switches would detect this and automatically disable the port and trigger an alert – this feature is called Spanning Tree.: In progress
<< Paul Beasley/Kamal Paul to analyse if this can be turned on in Ollerton – Paul to liaise with Kamal – look at the design/costs and what is required.>>
2 Can any alerting be set up for finding consistent network storm:  Rajesh: Not Feasible
<< Network is so busy in a storm that it won’t even have capacity to alert - CLOSED
3 Update NOC SOP’s to guide support to look for loops and network storms in case of any similar issue in the future : Rajesh: In progress
4 The PC that was utilizing a lot of network bandwidth still needs investigating : Matt/Michael: In progress
5 Review business comms for Ollerton – why was there a lack of business comms for this issue. Who is supposed to on the comms: Colin: In progress",12/8/2021,12/14/2021,12/14/2021,,,,,,,Human error - DC Operations,,A colleague had an issue with a printer on one of the postal desks. They have tried to rectify the fault by having a look at the cables to see if it was plugged in correctly. They have spotted a dangling cable and assumed it needed plugging into the network socket. Socket 138 was then plugged into socket 140 causing the network traffic increase.,,,,10/18/2022,October,2022,Sravan,Closed,,,2021,12,December,9/16/2025,1378,>60
12/6/2021,12/6/2021,88417045,01:40,5:26,03:46,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,MI,Delay in the overnight D&F batch,TCS,Technology Services,,WMS,RC unknown,59237,"At 22:30, BY reported that the files required for the overnight D&F processing had not interfaced into their landscape. Investigations revealed that a connectivity issue between the middleware VMs and the load balancer was preventing the files from getting transferred to BY. The load balancer was then bypassed, after which the files were transferred to BY successfully. The D&F orders were sent to the DCs a delay. A case has been logged with Microsoft to understand the root cause for the connectivity issue.","1.	Root cause with Microsoft – investigate any issues with the SFTP VM’s: Prathyush: RCA inconclusive. Microsoft could not find anything upon sending all possible logs.
2.	We have alerts for late processing but not for if the VM’s network route is not working.  Investigate of there is any possibility of creating alerts if there is no connectivity.: Alerts configured. 
3.	Review the alerts on the AnR dashboard and see if the alert timings can be made earlier/optimized: Shankar/Shan : <<Currently reviewing with A&R dashboard team on alert modifications>> Configured Amber alert.
4.	Look at a way to improve the engagement of support teams in case of any issues with BY overnight to include portfolio and integration teams: Nirmal/Shan/Raj : Closed: <<As per Service management the process has been executed multiple times.PCM to callout Middleware team on time.>>
5.	Review the process of extracting and sending the required files over to BY manually.  All SME’s should be trained in how to do this and have access to the relevant shared paths.  This should be done as a priority if initial investigations cannot find anything obvious.: Completed",12/7/2021,12/7/2021,,,,"No, Yes",Yes,No,Yes,RC unknown,,"In the D&F flow, there are two SFTP VM’s – under the internal load balancer.  The outbound connectivity from the VM towards internet is through PA FW.  This route stopped working.  Support found no obvious issues from our end.  Support removed both VM’s from the LB and adding them back and then the traffic started flowing.

Microsoft could not identify the root cause. RCA inconclusive",,,,5/5/2022,May,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1380,>60
12/3/2021,12/3/2021,88413089,01:00,7:49,06:49,Group Support, Finance,Supply Chain,"C&H and Intl Supply Chain, Foods Supply Chain",,MI,Overnight critical batches on 03/12 were delayed,TCS,SAP,,"WMS, SAP",RC unknown,59144,"At 00:34, it was observed that the FCO orders had not processed in SAP. Investigations revealed that around 60k messages had piled up in one of the queues in the SAP ECC system. The overnight critical batches completed with a delay which impacted the picking operations across C&H and Foods DCs.  The Foods Final orders and Customer orders were sent to the suppliers with a delay.  A sev1 case has been raised with vendor SAP to identify the root cause.","1.Root cause investigations continue with our teams and SAP vendor – can SAP provide further permanent fixes or work around suggestions? Remko: SAP did not provide a specific root cause, but confirmed it would be useful to progress additional load balancing of the jobs which is completed
2.Check if multi-tray messages can be assigned to separate queue as they are for stores and should not contend with DC despatch critical flow.  This is a longer-term improvement and depends on the results of the investigations from SAP vendor.: Karthik Srinivasan: SPM-9572- Platform Ops backlog Q1 2022 - SMQ2 Recommendations implementation
3.To explore the possibility to increase work processes so that adaptor interfaces have more processing power. This is a longer-term improvement and depends on the results of the investigations from SAP vendor.: Karthik Srinivasan: The deployment to production is planned through CRQ000000158652 - SMQ2 Queue Prioritization in upcoming May release (ie 20 May): Completed
4.Ensure that the FFO and Bradford dashboard gives us the correct data during batch delays beyond the SLA’s: Sakthi: Completed",12/3/2021,12/3/2021,,,,Yes,Yes,"Yes, Yes",Yes,3rd party issue,,"The adaptor interfaces were processing slowly. The slow progress impacted the processing of messages which caused the SLAs to be missed by 1.5 hrs (FCO, FFO) and 45 mins (A&R, Bradford, Milton Keynes). RCA is in progress with SAP.  OSS open with SAP.",,,SPM-9572-,5/5/2022,May,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1383,>60
12/3/2021,12/3/2021,88412699,09:20,10:20,01:00,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Slow retrievals at ECB impacting DC operations at Donington,Schaeffer,N/A,,WCS ,RC unknown,59142,"From 23:00, Donington observed slow retrievals at their ECB (Ecom Order Consolidation Buffer) and this was impacting picking and packing capabilities at the DC. The engineering and Tech teams were engaged to investigate the issue however, no abnormalities were found. The ECB retrieval at Donington was returned back to BAU from 10:20 and the DC resumed their picking and packing activities at full capacity. RCA is underway",NA,12/10/2021,NA,NA,NA,NA,No,No,No,No,3rd party issue,NA,"No issues identified from Customer IT. As per the design at SSI end, there are not enough logs to identify the root cause. RC inconclusive.",NA,NA,NA,12/12/2021,December,2021,Saloni,Closed,,,2021,12,December,9/16/2025,1383,>60
12/1/2021,12/1/2021,88408983,07:00,12:00,05:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Customer.com - Sparks Advent Calendar not showing for existing customers,SalesForce,Sparks,,Customer .com website,RC identified,58952,"The Sparks Advent Calendar campaign was due to go live today for all Sparks customers, however, teams while validating the offers noticed that they were only live for new customers - the offer was not available to existing customers.  When the scoring data was checked, it appeared that the date was set up without a time and the time defaulted to 12pm  (midday) rather than midnight.  A decision was taken that as there is not enough time to re-score, re-scoring will not be carried out. At midday, teams confirmed that the Sparks Advent Calendar offer was showing in both Android and iOS apps as expected. Re-scoring of days 2 – 12 has taken place to take account of the fact that the time has been corrected to midnight rather than midday. Teams will continue to monitor for another 24 hours.",The configuration has now been changed so that it defaults to 12am.,12/1/2021,12/1/2021,12/1/2021,12/1/2021,12/1/2021,No,No,No,Yes,3rd party issue,NA,The issue was caused by a configuration in Saleforce which defaults to 12pm start time if only start date is given and no start time is specified. ,NA,NA,NA,12/2/2021,December,2021,Saloni,Closed,,,2021,12,December,9/16/2025,1385,>60
12/1/2021,12/1/2021,88410229,12:15,12:40,00:25,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Customer.com -Sterling Order Management System Unavailable,TCS,Sterling,,,RC unknown,59039,"Support received alerts that there were multiple hung threads in the Sterling Order Management database and during quick health check the application was not accessible. This impacted stores raising assisted orders.  Contact Centre would have been unable to access thin client to amend orders.   Ollerton was unable access application to process returns.  There was a possible delay in the order flow to Donnington.  Initial analysis showed that there were multiple hung threads followed by all web and application servers going down.  To restore the service, teams cleared the hung threads by restarting the servers and initiating new instances.  The application has been stable since 12:40 this afternoon. No further issues identified.  Two separate cases have been raised with Microsoft to understand the cause.  Initial analysis points to possible network issues which is further being investigated.  Full root cause will be followed up under a problem investigation.","2112010050001672 - Case raised for DB issue
2112010060003057 - Case raised for checking AKS Cluster",,,,,,,,,,RC unknown,,Awaiting details from Venkat and Hisham,,,,5/5/2022,May,2022,Saloni,Closed,,,2021,12,December,9/16/2025,1385,>60
11/27/2021,11/27/2021,88403795,03:45,5:08,01:23,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Potential Delay to overnight Foods Flows,TCS,Quantum DevOps,,Bradford and MK NDC allocation,RC identified,58941,"One of the jobs in the Quantum batch which is responsible for loading the “actual life” file into Quantum database overran since it was using an inefficient execution plan. The job was hence bypassed to allow the overnight critical batches to proceed. The overnight Foods batches were delayed by 1.5 hours due to this issue. Later, it was observed that the NDC allocation messages had successfully processed out of ASO but were stuck in middleware as the ASO queue manager had crashed. The queue manager was then restarted to resume the message processing. The allocations for Bradford and Milton Keynes reached the NDCs by 06:33 and 06:21 respectively. GIST had also reported GR failures due to this issue and were advised to perform manual allocations to avoid any impact.","1. The actual “life job” was slow running as the scrub job was not working properly and hence over running with high memory utilisation.: Swagata Hazra /Dipti: Closed << Stats would not improve the plan>>
2. A backlog ticket has been raised to Quantum DevOps to create an alert if there is a delay in the starting of the “Q” processing job.: Swagata Hazra: Closed <<Alerting is now in place in production>>
3. The channel process had stopped and a case has been raised with IBM to find the cause: Raj Subramanian: Closed <<The process (MQ fastpath) that’s run the Channel process went down due to resource constraint in MQ POD.  IBM suggested to revisit resources – we have increased memory from 4gb to 8gb for the MQ POD. >>
4. Analyse a method to prevent memory over utilisation on MQ PODS: Lakshman: Closed
<<The reason why memory was being over utilised due to high number of long running connections from ASO application to ASO queue manager. In non-prod we have reduced max number of connections from 6 to 3 – we need to get the results of this testing and when and if it can be implemented in PROD>>
5. During any similar issue, all the layers in the middleware should be checked – on-prem, cloud etc.  A KT session should be run and the SOP needs to be amended and user access for all key SME’s needs to be verified.: Raj Subramanian – closed <<All access is there but resources had power cut in Chennai >>
6. Validate all middleware cloud component alerts in the foods cluster to ensure they are correctly assigned to portfolio or integration devops teams: Raj Subramanian: closed
<<Validation complete and all alerts have been rerouted to Foods support.  KT session with Foods team on completed.  Further ones to continue but action can be closed.>>
7. The efficient plan has now been pinned but eventually it will be inefficient and at that time the pinning can cause an issue - Kalidossan/Dipti: Closed
<<Rob Barnes has asked for periodic checks.  Analysis how often plans should be checked, how they should be check and what action should be taken.: Q product team confirmed that this plan is fine and no action required>>",11/27/2021,11/29/2021,11/29/2021,12/3/2021,12/3/2021,Yes,Yes,No,Yes,Data issue,,"Issue 1. The query was picking a different (inefficient) plan on the problematic day due to high number of data/rows in the table
Issue 2. In addition, the middleware channels dedicated to the ASO queue has stopped. BM confirmed that the channel had stooped due to a memory constraint. The memory of the pod has been doubled to prevent the issue from reoccurring.",,,,12/5/2021,December,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1389,>60
11/26/2021,12/2/2021,88402475,09:45,15:45,150:00,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Performance issues on SPPD impacting In-Store Fulfilment,TCS,ISF,,ISF,RC identified,58943,"At 09:45, stores started reporting that the In Store Fulfillment (ISF) Packing via the SPPD (Store Pick, Pack and Dispatch) application was slow. The technical teams observed that the number of picks and packs had increased due to the ISF plan being used and that the slowness was caused by packing being performed in parallel by multiple stores. A staggered restart of the application was performed at 16:00 in an attempt to fix the issue, however, some stores with larger volumes left to pack were still reporting slowness. This was expected since these stores had large volumes of orders on the pack screen which took longer to load. Three stores reported slowness with their packing screens over the weekend. On 29/11, the API Response was high throughout the day and 15 stores reported slowness during packing process. Impact: Around 2.5k orders that were supposed to be dispatched on Friday (26/11) were mis-promised 

 

On 30/11, a database configuration change was implemented along with the upliftment of the network bandwidth to manage the order volume. Stores that reported issues previously observed improvement in the packing process and the API response times was under the threshold throughout the day. Team continue to monitor the services for stability. ","As the SPPD application and it's infrastructure is very old, the next step is to migrate the application to Azure V2. This has been agreed with the Product Owners and business.",12/2/2021,12/5/2021,12/5/2021,TBC,TBC,No,No,No,No,Infrastructure issue / Hardware failure,,"We upgraded the network address translation instance which increased the network bandwidth for outbound SPPD traffic, this was the bottleneck hence the root cause.",,,,,January,1900,Saloni,Closed,,,2021,11,November,9/16/2025,1390,>60
11/26/2021,11/26/2021,88402637,08:50,17:00,08:10,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Customer.com Flowers PLP not showing full range of products,TCS,Bloomreach Support,,Customer .com website,RC identified,59021,"Business teams reported that the Flowers PLP were not showing the full range of products. Normally there should be around 70 products but there were only 12 products showing.  By 12pm the products were republished for a second time and they were present in the delta feed. The feed was sent to Bloomreach and the products were indexed.  After the Bloomreach indexing was completed and cache cleared, the full range of flowers was showing on the PLPs . Service restored at 12:45. Root cause investigation continues","Fix the SCL code to handle exceptions: In progress
Alerting has been configured to notify the exceptions: Completed",11/26/2021,11/29/2021,11/29/2021,11/29/2021,11/29/2021,"No, configured",No,No,Yes,Code/Product bug,,The issue was caused due to a bug in the SCL code to handle exceptions getting add-on products. The routing rules in Azure V2 have also contributed to this issue which is being investigated by Product Assembly team and Network team.,,,,11/29/2021,November,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1390,>60
11/24/2021,11/26/2021,88399545,07:30,15:30,08:00,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,SRD inaccessible due to infrastructure issue with BY,Blue Yonder,NA,,SRD,RC identified,58944,SRD was inaccessible from 07:30 due an infrastructure issue with Blue Yonder (BY) - the certificate authentication (CA) at BY was unable to generate the certificate for the BY Citrix server. This in turn caused the SRD application to be inaccessible for Customer users via SSO (Single Sign-On). BY implemented a workaround to bypass the SSO in the afternoon and later fixed the SSO issue in the evening. BY to share the detailed RCA.,"Reinforced customer communication aspect of BY's incident management processes - Completed
Enhancing alerting and modification of the core data center firewall(s) logging levels within the data center -Completed
",11/24/2021,12/13/2021,,,,"No, NA",No,Yes,No,3rd party issue,NA," Cisco (BY's vendor) confirmed that the service outage appeared to be the spanning-tree loop condition caused by the failure within the AIX service component. The offending device was isolated from the environment as part of the service restoration process to mitigate the potential of further risk. The nature of the failure resulted in a ripple effect throughout the network affecting a number of critical service components within the data center, ultimately impacting the solutions and services hosted in BY's Dallas Data Center.",NA,NA,NA,12/13/2021,December,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1392,>60
11/16/2021,11/16/2021,88385996,09:24,10:29,01:05,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Donington and Thorncliffe  unable to print labels for direct orders,Sorted,NA,,Label printing,RC identified,58884,"From 09:24, Donington and Thorncliffe reported issues with label printing for direct orders. Vendor Sorted was engaged to investigate and the issue got auto-resolved at 10:29. Investigations revealed that there was a sudden increase in the number of connections on the Security database at Sorted which caused the issue. From 12:00, the packing service was switched to Metapack as a proactive measure which had already been planned prior to the incident.","Continue root cause investigations as we don’t know the exact cause of the number of connections reaching maximum on the Security DB.: Multiple actions are being taken: In progress
1. Sorted to increase their securiy DB 
Sorted stated that another of their customers did have extensive polling and this needs investigating: Iain: Completed
<<The traffic was another customer sending repeated requests for tracking data. A short time fix is in place. Long term action will be picked up Sorted with the customer>>

Are the alerts obvious and set as sev 1’s?  Why did Sorted initially say they couldn’t see an obvious issues at their end? Iain: Completed

Sorted to create a method to refresh the authentication token within the cache manually rather than have to wait for one hour.: Completed
<<The change to increase the available workers has been implemented - The number of VMs was increased from 20-30, each increased from 2 to 6 cores. The elastic pool database server was increased from 6 cores to 12 cores, and the per-database use of cores increased from 2 cores to 10 cores. This increased the available workers to 1260.>>",11/16/2021,11/22/2021,11/22/2021,11/24/2021,11/24/2021,"No,Configured",No,No,Yes,3rd party issue,,"Sorted advised that at 09:22 they had a spike on their Security database since on of their customers were sending an abnormal number of requests to Storted. This led to all the “workers” being consumed for two minutes.  Sorted believe that this spike then led to the JSON web token becoming corrupted. The API management system refreshes its cache every hour and hence, services were restored after one hour exactly.",,,,11/24/2021,November,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1400,>60
11/3/2021,11/3/2021,88367295,08:24,11:54,03:30,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,MI,Customer.com made-to-order furniture (T39) not showing on the website,TCS,Sterling/ Bloomreach,,Customer .com website,RC identified,58847,"Business teams identified that most of the T39 products were missing from the website and support teams confirmed they were missing from the Bloomreach (BR) feed.  In the interim measure until the T39 products were back online, an Adobe Target holding page  (“Please Bear With Us”) was put in place on the impacted PLPs.  As a workaround, business teams did an emergency publish (EP) for all T39 products (447 strokes) and new feed was created and sent to BR which completed at 11:30.  Cache clears were then carried out and once completed, the products reflected on the website and the holding page was removed.  As the root cause was not clear on Wednesday, hypercare of nightly jobs was put in place.

Teams identified that products were again missing in the Thursday’s feed and immediately took remedial actions to avoid impact.  On Thursday the team identified that the root cause relates to some out-of-stock scenarios and the fix was deployed on Thursday night.  Since the fix, the issue has not reoccurred.  The code which was fixed by the FIND team had not changed last ~15 months, so a PIR was held with the upstream systems Sterling and WCS and it was found that Sterling had sent the zero stock which triggered the bug.  Sterling sending zero stock is related to changes for Bradford SOD in the release on Tuesday morning. A config change has been done to fix the issue in Sterling.","Bug in SCL code did not handle invalid data scenario: Fixed 
Refactoring the SCL code would help prevent this type of bugs ever surfacing. By simplifying the code, removing repetition, and applying the correct patterns, the code will be much easier to test and coverage will increase.: Scheduled in next sprint",11/3/2021,11/6/2021,11/6/2021,11/7/2021,11/7/2021,No,No,No,Yes,Code/Product bug,,There was an issue with inventory values being sent from Sterling to WCS for the Ireland website for the T39 department. The inventory value was being sent as 0 for IE rather than the correct inventory value. This then resulted in the application that processed the nightly feed for bloomreach to discard both the UK & IE products due to a bug in SCL for the scenario where they receive 0 stock IE inventory single UPC furniture products.,,,,,January,1900,Saloni,Closed,,,2021,11,November,9/16/2025,1413,>60
11/2/2021,11/2/2021,88366159,09:15,14:00,04:45,Customer Channels,Service Experience,Customer Channels,Service Experience,,SI,Customer.com Sterling orders processing slowly,TCS,Sterling,," Demand Notification (DN), Operational impact to Donington, suppliers and ISF stores  ",RC identified,58726,"At 09:15, alerting indicated a pile-up of backlog of order fulfilment (OF) messages in Sterling. The Demand Notification (DN) messages were processing at a slow rate as well. This issue caused the order flow to Donnington, suppliers and ISF (In-store Fulfilment) stores to be slower than usual, impacting their operations. The teams confirmed that the Sterling release deployed this morning was causing the slowness in processing. The Sterling release was then reverted on specific servers by 12:15 to fix the issue. The processing of DN messages was stable in Sterling since the reversion. All NDD (Next Day Delivery) DN messages were prioritised and processed in WMS by 14:00.",Alerting has been introduced for DN backlogs >2000 and if any DB queries run for more than 10 mins,11/2/2021,11/8/2021,NA,NA,NA,Yes,Yes,Yes,,Customer Tech change,CR,"The change introduced as a part of the Sterling OMS release caused the complex query running at the DB which inturn delayed the DN processing.
",CR#CM7000,,,11/10/2021,November,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1414,>60
11/2/2021,11/2/2021,88366841,14:38,15:16,00:38,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,KI,Packing unavailable in Castle Donington and Thorncliffe via Sorted,Sorted,NA,,Packing operations,RC identified,58731,Packing was unavailable in Castle Donnington and Thorncliffe via Sorted between 14:38 and 15:16. Sorted advised that a limit had been breached on their Shipment database and increased this limit to restore service. Sorted confirmed that housekeeping was not performed on the Shipment database and continue to investigate the root cause of the issue.,"1. Sorted are reviewing housekeeping data retention on the DB - Paul Maddox: completed

2. The Sorted Shipment DB has now been placed into the alerting group.  Alerting is being reviewed for the entire Sorted infrastructure relating to Customer - Paul Maddox : Completed

3. If any DB is changed or a new one added within Sorted, the DB will automatically be included into the alerting group - Paul Maddox - Completed

4. Sorted to review all storage sizes and dynamic growth for the databases to assess capacity - Paul Maddox: Completed
",11/2/2021,1/17/2022,,,,"No, Yes",No,No,No,3rd party issue,NA,Sorted have HA (high availability) enabled databases (DB) in the backend.The space allocated for the shipment DB was restricted rather than let it dynamically increase. The alerting was enabled for the DB storage group of which this particular shipement DB was not a part of. ,NA,NA,NA,1/17/2022,January,2022,Saloni,Closed,,,2021,11,November,9/16/2025,1414,>60
11/1/2021,11/1/2021,88364329,12:10,12:25,00:15,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,ADHOC,185999 unavailable ,Mitel,NA,,Service desk line - 185999 ,RC identified,NA,185999 unavailable from 11:32 to 12:29. The last call was taken at 11:32.  When users called 185999 they were presented with “there is a technical fault”.  IVR3 was rebooted by Mitel to restore services,"Investigate the possibility of improving the alerting for the Service Desk IVR’s and analyse the threshold levels of alerting – suggest improvements once the analysis is done: Andy Humm: Completed
Mitel to carry out a deeper dive into the Mitai process and why it could not communicate with IVR3 at the time of the issue.  Is the level of logging limiting the investigation? Andy Humm: Completed
Mitel have stated that they will investigate to see if the Mitai process can be monitored.: Andy Humm: Completed
Consider the approach of not presenting the “there is a technical fault” and presenting the caller with a menu despite some of the Mitai checks failing: Completed
Initiate a separate discussion regarding the how Mitel and Customer work together in the context of MI, RCA, alerting etc.: Paul Beasley/Eshwar : In Progress: Elizabeth to talk to Eshwar on the context
Mitel to confirm if all ports actually did fail on IVR 3: Completed 
Review if any of the Mitel infrastructure and connectivity relies on the Customer Palo Alto’s: Paul: In Progress: Paul and team to review and implement this in existing SOP’s
Can any alerts be created to alert if the desk doesn’t get any calls for a given period say 15 mins?: Completed
Unfortunately, Mitel are unable to do this as all calls are routed to external numbers. If the calls were routed to ACD paths/groups as they were in Stockley Park we could provide alerting to the Wallboard displays. Closed.
The boundaries of who owns the voice element in TCS is unclear.  Please advise.: Eshwar",11/1/2021,11/3/2021,NA,NA,NA,"No, Yes",No,No,NA,3rd party issue,,185999 goes via IVR 3 and 4. All logs showed that there was a heartbeat issue from MiVB (Voice server) to IVR3.  Mitai errors were then showing around time of the issue.  The MIVB saw that IVR3 was still available and therefore did not failover to IVR 4.,,,,11/5/2021,November,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1415,>60
11/1/2021,11/1/2021,88364326,11:15,14:32,03:17,GTS,Group Technology Services,All BU's,All BU's,,MI,Central network issue impacting multiple applications,Palo Alto,NA,, impact across portfolios,RC unknown,58832,"At 10:17, NOC team reported that the primary internet link at Stockley Park was causing latency.  At the same time, stores started reporting issues with gift card processing.  In an attempt to fix the issue, the teams worked with Vodafone to failover the services to the secondary DIA link within Stockley at 11:15.  After failover, the latency persisted. Investigations identified that we had issues with the primary instance of the Palo Alto (PA) firewall. To recover the issue, the teams failed over the services to the secondary PA.  All the impacted teams confirmed that the impact has been recovered from 11:31. At 22:00 Vodafone failed back to the Primary DIA link in Stockley Park.

 

The following services were impacted: Stores were experiencing issues with gift card processing and they experienced latency when using Honeywell apps.  In Donington, latency was observed on the pack benches and the order flow volume from OMS into WMS was low.  Stores were unable to see online orders on the SPPD screens for in-store fulfilment. Slowness was observed by the Czech DC while using the eSAP application.  GIST depots received the GR allocation response messages with a delay and Hemel and Thatcham had switched to backup allocation for 7 mins and 36 mins respectively.","1.Move back to the primary Palo Alto – suggested time is 03/11 6am.  This will be done via a MIM call - Paul: Completed


2. Implement hypercare on the primary Palo Alto and collect all logs/stats - Paul: Completed

3. There were two bits of traffic flow, one cloud to on prem and then the other was CD to Cloud – NOC team saw 2TB worth of data flowing through.  However from application perspective the teams saw a maximum of 50GB. - Paul/Raj/PA/Dipti - Teams to work together to identify what the data was and how much exactly was flowing through.  Could it have caused an impact on the PA?. Paul to ask PA to do a deep dive on the data spike: Completed <<PA are not able to provide anymore detailed information as this is not available on the firewall logs. PA need to be engaged earlier if this information is required.>>

+X17
6. Understand what Services go via PA for us to gather impact (similar to what we had for DS issue) for any future issue. We had a number of other impacts like scan and shop, OFP, mule post resolution - Dinesh/Vinay/Kamal - Vinay and team are working on fine tuning the impact spreadsheet for PA outages, this needs to be expedited.: Closed: Long term solution, hence raised a backlog - SM-00039.

7. What happened to alerting on middleware, why we did not pick the 249k pileup? - Raj - Raj says they did get an alert at 12:16 and it went to cloud integration team.  They found that messages were processing but slowly.  They found one queue manager not responding.  Team to see if any other alerts can be configured to alert us earlier.: Completed

8. There was delay in us treating this as MI, although Kamal informed us 10.15. In all the systems that we had impact, we need to understand if there was a delay in picking it up as a MIM - Paul - MIM were leading the issue from the beginning, however we would not have texted the issue out just based on the gift card impact.  Most of the impact came through after 11:11 when we failed over to the secondary DIA link – Paul and team to validate why the failover to the secondary would have caused any further impact.: CLosed

9. Why was gift cards impact only called out at the beginning?  Are there any alerts configured for failures in gift card processing? - Vinay/Mujeeb (ES)/Dilip Nair -SVS team: Completed
 ",11/1/2021,11/3/2021,11/3/2021,,,Yes,Yes,No,No,3rd party issue,NA,Palo Alto have proved that there is no path failure as they are not seeing the relevant entries in their logs.  All blades in the Palo Alto have had a health check with no issues found.,NA,NA,NA,12/19/2021,December,2021,Saloni,Closed,,,2021,11,November,9/16/2025,1415,>60
10/28/2021,10/28/2021,88357918,06:35,15:00,08:25,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Performance issues while printing labels in the packing areas at Castle Donington,Sorted,NA,,"Label printing, picking operations",RC identified,58813,"At 06:35, Donnington reported performance degradation in label printing via sorted. The Print and the ‘Pack At Pick’ (Royal Mail for Beauty Calendar) daemons were restarted at 09:03 and 09:22 respectively, after which an improvement in performance was observed. The ‘Pack At Pick’ (PAP) operations for Beauty advent calendar were stopped at 09:22 and resumed at 10:04 in a phased manner. However, a latency was observed again in the label printing and the PAP operations were stopped again at 12:00.  Investigations revealed that one of key database tables had 23M records where in only 200K were valid. The obsolete records were then purged. The label printing for a small number of PAP orders was then performed via Sorted and it was observed that the performance had improved.  However, to ensure that DC meets their targets for the day, they requested ”Beauty Boxes“  to be packed via Metapack instead of Sorted. This issue resulted in pick capability loss of 6K singles per hour and also caused slowness in label printing in the packing  areas. ","1. We need to investigate further to ensure that the above root cause can be confirmed with evidence backing the statement - Iain: Completed 
<<It has been confirmed that the latency was caused by the flow of a large volume of tracking data from REACT into PRO. A configuration throttle have been implemented to smoothen the  volume and no further spikes were observed>>

2.Was load testing done correctly before the PAP was deployed in Production? Can we create a testing environment for performance and load testing – investigate feasibility - OP/Iain Gregg - We don’t have a completely integrated environment available for testing between Customer and Sorted: Not feasible

3.Sorted implemented throttling to control the flow of data coming into their systems to allow the data to be processed with causing a back log and latency - Iain Gregg - Completed

4.A configurable/controllable throttling is being implemented on 01/11 - Iain Gregg - Completed

5.Investigate any other areas within Sorted that could result in this type of incident - Iain Gregg: Completed

6.Monitoring has been implemented by Sorted on the API end points with various thresholds set.  Fine tune the alerts and ensure key support stakeholders are part of alerts recipient group- Iain Gregg: Completed

7. The various common transactional tables needs review for purge logic and alerting (housekeeping) as in how often, how much etc - Jackie Martin

8. Investigate if we need to add any Oracle recommendations for improvements in performance - Jackie Martin: Completed

9. Identify how we can monitor the various API activities such as PAP, quote shipment etc within WMS - Jackie Martin

10. Decide on when we can move back to Sorted for PAP orders - Jackie Martin: In progress: Scheduled on 10/11",10/28/2021,10/30/2021,10/30/2021,11/1/2021,11/1/2021,"No, Configured",No,No,Yes,3rd party issue,NA,"Latency was observed within Sorted as there was an issue consuming a significant amount of imported tracking data flowing from React to Pro. Secondly, one of the key WMS database tables having high number of obsolete records caused blocking sessions which contributed to the latency ",NA,NA,NA,11/3/2021,November,2021,Saloni,Closed,,,2021,10,October,9/16/2025,1419,>60
10/27/2021,10/27/2021,88356765,12:06,15:15,03:09,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,GIST depots are receiving the allocation responses with a delay,TCS,Exponence ,,"ASO, SCRD, ORCA, FMD",RC identified,58805,"At 12:06, GIST reported that the depots were receiving allocation responses with a delay.  The issue was caused due to routing of all the foods application traffic via the firewall from the Azure backbone network, as part of a pre-requisite for the upcoming Natasha's Law changes to FMD (Foods Master data)/CYBAKE interfaces. Since the network connectivity via the firewall is relatively slower than that of the Azure backbone network, the latency was caused.  This routing was then reverted  to restore services. The ASO  application pods were restarted to recover the GR message flow to GIST and  ASO was switched back to live  allocations at 13:50. Six depots switched to backup allocations by 13:42 due to this issue and switched back to live allocations by 15:07. In addition, the UI of the following applications - ASO, SCRD, FMD, and ORCA exhibited slowness while the recovery was underway however no issues were reported by the application users . ","1. Moving the traffic to go through the firewall was not done via a change.  The change was approved for Thursday 28th October and was carried out on Wednesday 27th October. - Vidhya - Oversight has been noted and addressed

2.Testing the application was not carried out after the change was done apart from testing on FMD.  A thorough testing plan needs to be created before the full change can be deployed. - Vidhya: Closed

3. Before enabling any similar changes all pre-requisites should have been checked and verified such as the private link connectivity. 
Prior to the change going ahead again, the above needs to be fully configured and tested - Vidhya: CLosed

4.For the Cybake application feed, the teams need to enable a specific route via the Customer Azure firewall (for Natasha’s law) urgently.  This FMD change is being planned properly with a testing and regression plan. - Sam Welsh: Closed",10/27/2021,10/27/2021,10/27/2021,NA,NA,Yes,Yes,No,NA,Customer Tech change,CR,"The traffic for all the foods applications were routed to the Customer Azure Firewall on 27/10, whereas it was planned for 28/10. As the firewall was not configured to send the traffic on 27/10, it blocked the traffic flow causing no new GRs to be processed in ASO. 

 Secondly traffic for SCRD and ASO should not have reached the firewall unless the private link was configured properly, as it was not configured, traffic could not flow anyway.",CR#149598,Insufficient testing,NA,11/29/2021,November,2021,Aparna,Closed,,,2021,10,October,9/16/2025,1420,>60
10/25/2021,10/25/2021,88353060,12:15,15:05,02:50,GTS,Group Technology Services,"Customer Channels, Supply Chain","Service Experience, C&H and Intl Supply Chain",,MI,Primary internet link at Stockley park went down and failed over secondary,Vodafone,Network,,"Teams, Outlook, MyHR  Dect Phones at CD, Citrix",RC identified,58701,"At 12:15 25/10, alerting indicated that the primary internet link at Stockley park went down and immediately failed over to secondary. Stores were trading offline intermittently until 14:35. Direct Internet access at Castle Donington was intermittently down until 15:00, impacting applications such as Teams, Outlook, MyHR etc. Stoke DC reported issues when picking stock for a period of time. Vodafone was engaged and they confirmed that they had an issue with a router in their core network. As a work around Vodafone switched off the primary internet link (Stockley) at 14:35 as it was flapping and stability was achieved. Vodafone stated that they have had stability since 15:13. At 01:15 26/10, the Secondary link at SP went down due to a planned change CRQ1114689 – to replace the faulty fibre that reported over the weekend, impacting Citrix and Donington Dect phones . Vodafone was engaged and services were failed back to Primary link within SP at 2:20. After which the impacted services continue to remain stable. A full PIR is planned with Vodafone to review the entire issue","In conjunction with the technology vendor, Vodafone has tested and implemented a change to detect and initiate DDoS filtering at the sub-interface level for the DIAoMSP service. This ensures that any broadcast that triggers DDoS filtering and packet drop targets only the source sub-interface and not the whole line card: Completed on 30/10
",10/25/2021,10/26/2021,10/26/2021,30/10/2021,30/10/2021,Yes,Yes,"No, No",Yes,3rd party issue,NA,There was an uplift in ARP traffic (Address Resolution Protocol that connects IP to a MAC address) caused by a storm broadcast on one of the London nodes owned by Vodafone. The huge inflow of broadcast traffic triggered DDoS (Distributed Denial of Service) protection which caused packet drops and impacted multiple customer including Customer. The source of the ARP broadcast could not be determined.,NA,NA,NA,10/30/2021,October,2021,Aparna,Closed,,,2021,10,October,9/16/2025,1422,>60
10/20/2021,10/22/2021,88345151,04:06,3:30,47:24,Group Support, Finance, C&H and Intl,C&H and Intl Supply Chain,,MI,Incorrect orders sent to DCs,TCS,SAP ECC,,"WMS, Picking operations and Trailers",RC identified,58548,"On 20/10, DC's reported that they were receiving orders with older creation dates impacting multiple DC operations.  Due to this issue, Welham Green DC operations started the day’s work at 11:45.  Multiple trailers could not be received and cross-docked. Investigations found that the issue appeared to stem within SAP and the generation of old, duplicated orders were stopped. Workarounds continue to clear the old orders and reprocess any failed trailers. By 21/10, all the impacted trailers were cleared.","1. Business Operations should be getting the order number from the wholesaler and then the order should be created.  The discussion to change the process overall needs to be initiated - Kanmani Raja/DK (International): Shankar and Remko working. 

2. Ownership of the manual maintenance needs to be handed over to the business.  A tactical solution needs to be built so that the business can do this task - Adelson/Sathosh K. (SAP): 

3. Consider making the feed for the A&R dashboard 24/7 rather than stop at 7am with an alert to highlight there is a mismatch to investigate - Sakthi : Completed: Backlog id: CnH-WMS-00615

4. If there is an update to an order, then the whole order should not be processed and this logic needs to be implemented - Sakthi (WMS) - Closed : Backlog: CnH-WMS-00614",10/20/2021,10/20/2021,10/26/2021,TBC,TBC,"No, yes",No,Yes,TBC,Human error - Customer Tech,NA,"As a part of the International wholesale solution, the shipments are generally sent out to wholesale customer from SAP without the order number. On 20/10, the orders were replayed from SAP as usual. However, the dates on the variant were changed to pick the relevant requested orders. As the dates were not changed back, the older orders appeared in WMS.",NA,NA,NA,5/5/2022,May,2022,Saloni,Closed,,,2021,10,October,9/16/2025,1427,>60
10/14/2021,10/20/2021,88335365,10:00,16:00,150:00,Group Support, Finance, C&H and Intl,C&H and Intl Supply Chain,,SI,Six C&H DC’s are over stated in the C&H Stock integrity report,TCS,SAP Buy and Move Devops,,C&H Stock integrity report and in D&F,RC identified,58487,"It was identified that six C&H DC’s were over stated in the C&H Stock integrity report and in D&F. Investigations have found that there are failed idocs that have one or more older articles contained in them, these should not be there.  These have been already processed on a different date, time and under a different GR message reference and due to this the whole the idoc is failing.  We believe the issue started at the beginning of October.  A work around is in place to remove and archive the duplicate GR messages through a continuous job which is being run manually.  In the meantime, teams remain engaged to investigate the root cause and deploy a permanent fix.","
1. Devops team are looking into options of how we can stop the processing and implement an alert to advise us of the situation - Sarthak/Amit: <Team have come up with a tactical alerting solution to handle any future occurrence of the aggregation table corruption. : Closed

2. To allow select people to quickly have visibility of the stock integrity, the teams will consider sending an email in parallel to the BI solution (Remko/Naga/Sarthak) - Sandipan / Remko   : CLosed: <Automated email has been put in place for all the three dashboards which goes to all the respective SDMs>   

3. Identify if any alerting can be implemented which will help proactively identify any variances that may appear on the C&H stock integrity report by interrogating WMS and SAP: Karthick/Shankar/Remko: <Alerting for C&H Stock Integrity variance is already in place. Wrt the allocation shortages, WMS team is now working on including the automated checks for same in their morning routine. ETA yet to be given>

4. Identify if and how data corruption can be identified before it causes any downstream impact? Piyush Babu: <Included in Action 1.>: Completed",10/14/2021,10/14/2021,10/14/2021,11/01//2021,11/01//2021,"No, Configured",No,Yes,11/1/2021,Data issue,NA,SAP Devops team identified that the some of the data in the aggregation table had not been correctly updated as “processed” after the table lock issues were resolved.,NA,NA,NA,11/1/2021,November,2021,Saloni,Closed,,,2021,10,October,9/16/2025,1433,>60
10/8/2021,10/8/2021,88335845,11:42,12:55,01:13,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,MI,Sparks Unavailable In-store and online,TCS,Loyalty Services,,Sparks,RC identified,58517,"At 11:41, alerting indicated an increase in errors Sparks as it was unavailable in-store and online. Support disabled Sparks calls from .COM to alleviate the situation. Traffic was then restored by recovering the Loyalty Services health status. All flows – Web, POS and Apps were validated and confirmed to be working from 12:31, 12:14 and 12:55 respectively. The teams are currently monitoring for stability and investigating the root cause.","We need to increase capacity on the Loyalty Auth Service. This is planned for 15/10.  Need to confirm if testing has been completed. - Ivaylo - Completed

Non-Prod was four times capacity hence all tests were good.  It is now being reduced to bring it inline with Production. - Ivaylo - Completed

Testing to be carried for the increase in capacity - Ivaylo - Completed

The team has confirmed that there were NO issues in the engagement of support teams and in fact it was well managed with recovery achieved in less than an hour. 

Additional alerting to be put in place to notify on the threshold breach of the number of authentication calls to loyalty services:  Completed
",10/8/2021,10/14/2021,10/8/2021,10/15/2021,NA,Yes ,Yes,NA,NA,Design issue,NA,"The number of authentication calls to Loyalty services got doubled as part of Pluto implementation and during the final stages of the marketing campaign for TMO’s. The number of queries which the loyalty authentication service can process had reached its threshold and hence stopped further authentication calls to the server.
",NA,NA,NA,15/10/2021,15/10/2021,#VALUE!,Aparna,Closed,,,2021,10,October,9/16/2025,1439,>60
10/7/2021,10/7/2021,88335851,07:30,9:30,02:00,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Sparks unavailable for iOS App,TCS,Offer Management Platform Support,,Sparks,RC identified,58491,"Customers were getting a generic error while trying to access their Sparks hub or view any offers via an iOS device. The Sparks experience remained unimpacted on Web, in-store, Android App, or mWeb.  Customers with iOS devices were able to view their offers until 09:30. Investigations revealed that the offer setup for 27 TMOs (Tailor made offers) had a misconfigured URL for the images and hence iOS failed to recognize it, thus displaying the error.  A manual fix was performed on the impacted URLs to restore services at 09:30.","RPA to do changes in the Amended offers to capture 2 URLS for an offer - Priyabrata Completed


The change needs to be tested on all interfaces - android, ios, web etc - Closed-  Sekhar Gopinathan
 (This will be done after the live campaign ends on 21/10)

Offer set up needs to change the QA process to include testing offers on various device types (ios, android, web etc) - Completed for ios, web, but not enough resource to test on android - Alison Wright, Gemma Brockbank ",10/7/2021,10/7/2021,10/7/2021,NA,NA,"No, Not applicable",No ,"No, No",NA,Insufficient testing,NA,"In preparation for the migration from TIBCO to PLUTO, a change was done on the OMP(Offer Management Platform) for 2 URLs for an offer and export these in a single field. . This URL split logic worked only for new offers originating from OMP but it did not work as expected for the older offers amended in OMP due to insufficient testing. As these urls were split in tibco, it resulted in users seeing error and not being able to access other offers on the app.",NA,NA,NA,25/10/2021,25/10/2021,#VALUE!,Aparna,Closed,,,2021,10,October,9/16/2025,1440,>60
10/4/2021,10/15/2021,88321773,15:48,9:30,261:30,Group Platforms, Finance,Group Platforms,Finance,,SI,Foods Stock Integrity Variance between CSSM and SAP,TCS,SAP COE Projects,,CSSM,RC identified,58535,"After the phased rollout of the multi tray change for stores, it was found that there was Foods Stock Integrity Variance between CSSM and SAP. The total value of stock in SAP (ECC) is £60m while the total value of stock in CSSM was £70m.  We had a total of £31m of unprocessed idocs, some of which have already been manually processed.  Our plan was to archive the idocs that were manually processed.  These had a value of £21m (on Saturday 02/10 when the books were closed this was £17m).

 

On 06/10, the Absolute variance was brought down from £31m on the 3rd of Oct to £9m. BAU is £2m. The Net variance was -£20m on the 4th Oct and this was brought down to +£1.8m. BAU is +-£1m.  From 07/10 the total system stock integrity figure is £4.4m understated in SAP.  For reporting purposes, Finance excluded franchise stores and anything which is not directly owned by Customer which brings the total to £3.7m for the half year end accounts - this reflects an understated position in SAP.  Finance is much more comfortable with the position even though this is slightly higher than typical BAU position. Work will continue to analyze and improve this discrepancy over the coming days.",Identify if any alerting can be implemented which will help proactively identify any variances that may appear on the Foods stock integrity report by interrogating CSSM and SAP - this will be owned by bthe project team.,,,,,,,,,,Customer Tech change,CR,"This issue occurred due to carious scenarios that caused ASN's and GR's to fail after the multitray changes.
1.UPT from suppliers via GIST ASN was not what SAP is expecting,
2.GIST was not able to split the line items based on original estimated delivery date and therefore not able to share separate XML’s with SAP.
3.Goods not for retail sale were being included in the ASN which SAP cannot handle as it is not retail product","CRQ127521
CRQ127837
CRQ127037
CRQ128066",,,5/5/2022,May,2022,Saloni,Closed,,,2021,10,October,9/16/2025,1443,>60
10/1/2021,10/6/2021,88318117,18:47,16:00,117:12,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,MI,Issues with carrier label printing in Castle Donington and Thorncliffe,Sorted,NA,,Label printing,RC identified,58393,"From 18:20, the Castle Donington pack stations were unable to the print carrier labels for Hermes and Royal Mail.  A decision was made to revert the label printing to Metapack from Sorted. The Metapack print requests got processed by 01:56 after which the site confirmed that they were able to print the carrier labels without any issues. Further investigations revealed that Sorted was sending two tracking reference numbers for every label printing request, instead of one. The issue was replicated in non-prod and a fix was built however, it needs extensive testing before it can be deployed into production.   On 02/10, Sorted confirmed that there was a change deployed at their end in the allocation responses mechanism which resulted in sending multiple tracking reference number to WMS. This change was reverted to restore the service. It was agreed to continue the carrier label printing via Metapack over the weekend and a decision will be made on 4/10, Monday to switch back to Sorted.
 
Impact – The site was unable to pack any parcels across all sites / chambers until 01:56. 2.7K orders were miss promised resulting in a site CFR of 4.08%. The NDD proposition was moved to +24 hrs at 20:00 and was reverted at 22:00. 120K capability loss due to down time.  73K orders were duplicated which need to be manually reprocessed. ","1.Testing the reverted version from Sorted - Varun - Testing completed and the new version was successful.  Sorted have implemented the original version in their environment.: Closed

2.A decision to be made to switch the label printing from Metapack to Sorted - Colin/Martin - A decision has been made to transition back to Sorted from 07/10 – we will transition direct orders and a part of SYW located around the Leicester hub.

3.Change control to be put in place from Sorted an ensure that Customer is made aware of any changes - Colin -
(a) Sorted have stopped all changes to PROD temporarily.  Will resume Monday 11th October -Completed

(b)There is a daily change board.  Changes are now going Mon-Thurs before 3pm.  Sorted have implemented manual steps to prevent automated implementations. - Complete

(c)Features changes will be deployed with features turned off and Customer will be advised they are available for test and deployment.

(d)The new coding standard has been drafted and is being shared to engineering teams.   - Complete  

(e)Interim 15 min change catchup daily (this is in place) until change comms can be arranged - Completed

(f)Sorted will create a customer facing portal with change details, release notes etc.  In the meantime Sorted will send us a mail with Customer affecting changes - Completed

(g)All changes will have senior review. - Complete

(h)Sorted has confirmed their incident management process and they will ensure they work hand in hand with their change management team - Complete


4.If we decide to go back to Sorted  then plan and implement hypercare until the end of October- Jackie Martin - Plan completed – Complete

5.Review reversion plan to prevent duplications -  Jackie Martin - Plan completed and carried out a proof of concept in Prod successfully – Complete

6.We need a one pager for Sorted (our new carrier management system) trouble shooting document that will highlight the key SME's contact numbers, emails and also key technical points where we need to investigate. - Jackie Martin
Completed and sent to the ERS support teams, they are reviewing - Completed

",10/1/2021,10/2/2021,10/4/2021,NA,NA,"No,Yes",Backlog id,Yes,NA,3rd party change,NA,A change deployed at Sorted in the allocation response mechanism resulted in sending multiple tracking reference number to WMS  whereas it is designed to receive only one. ,NA,NA,,4/10/2021,April,2021,Saloni,Closed,,,2021,10,October,9/16/2025,1446,>60
9/28/2021,9/28/2021,88310921,00:30,23:44,11:14,Group Support, Finance,Commercial Trading,C&H Commercial Trading ,,MI,Delay in the overnight GMOR batches,TCS,SAP BI,,GMOR,RC unknown,58350,"Multiple GMOR jobs were over-running from 21:00 on 27/09. Investigations revealed that data commit into the GMOR tables was very slow which was causing jobs to overrun. SAP, Microsoft, and NetApp vendors were engaged. NetApp identified high CPU on one of the storage nodes hosted by Microsoft impacting the throughput of data transfer which was much slower than it should be. After shutting down the GMOR application, NetApp moved the storage to a different node.  The jobs were released, however, latency reoccurred soon after. SAP vendor engineers observed that the QoS data throughput limit was reaching near maximum and based on their recommendation, we have increased the size of the storage volume where GMOR is hosted.  By resizing the volume, we observed a much better throughput of data transfer. The jobs were released in a phased manner and all the downstream batch catch up for 27/09 were completed. The overnight GMOR GMTDB and BO reporting were made available with in SLA. The SSI intake data processing  got completed by 06:57.","Carry out a study of the jobs that were running on the application when the issues started. Do a comparison from previous days. Were there any changes a day or 2 prior. Try and replicate load in testing and see what happens. What jobs were running in parallel that maybe shouldn’t be there. Any transactional logging happening? Deepa/Robert /Mahesh closed
2 Get the early watch report from the start of migration and the one on the day of issue and look at trend. and study it for the day of the issue Robert and compare to previous weeks. Note that 28th and 29th is month end. Could month end be busier and hence the issue occurred? Deepa/Robert closed
3 Bob Conlan to provide a list of the devices connected to the storage volume - deepa.pattabiraman@marks-and- Bob Conlan (NetApp) closed
Bob will set up some monitoring on the HANA log and DATA log volumes on jobs that run throughout a typical GMOR night and then Bob can assist in a session where he can help us set a base line expectation for performance. Planned for: 5th October 7pm (2hrs) 1800 UTC and 00:00 2300 UTC (2hrs) are the busiest times And after we change the block size on 09/10 NetApp will monitor and trace again on 12th October - Gopi to arrange a session after 18th October. NetApp/Gopi Kishan closed
5 SAP Basis team to work with Priscilla Lee for system and performance alerts and analyse what we have and don’t have. The set up alerting in Solution Manager or Azure monitor to notify people of issues from application side – Job alerts are already in place. We need to consider system availability alert and performance alerts set. On the day of the issue, we only received job over running alerts. Rober/Mahesh and Priscilla Ongoing
6 NetApp provide documentation for NetApp metrics alerts NetApp Bon Conlan Ongoing
7 NetApp alerts available are: total read, total write and total throughput metrics. SAP basis to confirm if we already have these configured or need to. Mahesh and Robert Ongoing
NetApp strongly recommend changing the block size from 1mb to 256MB – Completed on 09/10
Ascertain if a smaller block size than 256K been used: Completed",28/09/2021,28/09/2021,TBC,TBC,TBC,"No, Yes",Yes,No,No,RC unknown,NA,"Detailed PIR was held with Microsoft, Netapps, SAP vendor teams. Root cause remains unknown at the moment. However, multiple actions have been agreed to identify the root cause.",NA,NA,NA,NA,NA,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1449,>60
9/28/2021,9/28/2021,88311300,10:30,13:18,03:12,Customer Channels,Selling Experience,Customer Channels,Selling Experience,,SI,Intermittent errors on CFTO slot selection,TCS,Enterprise Middleware,,CFTO collection App,RC identified,58368,"During site validations, support observed that CFTO slot selection was producing errors, in parallel, store colleagues also reported that they could not select slots. The response time for the slot selection spiked around 10:30 and caused the error to customers.  To reduce customer impact, slot selection was disabled and a message was published to state that slots can be selected during checkout.  From 12:06, customers were able to select the slots.  There was another spike between 13:00 – 13:18 where stores reported the issue reoccurred and was restored without intervention. Slot selection from the browse journey remains disabled and customers continue to select slots during checkout.  A case has been raised with Microsoft for further investigations.",A fix was implemented to  increase the number of threads from 1 to 160 threads per broker on 30/09.: Completed,28/09/2021,28/09/2021,28/09/2021,30/09/2021,NA,"No, Configured",No,No,Yes,Design issue,,"By default, the Enterprise Middleware service was configured with 1 thread per broker. On the CFTO campaign launch day, EM received much higher number of requests to be processed, which congested the thread causing the delay in response.",,,,30/09/2021,30/09/2021,#VALUE!,Aparna,Closed,,,2021,9,September,9/16/2025,1449,>60
9/28/2021,9/28/2021,88311034,08:18,10:08,01:20,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,MI,Sparks is offline in stores,TCS,Loyalty Services,,Sparks,RC unknown,58442,"From 08:18, Sparks was reported as offline in stores. Investigations revealed issues between e-receipt API and loyalty services.  Due to high memory utilization, 2 out of 3 CPS (Customer Preference Service) pods restarted automatically. The problematic CPS pods were restarted and the stores confirmed that the staff and customer discounts were working as expected on the tills from 10:08.","Enable health check alerts for Preference centre Pods  - Keerthi - To be enabled in Production from 29th Sep’21 - Completed


Setting up alerts on API timeout /response more than 1 second - Moutasi - This will be picked up on priority in the next sprint starting on Oct 20th


Loyalty Services code change to redo the timeout to 1 second for all CPS calls - Moutasi - To be picked with PM review - This will be picked up on priority in the next sprint starting on Oct 20th


Understand the overall setup in LS for Pluto implementation - Michael - To be picked in Q3 as part of LS decommission 
- This will be picked up on priority in the next sprint starting on Oct 20th",9/28/2021,9/28/2021,9/28/2021,9/29/2021,NA,"No, Applicable",No ,NA,NA,RC Unknown,,"A few months ago, circuit breakers were implemented, which stops further calls to loyalty services and Tibco, if the system senses a slow response time like it had done on 28/09.  Hence it was triggered for both staff discount and normal retail sparks calls on the day.  So, in essence, Sparks was up but the circuit breaker was preventing user calls being made to Sparks to protect the system. The cause for the slow response time could not be determined. The system load was normal and also no alerts were triggered to indicate any abnormalities. A hardware issue is currently being suspected.
",,,,30/09/2021,30/09/2021,#VALUE!,Aparna,Closed,,,2021,9,September,9/16/2025,1449,>60
9/22/2021,9/22/2021,88303015,02:30,2:50,00:20,Foods,Food Supply Chain,Foods,Foods Supply Chain,,MI,Delay to the overnight critical foods flows,TCS,Quantum DevOps,,ASO and WMS,RC identified,58307,The Quantum job responsible for creating forecasts and order plans was overrunning due to business peak pre-processing setup. Support identified UPC failures and skipped the peak pre-processing for the failed UPCs. These UPCs were then re-processed and the job got completed at 05:27. The peak order extracts for these failed UPCs have not been generated and hence will not be available to the users. Business comms have been sent out to notify the users on the delay in the Quantum UI availability. FFO completed at 06:44. Foods Bradford and MK NDC allocation completed at 07:35 and 06:47 successfully. The Q UI availability is being closely monitored.,"Update the table stats before the FOP runs, to ensure the job runs fine: Completed. Hypercare is in place to monitor the job performance on 24/09.
Add an Oracle hint to the affected query; this query resides in Java and will be amended at the most sensible opportunity: Devops: Completed",22/09/2021,22/09/2021,22/09/2021,23/09/2021,23/09/2021,Yes,Yes,Yes,yes,Design issue,,"During the FOP processing, as the database statistics of the job were outdated for one of the peak tables, the wrong execution plan was selected by Oracle and caused DB performance issue.",,,,30/09/2021,30/09/2021,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1455,>60
9/21/2021,9/21/2021,88302214,15:45,17:00,01:15,Customer Channels,Customer Channels,Customer Channels,Selling Experience,,SI,Central Salesforce issue impacting International Flagship Sites,SalesForce,.com,,Customer international flagship sites,RC identified,58308,"A central issue at Salesforce end caused performance issues on Customer International Flagship sites. Customers were unable to browse the website or checkout, impacting customer experience and loss of revenue. Salesforce applied a technical fix at their end to restore the services. After which the customers were able to checkout successfully. Holding page was applied for all flagship sites between 16:15 and 16:58. Awaiting detailed root cause from Salesforce.",Awaiting steps from Salesforce: com SM chased SF yesterday who has given an ETA of 2 weeks for updates,21/09/2021,21/09/2021,TBC,TBC,TBC,Yes,Yes,No,No,3rd party issue,NA,"A route leak by a small third-party ISP provider caused the return traffic from Salesforce to the Content Delivery Network [along with other Internet traffic] to take a suboptimal path across the small ISP, causing congestion on the network which resulted in performance issues. ",NA,NA,NA,05/052022,05/052022,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1456,>60
9/19/2021,9/21/2021,88299057,21:43,4:20,04:45,Group Platforms, Finance,Group Platforms,Finance,,MI, SAP ECC job and inbound messages stuck due to lock table overflow.,TCS,SAP,,"Bradford and MK NDC allocation, FFO & FCO, GMOR reports. EDW Day-1 reports, EDN generation, MP reporting. C&H  reporting",RC unknown,58406,"Multiple ECC jobs failed and a message pile up of 200,000 messages was observed due to a lock table overflow issue in SAP ECC. The ECC application services were restarted twice after which the held critical jobs were released in a controlled manner.? For Foods Final Orders, suppliers were advised to use the 28 day order plan.? The foods business have requested that we send them the actual FFO order plan however this has 3% of the orders missing and  the missing FFO orders for the supplier that was requested business had been retriggered successfully.  All C&H DC’s were asked to start picking at 07:35. For Bradford and Milton Keynes Foods DC's 90% of the orders completed at 09:10 and DC's were advised to start clustering.? ?The remaining 10% of orders had failed overnight and it was agreed to skip them as there is no safe way of retriggering them without duplicates. Multiple trailers across foods and C&H were delayed and support has processed all the pending trailers. The performance of SAP ECC processing remains stable. The overnight critical batches completed within their SLAs. Investigations by SAP vendor continue on the root cause.","Who owns SMQ2 and were there any alerts during the pileup?: Robert Kennady  - Completed - SMQ2 is a technical configuration inside SAP for message processing which is owned by SAP PO. Alerts didnt get triggered due to some metrics in GRAY status. Support have raised a case with SAP vendor and the configuration have been corrected. Alerts are getting triggered as expected after correcting the configuration.



What alerting can be put in place for the lock table filling up?: Remko - Completed 
A script has been created in OS level, the ITM agent will call the script and look for the lock table count. When the lock table count reaches 450K, an incident will be generated and call out will be given to the SAP Basis team. 



Lock table size – SAP wanted us to amend the lock table size (increase it) from 714K. We have asked for a specific value to increase to.: DHanvanttri Parameswaran (D3)
- Closed - The current value of  the lock table is 630234, and it can be increased to 714K if required. Support will increase it to 714k if at all any issues occur.



For the future, SAP team to note whether full or partial idocs are created and Update the knowledge base.: D3 - Cancelled- 
There is no way for SAP to track whether partial or full idocs are created. An anomaly has been created on that day and team is aware of this. SAP team will note this and team has been updated with this
.",9/19/2021,9/19/2021,,,,"No, Yes",Yes,"Yes, Yes",Yes,RC Unknown,CR,Root cause inconclusive.,146766,Insufficient testing,,15/10/2021,15/10/2021,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1458,>60
9/19/2021,9/19/2021,88297283,03:00,7:03,04:03,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,Delay to the Foods Bradford NDC allocation,TCS,ASO,,WMS,RC identified,58191,"Support identified that the Foods Bradford NDC allocation messages got triggered twice from ASO causing duplicates, which got processed into the downstream systems. To avoid over-allocation, a decision was taken to deallocate and remove all the orders from WMS. Support then re-triggered the expected 20 allocation messages from ASO which interfaced into WMS successfully by 07:03. No impact to the picking operations to the DC.  Investigations revealed that 1 out of the 20 allocation messages got stuck in ASO since several pods restarted as a part of the regular maintenance activity in AKS nodes. In addition, the status code was not updated properly in the database hence, upon retriggering the failed message, all the messages got re-triggered causing duplicates being sent to the downstream systems. ","ASO has asked the Infra team to hold the maintenance activity until a plan is in place: Completed
Why did the status code not get updated properly in the database: Santhosh/ASO: Completed:  It's because of the pod restart happened during maintenance activity
Infra team to have a plan while they resume the maintenance activity hereafter: Completed: This maintenance activity is currently scheduled between 10:00 and 15:00 when the critical flow like NDC is not running.",9/19/2021,9/19/2021,9/19/2021,9/19/2021,9/19/2021,Yes,Yes,"Yes,Yes",Yes,Infrastructure issue / Hardware failure,NA,"The issue was caused due to the following : 
1.Due to the regular maintenance activity on the AKS nodes which involves manual restart of several pods, 1 out of 20 allocation messages got stuck in ASO.
2.Since the database status code was not updated correctly, upon retriggering, all the 20 allocation messages got processed in ASO causing duplicates",NA,NA,NA,25/09/2021,25/09/2021,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1458,>60
9/17/2021,9/17/2021,88295119,07:28,9:00,01:32,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Issues with TSL DEF application and Reduced Later functionality,HEROKU,Colleague Devices Efficiency,,TSL DEF,RC identified,58187,"From 07:30, multiple store colleagues reported issues with the TSL DEF application and reduce later functionality. A central issue at HEROKU cloud services had caused the issue. HEROKU engineers identified an issue with their common runtime applications and provided a fix to restore services at 09:00.  Store colleagues were unable to perform the Reduced later functionality until 09:00. In addition, stores could not log safety compliance or carry out their slips and trips checks.",Alerting to be configured if the issue re-occurs: Not feasible. Applications are being moved to Cloud V2 this quarter.,9/17/2021,9/17/2021,9/17/2021,9/17/2021,NA,NA,NA,NA,NA,3rd party issue,,A central issue with the common runtime applications at HEROKU cloud services,,,,17/09/2021,17/09/2021,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1460,>60
9/15/2021,9/15/2021,88293178,18:10,20:40,02:30,C&H and Intl ,C&H & Intl Supply Chain, C&H and Intl,C&H and Intl Supply Chain,,SI,Issue in generating Carrier label for direct orders at Castle Donington,TCS,"Network, Vmware",,Label printing,RC identified,58172,"Carrier label for direct orders started failing at Castle Donington after a network change CRQ147386 DWDM to DEA OTV traffic cutover'. To mitigate any impact to the next day delivery, the proposition was moved to Metapack. Support teams identified that the connection between Stockley and Swindon broke after the deployment, causing the issue. Hence, the network change was reverted successfully to restore the services completely by 20:20. This issue resulted in packing capability loss of 24k singles at CD. 678 orders were miss promised due to this issue.","The server build handover checklist has been strengthened to engage VMware team to validate whether the server exists at right site as per the design: Completed
We are exploring to place an alert based on server naming convention as detailed here (MSHSRMNSUKP0 or MSHSRMNSUKPU or HLXP00 denotes Stockley and MSHSRMNSUKP8 or HLXP08 denotes Swindon). This will assist to position the server at right site even if it got misplaced unintentionally.: Dharma (Wintel)
Case raised with VMware to understand why Swindon VMs alone got impacted in first place.Wintel: Dharma
Communication - Confirm the build and handover process: Ravi
VM affinity rule exists: which will take care of server’s site once defined correctly at first instance. Re-certified all the servers in replicated clusters and ensured that servers are running as per their design at SW site.: Completed",19/09/2021,19/09/2021,19/09/2021,TBC,TBC,Yes (Supressed),Yes,No,No,Customer Tech change,CR,"As per design, the Donignton Mulesoft servers should be running on Stockley Park under CRQ000000141330_Mule Server for Carrier Management Independence Project, but however, was running on Swindon. In addition, the alerts remain supressed after the go-live due to a communication gap.  After the network change, CRQ147386: DWDM to DEA OTV traffic cutover, Jumbo frames (large packets) were not handled by new DEA network which caused  the servers in Swindon to go into hung state.",CRQ141330 and CRQ147386,Manual error,NA,1/26/2022,January,2022,Saloni,Closed,,,2021,9,September,9/16/2025,1462,>60
9/13/2021,9/16/2021,88289427,12:20,15:00,02:40,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Lost connectivity to VOD PODs,TCS,Exponence ,,"Scan and Shop, PWM",RC unknown,58292,"From 12:20 alerting indicated that connectivity had been lost to the VOD pods on Kubernetes V1 and CPU on the VOD DB had dropped to at 20% impacting Scan & Shop. This issue was due to one master node and other multiple agent nodes going into a “not ready” state.  From 12:51, the nodes started recovering themselves but a few required manual restarts. Support also added new nodes to the node pool and moved a couple of services (RTI & ITEM) to these nodes to maintain stability.  This issue was separate to the one which is occurred since 2nd September, where only some VOD pods were restarting and recreating themselves.  To alleviate this issue the virtual CPU's were doubled from 8 to 16 and this has prevented the impact from reoccurring.  Platform teams continue to investigate and monitor the environment.","Increase the CPU on the VOD pods from 8 to 16 and the issue did not re-occur
To safeguard the progression of transitioning to V2, we have decided to maintain our focus on the transition and stop root cause analysis for now.

",16/09/2021,NA,16/09/2021,25/10/2021,TBC,Yes,Yes,Yes,NA,RC Unknown,NA,Root cause is unidentified,NA,NA,1511,16/09/2021,16/09/2021,#VALUE!,Saloni,Closed,,,2021,9,September,9/16/2025,1464,>60
9/10/2021,9/13/2021,88289190,09:00,5:00,68:00,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Customer.com ISF capacity deflection not working,TCS,Sterling,,ISF,RC identified,58173,"On Friday 10th September,  analysis indicated that “capacity deflection” – an approach to deliberately force demand (in-day) into our store network ahead of EDC - was not deflecting the expected amount of orders required. Even though this did not cause any major impact to customers, it was required to be working before the EDC 12 hours outage on Tuesday 14th September, to manage the Order Well and to effectively use day to day store resources.
Various investigations were carried out over the weekend – comprehensively reviewing the functionality and logic. It was identified that this was not related to the Sterling release as initially suspected. The issue already existed but became visible as more departments were added to SIT (Stock In Transit) last week. The missing configuration meant SIT enabled items were routed to EDC and ignoring ISF stores, even when they were set above the in the priority. The missing configuration was added to resolve the issue on Monday 13th September.",Testing to be performed while introuducing any logic while deploying any future releases. : Completed: Teams will review the sourcing logic every two weeks to prevent this issue from re-occuring.,9/13/2021,9/13/2021,9/13/2021,NA,NA,"No, Not applicable",No ,"No, No",Yes,Customer Tech change,CR,"SIT (Stock in Transit) sourcing component was missing in the ISF priority logic( instead of ISF ship codes, the EDC ship code was added to the SIT distribution group)which was was introduced in error along with the Sterling release on 7th September.",CM#6684,Insufficient testing,NA,15/09/2021,15/09/2021,#VALUE!,Aparna,Closed,,,2021,9,September,9/16/2025,1467,>60
9/7/2021,9/8/2021,88281853,13:15,0:15,12:00,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,SI,Customer.com ISF stores exceeding capacities,TCS,Sterling,,.com BOSS,RC identified,58143,"Support identified that ISF (In Store Fulfilment) stores were exceeding their capacities after the 13:15 cut off on 07/09. Investigations revealed that the issue was caused by a code change introduced in the Sterling Release on 07/09. Support built a fix and deployed it by 00:15 to restore services. A total of 382 orders across 36 stores were cancelled today, 08/09 due to this issue.","Testing to be performed while providing any code fix during a deployment: Completed
Alerting in place to indentify when the ISF stores are exceeding capacities: Completed.",9/7/2021,9/7/2021,9/7/2021,9/7/2021,9/7/2021,"No, Not applicable",No,"No, no",Yes,Customer Tech change,CR,There was an existing issue in Sterling during the clock change in March 2021 related to the ISF capacity consumption going into negative even after the cut off crossed for the current date. The code fix provided to resolve this issue during the OMS release on 7th sept was problematic and hence caused the issue.  ,CM#6684,Insufficient testing,NA,15/09/2021,15/09/2021,#VALUE!,Aparna,Closed,,,2021,9,September,9/16/2025,1470,>60
8/27/2021,8/27/2021,88265799,02:00,4:32,02:32,Foods,Food Supply Chain,Foods,Foods Supply chain ,,SI,Delay to the Foods Bradford NDC allocation,TCS,ASO,,WMS,RC identified,58212,"Bradford NDC allocations were not processed out of ASO. Investigations revealed that the OSCS POD was in terminated status and another POD was in not ready status. The team tried to force deleted the problematic PODs to recreate the them and then re-triggered the allocation request, however the issue persisted. For recovery, support deleted the stale DB connections and stopped the GR flow temporarily. The allocations were then re-triggered and processed out of ASO at 04:32. Bradford NDC allocation completed successfully at 06:31.","The following actions agreed on the PIR will be completed by 13/09:

1.A not-started alert to be configured for Bradford and MK allocations in ASO- Hari H. (ASO)- An alert needs to get triggered if the allocation processing for Bradford and MK don’t start by the below times: Alerts have been configured and are working fine,: Completed
Bradford – 01:40
MK – 01:50
2. Setting up DTU utilization alerts for the ASO DB- Hari H. (ASO):Alerts have been configured and are working fine,: Completed
3.Looking into the feasibility of setting up automated housekeeping process for the problematic table- Santhosh V: Due to get completed on Q2 sprint 6: SCLT4 – 2296 and 2284
4.  2 out of the 11 OSCS pods (component of ASO) were not up and running. Although this did not cause this issue, we need to setup alerts to ensure that the teams are aware if any of the pods goes down and are not re-created.- Hari H. (ASO)- Hari to work with Prathyush and ASO DevOps for setting up this alert: Alerts have been configured and are working fine,: Completed",8/27/2021,8/27/2021,8/27/2021,6/9/2021,6/9/2021,"No, Yes",No,"Yes, Yes",Yes,Data issue,NA,High utilization in the ASO DB had caused the issue. The utilization spike was a result of queries running on a table taking longer than usual to complete since the table had a huge number of historical records (which were no longer needed),NA,NA,NA,6/9/2021,June,2021,Saloni,Closed,,,2021,8,August,9/16/2025,1481,>60
8/27/2021,8/27/2021,88265858,04:13,8:52,04:39,Commercial Trading,C&H Commercial Trading,Commercial Trading,C&H Commercial Trading,,SI,Delay in GMOR Overnight critical flows,TCS,GMOR,,GMOR,RC identified,58210,"The GMOR job responsible for loading the purchase flow back postings data was over-running owing to the high volume of data that it had to process. Investigations revealed that the inflated volume was a result of PO amendments made by business on 26/08 to cater to the logistical challenges caused by congestion in the port. This issue had delayed the availability of the GMOR BO reports however, the business users were already aware. The problematic job got completed at 08:43 and the GMOR BO reports were made available from 09:32. These changes are expected till Monday and this delay is expected, which has already been agreed with business.","Going forward, support will be performing the PO amendments in batches of a maximum of 3k POs to prevent the GMOR jobs from over-running.",8/27/2021,8/27/2021,8/27/2021,8/27/2021,8/27/2021,Yes,Yes,No,Yes,Human error - Business,NA,"PO amendments were requested by business on 26/08 to cater to the logistical challenges caused by congestion in the port. These amendments were made in batches of 13k in the SAP system which increased te runtime of the GMOR jobs since they can only handle upto 3k PO amendments per execution, without over-running.",NA,NA,NA,8/27/2021,August,2021,Saloni,Closed,,,2021,8,August,9/16/2025,1481,>60
8/26/2021,8/26/2021,88199127,TBC,TBC,TBC,Group Platforms, Finance,Group Platforms, Finance,,SI,Slow processing in POSDTA impacting availability of dotcom sales data in GMOR,TCS,POSDTA (SAP cloud),,GMOR,RC identified,58129,"Due to slow processing in POSDTA, the availability of dotcom sales data in GMOR had been delayed (still within SLA).  This occurred on three out of seven days last week, and two further occurrences a week before.  43% (31/08), 19% (01/09) and 27% (02/09) of the dotcom sales data was not available on until the following morning.
 
Product teams and SAP vendor are investigating.","Third application server added to POSDTA : Completed
Add the third application server in the network: In progress
Upgrade the POSDTA HANA database: Completed
Implement improved parameter settings as suggested by SAP: Completed
GMOR: Optimised job schedule to gain ~20 minutes: Completed
GMOR: shifted sales job start time to 3AM, gaining 30 minutes of runtime: Completed
 Improvement in the control-M schedule for the evening POSDTA jobs: Completed
Monitor system performance after these improvements to see if performance is now as expected: Completed

Longer term improvements are being tracked under risk 1613.",26/08/2021,15/09/2021,15/09/2021,15/09/2021,15/09/2021,Yes,Yes,Yes,Yes,Infrastructure issue / Hardware failure,NA,"The inbound ecom transactions were skipped for an hour due to memory issue in the SAP HANA DB causing a server outage for 90 mins. In addition, the slow processing of the outbound pipe jobs and locks present in the inbound jobs contributed further delay in processing.
",NA,NA,1631,20/09/2021,20/09/2021,#VALUE!,Saloni,Closed,,,2021,8,August,9/16/2025,1482,>60
8/26/2021,8/26/2021,88264404,01:29,5:55,04:26,Foods,Food Supply Chain,Customer Channels,Platform & Store Ops ,,SI,Stores unable to book-in deliveries from Barnsley depot,GIST,Foods,,CSSM,RC identified,58223,"Multiple stores reported that they were unable to book in deliveries from the Barnsley depot. CSSM support identified that they had not received any systemic deliveries from Barnsley depot after 01:29. Store counting was blocked for 112 stores from 07:37 to prevent the stock positions from getting corrupted. GIST IT was engaged who fixed the issue at their end and all the deliveries interfaced into CSSM at 07:58. From 08:20, a decision was taken to unblock store counting gradually based on stores which had completed receiving deliveries.","1. Checking if the alerting that had been configured in Barnsley has been set up in Cumbernauld as well - James S: Completed

2.We need to understand the cause for a delay in engagement of the GIST IT team - Customer MIM - The emails that had been sent to the GIST SD and GST MIM team will be shared with Darren H (GIST): Completed

3.The next version of DWS - DWS v70 goes live in the 2nd week of September and aims to introduce auto-restart function on the DWS integration such that it will automatically restart the JAVA job should it fail - Awaiting Gist IT resource assignment to achieve this: Completed

4.Considering that the interface I13154 will be made live in all depots pre-peak, GIST to ensure that the enhanced alerting config that had been set up for Barnsley be incorporated in the release plan for the remaining depots as well - James S: Completed

5.James to share the I1014 file (DWS stock integrity report) which will be shared once every 30 mins - James S -Sample file has been shared: Completed

6.Middleware and CSSM teams to check if the I1014 file could be used to determine if any despatch messages have not been generated or interfaced into the Customer landscape - The i014 interface is already being used to re-concile ASN deliveries. This re-con method has been stopped due to the SAP issue on 19/09.

'EAN check digit' check is in place for the Muller supplier setup which will ask the picker to re-enter the label number if the picker keys in an invalid character in the label number field. - Completed

Alerting has been enabled to monitor pile-ups in the I3154 interface - Completed",8/26/2021,8/26/2021,8/26/2021,27/09/2021,NA,Yes,Yes,No,NA,3rd party issue,NA,"By default, few pickers manually enter the label numbers for certain x-dock Muller milk pallets. On 25/08, while typing the 12-digit label number for one of the pallets, one of the pickers keyed in a ‘V’ instead of ‘0’. This label got processed successfully in GIST DWS (Data Warehousing System). The JAVA code which is responsible for processing these labels and sending the corresponding DRNs to the Customer landscape got failed at 01:29 causing pileups which resulted in the issue. GIST ITI support restarted the JAVA job by 07:57 which cleared the pile-ups and resumed the message flow.",NA,NA,NA,9/25/2021,September,2021,Saloni,Closed,,,2021,8,August,9/16/2025,1482,>60
8/24/2021,8/24/2021,88262351  ,19:20,19:51,00:31,GTS,Group Technology Services, C&H and Intl,C&H and Intl Supply Chain,,SI,Primary DP Firewall latency and failover to Secondary.,Palo Alto,Network,,"Metapack label printing, Citrix",RC identified,58101,"The Network support was engaged to investigate an issue with the Citrix DP (Digital Perimeter) portal connectivity.  Investigations revealed a latency with the primary DP firewall at Stockley from 19:20 and traffic successfully failed over to the secondary at 19:51.  This issue had impacted Metapak label printing at Donington and hence resulted in packing capability loss of 11k singles.
 
Network support advised that the issue occurred due to a bug in the current version and the teams are working urgently with Palo Alto to get the issue resolved via a version upgrade planned in September 2021.  The primary DP firewall at Stockley was recovered by the network team this morning and the traffic was failed back to it. Monitoring underway.","NOC awaiting an update on the hotfix release date from Palo Alto: In progress with Palo Alto
A risk has been raised for this issue.",24/08/2021,24/08/2021,24/08/2021,1/9/2021,TBC,No. Configured,No,Yes,No,3rd party issue,NA,A bug in the current version of Palo Alto that caused the latency in the primary PA box at Stockley Park,NA,NA,1610,1/9/2021,January,2021,Saloni,Closed,,,2021,8,August,9/16/2025,1484,>60
8/20/2021,8/21/2021,88257915,23:32,0:50,01:18,Customer Channels,Customer Engagement ,Customer Channels,Customer Engagement,,SI,Sparks intermittently unavailable,TIBCO,Sparks,,Sparks hub,RC identified,57903,"Alerting indicated that the loyalty calls were failing at checkout on the Customer website and the Sparks Hub was unavailable from 23:30. TIBO were engaged who identified that the VM hosting the Sparks APIs, webservers and database had gone down unexpectedly, causing the issue. The VMs were restarted by TIBCO to restore services at 00:50. Sparks Hub was unavailable between 23:30 and 00:50. This issue caused a small dip in orders and loss of sales.  Detailed root cause analysis underway.",Sypnotek to upgrade server hardware that hosted the vmware host : Completed on 01/11.,20/08/2021,22/08/2021,22/08/2021,31/10/2021,TBC,Yes,Yes,No,No,3rd party issue,NA,Hardware issue at Sypnotek's end (Datacentre vendor for TIBCO),NA,NA,NA,11/1/2021,November,2021,Saloni,Closed,,,2021,8,August,9/16/2025,1488,>60
8/19/2021,8/19/2021,88254892,06:00,7:30,01:30,Foods,Food Supply Chain,Foods,Foods Supply Chain,,SI,HHT connectivity issues at the Bradford Ambient Warehouse,TCS,Foods,,HHT's,RC unknown,57618,"On 19th August at 06:47, Bradford DC reported that their HHT and AMT's were not working. Investigations revealed locking sessions which may have caused the issue. The daemons were restarted by 07:15 and the session cleared. The DC confirmed the issue was resolved by 07:30.  The issue also occurred briefly on the morning of 18th August and was resolved very quickly by carrying out a daemon restart.
 
In our subsequent sessions with Blue Yonder, RedHat and support teams, they recommended further logging to trace where the locking sessions were coming from – this was enabled in Production. Other best practice suggestions were made by Blue Yonder and RedHat and our teams implemented these changes at 13:45, 20/08.  Hypercare monitoring continues.","No further actions to carry out
",19/08/2021,19/08/2021,,,,Yes,No,,,RC Unknown,,Root cause inconclusive.,,,,05/052022,05/052022,#VALUE!,Vinay,Closed,,,2021,8,August,9/16/2025,1489,>60
8/17/2021,8/17/2021,88252559,10:24,20:54,10:30,Customer Channels,Customer Channels,,,,SI,MM Flowers products not showing on PLP's or searchable,TCS,.com,,Customer .com website,RC identified,57687,"31 products from MM Flowers (supplier) were not searchable on the Customer Website or visible on PLP’s (Product listing pages). Investigations revealed an exception within the Search Control Layer indicating that the price was missing for an add-on item. This add-on item was added to all missing flower products yesterday, 16/08  and was removed from all 31 products by 17:20 after which, the products were available to sell on the PLP's. Around 19 products were not showing on the SRP's (Search Results Pages), however, they were made available after the Bloomreach delta processing and cache clear at 20:54. Detailed root cause analysis underway.",Bug fix has been deployed on 23/09 and no further issues observed.,17/08/2021,17/08/2021,19/08/2021,9/1/2021,23/09/2021,"No,No",No,No,No,Code/Product bug,NA,"The issue was caused due to a bug within Search Control Layer which causes the add -on item set up with any product to not be sellable on the website. The bug changes the stock indicator to 0, inturn causing the product not to be sent to Bloomreach and not be shown on the Website to be sold. ",NA,NA,NA,23/09/2021,23/09/2021,#VALUE!,Saloni,Closed,,,2021,8,August,9/16/2025,1491,>60
8/13/2021,8/13/2021,88248386,13:15,16:15,03:00,Customer Channels,Selling Experience,,,,MI,Store colleagues unable to pack BOSS orders,TCS,.com,,.com BOSS,RC identified,57766,Multiple store colleagues were unable to pack BOSS (Buy Online Sell in Store) orders. The issue was caused since the SPPD application was unable to communicate with the Carrier Gateway database which was restored by Microsoft at 16:15. 1286 out of 7208 BOSS orders were mis-promised due to this issue.,"Any manual configuration change request in Production via a SR must have a corresponding CR which has to go through CAB and be approved – Asokkumar Sreedharan (Exponence)
 - Completed ( Asokkumar has asked team to pick all types of prod changes that come via SR)
                                                          
In the Service Now Exponence portal, there are multiple scenarios where certain areas of infrastructure can be auto-removed once approved. Team to evaluate the different request scenarios that are made depending on which they decide what type of workflow changes are to be made – Asokkumar Sreedharan (In-progress)  the workflow changes in Service Now need some changes and will take some time. (  4 - 6 weeks) . Support have proposed some changes and have taken this to the change manager.                                                                                                          
                                                     Update the DL with active subscription owners list – Rajesh Ramakrishnan - complete
                                                      
The current business communications process needs to be reviewed especially during an urgent incident scenario – Closed (Process is agreed and in place) - Emma Rotherham - Retail ops have agreed to send required comms to stores during .com sev 1's and sev 2's",13/08/2021,8/19/2021,19/08/2021,,,Yes,Yes,No,Yes,Human error - Customer Tech,NA,The issue was caused after performing a service request to remove a server hosting the Foods Stock API both in Production and in Development.  The Carrier Gateway database was also hosted on the server and was removed with the assumption that there were no other services running on the server.,NA,NA,NA,8/19/2021,August,2021,Aparna,Closed,,,2021,8,August,9/16/2025,1495,>60
8/12/2021,8/13/2021,88247278,22:00,10:00,12:00,Group Support, Finance,,,,SI,POS Sales data processing failure,SAP,C&H,,Daily Sales reporting,RC identified,57770,"The cyclic job responsible for processing POS sales data out of SAP failed at 11:30 which caused a delay to all Daily sales reporting. The failed job was recovered, POS sales data was successfully processed out from SAP and the impacted reports were made available within their SLAs, except GMOR BO reports, which were made available by 10:00","Configuring ""Not-started"" alerts to ensure that the teams are notified if the iterations of the cyclic job - SAPBIIQ_001_DT is not running as expected: Not feasible since Control-M not started alerts cannot be cofigured for cyclic jobs.



The SAP POSDTA team to be familiarised with the lessons learnt (detailed in the problem resolution section) as agreed in the PIR.: Hariprasad Surisetty- Completed
(Hariprasad havae discussed with team on what needs to be done during multiple job failures)

 

Control-M dependency to be introduced between the GMOR ETL batch and the GMOR BO refresh jobs to ensure that the GMOR BO reports refresh only after the overnight GMOR ETL batches complete. Closed: WO1078368



In the SAP POSDM world, we had a scheduled job that ran every 3 hours and performed a re-con between SAGG and SAP POSDM. The SAP POSDTA team need to explore the feasibility of introducing a similar setup can be introduced in the current SAP POSDTA setup: Hariprasad Surisetty - In progress- Support updated that this recon would incur a technical debt to POSDTA and also adding additional recon which is not essential. Support also said that during multiple Control-M job failures if critical jobs have been validated then the above recon would not be required at all.",16/08/2021,16/08/2021,16/08/2021,,,"No, Yes",No,No,Yes,3rd party issue,NA,"The Control-M agent running on one of the SAP POSTDA application servers (on the SAP cloud) had gone down owing to an issue with the server. This caused 5 POSDTA jobs to fail. The job SAPBIIQ_001_DT which is responsible for processing the POS sales from POSDTA inbound and sending it to the other SAP systems had failed at 11:59, since the server was not responsive.So,it removed it's dependency with its successor job SAPPDIS_001_DT.Hence, SAPPDIS_001_DT did not run which in turn resulted in the subsequent iterations of SAPPDIS_001_DT not running.",NA,NA,NA,16/08/2021,16/08/2021,#VALUE!,Aparna,Closed,,,2021,8,August,9/16/2025,1496,>60
8/4/2021,8/4/2021,88242091,10:00,11:30,01:30,Customer Channels,Customer Channels,,,,SI,Bulk email and SMS messages not being sent from Mission control (Genesys),Anana,.com,,Genesys,RC identified,57744,Bulk emails and SMS messages sent by the Customer command center were not reaching customers despite showing as sent in Mission control (Genesys). The service was not working from 27th July after the UCS application upgrade. Anana restarted the proactive contact email servers which fixed the issue . Approximately 120k email messages were impacted due to this. ,"
Alerting needs to be created for errors when sending bulk emails entering the error queue: completed",4/8/2021,10/8/2021,10/8/2021,11/8/2021,11/8/2021,"No, Yes",No,NA,Yes,3rd party change,NA,"During the UCS upgrade at Anana end, the e-mail servers were disabled but proactive contact services were not restarted to apply new configuration and hence did not pick up the new UCS host. Therefore, the email server was unable to update UCS resulting to the emails going to the error queue.",NA,NA,NA,8/11/2021,August,2021,Vinay,Closed,,,2021,8,August,9/16/2025,1504,>60
8/1/2021,8/1/2021,88232018,09:34,11:31,01:57,Group Support,Corporate,,,,SI,Store colleagues unable to access People systems hub and e-learning portal,TCS,Infra,,Peoplesystem and elearning portal,RC identified,57720,"From 09:34 store colleagues were unable to access People systems hub and the e-learning portal. Support investigated and confirmed that the account which associates our Active Directory with the Digital perimeter for store front applications, expired on 31/07.  The password of this account was reinstated to restore services at 11:31.","
1 Analyse all the accounts to determine if any of the portfolio applications are using a user account as a service-account. If yes, the account would need to be replaced with a service account. Problem Management - Analysis of the F5 logs determined that 10000145 was the only store account used as a service account. The Problem Management team will coordinate with all SDMs to determine if any of their applications use non-service accounts.

2 Replacing 10000145 with a newly created Y-account - Kamal/Rajesh - Y account is ready, testing in prod is being discussed.

3 NOC to liaise with the Security Assurance team to obtain an exception for the account – 10000145 to stay active and in “Never expire” status until it has been replaced with the Y-account - Kamal/Rajesh

4 Determining if there are any leavers who had an A/B/C/N/P account and a stores account, but the stores account continued to remain active after them leaving Customer - Ram (AD Support)

5 Devising a process such that users with both A/B/C/N/P IDs and store IDs have their accounts disabled after they leave Customer, as part of the leavers process - Ram (AD Support)",1/8/2021,1/8/2021,,,,No,No,No,No,Design issue,NA,"The password for the stores account - 10000145 which associates our Active Directory with the Digital perimeter for store front applications had expired on 31/07.The P-ID of the user who owned this account also got disabled as part of the Leavers’ process after the user left the Customer account on 31/03/2020. The stores account - 10000145 however remained active, owing to the below reasons.
a. As agreed with Customer, the stores lifecycle script which is responsible for disabling the inactive store accounts has been placed on hold in view of the pandemic. The onus of manually disabling the inactive store IDs lies with the respective store managers.
b. The PID and store ID (10000145) for the user were not mapped to each other since there is no existing process governing this. Hence, the PID got deactivated as part of the Leavers’ process however, the account 10000145 continued to remain active.",NA,NA,NA,9/16/2021,September,2021,Vinay,Closed,,,2021,8,August,9/16/2025,1507,>60
7/31/2021,7/31/2021,88231466,14:00,15:41,01:41,Foods,Food Supply Chain,,,,SI,Bradford Foods DC unable to access the JDA application on HHTs,TCS,Foods,,HHT's,RC unknown,57618,"Bradford Foods DC reported that they were unable to access the JDA WMS application on their HHT’s from 14:40. Bradford advised they were unable to carry out picking, packing, despatching for the duration.  To mitigate the issue, a WMS daemon restart was done by 15:41 and the DC confirmed service restoration.  The site was also impacted by the same issue on Sunday 1st August from 22:00 to 23:43. ","WMS support and product teams to follow up root cause with Blue Yonder under log: BY log number: Kunal/Manoj:  02930195: In Progress

Linux support to create a script that monitors the Bradford RDT ports and alerts when these ports are being used by another process or are down: Jeeva/Linux: Closed

There is a job in place on Saturdays at 22:00 to restart RDT’s daemon in Bradford – restart takes 5 mins – can it be done daily 14:00 and 22:00 until we get a root cause – look at the feasibility - Daily RDT restarts have been stopped from 06/08.X8

Look further into the alert to see how the various scenarios can be catered for e.g: lunch, down time, meetings etc – to prevent false alerts: Kunal: An alerting (ITL alerting) is in place to monitor the number of transactions carried out by HHTs – it will alert if it goes below a certain threshold (less than 5 in 5 mins) – This did not trigger and needs verification: Cancelled (Not feasible).  

Compare OS level parameters and settings between Bradford and any other DC on Linux to analyse the differences: Main difference is that Bradford server has been up for more than a year (457 days) and are not fully up to date with patches (n-2 versions behind): Bradford app server restart was performed on 04/08 and the Linux version is being planned for 03/10. 

 “HHTs Stalling” POWER UP issue seen in Donington: Kunal/Manoj IPCS alerting (Messaging queue alerting) can be setup. Alerting Threshold: Piled-up message count> 200 and used bytes >2k for more than 5 mins. Will be implemented on 03/08: Closed

Increase the IPCS messaging queue limit from 16k to 64k: Completed on 10/08. Since we had observed a minor pile up on 07/08. However, this pile up did not impact operations.

There was a change on 12/7 to move the Bradford DC Corporate wireless users and HHTs to new Radius server: No significant errors observed on the radius server at the time of the incidents: Closed 

As recommended by BY, a ""lock watch"" script was enabled to help us identify any blocking sessions in the database: Completed

As recommended by BY, the hidden RDT commit  functions was enabled to facilitate faster commits in the DB and thereby reducing pileups in the messaging queue: Completed

Increase the Linux OS queue size parameters(ipcs -l) in the Bradford App server (hlxp0dc031) to match the WG configuration+X8 (hlxp0dc123): Completed

Analysis for comparing which RDT’s may be causing an issue and which RDT’s are commons in all pileups: 26 RDTs were identified however, upon physical inspection by the DC, these HHTs appeared to be working as expected.: Closed

 BY to document the investigations carried out so far along with  their recommendations including any known causes: Closed

Can a BY code clear the stale messages from the queue proactively to prevent pileups.: Confirmed to be not feasible.

An Oracle patch was deployed on 12th July – is it worth reverting the patch?: The DB logs from before and after the patching were compared and found to be identical – this is indicative of the fact that the DB patching had not contributed to the issue and can hence be ruled out.: CLosed 

New BY tool – RDT tool which can give us a visibility into the messages in the RDT messaging queues:  Needs to be tested in lower region.  

Logging indicates the presence of certain static messages on the IPCS queue which are not getting auto cleared. UNIX support to check with RedHat if these static messages can be identified: In progress: To be tested on 07/09.

It was found that Bradford was the only DC where the system was killing HHT sessions even where the user was not logged into the HHT. This was removed and replaced with killdae which looks for sessions where the user is logged into the HHT or  the HHT has not been used for 1 hour or more.: Completed
",2/8/2021,2/8/2021,,,,"No, Yes",No,No,Yes,RC Unknown,NA,Support believes the issue was due to the message queue generated by the HHT sessions. A detailed root cause analysis could not be identified. ,NA,Na,,8/10/2021,August,2021,Saloni/Vinay,Closed,,,2021,7,July,9/16/2025,1508,>60
7/30/2021,30/7/2021,88230456,15:15,16:05,00:50,Supply Chain,C&H & Intl Supply Chain,,,,SI,JDA WMS unavailable in Welham Green,TCS,C&H logistics,,WMS,RC identified,57624,"At 15:15 users in Welham Green reported that they were unable to use JDA WMS to carry out picking, packing, despatching  etc. This was caused by several blocking sessions which released themselves by 16:05 and DC confirmed service restoration.  RCA underway."," 1. Enhancing alerting for the library cache bin wait events (Boopathi) - Boopathi to check and confirm if any OEM alerting can be configured: Not feasible: Database have n number wait events, hence cannot be monitored. Also OEM doesn't have defined metrics for this library cache pin wait event.
 2. Ensuring that all active sessions are killed before commencing any change-related activities (Boopathi) - Going forward, Oracle support will check for any activity sessions prior to commencing any outage-related activity and will gracefully close kill sessions.: Closed
3. Awaiting a response from Oracle on the RCA for the increased number of wait events: The wait event is occurred because application code deployment, during the deployment some packages/jobs are running and accessing the database. To avoid such issues in future WMS support needs to stop all the application related services and jobs before deployment. : Completed: WMS has cascaded this to the team",7/30/2021,8/5/2021,8/5/2021,8/5/2021,8/5/2021,Yes,No,No,NA,Customer Tech change,CR,"On 30/07, BY patches 41 & 42 were deployed against CR # 144863. However, users did not log off during the agreed outage window. As part the pre-deployment activities, the active user sessions were killed from the WMS end however, a few sessions persisted on the DB. Hence, the wait event logs created by these active DB sessions filled up the library cache bin in the DB, resulting in the issue. However, the exact cause for the increased number of wait events remains unknown at the moment and a case has been raised with Oracle to understand this.",144863,change validations were not performed,NA,5/8/2021,May,2021,Saloni,Closed,,,2021,7,July,9/16/2025,1509,>60
7/30/2021,7/30/2021,88229292,06:10,7:25,01:15,Supply Chain,C&H & Intl Supply Chain,,,,SI,West Thurrock DC users are unable to connect to terminals and RDT's,TCS,C&H logistics,,RDTs and terminals,RC unknown,57713,"From 06:15 West Thurrock DC users were unable to connect to their terminals and RDT's to carry out picking, packing, despatching etc.  Support observed high CPU utilisation in the database server which was caused by multiple long running instances of a picking related query. On the day, the nature of the operations had become systemically busier and the problematic query had started referring to an inefficient execution plan. The Oracle team pinned a better execution plan at 07:25 after which the utilization reduced and the DC users confirmed service restoration. "," 1. Raise a case with Oracle to investigate the RC for the MOVE_TASKS table to have stale stats - (Boopathy): Closed : Orcale case confirmed that the cause for the stale stats could not be determined in this case. Orcacle recommended applying a SQL profile to pin an efficient execution plan which was already performed by Support on 30/07. 

 2. Checking the feasibility of increasing the frequency of the run stats job from twice every day. The runstats job has a runtime of <10 mins and will result in minor performance degradation when it runs- (Sakthi): In progress : Sakthi to check with Naga if the frequency of the runstats job can be increased.

 3. Enhancing the alerting for identifying long running sessions before it starts impacting the DC operations	 (Boopathy) -Not recommended by the teams since enabling this will result in multiple false alerts and we already have the locking sessions alerts in place: Cancelled 

4 Rechecking if the DC had been performing any different activity between 02:00 and 06:00 on 30/07 that could have caused the issue - Inconclusive: It is possible that the DC might have been performing multiple clustering operation on the day of the issue. However, this could not be confirmed.	",7/30/2021,7/30/2021,10/8/2021,TBC,TBC,Yes,No,No,No,Code/Product bug,NA,"High CPU utilisation in the database server which was caused by multiple long running instances of a picking related query. The queries were long running since they were using an inefficient execution plan. The Oracle engine, by design, had assigned a different execution plan to this query since it detected that the stats for one of the core tables – the MOVE_TASKS was stale. This plan turned out to be inefficient. 
The scheduled runstats job had run successfully at 02:00 on 30/07. The cause for the table to have MOVE_TASKS within 4 hours of the job running is currently unknown and a case will be raised with Oracle to this effect.",NA,NA,,7/30/2021,July,2021,Saloni,Closed,,,2021,7,July,9/16/2025,1509,>60
28/7/2021,7/28/21,88226097,03:10,5:06,02:54,Foods,Food Supply Chain,,,,SI,Delay In Foods Bradford NDC allocation,TCS,Foods,,Bradford DC allocation,RC identified,57604,"Foods Bradford allocation messages failed in middleware. Team identified that reference data was missing in the middleware due to a change in distribution channel value for a store '6499-Harrogate' in SAP by a business user on 27/07.  Support then reverted the distribution channel value and processed the NDC allocation successfully in WMS by 06:47. Due to this issue, the A&R orders were also skipped for this specific store.  Detailed RCA underway.","Provide a technical explanation as to why the system cannot be configured for UK stores to accept Irish returns within SAP : Extension of the customer master record to Ireland distribution channel would not help to clear the failed return order messages (idocs)  in SAP. Testing in the lower region proved that only the extension of the site master record would clear the failed messages, therefore, the only workaround is for the BSC Team to update the site master view to the Ireland channel as per existing process. : CLosed

This scenario needs to be presented to the finance business – to see if they have any other combinations of the problem statement.: George/Joey: 
Closed - George have discussed this with business and they confirmed that this won't reoccur again from their side.

Once the business have changed the attribute of a store to Ireland – SAP to ensure via mail communication that the store attributes has been changed back to UK. SAP to implement this new step in the process.: Remko: Completed
 - Additional step has been put in place for the SAP Sales team to remind BSC team to revert the change when requesting the workaround.

When there are middleware exceptions, the exception should be configured to show which store is causing the issue in the NDC foods flow: Raj - DeVops team is working on this, ETA 20th Sept.                                           
20/09: No progress yet, Raj is chasing with devops for the revised date
01/10: Devops team found a bug and the fix have been deployed on Nov 3rd.

Check the feasibility of switching ASO to process at store level rather than product level for NDC allocations(long term action).: ASO Product team: Santhosh Singh - Cancelled - Santhosh updated that this is a major change in ASO and it will not be approved as ASO is being decommissioned. They are aware of the risk and  have SOP in place. 
                                                    WMS team to note that any deletion of orders needs to be verified by the business via the BSP: SDMs. - This has been cascaded to the team.: Closed",7/28/2021,8/2/2021,8/2/2021,11/3/2021,NA,Yes,Yes,Yes,NA,Human error - Business,NA,"To allow refunds of goods  bought from Irish stores and returned to UK stores, the business users change the status of a UK store to an Irish store whilst the refund is being processed and after the refund is completed then the store is changed back to the UK store.
On the day of the issue, three UK stores were changed to Irish stores and the refunds were processed, but after this, the business only switched two out of the three stores back to UK stores and one was overlooked.",NA,NA,,11/3/2021,November,2021,Aparna,Closed,,,2021,7,July,9/16/2025,1511,>60
22/7/2021,23/07/2021,88219645,18:10,16:27,22:17,Foods,Food Supply Chain,,,,SI,Data duplication in ASO,TCS,Foods,,ASO,RC identified,57459,"As part of V2 migration preparation activities, the current production database was deployed in V2 ASO at 17:00 on 22/07. Due to this, the Q allocation responses were processed from both V1 & V2 creating the duplicates and over allocation in ASO. In addition, Faversham, Enfield and Cumbernauld depots went to backup. ASO support corrected the data and processed the frozen allocation to Bedworth successfully. The Depots switched back to live allocation by midnight. The configuration for two stores were also impacted the next day and teams had to perform an activity to switch the stores back to the correct serving depot","Pre-Deployment approval will be enabled to all V2 Preprod and prod pipelines  - Approval needed from Pushpa/Vivek/Saravanan/Santhosh to proceed.  Enable this rule by Monday 26/07: Completed
Access to V2 production resources will be restricted and app SPOCs and Santhosh will be added as co-owners to AD groups. They can further manage access for rest of the team. – Action for Neethu: Completed
FCE (Foods Cloud Enabler) team will not make any deployments to prod / pre-prod regions in V2 AKS. It should be triggered by app team and FCE can support: Completed
Application team to use pre-prod key vault for parallel testing in prod until go-live. Team need to ensure that pre-prod KV are updated correctly: Completed
FCE team’s access to production will be revoked after completing V2 migration : Completed",22/07/2021,22/07/2021,22/07/2021,26/07/2021,26/07/2021,Yes,Yes,Yes,Yes,Human error - Customer Tech,NA,"During the Prod pipeline testing, Foods Cloud Enabler team errorneously deployed ASO services in V2 Prod instead of pre-prod.  This resulted in allocation messages getting processed from both from V1 and V2 queue causing duplication in ASO.",NA,NA,NA,26/07/2021,26/07/2021,#VALUE!,Saloni,Closed,,,2021,7,July,9/16/2025,1517,>60
22/7/2021,22/7/2021,88218419,02:27,5:03,02:36,Supply Chain,C&H & Intl Supply Chain,,,,MI,Connectivity issues with the HHTs across the DCs,TCS,Infra,,HHT's,RC identified,57454,"Multiple DCs reported connectivity issues with the HHTs from 02:27. To fix the issue, the CRL (Certificate Revocation list) in the CA (Certificate authority) servers was refreshed followed by the server reboot and restart of the services at 05:03. The sites confirmed to be able to connect the HHTs to the network without any issues. Detailed RCA underway.","Automation is being planned and implemented for the second part of the CRL renewal to remove manual interventions as far as possible - Partial script is ready to pull dates from crl files (Support have raised a backlog - INF-WIN-00100 as this can be done only before the next renewal in Jan 15)


Explore feasibility of configuring alerting to highlight CRL is using an old certificate - In progress (ETA : Sept 30)
- Support need to write a script to trigger alert when those dates are nearning by 

 (Support have raised a backlog - INF-WIN-00100 as this can be done only before the next renewal in Jan 15)

Strengthen the SOP by adding more check points - In progress
 ( will be done after the script is done) (Support have raised a backlog - INF-WIN-00100 as this can be done only before the next renewal in Jan 15)

Wintel AD team to manually perform checks 24/48 hours before expiry to ensure all new certificate components are in the right place - Dharma updated that  AD team has to follow this in the next renewal which is in Jan 2022.(Support have raised a backlog - INF-WIN-00100 as this can be done only before the next renewal in Jan 15)",22/07/2021,22/07/2021,TBC,TBC,TBC,"Yes, No",No ,"Yes, No",No,Customer Tech change,CR,"As part of the half yearly CRL refresh activity, the new CRL was placed in an incorrect folder instead of the correct CRL share",144067,NA,,10/5/2021,October,2021,Aparna,Closed,,,2021,7,July,9/16/2025,1517,>60
22/7/2021,22/7/2021,88219582,16:40,17:20,00:40,GTS,Group Technology Services,,,,SI,Multiple applications hosted on Akamai inaccessible,Akamai,.com,,"UI of ASO, SCRD, RDA, SSI and TMS 
 Salesforce Marketing and Bloomreach accessibility",RC identified,57456,"Multiple applications hosted on Akamai were inaccessible between 16:40 and 17:40, due to a global issue at their end caused by their Edge DNS being unavailable. The issue caused user interfaces for ASO, SCRD, RDA, SSI and TMS applications to be inaccessible. For Dot Com, an order dip was observed. In addition, issues were observed in signing-in, Salesforce Marketing and Bloomreach accessibility",Akamai to review their software update process to prevent further disruptions,22/07/2021,22/07/2021,22/07/2021,22/07/2021,22/07/2021,Yes,Yes,No,Yes,3rd party change,NA,"The disruption lasted up to an hour and was caused by a bug in the domain name system (DNS) service, which allows web addresses to take users to their destinations, that was triggered during a software update",NA,NA,NA,23/07/2021,23/07/2021,#VALUE!,Aparna,Closed,,,2021,7,July,9/16/2025,1517,>60
20/7/2021,23/7/2021,88208481,14:28,20:10,05:42,Group Support,Corporate,,,,SI,Timesheets missing additional hours,SDWorx,HR,,Timesheet,RC identified,57520,"After investigating a single employee with an overpayment raised by Colleague Services, SD Worx found that 237 colleagues had missing hours or late codes in HRe in late June. This was due to an overlap in the timesheet export and timesheet import process thought to have been caused by remediation of a previous incident related to the business pay review and annualization processes. SD Worx carried out corrective measures for the impacted colleagues after validation -  any colleague who had received a manual payment to be excluded and a list of those requiring paying or deductions. These corrections were loaded ahead of the trial run on 26th July. RCA underway.",Include a 15 minutes slot in batch schedule between timesheet export process completing and timesheet import process starting to avoid any overlapping issue in future: Completed,20/07/2021,20/07/2021,24/07/2021,28/07/2021,28/07/2021,"No, Not applicable",No ,Yes,Yes,3rd party issue,NA,"The timesheet import process (importing timesheet data from WFM to HRe) is
dependent on timesheet export (extraction of timesheet data from WFM) to
complete.
On 27/06, there was an overlap of 4 minutes between the timesheet export and
timesheet import process. This resulted in some timesheet data to not get
imported into HRe, because they were still being extracted from WFM when
the timesheet import process started. ",NA,NA,NA,28/07/2021,28/07/2021,#VALUE!,Saloni,Closed,,,2021,7,July,9/16/2025,1519,>60
15/7/2021,15/7/2021,88209625,07:00,17:06,10:06,Customer Channels,Customer Channels,,,,MI,Sale Launch Delayed,TCS,.com,,Customer .com website,RC identified,57442,"The online sale launch was delayed due to long running nightly jobs.  One of the nightly jobs responsible for calculating promotions had overrun and needed to be restarted at 07:28. The long running nightly job (Precalc) completed at 12:23. Due to the delay in nightly jobs, the sales price did not show on Basket and PDP until 14:00.  Sales content on the website for Homepage/DLP/Offers went live at 15:15. The sales content scheduled for the apps was confirmed as visible to customers at 17:09. A decision was taken to resume the paused marketing emails sends for UK at 17:45 and IE at 18:15. ","Support to work with business and see if inactive sparks promotions can be deactivated/deleted in WCS: Completed

Support to increase the pre calc job time out value from 2:46 mins to 5 hours to ensure we have enough buffer time during scenarios where the job tends to time out on sale days: Cancelled: After the housekeeping done for promotions, the job is taking around 15 - 20 minutes on BAU days and on last sale takedown day, it took 25 minutes to complete. Hence, this option has been rolled out.
Ability to forecast number of records to be processed for pre cal job to identify the tentative time of the job completion: Completed",15/07/2021,16/07/2021,16/07/2021,NA,NA,Yes,Yes,No,No,Data issue,NA,The increased number of Sparks promotions and them being associated to a large number of products have contributed to this.,NA,NA,NA,19/08/2021,19/08/2021,#VALUE!,Saloni,Closed,,,2021,7,July,9/16/2025,1524,>60
15/7/2021,15/7/2021,88210486,17:24,18:00,00:36,Customer Channels,In-Store and Omni Channel Commerce,,,,MI,Stores reported fatal error on tills,TCS,Retail,,POS,RC unknown,57503,"Multiple stores reported the error message – A fatal error has occurred and the application will now terminate.  A member of the loss prevention team accessed POS back office at 17:24 to amend a current message which already exists to give store colleagues guidance about till scams.  The correct process may not have been followed initially and the wording was revised at a second time at 17:29.  The message that was created at 17:24 was blank and would have caused the fatal error on the tills.  As the message was quickly amended by the user, this limited the impact to our till estate.  Tills were validated to ensure everything remained stable. The team will also investigate the options for putting validations in place to prevent blank messages being deployed to tills.",Support identified a patch which will let the tills to ignore the missing “Last amended timestamp” if at all it receives the bad delta going forward. Patch testing underway: Testing is complete and the change is planned for 1st Sep after store trading hrs.: Completed on 01/09,15/07/2021,26/07/2021,26/07/2021,TBC,TBC,"No, Not applicable",No,Yes,TBC,RC Unknown,NA,"The affected tills had received a “bad delta” from the Back office. The delta did not have “Last amended timestamp”, which the tills expected. Upon receiving the bad delta, the tills experienced a “Null Pointer exception”. The cause for the bad delta sent on 15/07 is inconclusive, since it could not be recreated.",NA,NA,NA,1/9/2021,January,2021,Saloni,Closed,,,2021,7,July,9/16/2025,1524,>60
14/7/2021,7/15/2021,88209138,18:46,3:55,09:09,Customer Channels,Customer Channels,,,,MI,Sign-in page isn't working for .COM,TCS,.com,,Customer .com website,RC identified,57501,The sign-in page for .COM stopped working from 18:45 (14/7).  This resulted in an overall dip of 60% of orders during the incident time frame. The content data was corrected to resolve the issue at 03:55 (15/07). The cause was confirmed to be hyperlink referencing being lost when content was being published between AEM (Adobe Experience Manager) and WCS as an emergency deployment. ,"The deployment teams to double check if all the content configs are updated after any upgrade.
Synch between the AEM auth and pub after any deployments.",15/07/2021,15/07/2021,15/07/2021,15/07/2021,15/07/2021,Yes,Yes,No,Yes,Customer Tech change,CR,"After the AEM upgrade on 25/06, the Tier 1 content config was not configured properly on the AEM pub, wherewas it was configured correctly in AEM auth. On 15/07,  when the Tier 1 content was triggered as a part of the Sale Launch, the HREF (hypertext reference) links were missing as part of the default setup in AEM pub.
",CM6259,TBC,NA,15/07/2021,15/07/2021,#VALUE!,Aparna,Closed,,,2021,7,July,9/16/2025,1525,>60
10/7/2021,7/10/2021,88203095,03:00,6:47,03:47,Foods,Food Supply Chain,,,,SI,Delay to the Foods Bradford NDC allocation.,TCS,Foods,,Bradford DC allocation,RC identified,57407,"Alerting indicated that the Bradford NDC allocations had not processed out of the Foods ASO system. To improve the batch performance, a process was introduced in ASO on 05/07 that reduced the number of batches sent to downstream systems by merging three batches into one. This new process in ASO expects to receive allocations from Quantum within 30 minutes however, due to an overnight job failure in Quantum, the allocations were sent to ASO with a delay, causing the ASO process to not run. Support manually triggered this process for sending allocations to the downstream systems. The Bradford NDC allocations reached the DC by 06:47 (SLA: 06:00","All parties to be advised - Ensure that any discussions regarding using backup allocation/28 day order should start at 02:30 and FCS should be advised. - Closed

The time limit for automatically sending orders out from Quantum to ASO has been extended to a maximum of 120 minutes, after which a SOP will instruct support to manually trigger the orders.  Investigate if this trigger can be automated. - Closed

PCM to be added to the automated mail alerts for Bradford NDC allocations. - Closed",7/10/2021,7/15/2021,7/15/2021,7/15/2021,7/15/2021,Yes,Yes,"No, No",Yes,Customer Tech change,CR,"The new process in ASO expects to receive allocations from Quantum within 30 minutes however, due to an overnight job failure in Quantum, the allocations were sent to ASO with a delay, causing the issue.",142426,Insufficient testing,NA,15/07/2021,15/07/2021,#VALUE!,Rehan,Closed,,,2021,7,July,9/16/2025,1529,>60
10/7/2021,7/10/2021,88203170,08:08,10:53,02:45,Customer Channels,Customer Channels,,,,SI,Increase in errors on PDP Pages,TCS,.com,,Customer .com website,RC identified,57382,Alerting indicated an increase in errors on the Product Details Pages (PDP) from 08:00. The issue occurred as one of certificates had not renewed correctly. The certificate renewal was reverted by 10:52 to restore services. Marketing emails to customers were paused and then resumed by 11:09.,The teams have documented the process of correctly renewing and vaidting the certificate renewals.,7/10/2021,7/10/2021,7/10/2021,7/10/2021,7/10/2021,Yes,Yes,"Yes, Yes",NA,Customer Tech change,CR,"The root certificate was renewed however, the teams missed to renew the intermedicate certificate",CM-6412,Post-change validations were not performed,NA,7/10/2021,July,2021,Vinay,Closed,,,2021,7,July,9/16/2025,1529,>60
5/7/2021,7/6/2021,88198345,17:00,3:05,10:05,Customer Channels,Customer Engagement ,,,,MI,No service or marketing emails sent from SalesForce Marketing Platform,Salesforce,Loyalty & Marketing,,Salesforce,RC identified,57406,"E-mails sent from the SalesForce (SF) Marketing platform were not being received as expected. Service emails, marketing emails for Foods, marketing trigger journey mails and order confirmation mails to customers were affected. SF identified the cause to be a central DB issue at their end impacting multiple customers, including Customer. SF applied new indexes and rebuilt the database to restores services by 03:05 on 06/07. All critical emails such as order confirmation, transactional mails and scheduled C&H campaign mails were then sent successfully.  Marketing emails resumed when a SFTP issue was resolved by the reversal of a user set up which was incorrectly configured by SF.","Create an alert for long claim times - Complete

Update internal dashboards for more accurate tracking of this type of issue.- Complete
Investigating the monitoring in place at the time of the incident to identify areas for improvement - Completed
 Analysis of the current incident playbooks for performance degradations to determine options for faster resolution in future incidents - Completed

Create an alert that is triggered on an aggregate percentage of SQL calls that time out -Completed
Update to internal playbooks for recovery on long claim times -Completed
   Long-term goal to migrate to a SQL resiliency library developed to provide better insights, analytics, and resilience into SQL, prioritizing high impacting areas Completed
 Roll out the new index that was added and add the Option_Unknown to the affected SQL query to avoid the plan from changing unexpectedly - Completed",7/6/2021,7/15/2021,6/8/2021,6/8/2021,9/8/2021,No,No,"No, No",Yes,3rd party issue,NA,"1. An increase in database contention caused by the throttled route
2. The database engine made  an automatic change to the execution plan for a single stored procedure. The plan impacted the email triggers from SFMC",NA,,,6/8/2021,June,2021,Aparna,Closed,,,2021,7,July,9/16/2025,1534,>60
7/1/2021,7/1/2021,88191267,08:43,10:40,01:57,Supply Chain,C&H & Intl Supply Chain,,,,SI,Demand and Fulfillment (D&F) user interface unavailable,TCS,C&H logistics,,D&F,RC unknown,57379,The user interface (UI) for the C&H Demand and Fulfilment (D&F) application was unavailable to colleagues between 08:20 and 10:40. Blue Yonder restarted their application services by 10:40 to restore services. Blue Yonder to perform a detailed RCA.,"BY to take a decision to terminate the backup process if the batch completion delays in future - If the backup tends to overrun in the future, the decision to terminate the backup will be taken after analyzing the progress of the backup job completion.
BY unable to replicate the issue that caused the calc plan job to fail.",7/1/2021,7/7/2021,NA,NA,NA,Yes,Yes,"Yes, Yes",NA,RC Unknown,NA,"The D&F batch completion was delayed due to a possible data issue with a few SKUs in one of the calc plan jobs. This delayed the daily system backup which over-ran on the database consuming all the available resources ,in turn making the database server unresponsive. Hence, the database connections from the app servers dropped causing the D&F UI to be unavailable. The data issue could not be replicated in the lower environment",NA,NA,NA,12/7/2021,December,2021,Saloni,Closed,,,2021,7,July,9/16/2025,1538,>60
01/07/2021,7/1/2021,TBC,08:39,10:08,01:29,Customer Channels,Customer Channels,,,,SI,Incorrect Facets Displaying on PLPs,Bloomreach,.com,,Customer .com website,RC identified,57384,"Business users reported seeing irrelevant filters (Facets) on numerous Product Listing Pages (PLPs). The issue was caused by an unplanned release at Bloomreach (our third-party search provider) which was reverted by 07:15. In addition, a full cache clear was performed on the PLP pages and the search API by 10:00 to restore services. Bloomreach will be performing a detailed RCA.
","
Bloomreach QA team has identified the test scenarios will be including these in testing: Closed
Create automated test scripts changes for customization for future release: Closed
Customer Team to clear the cache after every release: Done : Closed
",7/1/2021,7/3/2021,7/1/2021,NA,NA,"No, Not applicable","No, ","No, No",Yes,3rd party change,NA,Bloomreach had an unplanned release to fix an issue related to conflict resolution of customizations at their end. This caused  incorrect facets get displayed in the PDP pages.,NA,NA,NA,3/7/2021,March,2021,Saloni,Closed,,,2021,7,July,9/16/2025,1538,>60
25/06/2021,25/06/2021,88183686,07:43,13:00,05:17,Supply Chain,C&H & Intl Supply Chain,,,,MI,Over-allocation of orders at Welham Green DC,TCS,C&H logistics,,WMS,RC identified,57216,"Welham Green DC reported they received 349k boxed orders, however, the planned capacity was 44k. As the delivery schedules were missed, orders for Donington, International and all UK retail stores were all cancelled from the DC. The issue was caused by a change that was implemented over night and caused over allocation in WMS due to an issue with the algorithm. This change was reverted, and some orders were tested and allocated correctly. A work around was carried out to remove the entire allocation and the orders were kept ready for re-clustering - the decision was then made by Logistics to cancel all orders for today.","Review overall deployment strategy for WMS Deployments – engage more senior engineers / SMEs to carry out thorough reviews and validation before deployment into production: WMS product team - Closed

Review and amend hypercare procedures and policies for each change: WMS product team Review and updates all existing SOPs with latest capabilities which were delivered in last 12 months (i.e. Locus, ScanTowers, 6Rivers etc). This is to speed up overall recovery and reduce MTTR: WMS product team -Closed

Create a Control M alert for validating orders created comparing against allocated.: WMS ERS Team: Alert have been set up for all the C&H sites, setting up the same in Foods WMS sites are in progress

Setup RDT validations to check for over picking performed by DC users: WMS product team Hold sessions with the DC to ensure they understand the correct manner to raise issues to IT in a clear and timely manner: Naga: Naga covering as part of regular sessions he holds with the DC",6/25/2021,6/28/2021,NA,NA,NA,"No, Configured",No,"No, No",No,Customer Tech change,CR,"The issue was caused by a change that was implemented over night and caused over allocation in WMS due to an issue with the algorithm. This change was reverted, and some orders were tested and allocated correctly.",142926,Insufficient testing,NA,7/8/2021,July,2021,Saloni,Closed,,,2021,6,June,9/16/2025,1544,>60
23/06/2021,6/23/2021,88182126,14:34,15:35,01:01,Commercial Trading,C&H Commercial Trading,Commercial Trading,C&H Commercial Trading,,SI,SSI application unavailable,TCS,C&H logistics,,SSI,RC unknown,57308,SSI was inaccessible between 14:24 and 15:35. SSI users would have been unable to raise purchase orders and perform sales/stock planning for this duration. The SSI (Sales and Stock Intake) application was unavailable after an increased number of threads (systemic processes) were observed on the application pods. The entire SSI pod stack was restarted to restore services. ,Teams are prioritizing the migration of SSI from AKS V1 to V2.,6/23/2021,6/30/2021,6/30/2021,TBC,TBC,Yes,Yes,"Yes, Yes",No,RC Unknown,NA,11 out of the 14 CnH Ignite pods restarted automatically due to a certain increase in memory utilization. Cause of the sudden increase is believed to be the inherent limitations of AKS V1. ,NA,NA,1511,6/30/2021,June,2021,Vinay,Closed,,,2021,6,June,9/16/2025,1546,>60
6/16/2021,6/16/2021,88173718,15:38,15:58,00:20,Customer Channels,Customer Channels,,,,SI, Drop In Order Volumes Due to Payment Related Error Message,Worldline,.com,,Customer .com website,RC identified,57103,"Between 15:38 and 16:46, a sharp increase in the number of payment processing errors was observed while placing an order.  Customers received an error advising them that there is a problem processing their payment.  Retail Dashboard in-days sales reporting was impacted but is now fully up to date. An internal system at Ingenico generated a lot of errors which activated Ingenico's own DDOS (Distributed Denial of Service) protection and banned some IP's and the token server was not able to communicate. These IP's were unblocked by Ingenico and service was restored.  One of the WCS Halls (Hall 2) did not update correctly with the repointing of the payment traffic after the Ingenico issue and there were payment related errors between 16:11 – 16:36.   However impacted orders during this second timeframe was significantly less than the initial order drop.","Awaiting answers for a few follow-up questions from Ingenico-Completed
Disabling the blacklist of the global token server during such scenarios: Ingenico - Completed
Increasing the IP connections: Ingenico -  Completed
ALerting to be introduced at GI(Global Infra end) to proactively identify any issues with their token server : Ignenico - Completed",6/16/2021,6/16/2021,6/16/2021,6/30/2021,6/30/2021,"No, Configured",Yes - the incident was highliged bu an alert at our end and not at Ingenico,"No, No",Yes,3rd party issue,NA,An internal system at Ingenico generated a lot of errors which activated Ingenico's own DDOS (Distributed Denial of Service) protection and banned some IP's and the token server was not able to communicate. These IP's were unblocked by Ingenico and service was restored. ,NA,NA,NA,6/30/2021,June,2021,Saloni,Closed,,,2021,6,June,9/16/2025,1553,>60
6/16/2021,6/16/2021,88172176,09:24,12:30,03:06,Customer Channels,Customer Channels,,,,SI,Customer.com - Irrelevant Facets Displaying on PLPs,TCS,.com,,Customer .com website,RC identified,56945,Business users reported seeing irrelevant filters (Facets) on numerous Product Listing Pages (PLPs). The issue was impacting customer experience on the site. The issue was caused by two attributes that inadvertently appeared on the site in the morning following the Bloomreach (BR) nightly jobs. One of attributes was hidden globally at BR's end. The second attribute did not globally hidden as expected following the nightly job completion on 17/06. A case was raised with BR and they hid the second attribute at 11:00 on 17/06.,Enhanced testing and deployment processes to be incorporated for all future SCL deployments,6/16/2021,6/21/2021,NA,NA,NA,"No, cant be configured",No,"No, No",NA,Unauthorised Change ,NA,An unplanned SCL deployment at our end had included a new attribute in field.,NA,NA,NA,6/21/2021,June,2021,Aparna,Closed,,,2021,6,June,9/16/2025,1553,>60
14/06/2021,6/14/2021,88170874,16:43,6/14/2021 19:00,02:17,Store Operations,Platform & Store Ops,,,,SI,"Multiple store colleagues unable to login to Honeywells using the device PIN code
",TCS,Retail,,Honeywell,RC identified,57005,"From 16:35, some store colleagues reported that they were getting a pin code error while unlocking Honeywell devices. Only new sessions were impacted, devices already in use were not impacted by this issue. The majority of the honeywell devices were in use on the shop floor during the incident. ","The team is to update the current documentation and processes to complete this type of activity, so that this type of action will not occur again.  ",6/14/2021,6/14/2021,6/14/2021,NA,NA,"No, Not applicable",No,"Yes,Yes",No,Human error - Customer Tech,TBC,"Whilst setting up a new pin code for one store within Airwatch, a significant UI change resulted in the same pin code getting assigned to the whole estate in error. This was quickly reverted to restore services. Airwatch processes have been updated to reflect the new UI to prevent further occurrences. ",,,,6/15/2021,June,2021,Aparna,Closed,,,2021,6,June,9/16/2025,1555,>60
04/06/2021,6/4/2021,88158626,08:19,11:19,03:00,Customer Channels,Customer Channels,,,,SI,Intermittent EgiftCard order failures,TCS,.Com,,E-gift card,RC identified,56915,"During the manual checks, support teams identified that huge number of egiftcard despatch emails failed due to mandatory links missing. This impacted any new orders created intermittently since 03/04.",Making the config change in Sterling to prevent Sterling from sending empy builder requests to EM - Completed on 29/06,6/4/2021,6/28/2021,6/28/2021,6/29/2021,6/29/2021,Yes,Yes,"Yes, Yes",Yes,Design issue,NA,One of the EM brokers service was unable to process requests as it become unresponsive. Sterling had been sending empty builder requests to EM causing increased resource utilization on the EM resulting in the issue.,NA,NA,NA,6/29/2021,June,2021,Aparna,Closed,,,2021,6,June,9/16/2025,1565,>60
04/06/2021,6/4/2021,88159013,17:00,22:40,05:40,Customer Channels,Customer Channels,,,,SI,Business users unable to build content for the weekend,TCS,.Com,,AEM,RC identified,56802,"The AEM application was down preventing business users from building website content. This impacted the ability to update C&H, Foods and Style and Living content across UK or Ireland for briefs going live on multiple platforms. This included the Summer Beauty Bag that was to be taken down across the site and Homepage amendments.",CTL team confirmed that they will follow the correct steps while restarting the AEM server. This will be cascaded to the team - (Venkatasivanarayana Darsi confirmed on this),6/4/2021,6/4/2021,6/4/2021,NA,NA,"No, Not applicable",No,"No, No",NA,Customer Tech change,CR,Human error by the CTL Infra team while performing a prerequisite task for the AEM upgrade - the AEM server restart was performed using an ID which did not have the root permissions,CM-6259,Incorrect deployment plan,NA,16,January,1900,Aparna,Closed,,,2021,6,June,9/16/2025,1565,>60
03/06/2021,6/4/2021,88157178,19:45,14:30,18:45,GTS,Group Technology Services,,,,SI,All jobs on Datastage node P1DHDS001 failing,TCS,Integration,,All DS jobs,RC unknown,56832,Proactive alerting indicated that all jobs on one of the DataStage (DS) nodes were failing from 19:45. ,"1 Which job had gone into the infinite loop and why ? - The root cause for this job to go into an infinite loop could not be determined (it was deleted as a part of the recovery). As per the script logic, if the number of file with a specific pattern in that path is greater than 10 this infinite loop is possible.
 The possibility of having more than 10 files are mostly due to some manual action. Because as per the script only 3 files will be available in that location.
Support suspects that  this was caused due to some incorrect manual run of the job or manual file creation by someone in tha
2.Do any other jobs use a similar chunking process ? No – only this job uses this bespoke chunking mechanism
3.Review the datstage impact in the AIM tool - Completed - Validated and revised the Impact statement as per the recent changes implemented in DS and cleansed the existing impact from Integration end.
4. Business is mainting a price list in backend table and SAP can extract and share it.  - Completed
5. An extra validation has been introduced so that the job will fail if the 10-file threshold has breached - Ratnam - Completed (added extra condition in the script to make the job fail and it won't go in loop)
6.The VoD – relating to Scan and Shop had the previous day’s data for both C&H and Foods. Can the data for VoD be aligned with POS for all future issues with current prices - Completed , POS have come up with a step to include this.
7. iNode storage utilisation alerting for DS nodes to be configured as a Sev-1 – at 80% full - Sathish - Completed
8. The iNode storage utilisation polling interval needs to be changed from 30 mins to 15 mins - Mani - Completed
9. The recovery of i3003 interface to be prioritized if the issue re-occurs. - Vignesh to re-iterate this to the team – Completed",6/3/2021,6/3/2021,6/4/2021,6/4/2021,6/4/2021,Yes,Yes,"No, No",Yes,RC Unknown,NA,The iNode for the GPFS file system space was full due to approximately 1 million 0kb files being generated as one of the datastage jobs went into a loop.,NA,NA,NA,6/4/2021,June,2021,Aparna,Closed,,,2021,6,June,9/16/2025,1566,>60
02/06/2021,6/2/2021,88153785,15:00,6/2/2021 15:28,02:07,Store Operations,Platform & Store Ops,,,,SI,Performance issues in Intelligent Waste,TCS/Customer,Retail,,Intelligent Waste,RC identified,56913,"From 15:00 on 1st June, users in stores reported performance issues when using the Intelligent Waste app on their Honeywells.  
The new functionality that calculates waste and any price reductions had caused the increased usage.  
To restore the service, the Intelligent Waste database subscription was upgraded to handle more DTU’s (data transaction units) 
and the new waste reduction calculation job and proactive Honeywell notifications were disabled. 
Users were experiencing performance issues the next day also and the Intelligent Waste app was reverted to the previous version on all honeywells.",The project team will now take this forward and the PIR can be closed - the change was reverted.,6/2/2021,6/2/2021,NA,NA,NA,"No, Not applicable","No, ","No, No",No,Customer Tech change,CR,DB calls during the peak were taking too long to respond. We were using transaction type connection on certain places which possibly held the DB sessions longer than required. We also found some traces of deadlocks as well. Database handling looks to be the clear culprit behind the incident which got aggravated by how Springboot DB framework was making queries in the background. On further investigation we found JPA framework was making more queries in background than we thought it would.,141261,Insufficient testing,NA,6/7/2021,June,2021,Saloni,Closed,,,2021,6,June,9/16/2025,1567,>60
28/05/2021,5/28/2021,88149290,07:30,17:02,09:32,Customer Channels,Service Experience,Customer Channels,Service Experience,,MI,EgiftCard Backend Service Unavailable,TCS,.Com,,E-gift card,RC identified,56688,"Following a planned deployment to the e-gift card backend, the service was unavailable and not processing any purchased e-gift cards. The issue impacted customers attempting to retrieve their gift cards whether old or new. Service was recovered by restoring from a backup image of the application.","
Dev team will not be using the legacy jenkins script for deployment(as it was using one deployment script which was coupled with aws infra). It is in their pipeline to migrate e-gift card to azure v2.
They will do manual deployment for prod till the e-gift card is migrated to azure v2.(aws part of appln w beill moved to azure v2.)
 The ownership of the deployment script to be defined - Saurabh confirmed that going forward this will be picked up by the devop",6/3/2021,6/8/2021,6/8/2021,NA,NA,"No, Not applicable",No,"Yes, Yes",Yes,Customer Tech change,,The script used during the AWS eGiftCard production deployment was using potentially obsolete parameters (Jenkins which creates issue),CM6228,,,,January,1900,Aparna,Closed,,,2021,5,May,9/16/2025,1572,>60
27/05/2021,5/27/2021,88148420,15:15,17:00,01:45,Store Operations,Platform & Store Ops,,,,SI,Queue pile up on various POS transaction queues,TCS,Retail,,POS,RC identified,56771,"On 27/05, MIM were engaged at 15:15 as queue pile ups were seen on various POS transaction queues on the beanstore trade servers.  The servers were stopped to allow the queues to clear and then restarted. During the restart, tills would have not been connected to the trade servers however transactions would still go through. The queues were cleared by 17:00 and any risk was mitigated.  Due to the issue retail dashboard sales were impacted until all messages were processed.","1.Only messages that the system cannot process due to lack of data should be going into the dead queue and as such a defect has been raised to Flooid
 - Fix is being tested now in non-prod and will go into prod on  5th July. 
2.Alerting to be placed on all the Beanstore trade queues to prevent pileups - Completed
 - alert configured with threshold limit 50K. 
3.Find out a way to identify which subscription messages in the intervention queue should go to: OLTP, MQ or Topic
 - Low priority, no ETA as of now (Backlog Id : RPOS-2055)
4.Defect created to fix the issue with bulk-replay from POS – the bulk method is missing some messages in the replay - Bala have created a bug  and passed to Seeva
 , this will also be long term and doesnt have an ETA. (Backlog Id : RPOS-2055 )",5/27/2021,5/28/2021,5/27/2021,7/5/2021,7/5/2021,Yes,Yes,"Yes, Yes",Yes,Customer Tech change,CR,"All sales messages should go to an input queue.  Each service such as Loyalty, Gift card and credit vouchers has a dead queue.    If the messages cannot be processed, then they go into the dead queue.  The dead queue has a limit of 50K messages per node.  During the incident recovery on 27th May, a defect was found in the Dead Queue - if each sales transaction has got the attribute for Loyalty, Gift card or credit voucher – then it should go into the input queue and then process queue.  In this case the Topic service was putting all transaction messages in the Dead queue.  This led to the Dead queue and subsequent queues as above to fill up.  When the Dead queue was cleared down on 27th May apart from the messages that should genuinely be in the Dead queue, other actual transaction messages amount to the £846K were also cleared leading to the variance.",137133,NA,NA,29/05/2021,29/05/2021,#VALUE!,Aparna,Closed,,,2021,5,May,9/16/2025,1573,>60
25/05/2021,5/30/2021,88145764,10:30,13:53, 123:23,Commercial Trading,C&H Commercial Trading,Supply Chain,Foods and C&H Supply Chain,,MI,Business Objects Personal Folders Missing for Users,TCS,SAP (Data & Digital),,Business Objects Reports,RC identified,56691,"At 10:30 on 25/05, users reported that they were unable to see their personal folders within the Business Objects (BO) reporting application. Users who have reported this issue are from C&H Logistics , finance and FAQT – other users have not reported issues at present but may potentially be impacted if they have personal folders in BO. Only self-service reports placed in user’s personal folders with BO are impacted, BO itself has no problems. By 27/05, support teams have restored the personal folders from backup and are restoring these to user's personal folders in Production in order of priority for users who have raised a log and we are planning to do a bulk upload for all other affected users.","A monthly audit tracker needs to be created and maintained that provides user details for those who have personal folders, how many users are still active etc: Completed

Housekeeping will continue on a daily basis to disable the ID and remove the associated AD groups of a leaver’s user profile: This will be an ongoing activity that needs to be performed by Access Management

The step-by-step recovery performed as a part of this incident needs to be documented in the SOP: Completed

Create a new SAP role in BO and tag the HO/Store users to it so that if the OU gets renamed accidentally in future, it will be easier to recover the lost reports – Completed

Alerts to be configured if an OU is renamed – Completed

Access restrictions to be introduced so that Access Management colleagues can work only on user groups and contacts and not be able to amend organisational units.: In progress: Dharma

Looking into the possibility of introducing a validation that causes the job to fail if the OU name is found to be different: Not Feasible",5/25/2021,5/25/2021,03/07/2021,NA,NA,"No, Configured","No, ","Yes, Yes",NA,Human error - Customer Tech,WO,"While changing the user ID from a 'C' ID to a 'P' ID, the Head Office  organisational unit was accidentally changed from 'HO' to 'C'.",1044781,Human error,NA,6/29/2021,June,2021,Saloni,Closed,,,2021,5,May,9/16/2025,1575,>60
18/05/2021,5/18/2021,88137253,08:00,12:40,04:40,Foods,Food Supply Chain,,,,SI,EDNs from Milton Keynes and Bradford NDC failing in GIST,TCS,Integration,,"WMS, CSSM",RC identified,56638,EDNs (Electronic Delivery Notes) from Milton Keynes and Bradford NDC were failing in GIST. This impacted stock integrity in stores from 05:30 to 12:40. A change was reverted in middleware as it caused the message ID format in the XML messages to change.  CSSM team continue with their stock corrections as and when pallets are received by stores - there is no impact to stores and the CSSM stock will no longer be corrupted.,"End-to-end testing to be performed for all subsequent changes to ensure that issues of this nature can be prevented, going forward - Raj and Nanda

The errors on the GIST DWS to be more targeted to aid easy identification of the issue - Mark W

Introducing alerts to ensure that issues of this nature are detected in a timely manner which can enable the teams to commence recovery and thereby mitigate impact - Mark W",5/18/2021,5/18/2021,5/18/2021,,,"No, Configured",No,"No, No",NA,Customer Tech change,Yes,A change had been implemented in middleware to change length of the tracking ID for the i172 messages from 12-characters to 16-characters. The GIST DWS system rejected the messages with 16-characters,140614,E2E validarion across all consumers was not performed during UAT,NA,10/17/2021,October,2021,VInay,Closed,,,2021,5,May,9/16/2025,1582,>60
17/05/2021,17/05/2021,88135733,07:30,10:15,03:40,Foods,Food Supply Chain,,,,SI,GIST not receiving the trailer ASN messages,OpenText,Foods,,ASO,RC identified,56636,"GIST reported that they did not receive the ASN (Advance Shipment Note) messages, impacting the trailer shipments. At around 10:15, GIST confirmed that ASN messages started processing successfully. On 19/05, GIST reported that the Thatcham depot had issues in ASN processing. This is believed to have been caused by an issue in OT that lasted between 12:09 and 13:54 on 17/05. GIST confirmed resolution after OT fixed their issue.","

Review monitoring and alerting for improvement opportunities - Completed

Add another node to the EMEA MS SMG cluster - Completed

Apply patch to address thread issues - Completed",5/19/2021,9/30/2021,,,,"No, Yes",No,"No, No",No,3rd party issue,NA,"Due to a latency issue on the OT database on 17/05, the queries slowed down causing the jobs to run longer than usual. In addition, there were multiple SSL and transaction failures and an influx of load during the incident causing the issue at Customer end.",NA,NA,NA,09/30/2021,September,2021,Vinay,Closed,,,2021,5,May,9/16/2025,1583,>60
14/05/2021,5/14/2021,88132541,04:15,8:30,04:15,GTS,Group Technology Services,,,,SI,All Control-M jobs for order date 13/05 held incorrectly,TCS,Infra,,Control M,RC identified,56717," As part of the planned SAP release, instead of holding the relevant SAP control M jobs, all Control-M jobs were held inadvertently. This delayed the order release for C&H and Foods DCs. Also, the DCs could not systemically despatch stock to stores for 2 hours during this issue. Further impact details are being verified with the business. All jobs were released to restore service","Was the maker checker process followed after the jobs were held - PCM filtered out the SAP jobs alone and validated that they had been held. 

Suganya to re-iterate the correct process of validating the held jobs as part of action number 7. - Closed



Could there be any alerting to advise if the jobs have been incorrectly held - Not possible to set up alert for jobs incorrectly held jobs as no logic is available. Cancelled since action is not feasbile



Set up restrictions to ensure that all the jobs in Control-M cannot be held together -Cancelled since action is not feasbile




Checking the possibility of setting up quantitative resources for the relevant jobs so that they can be prevented from running prior to a release activity. For example, a quantitative resource can be setup for all SAP jobs which can be used to prevent the SAP jobs from running as required. - During an activity, the jobs are held at a time, however, post the activity, the job cannot be released in a phase manner. This might cause performance degradation of the SAP system since the jobs will be executing at the same time as advised by the Support.  Quantitative Resource cannot be set up on a folder by folder basis. - Cancelled since action is not feasbile


Alerting for the WMS clustering and other critical jobs if they have not run for more than 15 minutes? - WMS have introduced a scripting for all the critical jobs to look up a key table in WMS from which transactions are send to downstreams. If it goes beyond 5 mins, WMS will get an alert. - Closed 
Suganya to discuss with Santhosh and team to look into the possibility of automating the control-m folder holds - Santhosh is working with the Ignio support to set up the automation alerts. Testing is completed succesfully in non-prod and Suganya have requested ignio team  to make some more changes after which implementation will be done in prod.  Support have raised a backlog id - (INF - PCM - 00030)                                                                                  Suganya to advice the team to use """"all jobs"""" viewpoint while validating job holds to ensure that no jobs have been held incorrectly - Suganya have cascaded this information to the team - Closed",5/14/2021,5/14/2021,5/14/2021,7/9/2021, ,Yes,Yes,"No, No",No,Customer Tech change,CR,"As part of the planned SAP release, instead of holding the relevant SAP control M jobs, all Control-M jobs were held inadvertently",139654 ,"As a part of the SAP Release activity, all jobs in Control-M with OD 13/05 were held in error",NA,9/8/2021,September,2021,Aparna,Closed,,,2021,5,May,9/16/2025,7,0-30
10/05/2021,5/10/2021,88127035,06:30,9:43,03:13,Customer Channels,Customer Channels,,,,SI,Problems entering card details in checkout,TCS,.Com,,Customer .com website,RC identified,56603,Multiple alerts indicated that the customers were unable to enter payment card details in checkout. A database change performed on 10/05 caused the issue which was reverted to restore services. This led to a 50% drop in the number of orders,Payment and checkout related testing will be performed prior to all subsequent deployments.,5/10/2021,5/10/2021,5/10/2021,5/10/2021,5/10/2021,Yes,Yes,"No, No",NA,Customer Tech change,CR,A physical policy change in DB was carried out to add “SparksPay”. The required app configuration entry was made in the DB to support the new payment method without clearing the WCS cache. The old code (R107) expected the DB entries relating to the previous (R107) app configuration in the WCS cache and therefore the failure occurred., CM6139,The payment checkout scenarios was not tested during the UAT,NA,5/11/2021,May,2021,Aparna,Closed,,,2021,5,May,9/16/2025,1590,>60
07/05/2021,07/05/2021,88123726,06:26,15:00,08:34,Customer Channels,Customer Channels,,,,SI,PIM Application is unavailable,TCS,.Com,,PIM,RC identified,56421,PIM not accessible after a planned certificate renewal. Application was restarted but further issues were encountered. A file system permissions issue was resolved to restore services. Users were not able to make any changes to products/attributes until recovery.,25/06 : CTL team along with RedHat have fixed the mountpoint failure issue by performing steps recommended by RedHat against CM-6330,5/7/2021,NA,6/9/2021,6/25/2021,6/25/2021,"No, Not applicable",No,Yes,Yes,Customer Tech change,CR,Microsoft and RedHat have advised that the restart did not happen  as expected due to a mountpoint failure in AzFS (Azure Federation Services),CM-6128 ,Unexpected error during deployment,NA,6/25/2021,June,2021,Aparna,Closed,,,2021,5,May,9/16/2025,14,0-30
04/05/2021,05/05/2021,88120710,21:32,3:05,05:33,Customer Channels,Customer Channels,,,,SI,Payment Failures causing .com orders to go offline,Worldline,.com,,Customer dot com website & POS,RC identified,56278,"Support received multiple alerts indicating the payment calls were failing for all .com orders from 21:32 on 04/05. In addition, Ingenico payment system was throwing an internal Server Error for all the de-token calls from Customer. Investigations revealed that a storage upgrade at Ingenico end had caused the issue. Support confirmed service restoration at 03:05 and all the offline orders got processed successfully. ~2.5K Orders were auto cancelled due to the Error response from Ingenico during the 1st and 2nd Payment retry attempt. These customers would have received a payment decline email as the payment is not processed.","Dedicated taskforce to review and identify enhancements of overall GTS solution (including hosted API gateway) which will include the service resiliency in addition to performance requirements: Closed
Review the Charon HaProxy which provides DDOS protection mechanism, to tune/change the aggressive ban threshold. Or to add new functionality to GTS application to favorize the legitimate GTS calls coming to HaProxy DDOS filter.: Closed
Review & Implement additional platform monitoring checks for additional monitoring of overall GTS solution (including Charon hosted API gateway) with further integration with PagerDuty for alerting: In progress: Completed
Review the process of incident escalation and handling by MCS Incident Manager and Instore AXIS on-duty manager: Closed
Create and test the internal process major incidents.: Closed",05/05/2021,05/05/2021,NA,NA,NA,Yes,Yes,"No, No",Yes,3rd party change,No,"The investigation performed by Ingenico’s Database team found that the incident was triggered by a storage upgrade that was announced by Ingenico as planned and non-disruptive maintenance and implemented on 4th May. This upgrade impacted Ingenico’s DC2 datacentre. The automatic failover to another DC was not triggered because the database was still active and operational in DC2. When Ingenico switched from DC2 to DC1, the VMs activated in DC1 but triggered the DDOS (distributed denial of service) protection mechanism and due to the influx of calls with error code “500”, this resulted in the blocking of IP addresses of the relevant gateway and batch services. This weakness of Ingenico architecture affected the service availability impacting tokenization and de-tokenization calls for Marks & Spencer.",NA,NA,NA,25/07/2021,25/07/2021,#VALUE!,Saloni,Closed,,,2021,5,May,9/16/2025,1596,>60
4/22/2021,23/04/2021,88108387,16:30,20:30,28:00,Customer Channels,Customer Channels,,,,SI,Customer.com -  In store furniture orders payment failures,TCS,.com,,Customer .com website,RC identified,56350,"Since 12/04, stores fully re-opened the payments for 'Furniture' orders placed in store which have discounts involved, 4 UPC'S have been failing in Sterling/OMS. The payments were failing because the 'line charge' amount received in the payment from POS does not match with the 'line charge' amount held in the order within Sterling. On 23/04, 14 orders were impacted by this issue and support manually resolve the hold for these to allow fulfilment. Meanwhile, a short-term solution (change promo set up of the £1 feet promotion) was tested and the promotions team made the required changes to put this live to all stores for yesterday, 24/04. Comms were sent to the stores with instructions.","Sterling dev team worked on a code fix which went into production on Wednesday 19th, and was validated to be working as expected. Support is currently in the process of validating some orders with the spend and save promos which will be completed today.",4/22/2021,4/22/2021,4/22/2021,NA,5/19/2021,"No, Not applicable",No,"Yes, Yes",Yes,Design issue,NA,The amounts do not match because Sterling & POS apply promotions/discounts in different ways resulting in the apportionment of the line charges to be different  ,NA,NA,NA,5/24/2021,May,2021,Aparna,Closed,,,2021,4,April,9/16/2025,1608,>60
4/22/2021,22/04/2021,88107136,00:45,6:04,05:19,Foods,Food Supply Chain,,,,SI,Delay to the overnight Quantum batch,TCS,Foods,, Bradford DC,RC unknown,56221,The Quantum job responsible for processing the ITLs (Inventory Transaction lists) from the suppliers into NDC had failed causing a significant delay in the overnight batch.,"No pending actions. Darth Vader release (Apr 5th) has reverted the hot fix done. Permanent fix is planned for Greedo release.
The permanent fix is currently discusssed and released on June 28th Greedo release",22/04/2021,4/22/2021,22/04/2021,4/22/2021,4/22/2021,Yes,Yes,"Yes, Yes",Yes,RC Unknown,NA,"The job failed due to duplicate records in the ITL table. A hotfix  was applied on 22/04 which eliminates any bad/duplicate records from the final table to a new one, continues to be in place. 
Support teams are investigating the cause behind the duplicate records in the ITL table.",NA,NA,NA,5/1/2021,May,2021,Saloni,Closed,,,2021,4,April,9/16/2025,1608,>60
4/21/2021,21/04/2021,88107210,17:30,17:50,00:20,Customer Channels,Customer Channels,,,,SI,Customer.com - PayPal - One of the payment methods unavailable,PayPal,.com,,Customer .com website,RC identified,56322,Through alerting teams identified that there are issues with paypal as payment method on the .com website. No orders placed with Paypal currently.,Paypal central issue - case closed,4/16/2021,4/16/2021,NA,NA,NA,Yes,Yes,"Yes, Yes",No,3rd party issue,NA,The business is chasing Paypal for a root cause statement - it was a cental issue at their end affecting multiple customers of theirs,NA,NA,NA,5/6/2021,May,2021,Aparna,Closed,,,2021,4,April,9/16/2025,1609,>60
19/04/2021,19/04/2021,88102761,12:23,13:15,00:52,Customer Channels,Customer Channels,,,,SI,Customer.com website showing pages out of alignment,TCS,.com,,Customer .com website,RC identified,56094,Customer.com Website showing pages out of alignment,"In order to prevent errors resulting from this scenario being cached, a front end fix was deployed in R-108 on 11/05. Support confirmed they havent received any alert so far.
The Splunk alerting has been configured as well to arrest these issues in a timely manner going forward",19/04/2021,19/04/2021,23/04/2021,11/05/2021,11/05/2021,Yes,Yes,Yes,Yes,Code/Product bug,NA,"We know that there was an error relating to MSTopNavTier3, this error caused a large page response to be cached. We have set up a splunk alert to capture this scenario",NA,NA,NA,5/11/2021,May,2021,Aparna,Closed,,,2021,4,April,9/16/2025,1611,>60
18/04/2021,18/04/2021,88102470,13:08,15:51,02:56,Store Operations,Platform & Store Ops,Customer Channels,Platform & Store Ops ,,MI,Issues in store CSSM counting due to stock duplication,TCS,Retail,,CSSM,RC identified,56196,Issues in store CSSM counting due to stock duplication ,"Enhancing the testing process by including additional test-cases to ensure that issues of this nature are identified and fixed pre-deployment -  CSSM Devops

After deployment, DevOps and support teams to validate metrics that are specific to the newly deployed functionality. This will help in identifying the issue earlier. For e.g.: after this change has been re-deployed, support to gather metrics pertaining to store deliveries and validate these for abnormalities - CSSM Support and DevOps

After deployment, support to perform spot-checks on counts, adjustments etc - CSSM Support

Mock Testing / Simulate Deployment changes in production post deployment - DevOPs team to analyse and feedback feasibility on this",18/04/2021,18/04/2021,21/04/2021,6/7/2021,Vinay to check with Bharath/Vel,"No, Configured",No,No,No,Customer Tech change,Yes,"DB stored procedure that had been implemented as a part of CSSM release was working as expected for all cross-dock store deliveries but was however, doubling the systemic stock levels for all Direct store deliveries (direct deliveries from NDC to stores). The stored procedure had not been tested for the direct store delivery scenario prior to the deployment.",138104,The test scenario for Direct Store deliveries was not tested as a part of UAT,NA,,January,1900,Vinay,Closed,,,2021,4,April,9/16/2025,1612,>60
16/04/2021,16/04/2021,88100622,15:45,17:25,01:40,GTS,Group Technology Services,,,,MI,Multiple applications hosted on the Azure cloud V1 were unavailable,Microsoft,"Foods, Retail, Foods, International",,"ASO, SCRD, ORCA/FMD, BINSTOCK, Scan & Shop, MPG, PWM, Intelligent waste, SSI, IBT",RC identified,56092,Support advised that multiple applications hosted on the Azure cloud V1 were unavailable.,"In order to prevent recurrence of the issue, these host updates have been paused until the issue is fully resolved.",16/04/2021,4/20/2021,4/20/2021,NA,NA,Yes,Yes,"No, No",No,3rd party change,No,Microsoft have advised that they have carried out an upgrade activity at the time of the issue.,NA,,NA,4/20/2021,April,2021,Vinay,Closed,,,2021,4,April,9/16/2025,1614,>60
13/04/2021,14/04/2021,88096821,21:25,23:00,01:35,Supply Chain,C&H & Intl Supply Chain,,,,MI,Castle Donington WMS Application Autobagger Deployment & WCS Database Degradation,TCS/Schaeffer,C&H logistics,,"WCS, WMS",RC identified,56091,"As part of the Castle Donington release – two different impacts were prevalent as a result – one for WMS and the other for WCS.
 
From 12:00 on 13/04 a message was observed stating ""LPN required to pack"" on Mezzanine pack benches.  This caused delays when acknowledging the message to complete the pack task.
 
From 19:00, a spike was observed in the pending events from WCS->WMS and vice versa. This was slowing down the automation process at the DC. Investigations revealed a high CPU utilisation on the WCS database.","Between 20:50 and 21:10, there was a sudden spike in traffic into the WCS DB via the application network interface (bond-0). Oracle vendor to investigate this issue - Closed - It has been confirmed that this issue too was due to the replication being switched on.
 
Considering that the users had to notify IT about the spike in the pending events, the option of introducing an alerting for the pending events needs to be investigated - James (SSI) - Closed - Alerting is not possible due to design limitations
 
 SSI received their first systemic alerting only at 19:35. The overall alerting and monitoring hence needs to be holistically revisited and fine-tuned to ensure that issues of this nature are identified earlier-on - James (SSI) and Ravi C - Open - Will be tracked against the WCS migration project
 
 The WCS DB1 was using an increased amount of memory for the duration of the issue. Thomas to investigate if this was caused by a known bug in Oracle wherein the DataGaurd causes the DB memory utilization to increase - Thomas (SSI) - Closed. This action is not required anymore
 
 Analysis required to determine if the WCS application and replication traffic need to flow through separate interfaces. If yes, a plan needs to be devised to implement this. - Thomas (SSI) and Ian (SSI) - Open - Will be tracked against the WCS migration project
 
 Reviewing how we can better troubleshoot WCS performance issues. Standard procedures to be devised which will help the teams narrow-down on the issue in a timely manner. For e.g. : for WMS-related issues, support has an SOP from JDA to troubleshoot the DB/app issues. - James (SSI) and Ravi C - Open - Will be tracked against the WCS migration project
 
Replication of the memlock change scenario in the WCS stand-by DB - Closed - This could successfully be replicated on 16th April 2021
 
 A plan to switch the WCS DB replication back on in a controlled manner, when deemed fit - Thomas (SSI) and Ian (SSI) - Open - Will be tracked against the WCS migration project
 
Implementing the recommendations in the WCS DB to ensure that the memlock parameter does not change depending on the manner in which the WCS DB has bee restarted - Dipti/Boopathi - OPEN - testing environment currently unavailable due to another issue. ETA for recovery is 2nd June ",4/13/2021,4/16/2021,4/21/2021,5/7/2021,,Yes,Yes,No,No,Customer Tech change,Yes,"WMS - Code from Blue Yonder contained an incorrect syntax which caused the new functionality to not recognise Autobagger, causing the error message.  This scenario was not picked up during test.  The current code has been corrected by Blue Yonder and will be retested.  Corrective actions on testing have also been put in place.
 
WCS - Following the deployment of WCS replication via DataGauard, a default memory value of 64kb had replaced the default value of 'unlimited' within the MEMLOCK setting.  This caused the DB performance degradation.  Further analysis in how this default value was inserted is currently under way.",139131,The restart of the SSI DB was performed using the SQLPlus command as opposed to the recommended utility,NA,,January,1900,Vinay,Closed,,,2021,4,April,9/16/2025,1617,>60
12/04/2021,12/04/2021,88095007,13:20,14:16,00:56,Customer Channels,Customer Engagement ,,,,MI,Sparks showing as offline in store and online,Tibco,Retail,,Sparks,RC identified,56197,Sparks intermittently unavailable online and in-store.,"Update 04/28 :An index was added by TIBCO to the ShopperOfferForAwardEngineProc stored procedure and a performance increase was observed in AppDyanmics. Several thousand live offers that are no longer attached to IRCT scoring data has been removed by CRM team.
",4/12/2021,4/13/2021,4/13/2021,4/13/2021,4/13/2021,Yes,Yes,"No, No",Yes,3rd party issue,NA,"ShopperOfferForAwardEngine SQL stored procedure took longer, as it executed with multiple execution plan. 
Over time and with the dataset being changed, a stored procedure may not run as efficient and optimizations are needed to keep the application in peak performance.
",NA,NA,NA,4/16/2021,April,2021,Aparna,Closed,,,2021,4,April,9/16/2025,1618,>60
11/04/2021,11/04/2021,88092869,02:40,10:16,07:36,Supply Chain,C&H & Intl Supply Chain,,,,MI,Delay in the overnight D&F batch,BY,C&H logistics,,D&F,RC identified,56096,Delay in the overnight D&F batch,"AnR order flow and 3PL I0177 job Control M changes have been completed and also I0180- FSV flows which are scheduled to run at 04:00 am these have now been extended 
 Regarding SFTP server monitoring, There are monitor alerts already in place to check for server issues.",4/11/2021,4/11/2021,4/11/2021,4/11/2021,4/11/2021,Yes,Yes,"Yes, No",Yes,3rd party issue,NA,"The problem occurred because of a faulty optic on the AM4(Z-side), Initially it was continuous flapping and the vendor was informed about it and on 11-April when it was continuously flapping the issue with the hardware was identified and replaced the optic and normalized the circuit.",NA,NA,NA,4/23/2021,April,2021,Rehan,Closed,,,2021,4,April,9/16/2025,1619,>60
10/04/2021,10/04/2021,88097774,09:00,9:00,00:00,GTS,Group Technology Services,,,,SI,Teams Ring 3 latency,Microsoft,Microsoft,,Teams,RC identified,56377,"Over the past few weeks, multiple users have reported issues with the Teams desktop application – latency, frequent crashes, lag in audio/video calls etc. System logs from affected users were sent to Microsoft.  Microsoft advised: Customer is part of the Beta testing release program (Ring 3, also known as TAP). The list of features will be different for the beta version of Teams as compared to general customers.
On 23/04, after several discussion with Microsoft, their product team advised that there have been known issues with performance/latency/attendee lists not displaying correctly on the versions of Teams that Customer run on (RING3). ",The workaround to move users to Ring4 has been suspended and any affected key users will have their Teams application uninstalled and then reinstalled as recommended by Microsoft. ,4/26/2021,4/26/2021,4/26/2021,NA,NA,NA,NA,"No, No",No,Code/Product bug,No,"On 23/04, after several discussion with Microsoft, their product team advised that there have been known issues with performance/latency/attendee lists not displaying correctly on the versions of Teams that Customer run on (RING3).",NA,NA,NA,4/26/2021,April,2021,Rehan,Closed,,,2021,4,April,9/16/2025,1620,>60
02/04/2021,02/04/2021,88078227,11:05,13:10,02:05,Store Operations,Platform & Store Ops,,,,MI,Intermittent connectivity issues with Scan & shop and Mobile Pay go,TCS,Cloud,,"Mobile Pay Go, Scan and Shop, Vision On Demand and POS Basket",RC unknown,56172,Multiple stores reported issues with the Scan and Shop and Mobile Pay Go (MPG) applications.,"•	App team to work with Platform support to configure the CPU and memory limit values on their pods to ensure that their pods are prioritised during resource crunche - We are setting limits in v2.  In v1 , platform team suggested to have dedicated node for VOD based on their root cause analysis. So this is not valid action item anymore in v1. - Closed
•	Increase the number of VoD pods in production to mitigate the business impact during pod restarts  - Covered in action # 1
•	Work with Microsoft to address the capacity management and memory leak issues in AKS v1 nodes - Caused by the existing bugs in AKS V1
",4/2/2021,4/7/2021,4/7/2021,Rehan to get the details from Venky,,"No, NA",No,No,No,RC Unknown,NA,Not enough logs on the VoD end to identify the cause.,NA,NA,1511,4/7/2021,April,2021,Rehan,Closed,,,2021,4,April,9/16/2025,1628,>60
01/04/2021,01/04/2021,88083140,18:00,18:00,00:00,GTS,Group Technology Services,,,,SI,Remote users unable to access Microsoft desktop applications using Customer Windows 7 laptops,Microsoft,Technology,,Microsoft destop applications,RC unknown,56325,Some users who are using an Customer windows 7 laptop are unable to access Microsoft desktop applications while working remotely.,"Replacement Windows 10 laptops are being arranged for the impacted Customer users by the WPT team, wherever possible.

The impacted TCS users with a Windows 7 laptop will be provided with a replacement TCS laptop",4/1/2021,4/1/2021,NA,NA,NA,NA,NA,No,No,RC Unknown,No,Unknown,NA,NA,914,4/1/2021,April,2021,Vinay,Closed,,,2021,4,April,9/16/2025,1629,>60
29/03/2021,29/03/2021,88072613,09:00,12:00,03:00,Store Operations,Platform & Store Ops,,,,SI,Customers unable to use the MPG service in stores,TCS,Colleague Devices Efficiency,,Mobile Pay Go,RC identified,56143,"Customers unable to use the MPG service in stores. The team identified an issue with server timings, potentially due to bank holiday/clocks change. The store API latest version was redeployed to fix the issue.","David confirmed that the common api populated by .com+ is generally enriched by another api called mpg-Stores API (soon to be names as enriched stores API).

It was this API which encountered the timing issue, hence only Scan and Shop (formerly MPG) was affected. An internal change was implemented which will now adjust the time zone automatically and thereby prevent the issue from occurring in future.",3/29/2021,3/29/2021,3/29/2021,NA,3/29/2021,"No, cant be configured",No,"No, No",Yes,Customer Tech change,CR,"On 28th March, after the clock change, the server api.marksandspencer.com: this is a common server to Customer and has been working like this since years, did not reflect the correct time and morning 10am BST was reflecting as 9am in the server. Hence the stores which were open at 10:00 were showing closed in the system. 
The  multi store change was deployed on 23rd March and was working till 28th March. However, after the clock change, time check failed and hence all the stores were displaying as closed, causing the issue. ",138037,NA,NA,3/29/2021,March,2021,Saloni,Closed,,,2021,3,March,9/16/2025,1632,>60
29/03/2021,29/03/2021,88072239,08:50,20:00,11:10,Foods,Food Supply Chain,,,,SI,Variance observed in the Quantum Foods order plans,TCS,Quantum DevOps,,Quantum,RC identified,56042,The foods business reported an order drop for some suppliers in the overnight Quantum order plan. The root cause of the issue is likely to be related to a problem with the recovery of Quantum job that failed overnight (Sunday night) due to clock change.  Order planning was calculating for 28/3 using the supply chain date of week commencing 29/3 thus causing a conflict.  This led to variances in the order levels being sent to suppliers.,,3/29/2021,3/29/2021,,,,,,,,Customer Tech change,,The root cause of the issue is related to a problem with the recovery of a Quantum job that failed overnight (Sunday night) due to clock change.  ,,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1632,>60
28/03/2021,28/03/2021,88071191,09:00,13:15,04:15,Store Operations,Platform & Store Ops,,,,SI,Customer.com - BOSS stores received more picks than the actual levels of stock available ,TCS,Sterling,,.com BOSS,RC identified,56107,Some BOSS stores reported that they had received more picks than the actual levels of stock available.  Investigations confirmed that Sterling continued to place orders yesterday against lines which ran out of stock.  The inventory processing had not worked as expected for lines of stock that had sold out.  This impacted 218 stores 4490 lines and resulted in 2521 cancelled orders on 28/03.,"24/06: Emma shared the risk id -  921 .  We have alerts in place already.
11/06 : Sterling IBM updated that it is not a product issue and it is an issue with our code or configuration. 
Sterling support & dev team working together to confirm this. Next meeting scheduled on Tuesday , 15th June.",3/28/2021,3/28/2021,NA,7/18/2021,NA,Yes,Yes,No,No,Code/Product bug,NA,The issue was caused by clock change,NA,NA,921,6/24/2021,June,2021,Aparna,Closed,,,2021,3,March,9/16/2025,1633,>60
26/03/2021,26/03/2021,88067853,09:30,10:30,01:00,Store Operations,Platform & Store Ops,,,,SI,Customer.com - BOSS stores exceeding capacities,TCS,Sterling,,.com BOSS,RC identified,56106,"A few stores reported that they had received more orders through BOSS than their agreed capacity. To mitigate any further impact, any stores which had consumed their capacity for the day were switched off. Stores managed to pick most of the additional orders by the pick expiry time. Only 300 orders failed and cancelled out of the 7235 additional orders.","24/06: Emma shared the risk id -  921 .  We have alerts in place already.
11/06 : Sterling IBM updated that it is not a product issue and it is an issue with our code or configuration. 
Sterling support & dev team working together to confirm this. Next meeting scheduled on Tuesday , 15th June.",3/26/2021,3/28/2021,NA,7/18/2021,NA,Yes,Yes,No,No,Code/Product bug,NA,"We know that the issue was caused by a bug caused by clockchange, however we do not know why this bug triggered before the clockchange",NA,NA,921,6/24/2021,June,2021,Aparna,Closed,,,2021,3,March,9/16/2025,1635,>60
23/03/2021,23/03/2021,88063754,14:07,19:25,05:18,Customer Channels,Customer Channels,,,,SI,Customer.com - No interest free credit furniture orders could not be taken,BNP,BNP,,.com ordering ,RC identified,55512,"88063754 - When clicking on the Interest Free Credit (IFC) option in checkout, the customer was presented with a 404 error page. The Interest Free Credit service is supported by BNP who were engaged to investigate. BNP advised that a planned security fix was deployed on 21st March to reduce the length of the query string. BNP increased the length of the query string to restore services by 19:00. ","Sterling Support & Sterling IBM to progress on this.IBM needs to confirm why the issue started midnight Thursday, this year compared to the previous years. ",3/23/2021,3/26/2021,3/23/2021,,,,,,,3rd party change,,"Investigations confirmed that the cause of the error was related to a security change that was deployed as part of the release on 21 March. The change had restricted the query string length to 512 characters. In some circumstances, the query string being passed exceeds this limit and is truncated. Regression tests as part of the release had not exceeded this limit, and therefore the error has not been replicated.
As the error occurred before the journey reached the UI layer, a 404 error was displayed as the URL being passed was not valid.",,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1638,>60
22/03/2021,22/03/2021,88061963,11:03,12:21,01:18,Foods,Food Supply Chain,,,,SI,Foods JDA (SRD) inaccesible,TCS,Database,,Foods JDA SRD,RC identified,56104,"The Foods JDA (SRD) application used to view and design store planograms was inaccessible after the storage for the application was migrated from Hitachi G1000 to PureStorage as part of a planned change. When the disks were disconnected, the application became inaccessible. The disks were then remapped and the application was back up at 12:21. ",The issue is being prioritized and IBM requested more time to investigate.Further details will be given on this week's call.,3/22/2021,3/23/2021,3/22/2021,,,,,,,Customer Tech change,Yes,"As part of the change, all G1000 disks were supposed to be removed from the ASM disk group, however one of the disks was left in the group. 
 
",137666,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1639,>60
17/03/2021,17/03/2021,88056752,16:03,19:22,01:20,Customer Channels,Customer Engagement ,,,,MI,Customer.com - Sparks unavailable online and in-store ,Tibco,Sparks Hub Product Support,,Sparks hub,RC identified,55460,Sparks was unavailable online and in-store.,Damaged fibre uplink in the Tibco’s ISP (Century Link) infrastructure,3/17/2021,3/17/2021,,,,,,,,3rd party issue,,Damaged fibre uplink in the Tibco’s ISP (Century Link) infrastructure,,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1644,>60
15/03/2021,16/03/2021,88053855,19:15,9:25,14:10,GTS,Group Technology Services,,,,MI,Azure authentication unavailable due to a central Microsoft issue,Microsoft,Microsoft,,"Slips and trips, Date Expired Food, Teams and Outlook",RC identified,55456,"At 20:12 on 15/03, multiple store users reported that they could not access “TSL, slips and trips” and also “date expired food”.",,3/16/2021,3/16/2021,,,,,,,,Customer Tech change,Yes,"Over the last few weeks, a particular key was marked as “retain” for longer than normal to support a complex cross-cloud migration. This exposed a bug where the automation incorrectly ignored that “retain” state, leading it to remove that particular key.",,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1646,>60
11/03/2021,11/03/2021,88047116,,,,Store Operations,Platform & Store Ops,,,,SI,Customer.com - Unable to Locate Certain Stores on Customer.com Store Finder,TCS,.COM,,Store finder app on the .com website,RC identified,55440,"Shouts received from multiple stores reporting that they are unable to locate their store on Customer.com’s Store Finder feature. Reports have also been received regarding issues with accessing Mobile Pay go and issues with customers attempting to book a slot for the Book & Shop service via the website.
Recovery actions: Following investigation, it has been identified that a change in the WCS release last Tuesday has resulted in the negative minus symbol “-” being removed from Latitude and Longitude values each time a store makes an amendment to their opening or closing hours using Customer.com+ (In-Store Assisted Ordering Service).
The removal of the negative symbol causes the store to drop off the store finder map.
It was identified that approximately 120 stores have made amendments since the release was deployed, however, it is not clear how many from this number are actually impacted as we are unable to confirm which of these had original negative values for latitude & longitude.
The team are planning on reverting the change made in the release and are currently working with the deployment team to assess how long this will take.
Once the change has been reverted, a plan is being put in place to correct the information for all impacted stores.

10/03 17:00
The reversion is currently being tested in the lower environments and validations are currently in progress
If validations and regression Is completed successfully without any issues the team aim to deploy to production later this evening.
The clean-up activity for the current impacted stores will be performed once the deployment has completed.

11/03 10:10
The deployment was completed and customer facing at 20:07 last night and validations confirmed that the issue is no longer occurring.
We believe a total of 73 stores were impacted by the issue and the values for these stores have been corrected by a database query earlier this morning.
Service is now fully restored.","Next actions:
1. More rigorous review process for any future security related changes and potentially adding additional testing cases to ensure issues do not surface into production - Agreed
2. With regards to alerting – Since the store finder latitude and longitude values are still legitimate in their incorrectly modified sate it will be a difficult scenario to alert against. We will explore further if there are any options to notify us sooner if any issue like this were to arise in the future.",11/03/2021,12/03/2021,,,,,,,,Customer Tech change,Yes,"
Investigations revealed that a change in the WCS release on 02/03 resulted in the minus symbol “-” being removed from the store coordinates each time a store makes an amendment to their opening or closing hours using Customer.com+. ",CM5832,Lack of proper testing,,15/3/2021,15/3/2021,#VALUE!,,Closed,,,2021,3,March,9/16/2025,1650,>60
11/03/2021,11/03/2021,88049759,21:10,21:43,00:33,Customer Channels,Customer Channels,,,,MI,Dip in number of orders taken on the website,Microsoft,.COM,,Customer .com website,RC unknown,55344,"Alerting identified a drop in orders across all Customer.Com ordering channels between 21:10 – 21:43 (33 mins)
Recovery actions: Blocking sessions were observed on the WCS database and the CPU had reached its maximum limit. These sessions were cleared and the CPU and order processing started to return to normal. By 21:43, order processing was back to BAU levels. The monitoring tools and alerting were showing no further issues. Support found that the blocking sessions were caused by multiple POD’s restarting at the start of the issue. A Sev-A case was raised with Microsoft to help confirm root cause. Microsoft have confirmed that they also faced an issue at the same time as we started experiencing impact. The RCA with Microsoft will be tracked via the problem management process.",Microsoft to arrange a mock incident to allow for mutual learning on both sides.,11/03/2021,08/04/2021,,,,,,,,RC unknown,,"Microsoft checked the diagnostic logs for the impacted clusters however, they were unable to establish a definitive RCA",,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1650,>60
04/03/2021,12/03/2021,88040015,12:41,9:00,188:19,Group Support, Finance,,,,SI,Finance billing documents issue ,TCS,SAP BI,,Finance billing document,RC identified,56141,"A number of historical billing documents which were not released into accounting due to various valid reasons, were inadvertently released into accounting on 4th March.  This was due to an incorrect variant setup when SAP support intended to release only the previous day's invoices as part of a service improvement. To prevent the issue, the job variant has been setup correctly to only release the previous day's billing documents.

Impact: £42 million worth of historical billing documents posted into Customer Billing Account
",,04/03/2021,04/03/2021,04/03/2021,,,,,,,Customer Tech change,Yes,"After the SAP release, the job variant was configured incorrectly leading to the issue.",,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1656,>60
02/03/2021,02/03/2021,88035854,07:00,10:30,03:30,Supply Chain,C&H & Intl Supply Chain,,,,MI,AMT connectivity lost at Welham Green DC,TCS,IS - Database/WMS Support,,AMT,RC identified,55309,"At 07:10, the Welham Green DC reported that all the AMTs at the DC lost connectivity
Impact: The Welham Green DC operations were impacted between 07:00 and 10:30 on 02/03
Resolution Notes: Oracle support identified that the FRA (fast recovery area) for the WG DB which is responsible for maintaining the archival files, had reached its capacity. The incident team uplifted the size of FRA filesystem, followed by a restart of the AMT daemons. The WG operations team then confirmed that colleagues could use their AMT's after 10:30.","We now need to work with WMS support to ascertain what caused the level 5 logging to turn on and off?	It was found that level 5 logging was enabled during the incident.  WMS support have advised that this needs in depth analysis and that this is scheduled to carried out as part of the PI planning sessions in Q1.
Is there a difference in the DB configuration between SP and SW? 	It has been confirmed that the archiving was enabled in SP but not in Swindon.  It is now enabled in both.  All other configurations were the same in SP and Swindon.  All other DC’s have also been checked to ensure archiving is enabled.
Why is there a difference is performance when the application is hosted in SP as opposed to Swindon?  We had latency issues during the entire week. The assumption was that with the app servers being in SP and having the DB in SW, the performance should be no different. However we saw issues. Can we look at some latency stats and get an idea as to why there was a difference in performance?

Can any alerting be implemented to notify us if level 5 logging turns itself on	WMS support have advised that there are many erroneous alerts occurring in the environment and for now it is not best to add new alerts until a full clean up has been carried out.

Why is there a difference in performance when the DB is hosted in Stockley and the application is hosted in SWindon",02/03/2021,05/03/2021,,,,,,,,Customer Tech change,Yes," 
To recover another issue after the maintenance patching on the WMS Welham Green database, we failed over to the Swindon instance. Level 5 logging was found to be active and also file archiving was not occurring in Swindon as it was set to manual, therefore the table space filled-up. Investigations continue as to why the Level-5 logging had been enabled.",136626,,,,January,1900,,Closed,,,2021,3,March,9/16/2025,1659,>60
01/03/2021,02/03/2021,88034835,10:25,2:20,16:55,GTS,Group Technology Services,,,,SI,Calls not routing to four business service lines,Anywhere365,Anywhere365,,"02087181199 (C&H stores support), 0208 718 8640 (C&H Transport Line) and (0333 200 5510 - store finance support and cash in transit line)",RC unknown,55163,"Calls not coming through to 02087181199 (C&H stores support), 0208 718 8640 (C&H Transport Line) and (0333 200 5510 - store finance support and cash in transit line)
Impact: C&H and Finance users were unable to reach the impacted lines and consequently, get their issues addressed for the duration of the incident.
Resolution Notes:
•	The Anywhere365 team restarted one of their services to fix the issue by 02:20, 02/03. The direct lines were then tested and the calls went through successfully. 
•	At 09:00 this morning, we received a confirmation that calls were being successfully received on the affected lines","Next actions:Franca Potter to go back to engineering teams in A365 to confirm definitively who owns the skype numbers 02039363790 and 02039364679 - A365 - Complete - they advised they dont own the numbers - CLOSED

Get an understanding of the four lines and how they failed – do they map to the actual UCC’s that were restarted at A365 
Check for any other BSC numbers and list them and map them to owners - Stephen Bolton: CLOSED

Once actions 1 & 2 have been completed and the information is to hand we need the confirmed root cause of the issue from A365
Mick Turnbull to advise A365 engineering to keep the portal up to date with all actions completed on their side - CLOSED

Rehan to provide A365 with the names and numbers of people who are able to call A365 and raise logs/make escalations.  A365 will then populate their whitelist with these details - Closed

Mick Turnbull to arrange a conversation between head of support at A365 and Stephen Bolton (Customer product owner) so that they can discuss and aligned the mapping of priorities on both sides - Mick Turnbull: Offline


Franca Potter to go back to engineering teams in A365 to confirm definitively who owns the skype numbers 02039363790 and 02039364679 - A365 - Complete - they advised they dont own the numbers - CLOSED

Get an understanding of the four lines and how they failed – do they map to the actual UCC’s that were restarted at A365 
Check for any other BSC numbers and list them and map them to owners - Stephen Bolton: Spoke to him, he will come back to us with a flow chat of what he knows

Once actions 1 & 2 have been completed and the information is to hand we need the confirmed root cause of the issue from A365
Mick Turnbull to advise A365 engineering to keep the portal up to date with all actions completed on their side - Stephen Bolton is working on it and provide something tomorrow morning

Rehan to provide A365 with the names and numbers of people who are able to call A365 and raise logs/make escalations.  A365 will then populate their whitelist with these details - Closed

Mick Turnbull to arrange a conversation between head of support at A365 and Stephen Bolton (Customer product owner) so that they can discuss and aligned the mapping of priorities on both sides - Mick Turnbull",02/03/2021,17/03/2021,NA,NA,NA,"No, Not applicable",No,"No, No",NA,3rd party issue,NA, Awaiting RC from Anywhere365,NA,NA,NA,5/14/2021,May,2021,Saloni,Closed,,,2021,3,March,9/16/2025,1660,>60
27/02/2021,27/02/2021,88032570,,,,Supply Chain,C&H & Intl Supply Chain,,,,SI,Multiple stores reported that they were encountering errors while printing Royal Mail carrier labels,Royal Mail,.COM,,.com BOSS,RC identified,55220,Multiple stores reported that they were encountering errors while printing Royal Mail (RM) carrier labels since 11:04. RM advised that they had an issue with the API at their end and services were restored from 15:50.  Up to a maximum of 233 orders scheduled for delivery on 28/02 could have missed promise.,"Next actions:
19/3: Update received from RM
19/3: The reasons for the failure have been identified and mitigated against future occurrence. I have been informed that no additional information is available to be shared.
* The out of hours RM contact details are to be reviewed. 
*Both Dan and Charlotte were not reachable during the incident, do they still support out of hours or is there a different point of contact?
-------Customer Solutions work Monday – Friday. Myself and Dan provide adhoc out of hours support on a purely voluntary basis. On weekends/evenings I escalate issues raised via our Customer/RM whatsapp group to the necessary contacts/suppliers but all support queries should be emailed into our central mailbox APIm.support.and.approvals@royalmail.com so that if required, other members of the team can also pick these up. 
*Although, Royal Mail was already aware of the ongoing API issues, no pro-active/outage notification was sent to Customer? How do we address this going forward?
---------The Royal Mail Portal is where outage information/updates are shared publicly. However, per your point below we are aware this was not updated over the weekend and have raised the point with the responsible teams. We are addressing this internally but I do not have a steer on how this will be managed going forward at this time. 
*The outage/incident details were not updated on the Royal Mail portal (https://status.intelligentshipper.net/proshipping) over the weekend, however, I see an update this morning at 10:00 AM.  Can the incident details be updated on the portal on a timely manner so we know there is an issue at your end.
---------As above",27/02/2021,19/03/2021,,,,,,,,3rd party issue,,"
Royal Mail informed that the impact to their systems over this specific weekend was down to a hardware failure and an intermittent connectivity issue. ",,,,3/19/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,22,0-30
27/02/2021,25/02/2021,88029767,09:00,9:00,00:00,Group Support, Finance,,,,SI,Delay in availability of P/L financial reports,TCS,SAP BI,,SAP BI,RC identified,55224,"One-day delay to the month-end operating cost allocation activities whilst support were working on populating missing cost allocation data and this subsequently delayed the publication of P/L financial reports. The cost data was inaccurate due to the following issues:
• Failed GMOR to EIH (BigData) interface on 31st January
• The allocation run in EIH (Big Data) did not update the “allocated costs” table
","Next actions:
Follow up by April mid or end
* Configuring not started alerts - •	Set an alert in case a job does not run on its allocated start day - Completed
*   Set up a Control-M resource dependency for the job to ensure that two instances of the job don't run I parallel, going forward -	Update the job start condition to not allow an automated re-start beyond its start day - Completed
* 	Put in place an automated reconciliation between number of records picked up by the GMOR job versus the count in the GMOR outbound file for that job (WIP via Backlog item SAP-GMORBW-00028 since this requires development. ETA Q1) - Completed
Update 7/5 : CR 137010 raised and completed
",25/02/2021,25/2/2021,,,,,,,,Design issue,,"
On Sunday 31st January there was no job failure as such, but rather the Saturday and Sunday daily jobs starting at the same time and day - which led to the output file data for that one run not being created (the other GMOR jobs during the same day did generate data, which is why the data drop went unnoticed).",,,,5/7/2021,May,2021,,Closed,,,2021,2,February,9/16/2025,1662,>60
24/02/2021,24/02/2021,88028826,,,,Customer Channels,Customer Channels,,,,MI,Sofa Product Detail Pages (PDP’s) led to an error stating  'Sorry something went wrong',Customer,.COM,,Customisable Sofa orders,RC identified,55129,"
The majority of sofa PDPs were leading to a ‘Sorry something went wrong’ error page. All sofas which can be customised (e.g., fabric /legs can be specified by the customer) were impacted. Sofas that can be purchased as a single UPC were not impacted.
Impact: 
•	Poor customer experience and potential loss of sales.
•	Approximately up to 80% of sofas could have been impacted.
Recovery action:
An issue was identified with the sofa colour data which was updated as part of nightly processing.  As part of the process to change colours; changes had been made to rename the colour Alabaster to Moonstone.  Although the colour was changed in SAP on 13th January, the colour was not changed within PIM.  
 
On 23/2, a separate change in SAP was carried out and this appears to have re-triggered the Alabaster/Moonstone change to PIM, however, as the Moonstone was not set up in PIM this resulted in the error. To fix the issue, the Moonstone colour was added into PIM.  19 further strokes of the same colour were corrected and republished from PIM.  A cache clear was performed to allow the sofas to be displayed correctly.","Next actions:
15/3: Followed up with team

- 1.	Review and clean up all the unwanted Sev-1 Pager Duty alerts triggered from Log Entries (especially after the nightly batch completion) raised for PDP team - Completed

- 2.	Closely monitor and action all the Sev-1 Pager Duty alerts without any misses  - Completed

- 3. Code change in PDP to ensure the Sofa Builder functionality does not break due to an issue with a single colour attribute - Not required as it was found that RC was due to incorrect data sent
25/3: update from Israel - When analysed the root cause stemmed from data not being sent correctly and even though we have error handling, there isn’t a nice way to render without that information being correct due to the inherent design of the app. My understanding was that the issue would be fixed upstream.

- 4. Explore New Relic alerting for this specific journey / scenario  - Completed
30/3: Thresholds set are below
404 Alert is set to 150 error in 5mins
500 Alert is set to 100 errors in 5mins

- 5. Explore Dept and Category Level cache clear for Enriched Product and Product Assembly APIs - Closed
18/3: Update from Elizabeth - We have raised a ticket in our backlog to look at clearing the cache based on dept/category. It’s not seen as a priority.
That’s it on the API side.
",24/02/2021,24/2/2021,,,,,,,,Human error - Business,,"
Missed manual step in the business process where a colour name(Moonstone) changed in SAP was not communicated / changed in PIM.",,,,3/30/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1665,>60
23/02/2021,23/02/2021,88027710,,,,Customer Channels,Customer Channels,,,,SI,"Customers were getting zero search results on some products and search result pages"" on the Customer .Com website","Bloomreach
",.COM,,Product Listing Pages (PLP) and Search Results Pages (SRP) ,RC identified,55208,"Customers were getting zero search results on some ""Product Listing Pages"" and ""Search Result Pages"" on the Customer .Com website. No visible impact was observed on the orders as the customers were able to place orders via Product Detail Pages (PDP) pages. Services were fully restored after clearing the cache on both the pages. Bloomreach (External Search Service Provider) confirmed that a routine change requested by the business was made on a configuration file at Bloomreach's end resulting in the issue.","Next actions:
Bloomreach to perform any feed/attribute configuration changes in off-peak hours to avoid inconsistent results.",24/02/2021,01/03/2021,,,,,,,,3rd party issue,,"
Upon Customer request for attribute changes[ benefit, skinType, beautySkinTone, age, fit (Single to Multi with || as separator value) ], Bloomreach executed the changes to feed/attribute configuration and indexed the full feed. The cause identified was that the full feed was indexed in between the deltas and which resulted in an inconsistency in results. The full feed was triggered inadvertently",,,,3/1/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1666,>60
16/02/2021,16/02/2021,88017070,,,,Store Operations,Platform & Store Ops,,,,SI,Stores getting errors while packing orders using BOSS ,Royal Mail,.COM,,.com BOSS,RC identified,54912,"Multiple stores are getting the following error while pacing orders using BOSS – “There was some problem with packing, please call the C&H support line at 02081181199 to correct this order”.
Impact: The below impacts were observed between 10:09 and 13:35
•	Stores colleagues were unable to pack orders using BOSS.
•	The shipping operations at the Gift card DC were impacted.
Resolution notes:
•	Investigations revealed that the response times for the Royal mail carrier APIs were taking longer than usual.
•	A Sev-1 case was raised with Royal Mail for recovering the issue.
•	Royal Mail (RM) fixed the issue by 13:35 and the stores are able to pack orders using BOSS successfully.
•	The Gift card DC also confirmed that they are able to ship orders successfully.","Next actions:
Update from Royal mail
* The services were restarted and further corrective actions taken and service was restored. Services continue to be monitored to ensure they remain stable.",16/02/2021,19/02/2021,,,,,,,,3rd party issue,,"
This was due to middleware issue with the Business Integration Gateway at Royal mail end which impacted all of their customer facing APIs including Shipping API V2.",,,,2/22/2021,February,2021,,Closed,,,2021,2,February,9/16/2025,1673,>60
15/02/2021,15/02/2021,88014999,,,,GTS,Group Technology Services,,,,SI,Calls to the service desk line fail after selecting the IVR options,Mitel,Mitel,,Service desk line - 185999 ,RC identified,54901,"At 10:28, MIM was made aware that calls made to the service desk line - 185999 were failing after selecting the IVR options – the users get a message stating “We have encountered a technical issue”.
Impact: Store and Head Office colleagues were unable to reach the Service Desk line between 10:00 and 10:50.
Current Update: 
•	Investigations from Mitel revealed that one of the services at their London data center was down, causing the calls to fail once IVR options were selected.
•	Mitel manually brought the service back up to fix the issue and calls to the service desk are successful since from 10:50.","Next actions:
Update: 1/3 after the meeting
* Any outage in the Mitel landscape impacting Customer Service Desk/Stores/DCs needs to be immediately communicated to the Major Incident Management (MIM) team – Agreed - Andy / Mitel Support
(MIM Team can be reached on 02087185533 or 07725734786; Email - IAS.IncidentManagement@marks-and-spencer.com; IASOperationsMana@marks-and-spencer.com)
• Any changes going forward , reboot theservers to ensure the servers are in the sync - Agreed
•         Check if the calls were successfully routing via IVR1 server in London Data Center during the incident window – Closed - We have one IVR server in London and one in Manchester, this is not load shared (i.e. not active-active) - Phil Lanham
•         IVR2 server – was it in a hung state or completely down?  why did the services not failover to Manchester DC? – Closed -  IVR server was in a hung state and hence the services did not fail over to Manchester DC - Phil Lanham",15/02/2021,22/02/2021,,,,,,,,3rd party issue,," This issue was due to the MiCC (Call centre programming server 10.137.64.50) and IVR server not syncing for 41 days. It normally syncs automatically, there was a planned change on the IVR around the start of January.
If there was a change on the IVR, it might have needed a restart of the server to push this through. This is why it started to sync after the restart.
It exceeded a threshold which is why the 2 servers not syncing only received an issue on this day.  As the IVR3 (10.137.64.52) server wasn’t actually down, this was the reason it didn’t fail over to IVR4 (10.137.192.52
",,,,3/1/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1674,>60
12/02/2021,19/02/2021,88012474,,,,Store Operations,Platform & Store Ops,,,,SI,Latency observed in BOSS while performing packing operations,TCS,.COM,,SPPD BOSS,RC identified,54703,"From 10 AM on 12/02, multiple stores were experiencing latency while performing packing operations in BOSS 
Impact: Multiple store users are experiencing latency in BOSS while performing packing operations
Current Update

•	The API responses remained stable and the stores have not reported any issues with packing in SPPD
•	The Java driver upgrade testing has completed successfully in the non-prod. This will further be reviewed and applied in to production early next week. 
Previous Update:

•	The API responses remained stable today and the stores have not reported any issues. 
•	The Java driver upgrade is currently being tested in non-prod and will be deployed into production once the testing is successful. 
•	Business have confirmed that the BOSS CFR has been steady last week, however this issue has increased the pick backlog. 
•	The teams will continue to monitor the performance of the SPPD application until EOD, today.","Next actions:
Next update: 5/3
* Change label printing delay from 5s to 1s - Completed
* Action to upgrade the java drivers to 3.11 as the current version of java drivers is not compatible with the version of Mongo Atlas DB - 24/2: Java driver upgrade Completed - Team is currently monitoring",19/02/2021,22/02/2021,,,,,,,,Customer Tech change,Yes,"
This latency was due to the 5 seconds label printing delay implemented in old AWS Mongo DB which is not required for new mongo atlas DB.",CM5715,,,3/1/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1677,>60
11/02/2021,11/02/2021,88010672,,,,Foods,Food Supply Chain,,,,SI,No frozen allocations for Irish stores on 10th February,TCS,SCRD,,"ASO,SCRD",RC identified,54604,"The frozen delivery method was deleted for two new product groups after a routine site refresh in SCRD due to a bug resulting in no allocations for Irish stores on 10th February.  As part of the Bedworth polling schedule changes for Brexit, two new frozen product groups - F1 & F2 were created to meet EHC and Fish (VET) categories requirements.  Support reinstated the delivery method for the two new product groups.  Once the delivery method was added back for the two new product groups, allocations went through normally","Next actions:
* Fix to be implemenetd - Completed - Fix has been developed and deployed in production to exclude the F1 and F2 del methods along with the existing F method. This will address the scenario strategically. No further action required for this issue. 

As a pre-cautionary step to prevent the similar issues in production, the below actions has been taken as part of upcoming releases and changes. All tthese are not in specific to this issue - Added in backlog by the team
?	Spike analysis and outcomes to be shared with other SCL teams including testing team and Ops. So that any solution gaps can be called out and included early on
?	Always include I16 and I5 changes irrespective of any changes in SCRD as part of regressing testing 
?	Review the code to find all the places where any data is getting deleted, document where we are deleting any data or any hardcoding is present. => To be added as backlog item and prioritised in one of the sprints”



",11/02/2021,11/02/2021,,,,,,,,Customer Tech change,Yes,"
The frozen delivery method was deleted for two new product groups after a routine site refresh in SCRD due to a bug resulting in no allocations for Irish stores on 10th February.
",130645,,,2/16/2021,February,2021,,Closed,,,2021,2,February,9/16/2025,1678,>60
08/02/2021,09/02/2021,88008351,,,,Customer Channels,Customer Channels,,,,SI,Order transmission delays from the website to the fulfillment,"HCL
",.COM,,Orders with combination of multiple lines and promotions,RC identified,54268,Batch processing issues were causing delays in order export from the website to fulfilment. Two large orders with the combination of multiple line items and quantities were causing the entire batch to fail.  The problematic orders were isolated and the rest of the order batch processed successfully. ,"Next actions:
Last update: 4/3/2021: Received confirmation from Hisham to close the PR.

*  WCS v9 Upgrade team already raised a case with HCL and they are trying to replicate the same in lower environment and to implemenet fix- Completed
2/3: patch deployed successfully in PROD. Team will monitor
23/02 - HCL provided a patch update to the JDBC driver and it is being tested, if all goes well it will be deployed as part of the March 2 WCS release.
18/2: HCL has asked for JDBC logs for further triaging the issue (based on the call on 17th Feb 2021 with HCL) which we will be sharing with them and there will be a working session that will be scheduled on the issue.
18/2: replicated in lower environment, waiting for fix-

*  Setup Splunk alert to capture this issue - Completed - The alert got fired yesterday ( 11/2 ) and this was actioned immediately. (Same customer placed an order with similar items with the promotions) 
(Alert Name - ""Error while composing Order XML: Immediate actions required"").
",09/02/2021,12/02/2021,,,,,,,,Code/Product bug,,"
A product bug relating to multiple line items and combinations has been identified after the WCS v9 upgrade and support is working with the product vendor HCL for a fix.",,,,3/4/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1681,>60
07/02/2021,07/02/2021,88000372,,,,,C&H & Intl Supply Chain,,,,SI,30% of users experienced issues logging into Demand and Fulfilment (D&F),BY,C&H,,D&F,RC identified,54176,"Users were experiencing issues logging into the D&F
Impact: Intermittently around 30% of users had issues logging into the D&F application affecting the users day-to day work and delayed the stock planning.
Resolution Notes:

•	Blue Yonder was engaged. Whilst investigations continued, as a workaround, Blue Yonder were restarting the affected services regularly during quieter periods and this then allowed the majority of users to log back in. 
•	Investigations found that the issue was arising from the web service being unable to handle unnecessary call-outs to the application servers, which were causing the application servers to overload and lock out any further user sessions.  An initial patch to fix the issue was applied yesterday morning but this did not resolve the problem. 
•	The product team provided a further emergency patch which was tested successfully on 03/02. The patch allowed the application to ignore the erroneous messages. 
•	The patch was implemented this morning and all users were successfully able to log into the application without any issues.
•	The application was monitored for stability till EOD and no issues were observed. This incident has been set to resolved.","Next actions:
Last update: 15/3: Received confirmation from Naga to close PR.
Blue Yonder support is looking at the options for putting monitoring and alerting into place - Completed
15/3: Update from Ganesh: There are monitor alerts set in place for resource usage. ",03/02/2021,05/02/2021,,,,,,,,Customer Tech change,Yes,"
A web service was found to be unable to handle the amount of calls made to the application servers, causing the servers to overload and lock out any further user sessions",133352,,,15/3/2021,15/3/2021,#VALUE!,,Closed,,,2021,2,February,9/16/2025,90,>60
07/02/2021,07/02/2021,88005574,01:30,0.234027778,04:07,,C&H & Intl Supply Chain,,,,MI,Delay in the overnight D&F batch,BY,C&H,,D&F,RC identified,54403," Zero orders were generated for all the DCs as reflected in the D&F dashboard.
Impact: The SLA's for Castle Donington and other DCs of 04:00 and 05:00 respectively were missed.  This meant that the DC's were not able release the pick to stores in a timely manner.
Recovery action:
Blue Yonder support were engaged and confirmed that one of their calc plan jobs had failed with a time-out error at 01:48.  Support re-triggered the failed calc plan job however this did not work.  
 
Investigations then continued as to why the calc plan job was failing and Blue Yonder support found and removed a problematic SKU from the calc plan processing, which was believed to be the cause of the issue. At 03:45, Blue Yonder re-triggered the jobs from the start and the jobs completed by 04:41.
 
The incident team validated the files before they were sent to the downstream systems. The D&F orders reached Castle Donington and other DCs at 05:37 and 05:31 respectively.","Next actions:


•	Fulfillment PD are working on a fix to handle the data condition and report a SKUException – Completed
26/4: The application patch which resolves the data condition was successfully applied to production instance on 24-Aapril as planned.
19/4: The patch that resolves this data condition, is scheduled to be installed on PROD this weekend (24th April).
15/3: This will be a fix through application patch. The fix is expected in the next release which is planned in Mid of April 2021. We will inform in case there is any delay.
19/2: This will take time to fix 
•	Implementation team & Support are working with Platform PD, to change SRE properties to allow the batch to move on, if a low volume of unhandled exceptions occur – Closed
15/3: This change is already implemented in production environment

•	Clean up the unwanted over-running alerts – Completed (Confirmed with Preethi from SAAS migration team)",07/02/2021,11/02/2020,,,,,,,,Code/Product bug,,"
The system was unable to create recship (recommended shipments) as the scheduled ship date of the product (SKU) is at the end of the recship horizon and coincides with a closed calendar.  The job went into a hung state and over-ran due to this condition.",,,,4/29/2021,April,2021,,Closed,,,2021,2,February,9/16/2025,86,>60
06/02/2021,06/02/2021,88005265,,,,Foods,Food Supply Chain,,,,SI,Milton Keynes NDC allocations delayed,Customer,FOODS,,MK NDC allocation,RC identified,54401,A count of 62k singles had not been sent to the Milton Keynes DC by 06:00 AM as 4 middleware exceptions between ASO to SAP were blocking the flow of allocation messages. These exceptions were a result of an incorrect store/supply chain reference data  against the Waterside Head Office store. Support isolated the allocations for the problematic store and retriggered the messages again from ASO. MK allocations reached the DC by 06:58.,"Next actions:
Last update 4/3
•	Source application name being recorded in Audit and exceptions for FOI1137 in Integration ACE also has to be corrected to ASO instead of Q - Integration Devops -  Completed
4/3: The change has been promoted and working as expected
22/2 - Will be completed by Thurday 25/2
12/2: testing the changes now. We will promote the changes at the end of our Sprint 3.

•	Integration team to have call with Foods/ASO support to get clarified on the interface numbers – Integration Devops team - Completed
•	Check and correct the stores configured incorrectly - Completed
12/2: found out one more store that was configured incorrectly and we removed the schedules for that store.
•	Configure an alert in ASO to search for any Bradford/MK allocations for these kind of stores in the Quantum backup allocation interface to be alerted proactively and will have time to take necessary action to avoid these failures  -Completed",06/02/2021,06/02/2021,,,,,,,,Human error - Business,,"
The exceptions were a result of an incorrect store/supply chain reference data  against the Waterside Head Office store.",,,,3/4/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,87,>60
05/02/2021,05/02/2021,88005065,,,,,Customer Channels,,,,MI,Drop in order rate on Customer.com website,TCS,.COM,,Customer .com website,RC identified,54408,"The minute by minute orders view on the Splunk dashboard highlighted a drop in orders coming in for the desktop version of the website customers from 50 orders per min to 15 orders per min.
Impact:
•	Poor customer experience on desktop channel.  Mobile channels were not impacted.
•	Potential loss of sales on desktop however there was a slight increase on mobile.
•	As the impacted channel was contributing to 50% of the sales and there was intermittent recovery for a while, it is estimated that this issue was responsible for a drop of 2500-3000 orders.
Recovery action: There was an initial drop from 19:15 to 19:45 and then the order rate recovered. However, the order rate dropped again at 20:00. Investigations found cache corruption and support cleared one of the caches (base cache) and this resolved the issue. 
 
There was a planned deployment on 05/02 to fix a promotion issue related to the Spend and Save campaign.
 
However, it is not clear if this had any impact or caused the cache corruption. The order rate on the website remained stable since the cache was cleared around 20:45 05/02 and the number of orders across all selling channels went back to BAU volumes. ","Next actions:
Last update: 4/3: received conformation from Hisham to close the PR.

*  In the MSURLDataBean.java existing logic(v7), based on True-Client-IP we load the static assets(css/js etc)  endpoints for either Origin or AKAMAI. We will change the condition by replacing the test for “True-Client-IP” with a base domain name test so that origin or public name is chosen based on the canonical domain. This will avoid this specific rare occurrence. We will have to do proper testing in preprod environments for the switching between different halls to ensure that there is no other impact.
 25/2: Raised a Tech Debt and assigned to check out product, they will take care aspart of product backlog. No ETAs , workaround : Deployment will include one additional step which is Base cache clear to avoid this issue untill product team fix this.",05/02/2021,06/02/2021,,,,,,,,Customer Tech change,Yes,"
Page delivered from WCS for the marksandspencer.com domain, CSS and JS links with the origin domain name were included in the page, which led to the failure caused when someone tries to test the origin site with the “Host” header as www.marksandspencer.com but without the “True-Client-IP” header.",CM5725,,,3/4/2021,March,2021,,Closed,,,2021,2,February,9/16/2025,1684,>60
04/02/2021,04/02/2021,88003239,,,,,Platform & Store Ops,,,,SI,Some BOSS stores unable to access SPPD on workstations  ,TCS,.COM,,BOSS(also called as Fernando),RC identified,54196,"At 13:33, SD reported that the BOSS application was inaccessible on store workstations for multiple store users. BOSS was however accessible via the Honeywells.
Impact: Packing might have been delayed in the affected stores. Not all stores that’s use SPPD were affected.
Resolution Notes:
•	Investigations revealed that the CPU utilization on the primary Mongo DB node XE4QG 001 was high which caused delays in the query processing.
•	A Sev-A case was raised with the vendor Atlas and the config on the DB server was upgraded from M30 to M40 last night. As recommended by the vendor, the teams deployed indexes this morning to improve the query performance.
•	Teams continued to monitor the application & database performance -CPU, memory & input/output operations were within normal thresholds. Picking and packing operations remained stable as well.","Next actions:
Last update: 24/2
* Upgrade config on DB server  from M30 to M40 - Completed
* Deployement of  indexes to improve the query performance  -Completed
*   A perimeter change will be made on the SPPD nodes as both read & write is happening on a single node and this will be changed so this is happening on two nodes - Closed
24/2 - Checked with Support, no plans to do this at the moment, confirmed to close this action
15/2 - Discussion ongoing",04/02/2021,04/02/2021,,,,,,,,Customer Tech change,Yes,"
CPU utilization on the primary Mongo DB node XE4QG 001 was high which caused delays in the query processing.",CM5715,,,1/3/2021,January,2021,,Closed,,,2021,2,February,9/16/2025,1685,>60
02/02/2021,02/02/2021,87998916,,,,,Group Technology Services,,,,MI,Multiple stores hard down,TCS,IS-Network,,Stores Network,RC identified,54157,"MIM was made aware that multiple stores were hard down and the phone lines for three DCs were not working 
Impact: 
•	Approximately 550 stores were hard down across UK and Ireland between 22:25 and 00:15. Fridge monitoring for these stores was impacted for the duration.
•	Castle Donington, Bradford and Welham DCs advised that their DECT phone lines were impacted.
Recovery action:
At 22:25, Network support received alerts indicating that approximately 550 stores were hard down. As the majority of stores alerts were indicating an issue with BT stores, the BT service desk was called to ascertain if they had any central issue, in parallel the network teams continued to investigate.
 
Through investigations, it was identified that the configuration on the Stockley Park UK POP concentrator was corrupted. To fix the issue, network support rolled back to the last known good configuration. The impacted stores were back online from 00:15 and the DECT phones at the impacted DCs started working. City FM, who manage the fridge monitoring confirmed service restoration. ","Next actions:
Latest update: 11/3
•	List of additional pre-check plans and procedure to be shared to avoid manual errors during the configuration changes in WAN Migration - Completed
•	Enable Full logging to capture the successful configuration - Enabling command account logging in concentrator to audit for miss/unauthorized line configuration - Completed
•	Checking the possibility of implementing configuration removal through automated script – Completed
5/3: Configuration automated for WAN migration using Solarwinds script 
23/2: The script has been tested and it is planned to go into production tonight, it will be used on one store only and the results monitored.

•	Validating the MIB file to check if there is any possibility of alerting for reduced configuration files – Completed
11/3: Fixed the issues, receiving the report daily
5/3: Additional mail trigger on receiving the last configuration change comparison daily report. -Completed but not working for last few day, hopefully will sorted out today.
23/2 - The alerting should be available by the end of the week(28/2)",02/02/2021,04/02/2021,,,,,,,,Customer Tech change,Yes," On Monday 1st February, during the WAN migration change, a critical part of the configuration of the UK POP SRX router was deleted due to a possible human error. Measures are being put in place to mitigate the risk of this type of error occurring again.",134874,,,15/3/2021,15/3/2021,#VALUE!,,Closed,,,2021,2,February,9/16/2025,1687,>60
01/02/2021,30/01/2021,"87995872
87995447",11:07               11:35,17:59               12:50,06:52          01:15,,Customer Channels,,,,MI,Performance issues in Customer.com website and the Parcel scanning app,"Customer
",.COM,,Customer.com website & parcel scanning application,RC identified,54225," At 11:36 on 30/01, Multiple issues on the Customer.com website including slow performance and problems adding to bag. In parallel, store colleagues experienced service degradation whilst using the parcel scanning app.  
Impact:
Dotcom: Poor customer experience on .com,  .com+ and international websites between 11:07 and 12:50.
Retail: Store colleagues were intermittently unable to perform parcel scanning between 11:07 and 12:50.
Castle Donington - Pack capability loss of 27k (9.5% against the 280k target). SCS pick capability loss of 10k.  
Recovery Actions: Order Management System (OMS) support identified hung threads in the OMS application servers from 11:20 on 30/01. Support investigated and found that a product (harper small footstool) was incorrectly priced at £1 which led to an abnormal surge in traffic and a very high order volume on Customer.com.  This led to hung threads in the OMS servers resulting in performance issues.  Support removed the product from the website after which traffic reduced and orders dropped back to BAU volumes.
 
Support further identified that the order flow from OMS to Donington had slowed down due to the high volume of unprocessed furniture orders in OMS. To improve order processing, support performed a number of improvement including increasing the number of pods and also de-prioritised furniture orders . The backlogged Donington DN (Delivery Note) messages from OMS to Donington were processed by 18:00 hrs.  ","Latest Update:Sep - 9th
 
2.Provide a solution to turn fast selling items into HOT SKU automatically - Sterling Product Team 
9/9: Raised risk 1618 as it is unlikely this is going to get resolved any time soon. Discussions are still going on between the teams but not sure what can realistically we done in the short term to resolve this.
14/7: IBM confirmed reversion is possible, testing is still in progress
  
11/6:Issue is with the optimized locking feature within the Sterling V10 version. Checking with IBM if can we revert to the 9.3 properties. Once we get confirmation this will be tested .   
--------
1.Alerting for incorrect product set up issues, discuss with the Business / Architects on what alerting can be set up – Command Centre / Hisham / Product team: Adding alerts to PDP for set up issues (only one fabric showing, product incomplete show variations if one attribute is not available & full lines dropping off) –– Closed
14/7: Ashok has confirmed the logs are in place for this now, they just need to set up alerts to trigger on back of these logs. This area is due to move from Log Entries to New Relic shortly so will be picked up then but have no confirmed ETA from PO. I would say we can close this now. We have advised they should have this in place but it’s up to PO if they want to prioritize or not.   
Emma haven’t got any updates. 
Emma will chase with Dev Team and PO yet to provide an update.
25/5: The meeting has been pushed out to 8th June.
*  Fix alerting on WCS for Fast Selling Items - Completed
*  Alerting and Dashboard to/within Sterling for order backlog to downstream systems and fulfilment partners – Sterling / EM Support - Completed
18/2: The alert has been configured for Donnington and Other Ship nodes(except donnington) with threshold of 1000 and 2000 respectively. A dashboard has been developed to provide the DN backlogs with overall and Fulfillment partner level. The Order create thresholds for QueueAge and QueueDepth alerts have been changed with minimum threshold.
*  Dashboard with WCS order split by fulfilment partners and the status of orders progressing against the cut off times – WCS Support - Completed
24/2: We have created two alerts and one dashboard for this issue.
Alerts:
  1.       min by min alert for higher-order volume in wcs ? The script will check the last 15 mins orders(min by min) and the alert will be triggered if the order volume is more than 300 per minute(Sudden spike in .com/mobile/cs and ISA orders).           
  2.        Order volume spike alert? This script will compare the orders between the last 5 mins and before 5 mins(Consider the script is running at 10 AM then it will check 9.50 AM and 9:55 AM data ), if the order volume is more than 60% compared to before 5 mins orders then the alert will be triggered to support dl.
Dashboard: 
           Order creation stats by BU’s? The dashboard contains the below information and it will refresh every 30 mins.
                                                   BU’s, delivery date, delivery method/shipping method and top selling product(for each BU’s)
*  Agree on the stores communication process for .Com incidents impacting Retail - Hisham / Rosie / MIM team - Completed
19/4: We have a process in place which we follow for all .com incidents which cross over to Retail. This action can be closed from Tech. 
*  Alerting for incorrect product set up issues, discuss with the Business / Architects on what alerting can be set up – Command Centre / Hisham / Product team
Update 25/5: 
--Adding additional business validation rules in PIM – Closed - This was included in the PIM release deployed on 21st May, this can now be closed.   
--Make colour a mandatory field in PIM –Closed -  Was reviewed but due to the integrated design between SAP & PIM this is not possible. Action has been closed by PO’s. Please close this from Tech ends as well. 
--Adding alerts to PDP for set up issues (only one fabric showing, product incomplete show variations if one attribute is not available & full lines dropping off) –Will ask for an update in meeting on 8th June 
*  Options to prioritise the backlog orders in Sterling by fulfilment partner and next day delivery – Customer Promise Product Team
25/5: No update, focus is on point below
12/5: Still not prioritised as focus is working with Sterling IBM to resolve HOT SKU problem
19/4: This has not been discussed since as PO’s & Dev Team focused on HOT SKU action below currently 
12/3: This has been added to the backlog but not prioritised, team are currently focused on closing off point 3 here. To provide this functionality the Sterling Architect has advised this would require strategic changes to the application as a whole. My personal opinion here, for the amount of changes required here to make this possible I cannot see the funding and support will be provided by the Customer business.  
18/2: This has been discussed with the product team and waiting for the confirmation that it has been picked up in the sprint.

*  Feasibility of setting up APM alerts for OMS – Steve G / Engineering Team / Customer Promise Product Team
12/5: Sterling IBM asked for some Sterling 9.3 properties to be enabled, these were and PT test was run this week, this still has not resolved the issue, details currently be shared back to Sterling IBM.
19/4: In progress, Sterling Support have had training, parts of this have been deployed on OMS servers in prod already, further changes coming in next Sterling Release on 26th April. Hoping to start using this from 27th April
12/3: The licensing and costings have been signed off for this to be set up on OMS. Currently looking to get Sterling Support the required training, once provided will get the tool deployed.    
18/2: Will discuss with product team and confirm
*  Provide a solution to turn fast selling items into HOT SKU automatically - Sterling Product Team
25/5: We need to share the webos logs for the test to Sterling IBM. This wasn’t carried out at the time so team need to re-test which is planned later part of this week and then will share the logs as well. 
12/5: Sterling IBM asked for some Sterling 9.3 properties to be enabled, these were and PT test was run this week, this still has not resolved the issue, details currently be shared back to Sterling IBM. 
19/4: Sterling IBM have asked us to re-create the scenario again in our VnP environment with a certain set up, this is planned next week and results will be shared for Sterling IBM to review 
12/3: Scenario has been replicated in VnP, Sterling IBM provided some property changes and these were made and scenario was re-tested. These changes have not resolved the issue, we are currently following up with Sterling IBM on this.   
18/2: A case has been raised with the IBM team and got few recommendations for the HOTSKU properties file. Planning to replicate the issue scenario in VNP with and without recommended properties by IBM
*  Performance tests to be carried out in Sterling to replicate the production issue and understand the system performance/breaking point – Sterling Performance Testing team - Completed
12/3: Scenario was successfully replicated this action can now be closed off. 
18/2: Had a discussion with the performance testing team and they are replicating the scenario.
*  Review the infinite inventory stock positions and agree for changes - Hisham / Business - Completed
19/4: Test was performed to prove we can change this stock position as business see fit. Furniture event currently running at the moment, due to end tomorrow, business will then review again and arrange for Westbridge to make the required changes. This now sits with Command Centre, Furniture BU & Westbridge to manage. Action can be closed from Tech. 
*  Add an additional stage into the Upholstery launch process to confirm between BU and Dotcom Content team which PLM buying hierarchies have been used to set up new products – Business - Completed
*  Check the template type value assigned when setting up new furniture products in PIM, particularly for the multi-upc sofa products where the template for the referenced fabric/feet/cushion products has to be different from the shape product – Business - Completed""""""""
10/06: 2.Options to prioritise the backlog orders in Sterling by fulfilment partner and next day delivery – Customer Promise Product Team – As per the last update received from your end, this was not prioritized. – Cancelled (this will not be picked up and delivered, this request has been rejected. Reason being, the level of change required to the app to deliver this functionality, the cost does not warrant the benefit given this is something that would rarely be used with current incident stats)",30/01/2021,30/01/2021,,,,,,,,Human error - Business,," A particular product - harper small footstool - was incorrectly priced at £1, which resulted in an abnormally high surge in website traffic and abnormally high order volumes that caused the OMS application threads to hang.",,,1618,9/9/2021,September,2021,Dimple,Closed,,,2021,2,February,9/16/2025,1688,>60
30/01/2021,30/01/2021,87992447,14:40,6:50,64:60,Foods,Food Supply Chain,,,,SI,Multiple Bradford and MK EDN failures in SAP,"Customer
",SAP Buy & Move Support,,EDN failures impacting GIST & Ocado trailer receipts for Bradford stores,RC identified,54138,"At 11:23 on 28/01, Multiple Bradford and Milton Keynes EDNs had failed impacting the GIST and Ocado trailer receipts.
Impact: 
•	Manual receipt impact to GIST
•	Impact to stock positions at stores for Bulk and Cross-dock stock sent from Bradford NDC via GIST RDC’s.
Recovery actions:
At 14:40 on 27/01, three Ocado EDNs were in an unprocessed status since they had a ‘duplicate group ID’. This prevented multiple EDNs for Bradford and Milton Keynes DC from getting processed. Support isolated these problematic EDNs and processed the remaining ones by 13:00 on 28/01. This issue impacted stock position of the stores served directly or in X-docked manner by the Foods Bradford NDC. It also caused a manual receipt impact to GIST. A systemic re-con revealed that 642 pallets were missing in CSSM. The store counts for 139 stores were initially believed to be impacted and hence their counts were blocked in CSSM by 09:30 on 29/01.
 
GIST later confirmed that out of the 642 pallets that weren’t available in CSSM, they had systemically shipped 580 pallets. These however, did not reach CSSM due to an issue caused by a previously introduced ‘fix’ at GIST for a zero qty order issue in Customer deliveries. As a workaround, support adjusted these 580 pallets in CSSM. Support then systemically adjusted the stock for 14 additional pallets in consultation with GIST. The store counts for all the 139 stores have been unblocked.","Next actions:
*Work with the business to list the remainder of 3k articles to the virtual DC (Ocado) - Pending Adelson / SAP Project team -
4/8: FCS Team confirmed that all line status pending with category planners have been updated.
3/08: Escalated to FCS team
06/07: Dimple spoke to Adelson, the action with the business is still open, no ETA.
22/06: Vinay to speak to Adelson

2/6: Business yet to confirm if the remaining articles have been listed
16/4: Business is working on the same. Will update once we get a confirmation.
8/3: Business is working on the master data for the remaining articles. We shall update once we receive a response.
18/2: We are working with business to get the remaining 3k articles listed, this is expected to the closed next week
•	Alerting in SAP to capture any unprocessed idoc/EDN for more than 2 hours
o	Alerting in the existing trailer dashboard - Vasanth / SAP Buy and Move Support - In progress
26/4: CRQ000000136497 – Alert for critical interfaces is now live in production, this will help to find discrepencies within SAP
21/4: From Piyush: Since the alerting in existing trailer dashboard has been reverted SAP Team have develped  critical interface alerts; Vasanth & Support team is working on CRQ000000136497_Alert for critical interfaces
9/4: Alerting in existing trailer dashboard - We’ve moved the change, but it impacts the system’s performance. So, the change needs to be reviewed and go through the performance testing and it is planned on June Release. 
1/3: CR (CRQ000000136497 - Alert for Critical Interfaces) has been initiated and working on the development. 
o	Central iDoc framework - Vasanth / SAP Buy and Move Support working with Piyush from SAP CoE team (No ETA) 
21/4: The Idoc Maintenance Framework tool is still design phase and will be operational for pilot by end of Q1 only at the earliest 
•	Manual checks to be performed every 2 hours for any unprocessed/failed idocs - Vasanth / SAP Buy and Move Support - Agreed
1/3: We are now checking (as a part of BAU) for the failure and yet to be processed IDocs twice – for the first, second shift (IST)
•	Once the article is moved from the status 01 & 02 to any other status, will it be listed automatically -  Adelson - Completed

18/2: 00 to 02 status articles have also been listed to the new site now. Business is working on the remaining articles, we will keep you posted on the same.
We are analysing this, based on the analysis done thus far we would expect to list these articles as well but will confirm based on detailed analysis.",30/01/2021,30/01/2021,,,,,,,,Process gap,," 
Business process to map 3 newly introduced products to Ocado supply chain within SAP was missed out and this resulted in multiple failed EDNs for Bradford and Milton Keynes DC.",,,,,January,1900,Dimple,Closed,,,2021,1,January,9/16/2025,1690,>60
29/01/2021,29/01/2021,87993675,,,,,C&H & Intl Supply Chain,,,,SI,No D&F orders on the JDA dispatcher for Stoke NDC,"Customer
",C&H,,Stoke JDA dispatcher,RC identified,54134," Stoke NDC: 5885 users reported that they had not received any D&F  orders on their JDA dispatcher WMS system.
Impact: 
•	The D&F orders for Stoke NDC could not be sent within it's 05:00 AM SLA
•	Impact to the DC operations between 05:00 and 08:07.
•	Potential delay to the store deliveries.
Recovery actions: 
•	Support identified that the cap limit for the DC was set to zero, since the cap limit was not set by the planning team. However, the rec ship (recommended shipment) was 132k for Ecomm and 45k for stores. Hence, the orders were not released from the source(D&F). 
•	Support then amended the cap limit to 99999999 based on the rec ship capacity of the DC and re-triggered the orders from the source (D&F). All 551 orders successfully reached WMS by 08:07.","Next actions: 

Last update: 22/2
•	Set up a job and alerting in the new BY environment to check the cap limit on a daily basis  - Completed
22/2: The Control M changes to send cap limit report has been deployed in BY last Friday and starting from Saturday (20/02) the report is being sent out daily at 5 pm. The Control M changes to send cap limit report has been deployed in BY last Friday and starting from Saturday (20/02) the report is being sent out daily at 5 pm.
15/2: The Control M changes to enable this additional report is not yet deployed due to delays in deployment/ testing at BY end. This is expected to be deployed into Production by end of this week (19/02).
8/2: To be implemented by the end of this week (12/2)
•              IT Operations Incident Management / PCM - As part of overnight critical batch monitoring, whenever there are no orders to a specific DC, please can you cross check with support to ensure this is expected rather than ignoring or assuming there are no orders to the DC - Agreed",29/01/2021,29/01/2021,,,,,,,,Human error - Business,," 
The cause of the issue is that the cap limit for DC was incorrectly set as ‘Zero’ by the business planning team. ",,,,2/23/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,95,>60
27/01/2021,27/01/2021,87992198,,,,,Customer Channels,,,,SI,PDPs slow to load on iOS and Android Apps,TCS,.COM,,PDPs on Android & iOS,RC identified,54216,Alerts and command center shouts indicated issues with multiple PDPs being slow to load on iOS and Android apps. This resulted in poor customer experience and potential loss of sales.  Investigations revealed that one of the APIs was down (Catalogue API). This was causing timeouts and intermittently redirected users to the web-view.  One of the AWS instances was restarted to resolve the issue,"Next actions: 
Latest update: 2/2
•	Adjusting the scaling trigger to be more sensitive - We have created a ticket in Backlog (APITB-243) and prioritized by our PO. This is not closely related to this issue and it could prevent issues wrt CPU spikes
•	Create a runbook with the necessary steps to have a workaround in place if the alerting is triggered - Created Runbook https://confluence.marksandspencer.app/display/API/Product+Catalog+V2 - Completed
•              In long term, we are recommending consumers to migrate it to Product assembly as this catalog-v2 api  is legacy one which runs on AWS stack.",27/01/2021,03/02/2021,,,,,,,,Design issue,," 
The Catalog v2 API application servers could not serve the incoming traffic due to contention/blocked threads on the application servers.",,,,2/8/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,0,0-30
27/01/2021,27/01/2021,87991818,,,,,Corporate,,,,MI,HR Evolution portal inaccessible,SDWorx,SDWorx,,HRE (HR Evolution) portal ,RC identified,54131,"At 15:33 on 27/01, it was reported that the HRE (HR Evolution) portal was inaccessible.
Impact: Business users were unable perform their routine tasks on the HRE portal. In addition, store colleagues were unable to book holidays.
Recovery actions: SDWorx identified a central issue impacting multiple customers of theirs. Investigations from SDWorx revealed high resource utilisation on their database servers. A workaround was then implemented to make the HRE portal accessible to users therefore  mitigating the impact. 
 
SDWorx then confirmed issue was fixed permanently by 20:00. ","Next actions:
5/1: Received RC report from SDWorx
*  Need to check whether the high number of users was from other clients or specific to Customer - Completed -  High number included their different big projects - Indexing has been done to prevent this from happening again.
*  To have a review to understand if there are further enhancements that can be introduced - Infrastructure Architect -  Completed
*  2 further enhancements below  planned for early Q2: have been identified and assigned to the respective Product Owners for testing and future deployment to Production - Mark Rose – Portal Development 
*    “Amend the stored procedures 'GetLastestProcessedFile' and GetLastestProcessedEOYFile to help Portal efficiency.”
An amendment to these queries will be made to make them more efficient - Completed
*     “Add EmailAddress to Index on the SecurityUser table on Portal""
As the title suggests, this will add email addresses to an existing index to improve efficiency.",27/01/2021,05/02/2021,,,,,,,,3rd party issue,," Following investigation, it was found that there was an unexpected significantly high number of users accessing HRE via Single Sign On. This caused a performance bottleneck in the application, pinning CPU usage on the server at 100%. In order to optimize the process, an index was added to the Security User Table which resolved the issue.",,,,2/22/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,1693,>60
26/01/2021,26/01/2021,87990537,,,,,Customer Channels,,,,SI,No PayPal orders taken,TCS,.COM,,.com website - Paypal payment,RC identified,54118,"Alerting indicated that no PayPal orders were taken for approximately three hours.  
This issue was caused by a corrupt cache.  Once the cache was cleared the PayPal page rendered correctly and the PayPal payment method could be selected to pay for the order
It was only once the cache was cleared that PayPal orders started flowing.","Next actions: 
New alerts have been added to pick up this issue more proactively - Completed
Alert name: ""No orders place through one of the payment types"" created to check every 30 mins starting from 6AM to 11PM.",26/01/2021,26/01/2021,,,,,,,,Code/Product bug,," 
This issue was caused by a corrupt cache, once the cache was cleared the PayPal page rendered correctly and the PayPal payment method could be selected to pay for the order",,,,2/4/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,0,0-30
24/01/2021,24/01/2021,87988219,,,,,Customer Channels,,,,SI,Android version of Mobile App navigation redirecting to homepage,TCS,.COM,,"
Android version of Mobile App navigation",RC identified,54205," The Mobile App Android team reported an issue with the links for Android versions redirecting the customer back to the homepage instead of taking the customer to the correct category page.
Impact: 
•	Poor experience for the majority of Android App Browse customer journeys.
•	Potential loss in orders.
Recovery actions: 
After the WCS upgrade from v7 to v9, the majority of the navigation links changed to a format which the Android App was unable to recognize, resulting in the issue.

Support took a version of the files with the navigation links from WCS v7 (the format that Android recognizes) and placed it as a hard coded static file for the Android App via Akamai, but to no avail. Support then updated the file directly within the Android App configuration to restore services by 14:30. 

A permanent fix for WCS v9 will be applied on Monday 25th January.","Next actions: 
* Permanent fix is planned for 25/1/2020 - Completed",24/01/2020,24/01/2020,,,,,,,,Customer Tech change,Yes,"  The format of the navigation links on the Android app changed after the WCS upgrade from V7 to V9, which caused the issue.",CM5645,Lack of proper testing,,1/25/2021,January,2021,,Closed,,,2021,1,January,9/16/2025,0,0-30
23/01/2021,23/01/2021,87986839,,,,,Group Technology Services,,,,SI,Performance Degradation in the EDW database,TCS,IS-Unix,,PDOA EDW Database,RC identified,54209," At 09:30, MIM was made aware that multiple EDW jobs had failed; since 3 out of the 4 nodes of the EDW database cluster went to ‘pending online’ state.
Impact: Departmental hourly sales reports used by stores users were inaccessible between 08:30 and 11:59. However, only 2 store users reported this issue, since these are minimally accessed over the weekend.
Resolution Notes:
•	In an attempt to fix the issue, Support restarted all the affected nodes however these nodes continued to remain in ‘pending online’ status.
•	A sev-A case was raised with IBM for recovery and diagnostic logs were shared.
•	As advised by IBM, Support unmounted the filesystems and restarted the GPFS ( General parallel file system) to recover the problematic nodes by 11:59.
•	EDW PROD and DR databases are in sync since 12:00.
•	All the failed EDW jobs were recovered successfully by 14:23.
•	The failed EDW weekly backup job has been triggered and will be monitored till completion.
•	RCA is underway with IBM.","Next stesp:
28/1: Risk ID: 1076 - PDOA appliance running in N-2 fix pack version.  Need to upgrade to the latest version PDOA v1.1 Fix Pack 3 to address the open vulnerabilities - ALready in place so closing the PR.

27/1: update from Elizabeth on upgrade:
Customer declined to fund the stack upgrade in 20/21. Mark said he would look at funding in 21/22. It’s on our list to request the funds in Q1.
27/1: Recommendation from IBM: We recommend that you upgrade to PDOA V1.1 FP3 which includes GPFS 4.2.3.17.  This will bring up to a GPFS level that includes fixes for errors similar to what occurred here.   
Then upcoming PDOA V1.1 FP4 is released we would recommend that you upgrade to PDOA v1.1 FP4 as well which will upgrade GPFS to V5 which includes more fixes and is better supported by our GPFS support team. This suggests that the specific assert error is rare and does not occur frequently.   Given this is a code issue, there is not an issue with hardware or configuration on huxp008 that. Another upgrade recommendation is to upgrade the PDOA ha toolkit to the latest version.  In general, this will help TSA failover more reliably and help avoid unexpected problems


25/1: As per the update from IBM issue might be with the GPFS layer or cluster level.",26/01/2021,27/01/2021,,,,,,,,Code/Product bug,,"  This was due to GPFS assert on node huxp008. It doesn't perfectly match a known defect, but it is close to several other defects where the code has been fixed.  
",,,1076,1/27/2021,January,2021,,Closed,,,2021,1,January,9/16/2025,0,0-30
13/01/2021,13/01/2021,87972177,,,,,Customer Channels,,,,MI,Voice calls not routing to colleague services helpdesk and Dot Com contact centre,Anana,.COM,,Genesys IWS,RC identified,53493,"At 08:26 on 13/01, MIM were made aware of an issue with the voice calls not routing to colleague services helpdesk and .com contact centre agents due to an issue at the Anana end.
Impact: 
·         No voice calls to .com contact centres and colleague services.
·         Poor customer experience.
·         Contact centres and Colleague services were unable to make outgoing calls.
Recovery Actions:
At around 07:30 on 13/01, Anana received an alert indicating that multiple inbound and outbound voice calls were silent. In an attempt to fix the issue; Anana performed a failover of their MCP server (Media Controller Platform) and Resource Manager (RM) servers (RM load balances the Genesys platform services), however, the issue persisted. At 08:14, reports started coming in as calls were not being routed to the contact centre and colleague services helpdesk.
 
Investigations by Anana revealed that calls were failing in their Genesys layer. Anana had performed a Twilio routing change on Tuesday,12/01night. Anana reverted this change to check if this fixed the issue; however, the issues continued to persist. Further troubleshooting from Anana revealed an issue in one of their SBCs (Sessions Border Controller) at around 09:45 and they failed over services to the secondary SBC, in an attempt to fix the issue. A few test calls were then performed which revealed that the issues continued.
 
Upon further analysis, Anana identified the calls not routing to the contact centre and colleague services was a result of the RM failover not performed properly in the morning, which had caused an IP conflict. Anana then performed the RM failover in a controlled manner to fix the issue by 11:50.  The silent calls were a result of a missing VLAN in the Twilio physical interface on a switch.  Anana published the missing VLAN to fix the issue by 12:00. Test calls were then performed which were successful and the Global emergency message was then removed by 12:25, 13/01.","Next steps
15/2: Long term actions are enhancements, will be taken up by Anana
Update: 29/1:
1.	Introduce switch monitoring for symptoms of MAC address inconsistencies. Action complete.
2.	Improve post change testing/checking to detect issues. Action complete.
3.	Improve proxy set alerting and management.  Action complete.
4.	Simplify RM service architecture to remove human fallibility element. To be prioritised and agreed.
15/2: Long term actions, will be taken up by Anana
5.	Add the ability to bypass global emergency messaging to enable voice testing before the message comes off. To be prioritised and agreed.
15/2: Long term actions, will be taken up by Anana",13/01/2021,18/01/2021,,,,,,,,3rd party change,Yes," The incident was caused by an approved change which was to connect SIP networks between the two data centres as part of a resilience improvement initiative.  This follows a successful deployment in pre-production. The change caused a MAC address inconsistency across the VLT link which led to approximately 10% of  traffic being “black holed”.  The issue was resolved by adding a VLAN which meant traffic was no longer being discarded. This issue was resolved at 09:20 am.  Subsequently a second issue caused a larger impact (up to 50% of calls) to the CSSC after the failover of the RM service which caused a Genesys call routing issue. This was because the failover was not performed according to the documented procedure.
The outage is suspected to be caused by an Anana network routing change, followed by the recovery 
actions performed in an attempt to fix the issue. Awaiting detailed root cause from Anana.",Anana Change,Lack of proper testing,,2/15/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,111,>60
07/01/2021,07/01/2021,87965642,,,,,Customer Channels,,,,MI,Customers unable to checkout on international flagship sites,Global E,.COM,,Customer international flagship sites,RC identified,53475,"Customers on the Customer international flagship sites were unable to checkout their orders for a brief duration of 14 minutes.
Impact:
•	All flagship sites were affected.
•	Customers could not complete their orders. Customers would have seen error page after clicking on check out
•	Poor customer experience
•	Loss of sales
Recovery actions: Global-e and support teams were engaged immediately to investigate the issue. Global-e advised that they were experiencing system availability issues and were already working on fixing the problem. 
 
By 14:14 Global-e confirmed that they had fixed the issue and validation checks performed on the flagship sites confirmed complete service restoration.","Next steps:
11/2: Closing PR as the new service to prevent this is planned for Q2.
11/2: Updated received from Global e on the pending investigations
4/2: Followed up with Sankar - They have followed up with global e but there was no update.
11/1 - Received RCA from Global E

--Why the catalog download process hung
   Our investigation concluded that the catalog process hung because the SQL driver that is responsible for file uploads crashed.
--Why the restart of the primary DB resulted in the entire cluster becoming unstable.
     As our internal teams were unable to identify the root cause, we consulted directly with Microsoft’s MS SQL experts and also hired a specialized consulting firm to help with the investigation. Unfortunately, after examining both the machines themselves and the logs, both Microsoft and the consulting firm’s experts came up empty handed. They were unable to find the reason for the cluster to become unstable, with the only hypothesis being that the specific driver crash may have served as the cause. As such, their recommendation was for us to stop using this specific SQL driver for file uploads, and switch to a different method.
Therefore, our R&D and data groups started to work on the necessary developments to put in place a new type of service for catalog updates, that we can use instead of the SQL driver. The development is in advanced stages already and should be completed as part of the next sprints. Following QA, we expect it to be operational by Q2.
In the meantime, as a percussion, we put in place a special procedure by which whenever there is a need to upload catalogs, our tech support team stops the service manually and restarts it back when the upload is done.
To date, since the original incident, we have not experienced any additional ones of this sort.",07/01/2021,11/01/2021,,,,,,,,3rd party issue,," 
Accelerated DB restart performed on the production environment after testing successfully on their staging environment DB to fix the specific state the DB was stuck in resulted in all GE services becoming unavailable.
Global-e advised that the outage was due to a server issue in their environment. We await a detailed root cause from Global-e.",,,,2/11/2021,February,2021,,Closed,,,2021,1,January,9/16/2025,1713,>60
04/01/2021,04/01/2021,87960656,,,,,C&H & Intl Supply Chain,,,,SI,C&H warehouse data missing in EDW reports for 31/12/20,TCS,EDW,,"EDW GML Reports
Stock Order Delivery Receipt Actuals
Order fulfilment
Stock Receipt
Site delivery
Stock picking",RC identified,53524," C&H EDW GML reports are missing data for 31st Dec 2020. The data was received from source WMS but did not get processed to the reporting layer.  GML has self-service reporting, so users create their own reports from the data available in EDW reporting layer.","Next steps:
Last update: 4/3

* The date format in DataStage routine – ‘EDWGetBatchIdAndProcessingDateWithStatusByBatchId’ has to be updated to ISO format from US format. This will be implemented through a CR -CRQ000000133923 raised will be represented on 18/1/2020

* An automatic data validation and alerting will also be implemented to catch the similar issues in GML reporting. This will be done through Ops Backlog - Completed
4/3: Data validation is in place and alerting is in place - EDW DL will get an automated mail every day post GML flow completion with last 15 days value of each GML attributes  in a table format and an automated incident will be raised if today’s value exceeds the average of last 14 days to a certain threshold percentage, so that EDW team  can know the data discrepancy in report and take corrective action.
1/3: Alerting will be implemented on 1/3 through CR 136540
22/2: Getting automated emails having gml measures on a daily basis. ETA for alerting set up to be completed - 28/2",04/01/2021,04/01/2021,,,,,,,,Design issue,," This issue occurred due to the year transition from 2020 to 2021. The DataMart job has a US date format (MM/DD). Systemically, the job ensures it is picking the data for the correct date by comparing the current date (maximum value) and the date the previous day (minimum value). Therefore, 01/01 was considered as the minimum date and “12/31” was considered as the maximum date - which is incorrect. As a result, the transactions with “12/31” data were not picked by the DM job for processing.",,,,3/4/2021,March,2021,,Closed,,,2021,1,January,9/16/2025,1716,>60
28/12/2020,28/12/2020,87954950,,,,,,,,,SI,Latency observed in the Foods JDA Open Access Application,,,,,,,,,,,,,,,,,,,,,,,,,January,1900,,Closed,,,2020,12,December,9/16/2025,1723,>60
28/12/2020,28/12/2020,87954198,,,,,Platform & Store Ops,,,,SI,Intelligent Waste and TSL DEF applications unavailable,TCS,Colleague Devices Efficiency,,"Intelligent waste, TSL-DEF, Reduced Later",RC identified,53314,"Since 06:00 on 28/12, multiple stores started reporting that they were getting a timeout error message while trying to access the Intelligent Waste application.
Impact: 
•	Store colleagues were unable to access Intelligent Waste application and perform waste reduction between 06:00 and 07:33.
•	The TSL-DEF application and the Reduced later functionality in the Intelligent waste application was unavailable between 06:00 and 09:10.
Recovery actions: Support identified that the TSL-DEF (Trading Safely and Legally - Date Expired Food) application database was taking longer than usual to respond, causing timeout errors. As the Intelligent Waste application also connects to the same database to fetch location details for Reduce Later functionality, Intelligent Waste also experienced timeouts.
 
The TSL-DEF backend was put under maintenance mode to bring up the Intelligent waste application at 07:33. Store colleagues were able to access the Intelligent Waste application and perform waste reduction since. However, the TSL DEF application along with the Reduced Later functionality in the Intelligent waste app was still inaccessible.
 
As the peak hours for waste reduction activity was over, support teams agreed to take the TSL DEF backend out of the maintenance mode at 09:10. Stores confirmed that the TSL DEF Application and Reduced later functionality in Intelligent waste application were accessible since.
 
In order to fix the DB performance issue, a decision was taken to perform data purging on the Reduced later table in the database which completed at 10:30. The applications were then monitored for stability until this morning and no issues were observed.","Next Steps:
15/4: Closing as per confirmation from Rosie
*  ‘Reduce Later’ data sync will be disabled in Offline Helper by WE 24/01 which will alleviate the load on the TSL DEF database - Closed -
15/4: Rosie confirmed over email to close the PR.
17/4: Email sent to Rosie to check how to proceed as there is no ETA when the Offline helper sync in reduce later would be disabled
1/4: Contacted Rosie to check with Business whether this is going in ot nor.
12/3: Rohith informed PO is checking with Business
5/3: Checked with Rohith for update, he has not received any update, he will check with PO and confirm
18/2: Business raissed concern about removing this feature as stores say its convenient for them to have this feature. Discussions ongoing. No ETAs
Reduce Later is not recommended from intelligent waste app anymore and we can turn off this feature in Intelligent Waste. We already wrote stories to remove reduce later data sync in Offline helper and planning to deploy it to estate by end of sprint.
15/2: Checking with Rohith for PROD deployement ETA
8/2: deployed to Innovation. Will plan for prod deployment soon
25/1 - This is deployed to MVP stores now and will be deployed to all Innovation stores this week. Planning for a Estate release next week after monitoring app in Innovation for a week. ",28/12/2020,31/12/2020,,,,,,,,Design issue,," An unusual high volume of Reduce Later API calls caused the TSL DEF database to slow down and impacted the overall performance of the database.  The API calls from Intelligent Waste were timing out as a result and caused the application to become unresponsive.

We analysed this further and found why traffic surged at 6 AM. We are caching previous day’s reduce later data in Offline Helper app for a feature in Intelligent Waste. This cache gets invalidated at 6 AM and triggers a lot of extra calls from all the devices across estate. This added with more peak traffic caused slowness in DB. And if this call fails, it will be retried when any CSSM apps opened again. This aggravated the issue and caused enormous traffic.",,,,4/15/2021,April,2021,,Closed,,,2020,12,December,9/16/2025,108,>60
23/12/2020,23/12/2020,87950590,,,,,C&H Commercial Trading,,,,SI,Intermittent issues while performing data loads in New SSI application ,"Microsoft (120122326004111 & 120122326003497)
",Cloud Exponence Support,,SSI,RC identified,53289,"At 08:00 on 23/12, users reported that they were getting the error – “data failed to load in cache“ and were hence unable to load data in New SSI.
Impact: Users were unable to perform data loads in New SSI impacting C&H product planning.
Recovery actions: At 08:15, support observed that the database connection to the SSI product services was closed. Support teams restarted the application, ignite and interface pods by 08:45 which alleviated the issue temporarily. After 09:10, users again reported issues and support could replicate the problem however, the system was stable since 09:25. 
 
No further actions were carried out by support team and hence a Sev-A case was raised with Microsoft. A Microsoft engineer was engaged to investigate the issue along with support. Microsoft advised that three nodes were undergoing automatic updates for platform components and hence went into an unresponsive state and recovered at 05:03, 06;22 and 09:05 respectively.","Next Steps:
1/3: Prathyush (Cloud exponence team) to work with Cloud Platform team within themselves to implement the preventive measures suggested by IBM. Risk already in place for the SSI v2 migration

10/2: Checking with Prathyush on implementation of preventive measures suggested by MS

* To prepare for VM maintenance and reduce impact during Azure maintenance, try using Scheduled Events for Windows or Linux for such applications.
Actions Pending:
•	Investigate why the AKS pods restarted when the VMs paused due to a memory preserve update 
o	Andrew to investigate
o	Customer will send the logs to Andrew - Prathyush & Radhakrishnan to share the relevant logs / update the case 120122326004111 - Completed
 
•	Investigate how or if it is supported to enabled Scheduled Events for VMSS – Action on Maria
 
•	Have a look at the errors that Mohamed got (case 120122326003497) and check if we can find out if they correlate with the node restarts - Action on Mohamed & Maria
o	Customer, it would help if you can share some context as we cannot see the client instance names in the errors - Desh / Radhakrishnan
15/1: Email sent to Maria: Can we have what exactly you looking for on the client instance.
o	Can you please confirm if you still see similar errors (DB Connection Close Errors) after the 23rd December? - Desh / Radhakrishnan - 15/1: Email sent to Mariuachecked the log for last 7 days and we could see the DB connectivity Issue , attached the document for reference(mentioned the DB server name, pod name). Please let us know if you need any further details.
 
•	Dinesh will schedule a call to share our findings for next Tuesday 19th Jan - Action on Dinesh - Completed
 
•	Maria to share any update in RCA pending on k8s-agent0-22076845-vmss_68 (Networking taking over 3 minutes to be available after memory preserve update) -  Action on Maria",23/12/2020,24/12/2020,,,,,,,,3rd party issue,," Three Kubernetes nodes went into an unresponsive state as these nodes were undergoing automatic updates for platform components causing intermittent connectivity issues.

This occurrence was caused by an Azure initiated memory-preserving update action. This update is part of routine maintenance performed on the underlying hosts for this VM. During these updates, the VM is paused and then resumed. The network traffic to these VM instances took a little longer to resume.",,,,3/1/2021,March,2021,,Closed,,,2020,12,December,9/16/2025,1728,>60
10/12/2020,10/12/2020,87932330,,,,,Group Technology Services,,,,SI,Issue with the Service desk line,BT,Service Desk,, Service desk lines(185999 & 185704),RC identified,53212,"At 12:52, MIM was made aware of an issue with the Service desk number 185999 and SD escalation line 185704 where callers were encountering a message ‘there are no agents logged in the system, please call later’. 
Impact: Service desk and escalation lines were unavailable between 12:40 to 13:35. Users were advised to contact the service desk via email as a workaround.
Recovery actions : Service desk advised that agent consoles were showing that calls were coming in, however the voice calls could not be answered from 12:40. Service desk were able to receive inbound and make outbound calls successfully from 13:35. ","Next steps:
Update : 24/2
Working with Rosie to see if she can agree with Mitel on getting an engineer asap to fall back to the skype solution in case of issues with the BT platform - Completed
----------------------------
28/1: No further updates from Chennai Voice team.
Working with Rosie to see if she can agree with Mitel on getting an engineer asap to fall back to the skype solution in case of issues with the BT platform.
25/1: Emailed SD to check with Voice team for the perventive/mitigation plan
15/1: SD checking with Voice team
Update:8/1: Checked with SD for any further updates from Voice team
Need to check and agree with Mitel to move to the skype solution in order to minimize the outage window especially during the Christmas/New Year period  in the event of an outage from BT.",10/12/2020,17/12/2020,,,,,,,,3rd party issue,," Port failure on Nokia Firewall in London Global Switch. The port of the firewall was in a ‘hung state’ and it did not go fully down – therefore auto failover did not happen and manual intervention was required to restore the services. Firewall team was engaged and initiated a manual failover to the standby firewall which resolved the connectivity issue.
",,,,2/24/2021,February,2021,,Closed,,,2020,12,December,9/16/2025,1741,>60
07/12/2020,08/12/2020,87927602,,,,,Corporate,,,,MI,Absences not interfacing into timesheets,SDWorx,SDWorx,,Work Force Management,RC identified,53101,"Stores reported that when they went into accept employee timesheets, they noticed that absences were missing from timesheets for WC 29/11/2020.
Impact: 484 employees did not have accurate absences recorded on their timesheet – their pay was not be affected.  47 employees out of the 484 had their additional payments (such as overtime) delayed to Friday 18th December.
 
Additionally, on 08/12, it was reported that around 500 new starters were showing zero hours on their timesheets. The impacted records for the new starters were identified and re-triggered by SD Worx to fix the issue.
Recovery actions: SD Worx identified that any absences keyed in from 3rd December did not flow into WFM (Work Force Management). Stores had been re-keying the absences however, these too did not flow to WFM as the fault was still there and the rekeying had been causing duplicates. 
 
After further investigations, SD Worx confirmed that the issue was caused by a TLS encryption change at their end on 3rd December 2020 and the change was reverted on 07/12. 484 weekly employees were affected by this issue.  Attempts were made to retrigger the missing absences in batches of 50 by SD Worx however SD Worx encountered timeout errors when they retriggered the missing absence data since the system was being used during the day.
 
The re-trigger was again attempted during the early hours of 08/12 and all the 484 employee weekly absences got interfaced successfully overnight. The monthly employee’s data that was missing is being triggered but these are not part of the impact as the monthly run was not due for this month.
Root Cause: An infrastructure security change in the SD Worx environment resulted in absences failing to be interfaced from HRe into WFM","Update 24/12: Next actions:
*  To ensure there is end-to-end testing performed following for major infrastructure level change on the Non-Production environment - SD Worx Project Team - Lessons leart

* To hold a Hypercare session following high impacting infrastructure change
 Following the lessons learnt session held following occurrence of this MIM, for these types of changes, it has been agreed that the Hyper-care should be organized - SDW Environnent team – Laura T - Lessons learnt

Update 22/12
Questions raised with SDWorx:
1.	Why was the CAB process not followed for this infrastructure change?
---Please note that the CAB Approval process was actually followed. Kindly refer to CRQ000000131123 which was logged in remedy. The RFC was raised on 06/11 with proposed implementation date as 03/12. 
The tests that were performed were not extended to cover all scenarios. At the time, the change that was done on Non-Production, when the tests were performed, no issues were identified. The tests were performed by WFM experts and they had no indication that there would be some aspects that were not considered as part of the testing. After the change was promoted to the Live environment, there were tests that followed but which were based on the ones performed on Non-Production. It was only when a major incident INC000087929093 was raised, that it came to light that some aspects had not been considered.
2.	During the incident, we were told that the infrastructure change was reverted to restore the services, however, there is no mention of change reversion in the RC document, it talks about fix forward.
----Indeed, you are right. By referring to the term the issue was fixed, it would be more appropriate to say that we fixed the issue of updates not going through to HRE by in fact reverting back the TLS disablement. The TLS activity has now been postponed to next year – date is yet to be confirmed. 
3.	The document highlights that no issues were observed during testing after the change was carried out in Non-Prod, functional tests were also conducted after the production change which did not highlight any issues.  Are we saying, End to End testing was not carried out by the support teams?
-----There has been sessions that following the occurrence of the 2 MIMs and the project team has taken note of the shortcomings of the previous tests scenarios and will ensure that there is end to end testing and that all the relevant teams are involved to redefine the scope of testing, ensuring full integration end to end tests. 
4.	On actions, who is the SD Worx project team- need teams but also names of owners who will follow up on actions
[MIM] The TLS disablement project will consists of many teams but the principle project owner who will be ensuring all the relevant teams are engaged and the information is correctly co-ordinated amongst them will be from the environments team – Laura Troughton
5.          And on impact page 7, it says that support identified no pay impact but there was pay impact to employees who had additional payments
----The TLS MIM INC000087927602 actually was a pay-impact one where absences had to be carried forward to the following week’s payroll – this is where the employees had additional payment impacted whereas for MIM  INC000087929093, where you have highlighted in the timeline, we confirm there was no pay impact. We have confirmed with the Support team and out of the 1012 weekly employees, most of which were future dated employees. The only employees who would have been pay-impacted are provided below. They would have been pay impacted if the records were not re-triggered by the Support team.",08/12/2020,08/12/2020,,,,,,,,3rd party change,, An infrastructure security change in the SD Worx environment resulted in absences failing to be interfaced from HRe into WFM,,,,29/12/2020,29/12/2020,#VALUE!,,Closed,,,2020,12,December,9/16/2025,1744,>60
01/12/2020,01/12/2020,87918384,11:33,12:43,01:10,Foods,Food Supply Chain,,,,SI,Milton Keynes DC lost network connectivity,Vodafone,IS-Network,,JDA Dispatcher/ HHTs,RC identified,52800,"Around 12:15, MIM was made aware that Milton Keynes DC lost network connectivity.
Impact: The DC users lost AMT connectivity and JDA systems were down, impacting their BAU operations between 11:33 and 12:43. However, the DC confirmed that they usually pick more than necessary and hence the impact to the stores and allocation was minimal.
Resolution Notes: 
Network support confirmed that they received multiple alerts indicating that the switches at the DC had gone down from 11:33. Support confirmed that the switches came back up automatically at 12:43. 
 
Network connectivity at the DC restored and the DC operations resumed. 
 
The team believed at the time that an issue within Vodafone had caused the network issue at the DC. A Sev-1 ticket was raised with Vodafone however Vodafone confirmed that they could not find any issues within their environment. ","Latest update : 

1.	Arrange a network engineer at the site to label all the required cables and provide a walkthrough to the site contacts (XPO Shift Managers) to carry out a manual failover - Mustafa - Completed

2.	Risk to be raised in the Risk Register - Completed - Risk iD: 1532
No automatic failover to the backup link (4G solution) in case the temporary link goes down - Kasi

3.	Activate the Vodafone Primary Link - - Completed
a.	Verify and validate the configuration of the VF primary router / link is all good - Mustafa
b.	Work with the site to get a maintenance window of 2 hours (includes cut over to the VF primary link along with the failover test) – Completed - Ben -
10/2: Completed
25/1 - Postponed to 7th Feb
15/1: ETA: week commencing 18th Jan.
Update 21/12 - Outage agreed for post peak 
c.	Once the traffic is switched over the VF primary link, the temporary link will act as the back-up link and the failover will be automatic.

4.	Commission the Vodafone Secondary Link. - Completed
14/5: Activity completed on 9th June
17/5: Revised dates for final MK comms changes to allow site ti be BAU but still awaiting BT and VF confirmation: 6th June and 16th June- 10.00 - 14.00
19/4:  Revised dates for final MK comms changes to allow site ti be BAU but still awaiting BT and VF confirmation
5th May and 12th May - 10.00 - 14.00
8/4: Checked with Craig and he told the chnage for 7th was cancelled and will have  a meeting with Joey to confirm a revised date
4/3: MK site have confirmed Wednesday 7th April (10:00-14:00) as the next best time for a four hour outage. Waiting for BT to confirm if they need to use this slot to complete their cable work before we get back to planning the implementation activity for the 2nd line. 
1/3: Meeting with John Ariss, Craig and VF on 1/3 to discuss on this
18/2: Change cancelled due to unavailability of VF resources
15/2: Change planned for 18/2
a.	

5.	All network changes at Milton Keynes site should go through a Change control process – Mustafa / MK Project team - Agreed",01/12/2020,08/12/2020,,,,,,,,3rd party change,," The outage was caused due to a config change performed by Vodafone as requested by the MK Project team, in readiness to commission the new primary link.

The config change was done on the new primary router (not connected to Customer LAN) which caused the live traffic to favour the new primary link instead of the existing link.",,,1532,14/06/2021,June,2021,Dimple,Closed,,,2020,12,December,9/16/2025,1750,>60
26/11/2020,26/11/2020,87912297,,,,,Customer Channels,,,,SI,Customer.com - Hampers not showing on .com  website,Flamingo,.COM,,Customer .com website,RC identified,52889,"On 26/11, business teams advised that the majority of hampers were not showing on the .com website
Impact: Potential loss of sales, as customers were not be able to buy the hampers. This was a particular concern because 25/11 was the biggest day of hamper sales ever
Recovery actions: Support teams joined a major incident bridge call and found that the Start of Day file (SOD) sent from the supplier (Flamingo) had zero inventory for the hamper products, however there was physical stock of these products.
Stock levels were re-sent from Flamingo via the IAA feed. Once this was sent and the Bloomreach delta feed job had run, the hampers were visible on the website and customers could purchase them.","Next actions:

•	Cleaning of the corrupt data - Completed
•	Adding a step in the logic for the system to not allow a SOD to be processed and sent if the file contains errors within it, it will just fail again and require IT intervention to resolve - Completed
 
•	We are going to make a change to the SOD threshold alerting so it runs no matter what time we receive the SOD - will be done on Monday (30/11) - Completed - As per the update from Jackson this has been completed on 2/12",26/11/2020,29/11/2020,,,,,,,,3rd party issue,," Due to a data corruption issue at Flamingo, the SOD Inventory file sent to Customer was missing the hampers data. The Sterling application then changed the inventory for the missing lines to zero resulting in Hampers being unavailable on the website.",,,,04/12/2020,December,2020,,Closed,,,2020,11,November,9/16/2025,1755,>60
16/11/2020,18/11/2020,87897294,,,,,Platform & Store Ops,,,,MI,SPPD (BOSS) unavailable ,TCS,.COM,,.com BOSS,RC identified,52738,"Store users advised that they were unable to use the SPPD (BOSS) application to pick or pack on both their Honeywell devices or workstations.
Impact: BOSS stores were unable to carry out their BOSS operations; both picking and packing operations were impacted from 15:00 until 15:50 on 16/11.
Recovery Actions: 
At 15:12 on 16/11,  MIM were made aware of multiple calls coming into the service desk from stores, advising that they were unable to access the SPPD (BOSS) application.
 
Support was engaged and could see high CPU utilization and long running queries on the application database server; DB3.  Support carried out a restart of the DB Mongo service to try and restore service. This was done and the CPU utilization went down after the restart. Latency had also reduced when tests were carried out by the support team.
 
From 15:50, multiple stores confirmed that performance of the SPPD (BOSS) application was back to normal. Support remained on the call for an extended period of time to ensure stability.  A team was set up in parallel to ascertain the root cause of the long running queries and high CPU.
 
BOSS support was placed on hyper-care throughout the night and re-joined a bridge in the morning (17/11) to continue hyper-care when picking and packing started. 
 
On 17/11, Stores reported that they saw the following error when they were trying to pack parcels; 'Unable to fetch parcel label. Please try again"". Support could see that the store delivery label API call was intermittently failing. Approximately for every 20 calls, 2-3 were failing due to a time out error. When the store re-tried to get the label information, support saw that the 2nd call was going through successfully as the first call added an entry to the DB already.
 
The issue was due to the amount of data being held in the live tables within SPPD causing long running queries and analysis on the archiving was carried out. Support deployed the change in SPPD database to change the amount of live data that was archived by 20:30 on 17/11.
 
Following the change, support could see that it had a positive effect on the pick and pack API response times – average times before the change was 0.9 secs to an average of 0.1 secs after the change.","Next Actions: 
Latest update 4/2:   SPPD/Speedy database to upgrade it to a Mongo Atlas database completed
Latest Update:28/1 : On Wednesday 3rd February a change will be carried on the SPPD/Speedy database to upgrade it to a Mongo Atlas database. 

Latest update : 25/1: Followed up with Emma, waiting for update on the plan on migration.

1.	Reduce data retention period in live tables from 5 days to 3 days for BOSS and moving them to archive table - Completed
2.	Optimise DB performance by tuning queries - Completed
3.	Migrate BOSS database from the current setup to Atlas Mongo - Long term - Completed
27/11 - The plan to migrate the BOSS DB over to Mongo Atlas is currently in progress. 
It has been agreed the Dev Team will make the required changes to get this up and available but we will only move over to it if we really need to. 
Plan is to upgrade after peak but have a migration plan in place to follow in case we was to make the decision to move over to it sooner. 
Hopefully, we will be taken through the plan in the next couple of weeks. 
",18/11/2020,19/11/2020,,,,,,,,Design issue,," Over a period of time and as the usage of the BOSS application increased, performance degradation within the BOSS SPPD Database required the need for alternations to the way data is archived and for further performance tuning.",,,,04/02/2021,February,2021,,Closed,,,2020,11,November,9/16/2025,1765,>60
12/11/2020,15/11/2020,87893276,,,,,C&H Commercial Trading,,,,SI,Lower than expected sales values for .com C&H for week commencing 09/11 in GMOR,TCS,"5267 - PLM Project
",,GMOR,RC identified,52833,"On 12/11, users reported lower than expected sales values for .com (C&H) for week commencing 09/11 in GMOR
Impact: GMOR reporting .com C&H daily sales were understated for the week of 9/11 on a day-to-day basis therefore financial reporting for the week was inaccurate.
Recovery Actions:
On 12/11, business users reported that they were observing lower than expected sales values for .com C&H for week commencing 09/11 in GMOR.
 
Planned work was carried out on 7/11 to change the source of the GMOR C&H reports from OAGG to SAP ECC (CR # 122588) to allow the business to study GMOR .com C&H sales in a more granular manner including a split of sales for each individual international flagship website.
 
After the change was carried out, users reported that the sales in the new reports were lower when compared to the original OAGG source. Support and project teams confirmed that the variance was there because in the original solution the GMOR reports would get their data directly from one source, OAGG and the processing was very quick. With the source being ECC, the data processing jobs were cyclical and hence processing in the various systems took longer, therefore taking processing time across midnight. This meant that some of the sales would not be posted for the day required and would be posted the next day.
 
On 13/11, a decision was taken to revert the GMOR flagship change after securing an approval from the business. The Project team reverted the GMOR flagship change successfully by 14:56 on 14/11 after which the business validated the reports and confirmed that the data was as expected.
 
Support monitored the 14/11 overnight batch and confirmed that .com C&H sales in GMOR were as excepted for the entire week.","Next actions:
The change has been reverted until another solution becomes available",15/11/2020,15/11/2020,,,,,,,,Customer Tech change,, Planned work carried out to change the source of the .com C&H GMOR sales figures to differentiate between UK and Flagship sites,CRQ122588,,,17/11/2020,17/11/2020,#VALUE!,,Closed,,,2020,11,November,9/16/2025,1769,>60
05/11/2020,07/11/2020,87883906,,,,,Group Technology Services,,,,ADHOC,Two nodes on AKS v2 cluster went to “not ready” status,"Microsoft
(120110523002359)","Cloud exponence
",,Metapack services,RC identified,52704,"Around 20:30 on 05/11, MIM was made aware that multiple pods in the cloud B2B cluster went down along with the Metapack services failing in the Castle Donington DC. The cloud integration team advised that most of the nodes in the primary B2B cluster restarted themselves after which the nodes 2 and 4 went to “not ready” status automatically . 
Update 07/11:After performing a detailed root cause analysis, it has now been confirmed that an infrastructure change at Microsoft end has caused the nodes in the AKS cluster to go to ""not ready status"". Support had uncordoned the problematic pods which means the nodes have been added back into service from the maintenance mode successfully. The services continued to remain stable",RC identified,05/11/2020,,,,,,,,,,,"
The journey starts at two pieces of kubelet logs from the two nodes respectively:
Nov 05 20:06:28 aks-agentpool01-18411504-vmss000002 kubelet[109540]: E1105 20:06:28.958448  109540 controller.go:170controller.go:170]  failed to update node lease, error: Put https://shsrv-prod-dp-prod0001-eun-2fa8c30a.hcp.northeurope.azmk8s.io:443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/aks-agentpool01-18411504-vmss000002?timeout=10s: write tcp 10.150.242.6:60298->40.127.235.93:443: write: broken pipe
Nov 05 20:06:34 aks-agentpool01-18411504-vmss000004 kubelet[10671]: E1105 20:06:34.318813   10671 controller.go:170] failed to update node lease, error: Put https://shsrv-prod-dp-prod0001-eun-2fa8c30a.hcp.northeurope.azmk8s.io:443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/aks-agentpool01-18411504-vmss000004?timeout=10s: write tcp 10.150.242.9:37642->40.127.235.93:443: write: connection reset by peer
 
At the backend, we found that a service infrastructure upgrade started at the same clock. The operation will migrate the whole control plane from the current hosting machines to another machine set, thus may incur a brief time of inaccessibility for cluster nodes. Unfortunately, kubelets on those two nodes did not make it through that short period and fell offline. Though this accident is a corner case, we do feel sorry about it and will continue improving AKS service stability. 
In addition, an AKS cluster is a dynamic platform in which we recommend to always keep an extra capacity in case of hardly avoidable node failure. Proper cloud adoption should also be applied to your workloads to boost their availability.
 
Regarding the node reboot at 21:44 which was more than 1.5 hours later, it was triggered by AKS node remediation mechanism after a node remains unreachable to the control plane. A side note on this: we anticipate this remediation process to happen much sooner than that clock, while ARM request throttling at that time engraved the situation and added a delay.",,,,,January,1900,,Closed,,,2020,11,November,9/16/2025,1776,>60
01/11/2020,01/11/2020,87878273,,,,,C&H & Intl Supply Chain,,,,SI,Metapack services failing in the Castle Donington WMS application ,TCS,"Cloud exponence
",,Metapack label printing service,RC identified,52380,"Around 22:15, MIM was notified about the failure of Metapack services in the Castle Donington WMS application.
Impact: The packing operations at the DC was impacted between 21:49 and 23:00.
•	Operational Impact – Unable to pack across all chambers. Capability loss of 13.8K. SYW unable to process labels. 
•	Customer.COM Impact – 403 failed orders.Recovery Actions: 
The packing operations at the DC were stopped at 21:49 owing to this issue. The vendor Metapack was engaged and it was confirmed that the service was down between 21:49 and 22:58. Further investigations revealed that the Metapack pods (A pod is a mini-server or container running in a managed Kubernetes environment) went down at 21:49 along with the Nexus going down at the same time. Nexus is a repository where the images of the pods are stored and used for recreating the pods
 
Around 22:54, support re-created the Metapack pods using the image available in the Kubernetes cluster to restore the services.
 
The packing operations at the DC resumed at 23:00 and remained stable since.","Next actions:

5/3: Colin confirmed to close the PR and team will be tarcking the long term actions themselves.

1  Remove dependency on Nexus Repository for Pod restart - Integration DevOps - Completed

2   Uplift the pod memory from 1 GB to 2 GB on Tuesday, 03/11 at 06:00 AM - Cloud Integration - Completed

3   Validate and understand the reason for high memory utilization on the Metapack pod in the PT environment - Cloud Integration - Closed -
5/3: Closing as per the confirmation received from Colin
Update:25/2:
1.	We were provided with 2 alternative options to capture stats, first one was to use gcore tool, We are unable to run gcore command within the Pod, it fails with the syntax error on the first line itself.
2.	2nd option was to use mqsimemcheck script provided by IBM - We were able to run mqsimemcheck script and capture stats in Non Prod environment
3.	Meanwhile, in the last few weeks, We rolled out few changes
a.	ACE FP11 Rollout for all our 200+ K8s Pods
b.	'kubectl' version upgrade in the cluster - Rolled out by Platform Team
c.	Flexvolume drivers were migrated to CSI drivers to access AKV. 
4.	After all the above changes and rollout, We are not seeing any memory spike until now, it's been 8 days since the last change went in. Since there was no spike at all, we couldn't capture the stats. Pods memory utilization looks normal for past continuous 8 days, if it looks smooth for next 10 days, we'll update the IBM PMR and close the ticket.
15/2: We have rolled out the latest fix pack for Metapack interface as per IBM recommendation but we still couldn't generate the expected core dump file in Production, We are working with IBM to understand if there are any alternatives to proceed further on this. 
12/1: IBM has asked us to capture System core file for which we need to upgrade ACE Pods to Fix Pack 9 or greater. currently running ACE FP5 and we have built the docker image with latest fix pack FP11 as part of our planned activities. We'll rollout the FP11 changes to Non-Prod in the upcoming sprint (Sprint 1) and run few test cases to validate the functionalities. Post which, we are planning to roll-out ACE FP11 to Production for all the Pods in B2B in Sprint 2. Please note that fix pack may not resolve the issue and IBM expects a system core file for analysis which can be generated only in FP9+ versions. Hence, post the FP11 rollout we need to generate system core files and share it to IBM for further analysis
26/11: As of now, it has taken little bit of back foot due to the other business priorities of Brexit testing. Aravind will come back on it
24/11: We have created stub pods as same as Metapack service Pods and tried to replicate the memory spike scenario in Non-Prod by executing huge volume test but unfortunately, we couldn't achieve the production-like scenario. Hence, We'd need WMS Testing team's help to run the load test from WMS End to the actual pods so that we can monitor and observe the spike, also capture the following stats/traces required by IBM to take this analysis further.
Below are the stats we need to capture as part of our run. We kindly request your Team's help to run 70K + 70K (140K per day with a gap of few mins in the 2 runs) on Thursday and Friday so that we can capture these stats at our end.
11/11: Support observed spike in memory utilization after running the load test in the PT environment.  Further investigations in progress.  Next update end of sprint on 25/11.
No memory growth pattern observed in PT environment possibly due to variance in load. To reproduce the issue in non-production, we may need to run load test and enable traces when we find memory growth. This action is currently on hold to prioritize ‘Action 8’

4   Agree on who should be getting the alerts if a pod goes down along with Pager Duty call outs - Cloud Exponence / Cloud B2B Application Team - Completed - Sev1 Pager Duty alert configured to Integration ERS team in case of Pods going down in primary / secondary region (this alert has been configured for all the pods in the B2B Prod Namespace cluster)
Sev1 Pager Duty alert configured to Cloud Exponence team based on the pod status i.e. if the pod is in Pending / Unknown / Failed / Error status

5   Alerting on application pod memory utilization - Cloud B2B Application Team - Completed - CPU based alerting already in place.  Sev1 Pager Duty alerts have been configured to Integration ERS team for all the pods in the B2B namespace cluster.  Alerts will be triggered if the memory utilization on the pod goes beyond 80%

6   Re-route all the traffic to DR region if the metapack pod in the primary region is unavailable - Integration DevOps - Completed -
13/1: The change has been implemented successfully, and no issues observed post implementation so far change has been implemented successfully, and no issues observed post implementation so far
5/1: Colin to check and come back with the date to implement
23/11: Pending for Colin update
20/11: CRQ000000131591 has been raised for F5 network production deployment. Please let us when it can be scheduled and approval from CD end.
19/11: testing completed, checking with network team on the production go live implementation and approvals .
 11/11 - Network routing changes done in CATE environment (on F5), testing to be performed by CD, Cloud B2B and Network team
Network team checking the feasibility, should be done for all critical HTTP based applications in the B2B cluster.  ETA - TBC

7   Spin up an additional metapack pod in the Azure primary region - Integration DevOps -Completed - 10/11 - Testing complete, awaiting approvals from CD Business to deploy the additional metapack pod both in Prod and DR regions.   Uplift another 2 GB memory on the Metapack Pod i.e. 4 GB on each pod.
An additional metapack pod will be introducted in the primary region due to the criticality of the service.  This has been agreed and approved by the PO and Architects.  ETA - 11/11

8   Alerting and housekeeping on Nexus Repository - Cloud Tools - Completed - 11/11: Pager Duty alerting on Nexus space availability will be configured to the Cloud Tools team at threshold of 85%.  Once the alert is received, Cloud Tools team will follow up with the respective application teams for housekeeping / add additional space to mitigate the impact - ETA 13/11
Space Availability alerts to be set up on all the Nexus Repositorys.  Pager Duty alerting will be configured to the respective application teams as the app teams are responsible for housekeeping - ETA 06/11

9   Move from Nexus Repository to Azure Container Repository - Cloud Integration - TBC - Update from Integration Frameworks Product Owner - This action is not a priority as of now.  To be discussed further.

10   Implement monitoring / dashboard to identify the problematic component in the message flow between WMS --> B2B --> Metapack - OP / Colin Haworth - In Progress - 11/11: Metapack POD Health Check URLs being monitored on the ITM dashboard to identify issues in message flow between WMS and B2B components
Cloud B2B App team to work with OP / Colin and agree on what is needed for the dashboard

11   Establish a way to identify issues with pods in the primary / secondary Azure region - Cloud B2B Application Team - Completed - The pod down alerts will notify if the impacted pod belongs to primary / secondary region

12   Alerting configuration at Metapack when they are not receiving responses from Customer - OP / Colin Haworth - Closed- 
5/3: Received confirmation from Cloin to close PR as this will be along term action
11/11: Will be picked up with Metapack during the service review meetings.",01/11/2020,02/11/2020,,,,,,,,Capacity constraint ,,"

The metapack pod was evicted out of the node due to insufficient memory on the pod.   According to the current theory, high memory utilization on the metapack pod caused the pod to restart, however, the pod could not re-schedule itself as the Nexus Repository which holds the application image was unavailable.

The memory limit on the pod is set to 1 GB, although from the logs we could see the there was a gradual growth (and not a sudden spike) in the memory, the cause for the high memory utilization on the pod is still being investigated",,,,3/5/2021,March,2021,,Closed,,,2020,11,November,9/16/2025,1780,>60
30/10/2020,30/10/2020,87875547,,,,,C&H & Intl Supply Chain,,,,SI,Metapack services for Royal Mail 48-hour service was failing in CD and Thorncliffe DCs,"Metapack
(MET-67310-
L0T5R4)",C&H,,Metapack label printing-Royal Mail 48-hour service,RC identified,52293,"At 06:17, MIM was notified about the failure of Metapack services for carrier Royal Mail in the Castle Donington and Thorncliffe WMS application.
Impact: This issue impacted all packing since 05:41. Operations made the decision to stop packing for one hour whilst IT investigated. The actual issue only impacted the Royal Mail 48-hour service. The 24-hour service was re-enabled at 07:46. There were no missed promises due this issue.
Recovery actions : Parcels to be packed and sent by Royal Mail encountered an issue from 05:38 on the morning of 30/10 in Donington and Thorncliffe DC’s. Support engaged the vendor Metapack and they acknowledged that there was an issue with the consignment tracking number (SLID) at Royal Mail end.
 
In order to mitigate the impact to operations, Royal Mail services were removed from the packing operations at 06:41. Metapack raised a case with Royal Mail for the issue and also escalated to the Customer account manager on Royal Mail’s side. All impact was mitigated and Royal Mail packages were redirected to go via Hermes - except the Edit Beauty Box; as there is a commercial contract in place for this to be sent only via Royal Mail. It was confirmed that none of the Edit Beauty Boxes would miss promise as they are dispatched on a 14-day proposition.
 
Investigations found out that the issue only impacted the Royal Mail 48-hour service and hence support re-activated the 24-hour service from 07:20 and this further alleviated any load.
 
Metapack worked with Royal Mail and got new consignment numbers for the 48-hour service. These were applied by Metapack at 07:46 and Royal Mail labels were generating as normal.","Next Actions:
Latest update 23/11
Martin Coy confirmed to close the PR

•            Validate the consignment number range for all the carriers / services and ensure we are within the threshold (i.e. nothing close to breaching) - Completed - Paul Hughes / Metapack
•            Follow up with Royal Mail on Monday (02/11) to get the additional consignment number range for 48 hour service - Completed - Paul Hughes / Metapack - Requested an extra 5million number range but Royal Mail has give it to us in halves so you have 2.5million number has a over flow and the 2.5million as another overflow but we can only put in one at a time as they have different prefix’s so when the current overflow is low we will switch over to the other 2.5million. 

•            Snapshot view with the threshold and the current usage of consignment numbers for each Royal Mail service - Completed - Paul Hughes / Metapack - You currently do not have no number ranges below the 25% threshold. 
Royal mail have come back to us and gave us a new number range for the ‘Tracked High Volume 48 No Sig’ service. We have implemented the new range and will continue to monitor the number ranges. 

•            Review the gaps around process and alerting - Completed - Paul Hughes / Metapack - The gaps around the process have been reviewed and fed back to the support teams. 

•            Include CD IT Support/Key Stakeholders from Donington Operations on the Metapack alert emails for consignment number range threshold breach - Completed - Paul Hughes / Metapack -
19/11: Emails added successfully: CastleDoningtonITSupport@marks-and-spencer.com  & DL-TCSGMWMSSupport@marks-and-spencer.com

5/11:We are currently discussing making this change internally, while we are doing that we are providing Customer with a manual daily update to make sure that this action is covered. 

•            Daily checks from Metapack on the consignment number ranges throughout the Peak period - Completed",30/10/2020,30/10/2020,,,,,,,,Human error - 3rd party ,," Metapack ran out of consignment numbers for the Royal Mail 48 hour service due to the volume of orders being processed at Castle Donington (including the Beauty Edit Box despatched via Royal Mail).  Metapack worked with Royal Mail to obtain new consignment number range for the 48 hour service in order to resolve the issue.  

Metapack has an alerting mechanism in place to notify them of any threshold breach on the consignment number ranges for a specific service / carrier, however, in this case, due to a human error, the alerts were not actioned on time which resulted in a major incident.",,,,25/11/2020,25/11/2020,#VALUE!,,Closed,,,2020,10,October,9/16/2025,1782,>60
27/10/2020,27/10/2020,87871645,,,,,Customer Channels,,,,MI,Customer.com – Customers unable to add non-branded products to bag ,Customer,.COM,,Customer .com website,RC identified,52272,"Customers were unable to add non-branded products to the online bag. When they tried adding a product via the PDP (Product Detail Page) - nothing happened.
Impact:
•	Drop in orders
•	Number of products impacted were 4229 for C&H and 249 for Foods
•	Poor customer experience.
•	Increase in calls to Contact Centres.
•	Current marketing plans were on hold until the issue was resolved e.g., emails about Beauty Edit Box.
•	Apps and Assist App not impacted.
Recovery actions: The FESK PDP team identified that the issue was caused by a change to make the brand attribute mandatory against the product, however this should not have been mandatory. This attribute change was made ready for the brand launch of 'Nobody’s Child'. The issue became evident only after a further deployment on 26th October relating to the 'Contact Me When Available' (CMWA) feature.  The CMWA fix caused more PDPs to manifest this attribute problem.  The FESK PDP team developed a fix and this was deployed in production to resolve the issue.","Next Steps:

Last update: 23/11 - Update from Chris - Pending two actions added to backlog and will be done in furture.

*Identify why rollback did not work.  Fix any issue.  BRS-1020 created.
*Rehearsals for all members of PDP team to practice rolling back.
23/11: The rollback problem was found and we’ve since tested a rollback successfully. Also have work in progress that will make it more resilient.
6/11: Team had a session to investigate rollbacks and we identified a problem with conflicting processes. The ticket above is to be done soon  and includes improving the process to  remove  the  issue we saw and  to make rollbacks  a 1 click job. Validating the existing process so we can continue to release whilst this work is done.

*Look into automating build verification testing - meetings to be set up to discuss further.
*Review approach to early detection. Build NR dashboard with all critical information.
         -Using  manual approaches for now but this is  on the backlog to look at  in the future

*Fix forward timelines - In a critical situation how can we build to  production <15mins.
         -Using rollback as the primary strategy for sev1s. Ticket in backlog to look at this in the future

*Investigate why caching seemed to last until the full overnight cache - Completed
23/11: Caching session with David complete and we understand that there was a bad edge control setting that caused 24h cache not 8 as was believed, this is resolved now.
6/11 -Had session with David and ops team to investigate. Identified that FESK apps override edge control max age so the pages were cached for 24 hours. Find time have a ticket in place to apply this to foundation which we will pick up.

*Added manual step to in Deployment guide template to review any spikes in errors in new relic after the release - Done

*Add extra sanity/smoke testing, to be completed by both team members that approve merge of release branch.
*When there is a Sev1, someone from the dev team should always be available on the bridge.
          -Team is aware of these changes -Completed",27/10/2020,27/10/2020,,,,,,,,Customer Tech change,"CM5178 – 08th Oct change
CM5263 – 26th Oct change","  A change was implemented on 8th October when a brand exclusion list was set up on CMWA. However, this did not become apparent until 26th October when another error was fixed, which stopped CMWA from displaying.",,,,30/11/2020,30/11/2020,#VALUE!,,Closed,,,2020,10,October,9/16/2025,1785,>60
21/10/2020,21/10/2020,87864640,,,,,Retail,,,,ADHOC,Users unable to access Customer SharePoint,TCS,GSOC & Firewall,,Sharepoint,RC identified,52349,"At 14:40, service desk advised that store users were unable to access My World Stores URL 
mnscorp.sharepoint.com/sites/MWS within SharePoint.  A few head office users from the Salford Quays office and a few users on Customer laptops and Global Protect who were connected on their home networks were unable to access SharePoint.
Impact: A total of 31 users reported this issue:
• 29 Store users reported that they were unable to access My World Stores within SharePoint. This portal is heavily used by the store users for viewing announcements and other key info.
• 2 Head office users reported that they were unable to access SharePoint– 1 user was from the Salford Quays office the other had connected from home using an Customer laptop.
Recovery actions : On 21st Sep 2020, the Info Security team were made aware of a phishing email being sent to several Customer colleagues advising them to click on the a link. 

The team investigated and found this to be a malicious link that was meant to perform “credential harvesting” and hence advised their vendors NetGraft and Cyjax to this URL down.  This action was taking longer than expected and InfoSec therefore worked with Network support to block the URL in our firewall.  As this URL was hosted in the Microsoft domain, it resulted in all the Customer SharePoint URLs (URLs beginning with mnscorp.sharepoint.com) getting blocked.  The Network team unblocked the stores share-point URL by 15:39 to fix the issue and restore services.
","Next steps:
Being managed by Nikolay himself, Nikolay to co-ordinate with Network team and agree on a solution.

Latest update 13/11: Pending for nikolay availability to schedule call

Update 6/11 : Checkpoint call - No updates received

• Review the current process on how the URL block requests are made (any additional details to be provided along with the URL / IP Address to avoid any other impacts) - GSOC Team
• Loop IT Operations Management on all URL block requests for awareness/visibility - GSOC Team
• Impact assessment to be carried out on the URL block requests before implementing the same on the Palo Alto firewall - NOC team
",21/10/2020,21/10/2020,,,,,,,,Process gap,INC000087864716," 

Action approved by the Infosec team to protect the integrity of the Customer estate from a malicious link, which resulted in all URL’s beginning with mnscorp.sharepoint.com being blocked.

• On 21/09, GSOC (Global Security Operations Centre) team was made aware of phishing emails received by Customer colleagues.
• GSOC team has been working with their vendors (Netcraft & Cyjax) to take down the above URL since.  However, as this was taking longer than expected, GSOC approached Network Support to block this URL in the Palo Alto firewall on 21/10 (Sev2 - INC000087864716)
• GSOC confirmed that blocking suspicious URLs / IP Addresses is a standard BAU process and is usually done via an incident.
• Network Support also confirmed that the malicious URL requested by GSOC was blocked in the Palo Alto firewall and that there were no manual / human errors in this process.  However, the malicious URL that was blocked in the firewall resolved to an IP Address which was the same IP Address as mnscorp.sharepoint.com and hence resulted in all the sharepoint sites being blocked for Customer users.
",,,,,January,1900,,Closed,,,2020,10,October,9/16/2025,1791,>60
18/10/2020,23/10/2020,87855685,07:17,19:00,83:43,,FOODS,,,,MI,Intermittent wireless network connectivity issues on HHT devices in Bradford Foods DC,"TCS
(Zebra : 08390332)",Network,,HHT's,RC identified,52247,"After the completion of the Wi-Fi access point and network controller refresh activity (CRQ000000129262), Bradford Foods DC reported intermittent network connection issues with the HHT’s in the BWS/LLA/International and Ecomm chambers 
Impact:  
BWS, LLA, EComm and International areas in the DC - individual user’s operations were interrupted as the HHT and AP connectivity dropped and this led to an overall reduction in productivity over time
Recovery Actions:
 
18/10:
After initial triage, support made changes to the roaming parameter configuration and the WMS RDT daemons were restarted.  Support then increased the Access point signal strength in the BWS area.
 
A Cisco TAC case was opened and Network support worked with CISCO for further troubleshooting. 
 
Support worked with the site to secure a MAC address of one of the affected devices to enable debugging.  
 
As recommended by Cisco, support enabled WPA2 which is an encryption method. 
 
After enabling the additional encryption (WPA2) on the new wireless controller, the issue for most of the users had seemed to be resolved. However, around 15:30, site started reporting the issue again.  Support advised that the issue was due to the controller not accepting the change, causing the HHT's not to connect at 2.4GHz radio frequency.  Network support worked with Cisco on increasing power level of the 2.4GHz radio frequency.  The Access Points in LLA that were operating below 50% power level had their power increased. The DC advised that the issue was still quite apparent however their picking stats shows a marked improvement.
 
Network support believed that the signal from the Vulcano WiFi SSID from two particular Access Points (APs) in the Foods Office area was extending into the aisle areas in the warehouse and so the theory was that when an HHT moves from near the office (corner of the aisles area) and deeper into the aisles, the HHT gets disconnected as the APs in the aisles and the office area are served by different wireless controllers. The handover from one WLC to the other should be seamless. Support got an approval from DC and shut down 2 APs in Foods office area.  The DC uses two types of HHTs – Symbol 41n0 (new) and Motorola 4090 (old). After shutting down the 2 office APs, it was confirmed that the Symbol variant was not able to establish connectivity with the APs served by the new wireless controller whilst the Motorola HHTs could.  Therefore the two ap’s were turned back on.
 
19/10:
Further investigations continued as to why predominantly the Symbol 41n0 HHT’s could not connect/authenticate to the new wireless controller. Intermittent faults were also reported with the Motorola 4090’s; but, there were far fewer complaints. 
 
80 Motorola 4090’s were sourced and sent to Bradford.  The team replicated the problem with the 41N0 and studied all logs on the hand held and radius servers.   
 
Overall the DC confirmed to our team in the site that they were happy with the performance of the Motorola 4090’s.  We continued to reinforce to the DC users the need to use the 4090’s as far as possible.  
 
Escalations were made within Zebra, the manufacturers of the HHT’s. 
 
In depth fault finding continued and we examined factors like the build version, the date of manufacture of the HHT’s, whether the time setting was correct on the HHT’s etc.  A new “mini” wireless network with a new ID was created with one HHT in Production to help us fault find.
 
The DC reported issues with a small number of the finger scanners brought in from Castle Donington. Support teams managed to fix the scanner issue with the HHTs.
 
20/10:
After business confirmation in morning, at 08:30, we isolated the Vulcano SSID so that it was no longer usable.  It was disabled in the office areas of Bradford which meant it could no longer spread its signal into the warehouse areas.  After this action was taken, the DC reported that around half of their fork lift trucks and low level pickers could not connect with their HHT’s.  The Vulcano SSID was immediately reinstated and the DC confirmed stability again.
 
The incident team remained on the MIM call with vendors Zebra and Cisco to further investigate.
 
During the late morning and afternoon, DC users continued to remain very satisfied with the performance of the 4090 HHT’s.  Users advised that any glitches were occurring at a similar rate to before the cutover.  Investigations found that the issue indicated that the handhelds were not connecting to the new access points. 
 
The DC arranged for the fork lift truck drivers to also use 4090 HHT’s.  Our investigations and testing resulted in our upgrading the Zebra drivers on a test 41n0 HHT’s and this HHT then successfully connected to our test network .  We then took this test HHT on a walk about through LLA and BWS areas and it successfully roamed and switched AP’s during travel as it should.  We upgraded the Zebra driver on a handful of other 41n0’s with the same successful result.
 
Support successfully upgraded the network drivers on the Zebra 41N0 devices and handed them over to the DC. The DC confirmed that they were able to perform the operations using these devices without any issues.  The drivers were upgraded on a few FLT(Fork Lift Truck) devices (VC70N0) and handed over to the DC.
 
21/10:
Our team continued to work in tandem with DC staff to update HHT drivers and carry out operational testing.  All known handsets in the Bradford Foods DC were updated and checked.  XPO leadership confirmed that productivity in the DC had risen by +3% against plan when compared with their peak performance forecast prior to the Network Infrastructure Cutover - in short, XPO operated at 103% against expected performance numbers.
 
A wireless network configuration change was planned and tested which would make the roaming of a HHT within the building a much smoother operation between access points.
 
Support teams tested the clustering fix to prevent HHT connection drops while DC users travel with their HHT's to and from the Ambient and BWS areas. 
 
22/10:
Operations in the Bradford DC continued overnight and during the day with no issues reported.
 
After liaising with the DC, the business and all other stakeholders, the network team implemented a network configuration change that will make the roaming of a HHT within the building a much smoother operation between access points.  This change was completed at 14:00 and our team was on the ground testing alongside our DC colleagues. 
 
Our DC colleagues reported that during the day since the change, there were no disconnects 
The Vulcano SSID (in the office area) on the old controller remains enabled. The team carried out a period of monitoring with the DC for 24 hours and brought the issue to a resolution.
","Next steps:
Last update:1/3:
* Comprehensive test cases to be agreed with the SMEs and test results to be documented for all future changes – Completed - Martin B / Project team

* Comprehensive Testing – In line with recommendations from a AMT Product owner, recommended AMT devices to be tested prior to the WLC upgrade – Completed - Martin B / Project team

* Onsite Resource Planning – Planning for more than one (right) resource during implementation and fresh resources to be available onsite in case of issues after the critical wireless upgrade activities – Completed - Martin B / Project team

* Validate the inventory to understand the number of devices being used along with the different models – RIT / Project team -Completed

* Agree on a process with RIT to maintain Asset inventory – EUC / RIT - Completed

* Ownership of the roadmap / strategy for handhelds including the product ownership – Completed- Paul A / Michael B / Daf

* Gap in support ownership of AMT devices and software – Completed - Ramesh / Paul A

* Document and clarify all reported pre-existing issues (Wifi coverage, AMT roaming disconnects etc) prior to the WLC upgrade – Completed -  Martin B / Project team

* Understand the Zebra support contract; identify and close any gaps  - Sathish - Completed - Customer has contract only for CD. Support model is Mon -Fri (Timing: )

* Identify devices / terminals that are out of support (4090s) and raise appropriate risks – Completed -  Colin W / EUC

* Agree on an approach to upgrade the AMT devices periodically or on a need basis – Colin W / EUC - Completed

* Follow up with Zebra to understand the actual root cause – Completed - PM Team - Zebra Technicalsupport confirmed root cause could not be identified.

* Need advanced tools to diagnose complex wireless connectivity issues – Darren C 
1/3: Long term action. ETA:Q1/Q2 next year.
12/11: We have a piece of work in the next month to look at diagnostics for wireless. I expect it to be on going into Q4.

* Plan to replace Motorola 4090 devices since they are out of support - Closed - Joey: Risk raised:1519 ",18/10/2020,21/10/2020,,,,,,,,Customer Tech change,CRQ000000129262," 
The existing wireless drivers on the Zebra devices (41N0 & VC70N0) were not compatible with the new wireless controller and hence could not establish a successful connection.  The wireless drivers on these devices were upgraded to resolve the issue.",,,1519,09/06/2019,June,2019,,Closed,,,2020,10,October,9/16/2025,1794,>60
18/10/2020,16/10/2020,87854534,,,,Foods,Food Supply Chain,,,,MI,Overstated allocations for Ocado food orders in the Bradford Foods DC,Ocado,FOODS - Ocado project,,Over allocation of FOOD orders,RC identified,52336,"At about 11:04, MIM were notified that the Bradford Foods DC had received an Ocado allocation volume that was higher than the ordered volume.
Impact:
• The DC had stopped the Ocado picking process of the Foods products till 13:40.
• Ocado will be receiving an excess of 7278 cases that they had not ordered.
Resolution Notes : At 11:04, MIM were notified that the Bradford Foods DC had received Ocado allocations that were higher than the ordered volume. Support and DC deallocated the Ocado Foods orders that had been sent to the DC this morning. 
 
There were 11.7k cases comprising of 109k singles that had already been picked by the DC before the recovery had commenced and could not be deallocated. Investigations revealed that out of these 11.7k cases, Ocado had only ordered for 4.5k cases. The remaining 7.2k cases were excess. The incident team corrected the allocation quantities for the de-allocated Ocado UPCs directly in WMS which was completed by 13:40. The DC successfully allocated and clustered these orders by 14:36 and these are currently being picked. Ocado have agreed to accommodate the excess 7.2k cases that had already been picked by the DC.
","Next steps: 

Update 15/4
•	Check with Ocado why the empty file was sent – Completed - Due to network issue at their end empty file was received from Ocado, this issue has now been fixed by Ocado. Going forward they will not be sending the provisional file if it is empty.
•	Agreement to be made with Ocado to communicate to Customer if they send empty provisional order file /do not send the file/any issues at their end in sending file so that Customer can check for further actions – Completed - Sundaram - :
a.	Do not send the empty file to Customer (This is system generated) and Ocado should stop if there system generates empty file- Completed - Update 16/11: We have confirmation now from Ocado that they have deployed the fix and empty files would not reach us.

b.	Ocado has to manually prepare the file and share with Customer via eMail in such cases (This is temporary) - Completed - 16/12: This is now confirmed by Ocado that file be sent manually where there’s an empty file scenario at their end.

•	Back up process to avoid eMail ways of sharing the file so in Q4 we are planning to automate this file sharing. Ocado would manually prepare the file but would send it via SFTP to Customer.– In progress – Sundaram – ETA: Q4 ( February/March 2021)
15/4: This can be closed. There is no manual file transfers now and it is automated from Ocado. 
9/4: Checked with Promoth, he will check with Samantha and come back
15/3:  Had call with Joey & Promoth on 15/3-need to check with Samantha, but she is OOO
•	Review and fix the issues in alerting as it did not trigger during this issue. Need to reproduce the same in NON-PROD and check– Cloud Exponence – Completed – During the day of the incident an incident/alert was already open and hence it did not trigger a new alert. Configuration settings have been modified now in such a away that for each new issue an alert would to be triggered.
•	Review and close gaps in Ocado Service design – In progress – Joey Ocansey
o   SFTP Support Processes and incident management (Order files / Schawk, Business and Brand Bank movements) – Ambiguity around how the IT or business Issues will ‘actually’ be handled
o   OCADO SCL – No process or recovery in place for CFC Sales Order failure scenario (as seen as part of last week’s incident)
•             Investigate the need for ORCA to send the provisional order file along with the final order file to Quantum for order finalization - Completed – Promoth –18/1: We are fine if the OCADO has implemented the fix at their end.
16/12:  discussed with Siva Sakthi on this. Siva stressed to fix at source end (OCADO) and then followed by additional validation within ORCA V2 if required.",16/10/2020,16/10/2020,,,,,,,,3rd party issue,, An empty provisional order file received from Ocado on 15/10 due to an IT issue at their end resulted in Quantum processing the order files with into store date 16/10 causing over-allocations.,,,1498,4/15/2021,April,2021,,Closed,,,2020,10,October,9/16/2025,1794,>60
16/10/2020,13/10/2020,87842363,,,,Foods,Food Supply Chain,,,,MI,Connectivity issue with RDTs at Milton Keynes DC,TCS,IS-Unix,,RDTs,RC identified,52221,"At 01:55, Service Desk highlighted a connectivity issue in the RDTs at Milton Keynes DC. The DC users encountered an error message 'trying to connect to the dispatcher' while connecting to the RDTs.
Impact: The DC users were unable to perform picking and replenishment of orders using the RDTs from 01:15 - 07:26
Recovery actions:
The incident team initially suspected a network issue at the DC. The teams then  performed basic health checks and could not identify issues with the network, database or the WMS application. In an attempt to fix the issue, network support restarted an Access Point and performed checks on the RDT connected to it. Support confirmed connections were getting established at their end however, the DC users continued to experience the issue.
 
Thereafter, the RDT daemons were restarted to no avail. Support raised a vendor ticket with JDA for triage. Further investigations revealed that the password of the Y-account used by the RDTs to connect to WMS had expired at midnight, today (12/10). Support hence configured the password of this Y-account to never expire and the DC users have been able to use the RDTs since. This stability of the RDT connectivity was monitored until COP today and no issues were observed.
","Next steps:
Last update: 15/3: Had call with Joey & Promoth on 15/3, CR was apparoved and scheduled but due to some issues it was reschduled, waiting for new date, hence closing the PR

•         Identify and extend the password expiry date for all the Y and Generic accounts in all the new servers built after Dec 2019 – Completed - Jafar / Linux Team
 
•         Review all the service accounts and extend the password expiry date for Milton Keynes JDA Dispatcher Application Server (hlxp00dc140) – Completed – Jafar / Linux Team
 
•         Identify and extend the password expiry date for all the Y and Generic accounts for all the other Milton Keynes servers – Completed - Jafar / Linux Team
 
•         Monitoring and alerting for service account password expiry – Completed – Sathish / Linux Team - 4/11: Completed for all MK DC servers 
Netcool alert( Sev 3 incident) will be triggerred to the Unix team 7 days prior to their password expiry

•           Compare all the application related settings / parameters between Milton Keynes and Bradford JDA Dispatcher Application/DB servers and share with application team - In progress -
18/2: Waiting for Joey approval
25/1: Will check with Joey for approval. Not yet over. Since thre are too many issus in Foods he wants to delay it if thre is no urgency. 
8/12- Printer stting change will be done post peak
30/11 - printer settings change is still pending for approval
30/11 - Made few change in MK server OS Paramater cchanges similar like other WMS server and Printer settings also changed.
23/11: implement Redhat recommended printer tunings on MK. ETA: Current week
 Sathish / Linux Team -
 
•         Enable package logging on the RDTs to capture error logs even before connecting to the application – Completed - Sakthi / WMS Team - Update 9/11: Colin Wilson confirmed that it is not possible enable logging on the HHTs before connecting to the application.

 
•         Joey to share the process to on-call BSPs and MIMs for informing FCS during any Sev1 / MI impacting Foods DCs – Joey - Completed
 
•         Validate password expiry for Service Accounts for all the future server builds – Completed - Build Team
 
•         Review the process for security policy hardening / exemption of service accounts from password policy – Completed - Elizabeth G - Update from Ravi on 9/11
For any future security hardening on Infra Gold builds, recommendations will be reviewed by Infra team and agreed mutually with Infosec.  Anything that could cause an impact to Operations will go for a discussion with wider group.  Newly implemented policies will be flowed down to Build & Capex project team.  Any deviations/exceptions from this hardening to be dealt by build team with respective business portfolios during demand phase and signed off by Infosec & Infra SDMs.",27/01/2021,12/10/2020,,,,,,,,Process gap,, The DC users at Milton Keynes could not connect to the JDA Dispatcher application on their RDTs as the Y Account password (required for authentication into the application server) expired on 12/10 at 01:00 AM.,,,,15/3/2021,15/3/2021,#VALUE!,,Closed,,,2020,10,October,9/16/2025,1796,>60
15/10/2020,15/10/2020,87853754,,,,,Customer Channels,,,,MI,Customer.com - Drop in orders on Customer.com and holding page deployed,TCS,.COM,,WCS (Customer.com and CFTO websites ),RC identified,52232," Alerting identified that there were issues on the website and from 21:00, no orders were taken on Customer.com and CFTO websites.
Support identified that there were blocking sessions on the database which was preventing orders from being taken. Holding pages were deployed to Customer.com (UK and IE) , CFTO (UK and IE) and Sparks at 21:40 to protect the customer experience.
Impact:
• No orders taken on Customer.com or CFTO for both UK and IE.
• Poor customer experience.
Recovery actions:
All relevant support teams joined a major incident bridge.  Support teams identified contention dead locks on the database and the decision was taken to restart the database. 
 
After the database, restart the dead locks were no longer observed. The holding pages were taken down and support throttled traffic back onto the sites.
 
By 22:10, the traffic was back to 100% on both Customer.com and CFTO and service remained stable.  
","Next steps: 
Update: 27/1/2021:
All pending activity related to this incident is completed successfully on 23-01-2021.

Update: 6/11:
WCS DB upgrade planned in Jan 2021 ( possibly on the 23rd and 24th Jan). Testing of fix will not be carried out now but will be taken care in the upgrade
Update 26/10 From Sri on ETA - As and once we decide to do any such activities in future. 

A.      Future actions for  Support for Table Defragment activity - when undertaken:
 
Parameter fix suggested by Oracle Support to be tested prior to confirm it’s working - Completed
 
B.      Best Practice to minimize impact of any unknown Oracle bug:
 
Execution window   to be reassessed to an off business window when order volume is low.
",15/10/2020,20/10/2020,,,,,,,,Customer Tech change,Yes," RC is attributed to a bug intruduced in Oracle 11G. Dropping table fails with deadlock and waits on enq TM contention due to this foreign key and its due to bug fix of 5909305 in 11g and this changed the mode of PM lock from SS to SX.
",CM5172,,,29/01/2021,January,2021,,Closed,,,2020,10,October,9/16/2025,1797,>60
08/10/2020,12/10/2020,"87838631
",,,,,Group Technology Services,,,,MI,Multiple applications hosted on Kubernetes V1 cluster unavailable ,"TCS
(MS Ref: 120100826004091)","Cloud Exponence
",,"ASO,FMD,SCRD,ORCA,SSI,IBT,CFTO",RC unknown,52217,"From around 15:30, it was observed that the V1 Kubernetes ingress controller pods were restarting themselves impacting multiple cloud based applications.  
Impact:
 
The following applications were impacted between 15:30 and 16:18.
 
Foods/Logistics
• ASO, FMD, SCRD, ORCA user interfaces were inaccessible.
C&H/Logistics: 
• New SSI application was inaccessible 
Retail
• Intelligent Waste and  Assist applications were inaccessible to the store colleagues.
Customer.COM
• Customer.com for UK & Ireland, Mobile Apps & In-Store Assist App was experiencing intermittent errors.
• Customers received ""something went wrong"" Error page while searching for the products using UPCs on CFTO site
• Intermittent errors while searching for stores during CFTO Slot selection
International
• IBT application was inaccessible.
Recovery actions:
From 15:30 reports were coming in from multiple areas as described in the impact statement above to say that services were unavailable.  The cloud exponence team observed that the ingress controller pods on the Kubernetes platform were repeatedly restarting.  To try and mitigate impact caused by the restarting, the number of ingress controller pods was increased from 10 to 20 and this alleviated the impact.
 
From 16:18 applications were available however there were still some pods intermittently restarting although these were minor and didn’t impact the applications. Teams analysed any unhealthy nodes and restarted one node on which high number of pods restarts was observed. The last ingress controller pod restart was observed around 18:00.
 
Services remained stable since 16:18 on 08/10 and no further pod restarts were observed by support. 
","Next steps:

Ingress controller pods restart on both the nodes (VMSS_53 and VMSS_54) on 8th and 15th Oct 
• To eliminate the possibility of issues with underlying nodes, can we remove these nodes from the V1 cluster by moving the application pods to new nodes – TBC

Investigations on applications contributing / causing node issues
• Applications teams to carry out further analysis if at all any specific application(s) might have contributed or caused issues on the node during the time frame which resulted in the containers restarting - Completed - Cloud Exponence to co-ordinate this activity - As per Prometheus metrics the Volume is huge for 
1.	Bloomreach 
2.	Headless commerce
3.	Api digital services
• Spike in the application logging from 04/10 in LogEntries - Identify and narrow down the application(s) reason for spike in logging from 04/10 along with the specific application(s) -Cloud Exponence -  Completed – No abnormal spike observed in application logging

Short Term / Tactical Fixes - Cloud Platform Support
• Ingress Pods to be distributed across multiple nodes - Completed
• Deploy multiple ingress controllers for applications with heavy traffic - Tests to be completed by Oct end - Completed – 
19/1: Bloomreach, Digital API and HLC has been moved to an active/active solution in V1 AKS-E to support the peak traffic along with dedicated nginx controllers.
18/12: Hlc moved to new ingress, digital api services will be moving Monday(21/12), awaiting update from bloomreach
8/12 - have completed the necessary set up in production cluster last week. We have also intimated the same to the dotcom team. Expecting them to raise a CR and migrate their traffic to the new controllers this week. 
30/11: Com are the major stakeholders and we agreed to perform this activity for their portfolio applications hosted in AKS-E.
Platform network changes are getting rolled out today and application changes will be planned later this week.
20/11: Will be deployed in .COM applications this week ( 23-27 )
Cloud Platform Support (ETA 30/10) - 6/11: have done a sanity testing on the set up and is working fine.

As per Prometheus metrics the Volume is huge for 
1.	Bloomreach 
2.	Headless commerce
3.	Api digital services

We have sent mail to Bloomreach with steps to migrate to new ngixn controller. Will collaborate further to implement it in dev and perf. 
5/11: change was implemented yesterday by the firewall team in DEV and PERF V1 Region.  I have deployed the nginx instances in Perf and Dev Region. Next we have to identify applications whose traffic can be routed though the new controllers. I will share a report based on Traffic volume today. We can identify applications and test them in dev and perf
• Reduce the size of nodes in the cluster (from 32 cores/CPUs to 16 cores) - NA
• Check with Microsoft on alerting and monitoring on the Disk I/O, more application logs being written to LogEntries etc - Completed - Cloud Exponence - MS Ticket is closed. MS Update : I tried to do multiple researches POC’s to have a view to the IO metrics of each instances in the scale set but unfortunately, we don’t have this as of now
So we need to depend on MS to check what going on in the background on scale set .
",08/10/2020,15/10/2020,,,,,,,,RC Unknown,," 
The root cause of our incident impacting applications on the Kubernetes V1 cluster (both on 08/10 and 15/10) remains inconclusive.   Microsoft have confirmed they cannot see any issues on the underlying nodes on the Azure Platform.  However, they have pointed out that the underlying VMs are using the Standard HDD managed disks which are best suited for test / development environments and hence recommended to upgrade to Premium SSD disks.  Cloud Platform Support confirmed that the upgrade is complex and involves lot of efforts and hence are working with the relevant application teams to move their applications from V1 to V2 cluster.

The incident team suspects that the issue was caused by a high disk I/O on the underlying nodes of the Kubernetes cluster. A full root cause investigation is being carried out with the help of Microsoft.",,,,20/1/2021,20/1/2021,#VALUE!,,Closed,,,2020,10,October,9/16/2025,1804,>60
08/10/2020,08/10/2020,87838381,,,,,Platform & Store Ops,,,,SI,Multiple stores unable to perform targeted counts ,TCS,Retail,,CSSM,RC unknown,52201,"At around 14:21, multiple stores colleagues reported that they were unable to perform targeted counts in their Honeywell devices.
Impact: Multiple stores were unable to perform targeted counts between 14:21 and 15:18.
Recovery Actions:
Support teams observed that the job responsible for performing the targeted counts was invoking a rule that was causing the job to over-run.  The incident team initially suspected that the job was over-running as it was invoking a new rule (UPT mismatch rule) that was introduced yesterday and disabled it. However, the job continued to over-run.
 
Further investigations revealed that the job was invoking an old rule (DEF rule) which had been working fine for many years. Support teams disabled this rule and re-ran the job. The job completed successfully by 15:18.  Stores colleagues confirmed service restoration. The incident team have agreed to let the UPT mismatch rule and the DEF rule remain disabled until the RC of the issue has been identified.
","Next actions:
Update 16/10
Unable to trace the RCA, in future if the same issue reoccures, then the support needs to enable to log trace and capture the logs to analyse further.
Update 16/10:SQL team to raise a case with Microsoft with non-prod logs to understand why the select statement which intended to run as ‘loop join ’, running now as ‘Hash join’ (execution plan change)?  
13/10 Update: SQL team investigating on why the execution plan changed

• If the fix (force to use innerloopjoin instead of SQL engine to decide to use hash/loop join) was implemented yesterday and if the DEF rule was enabled? - Yes, we have deployed the fix and enabled the DEF rule.
• Share the stats around the runtime of the problematic job before and after implementing the fix - Yesterday the job took 3 min and 12 seconds which is expected. Before fix(without DEF rule on 13th Oct), this job took 2 min and 52 sec.
• Tentative ETA on when the new rules will be re-enabled? - We will enable new rule(UPT mismatch) by Monday(19th Oct).

*Identified the fix for this issue and planning to implement the fix by Wednesday(14th Oct).
This was functionally and performance tested in non-prod environment. All looks fine. Also we have verified the change in production(just select statement) and its completing the step by 21 sec. 
CR : CRQ000000129745
Fix : Force to use inner loop join instead of SQL engine to decide to use hash/loop join.

* Will enable only DEF rule(rule from 2018) tomorrow. Once fix is working fine, we will enable new rules.

Steps Taken to find the Root Cause:

1. Support have validated was there any changes made in the system – No Changes Made
2. Any patches / updates released by Microsoft – No Release / Changes
3. It works perfect in the test environment, when tested the SQL engine can decide to use hash/innerloop join. 

Next Steps: 

1. Support have implemented the fix -  force to use innerloopjoin instead of SQL engine to decide to use hash/loop join.
2. In future if the same error occurs, support team to enable the log trace and do the investigation. 
",08/10/2020,15/10/2020,,,,,,,,Infrastructure issue / Hardware failure,," The stored procedure used by the targeted counts functionality had employed an inefficient execution plan. A detailed RCA is underway.
We could see that one of the select statement in target count creation Store procedure started to executed with different execution plan than the expected one . It seems that the select statement which intended to run as ‘loop join ’, running now as ‘Hash join’ which affecting the performance. We don’t have concrete RCA on why this query changed its execution plan and CSSM product team working on the fix either to clear the current execution plan or to force fully run the query as ‘Loop join’ by enabling the ‘Inner loop join’ in the existing query. Product team can come up with the workaround by next week Monday or Tuesday , Until then we are proceeding with current plan of eliminating the DEF parameter from this job. 
It has been concluded that the root cause is unknown since we do not have log traces during the time of the issue.",,,,26/10/2020,26/10/2020,#VALUE!,,Closed,,,2020,10,October,9/16/2025,1804,>60
08/10/2020,,,,,,,Customer Channels,,,,ADHOC,Contact Centre - Increase in silent calls after weekly reboot of Citrix Servers,Anana,.COM,,Genesys IWS,RC identified,52319,"Issue: 
• Contact Centres reporting increase in silent calls after the weekly citrix servers reboot on Monday after business hours.

Background:
• The issue with silent calls seems to have started by the end of Feb / start of March after the Azure NetScaler Gateway was set up for Capita Users to be able to connect from home and access the applications due to the Covid situation.
• Contact Centres are reporting an increase in the number of silent calls (approximately 6% - 7%, whilst the BAU number of silent calls are 3%) every Tuesday after the reboot of citrix servers carried out on Monday around 23:30.  The increase in silent calls are observed only after the Citrix servers reboot and this has been proven by moving the citrix servers reboot from Sunday to Monday.
• The Citrix servers are rebooted on a weekly basis to avoid performance / caching issues.
","Chris McGrath will be owing this going forward, no assistance required from our end
Checkpoint call 13/11
Update 6/11:
Based on Anana's triage and discovery we managed to replicate the issue and collect packet capture from a failed session where user was experiencing silent call. These logs have been analyzed internally and we error message in the DTLS chain that is corresponding with the time when silent call issue occurred. Anana will forward their findings to Citrix and ask them to help is investigate and find the root cause. This finding suggests it isn't related to neither WDE or IWS and this is consistent with what we have seen on remote sessions with users
•  Its been agreed that Anana will continue working with Capita SA users that are having issues and update the document ""Triage data post WDE rollout updated 20201102"" with their findings in regards to the issue they are experiencing. What has been observed over the last 3 weeks is that the audio issues(silent calls/call quality/session freezing) seem to be following the users and isn't experienced randomly by everyone. This is suggesting the issue is related to either the equipment or connectivity that these agents are using.
•  On the back of the last weeks findings, Gcobani, Matthew and Marlon will arrange 2 new, re-imaged laptops to be tested by agents in the Capita office in SA. If no issues are experienced in the office then these laptops will be sent to the users that can't attend the office but are know to have silent call issues happening on daily basis( Zulpha Isaacs, Siphokazi Jolumpu , Sandiswa Mdiza). Once they receive new laptops they will use them on their daily shifts and report if they are having any issues still. If the problems resolve it would suggest something on their laptops was causing this issue to happen. If the issue is still present then it will suggest that there is networking issue that is causing these particular users to have problems.
•  It has been Agreed that Lukasz Kowalczyk will help us with finding out if there are any Capita UK users that are having the same issue(regardless if it it IWS or WDE) and will keep track of these users so that we can categorise their issues and help them if necessary. We haven't had any reports with silent call issues from UK workers just yet. I'll provide the template Excel tracker with current SA users as an example. Please use it for tracking UK user issues and collect the details about the version of Citrix Receiver/Workspace used.
•  Anana is going to re-group in one week time to re-evaluate the progress.

Update from Chris 5/11:

We have had it confirmed from CSSC Ops (Steve) that the WDE upgrade has not made a noticeable / material impact in reducing the volume of silent calls, and, in some cases, has created silent calls and the users had to revert back to IWS.

Given this is way off our hypothesis, can we put the spot light back on this issue to reset and plan out our next corrective actions?

We also need to understand this in the context that it was a very good week for Silent Calls – including a substantial reduction on Tuesday.

Update: 28th Oct’2020.
As a part of fix, Anana is rolling out the new agent interface WDE which contains a new SIP endpoint. Already Anana have tested this solution with around 20 agents and no issues observed so far. The same will be rolled-out before the end of next week.


• High level details of investigations / actions carried out on different components (Azure Network / Citrix / Hardware etc) so far - Anana - Completed - Lukasz Kowalczyk have sent the test result file carried out earlier and on 13th Oct’2020, the same is attached in this email
• Work with the contact centre agents to enable live packet tracing on the local clients and perform further analysis - Anana - Not required -
Update 21/10:Since RCA is identified and no further actions required for the pending actions  No Valuable diagnosis observed during the test performed on 13th Oct’2020. Another round of tests is planned on 20th Oct’2020 
• Understand the actions performed by the Contact Centre agents to fix the silent call issues on a Tuesday - Capita - Not required - 
Update 21/10: Since RCA is identified and no further actions required for the pending actions
Restarting the Citrix server in the users machine resolved the issue. For some users, restarting once fixed this issue, and for few users restarting more than once resolved the issue. In order to observe more, the users are advised to make a note of the host of the Citrix they are connecting through before restarting the Citrix.
* Marlon to send the list of agents affected with Silent Calls yesterday - Marlon -  In progress

* Lukasz to send the status of WDE roll-out.  - Lukasz - InProgress

* High level details of investigations / actions carried out on different components (Azure Network / Citrix / Hardware etc) so far - Anana - Completed - Lukasz Kowalczyk have sent the test result file carried out earlier and on 13th Oct’2020, the same is attached in this email

* Work with the contact centre agents to enable live packet tracing on the local clients and perform further analysis  - Anana - Not Required - Since RCA is identified and no further actions required for the pending actions
* Understand the actions performed by the Contact Centre agents to fix the silent call issues on a Tuesday  - Capita - Not Required - Since RCA is identified and no further actions required for the pending actions

• To clear all the Citrix active session from backend on Thursday overnight - Completed
• Check the feasibility and impact analysis of performing the citrix server reboots on a fortnightly basis - Anana / Capita  - Not required
• Check if the Colleague Services (HRSS) and RCS Chester contact centre users also impacted by the silent calls issue - Jaye/Steve - Completed - Although Chester users experience silent calls the same pattern following a reboot doesn’t occur for Chester users. There are other factors also due to the nature of the calls handled in Chester that creates a much ‘spikier’ picture as you can see.
• Calls from the RCS Chester Contact Centre go through the On-Prem Netscalers and the other CCs go through the Azure Netscalers - Completed - Jaye
",21/10/2020,21/10/2020,,,,,,,,,," 
1. The first is Citrix Receiver passing through audio devices as “Headsets” even when they are speaker devices or in-built audio chips (Realtek). This exacerbates the second bug.
2. The second is SIP Endpoint choosing a random headset as opposed to the current default device, which means that on occasion the wrong device is chosen (and as above this could be a disabled device).",,,,,January,1900,,Closed,,,2020,10,October,9/16/2025,1804,>60
06/10/2020,06/10/2020,87834671,,,,,Customer Channels,,,,SI,Customers getting checkout error on international flagship sites ,Global E,.COM,,International flagship sites /.COM,RC identified,52058,"Customers on international flagship sites were getting an error page when attempting to checkout.

The sites were fully browsable during the incident and only customers checking out during this time were impacted and would have seen an error page.
Impact: 
• No orders placed on Flagship sites from 07:50 – 08:03 and from 08:30 – 09:26.
• Poor customer experience.
• Estimated 170 orders would have been placed during this time based on previous days.
• Payment confirmation messages not received from 08:03 – 08:30 but these were replayed successfully.

Recovery Actions: 
Global-E were engaged and advised that they were seeing issues with their database. Checkout was unavailable from 07:50 to 08:03. Since 08:03 checkout was up, however couldn’t see order payment messages passed from Global-E to Demandware. At 08:30 reoccurrence of the checkout error was observed and it has remained down since. Global-E advised they are still working on the database issue on their side. Global-E informed that the checkout was back up from 09:26. Support validated and confirmed that the checkout was back up and orders were flowing. Any missed payment messages from 08:03 to 08:30 were replayed and orders re-processed.

","Next actions:

Update from Global E
Our tech teams are in the process of collecting the full technical information from the incident in order to pin-point the exact root cause, conduct a lesson learned investigation, and apply the necessary changes in production deployment processes and controls in order to prevent any such future incident.",06/10/2020,09/10/2020,,,,,,,,Human error - Customer Tech,,"  Human error during a planned deployment on the Global-e database caused high CPU utilization resulting in latency during checkouts.

Global-E advised that they had applied several fixes and upgrades to their production systems, one of which required substantial database changes by means of a dedicated pre-production script which caused the issue.
Unfortunately, during the deployment of pre-production scripts to the production environment, due to a human error, the DBA in charge did not notice that one of the major DB tables was locked and executed the script anyhow instead of aborting the process. As a result, our main DB server’s CPU raised to 100% causing latency on checkouts and the Global-e Admin application. Eventually the server exhausted its resources and could not server checkouts.",,,,,January,1900,,Closed,,,2020,10,October,9/16/2025,1806,>60
04/10/2020,05/10/2020,87832921,,,,,C&H & Intl Supply Chain,,,,MI,WCS unavailable in Castle Donington ,"SSI
(ITS 456384/MAS-15811)",C&H,,CD WCS,RC unknown,52060,"Around 09:00,  Castle Donington DC observed intermittent error message in the SCS (SCHAEFER Carousel System) in relation to the storage totes. An error message was displayed on the PTT ( Picking totes) station screens preventing colleagues from picking. SSI vendor was engaged for further investigations. 
Impact: 
• Unable to pick and pack resulting in the loss of capability between 11:15 and 14:05.
• Estimated impact – 25k Pick Capacity and 14k Pack Capacity loss. 
• 52 orders missed promise.
• CFR proposition was kept out by 24 hours after previous high sales.
Recovery Actions: 
Upon investigations vendors SSI identified that the PLC (controller) was not communicating correctly. In order to restore the services, SSI restarted two critical boxed services. However, one of the services (MFS Boxed) did not come back up successfully. Investigations revealed that the thread count for processing the tasks within the automation had breached resulting in WCS being unavailable from 11:15. SSI vendor advised that the thread count threshold needed to be increased followed by a restart of the messaging services with a full shutdown of the automation. CD IT was requested to liaise with operations to log all users out of the WCS system to allow the restart the messaging services. SSI restarted the services and they came up successfully at 13:45. After this PLC connections were brought back up and the site operation was resumed by 14:05. No errors were observed on the SCS solution after the restart.  SSI team will monitor the thread count manually every hour.
","Next Actions:
Latest update: 24/2: 

Next steps mentioned in PIR:

a) An improvement Jira (MAS-15812) has been raised to “add alerting to monitor the number of threads used by WAMAS services”. As discussed, this might not be feasible or be very difficult to implement but should be investigated - Completed - Alerting in place, email alerts will be received by RMIT team

b) SSI to look at how WAMAS can handle JMS error messages better. If the handling can be improved and threads handled appropriately then it could eliminate a re-occurrence. However as the ‘trigger point’ cannot be determined (due to it being mechanical or manual intervention) and the limitations of the current test environments, it might not be possible to test or reproduce the error to confirm if such a fix would work. MAS-15811 - Defect priority 3, No ETAs -Closed 24/2: Will be managed by SSI IT 

Update from James on 7/10:
A ticket has not been raised with Oracle and it seems to be an ACX issue. From the investigation so far it seems that mfsBoxedOutboundService isn’t handling errors as expected. Developers have escalated to our internal ACX support team.
 
I have raised an improvement Jira (ref: MAS-15812) to investigate what alerting can be put in place to monitor the number of threads WAMAS services are using. @Coy, Martin/@Haworth, Colin: I have gave Toyin a heads up regarding this and that it will need to be prioritised. It will also be worth picking up from your side and asking for resources to be allocated to it.",05/10/2020,09/10/2020,,,,,,,,RC Unknown,," SSI IT suspects that the issue could have caused by a PLC restart done by the engineering team in the early hours of the morning to fix another issue.

The number of max threads allowed for messagingService reached its limit of 3000. This meant no new threads could be created by other connections. Investigation continues as to what caused the thread count to increase. So far, the evidence suggests that the earlier PLC 15 issue has somehow caused the JMS queue to fill up. (This information is correct as of 11:30 09/10/2020 but could change as the investigation into what caused the thread count to increase continues).  
There was a Sev 2 raised 03/10/2020 (INC000087832196) against C15. After investigation from PLC support it appears that the FIFO buffer was full, and the PLC had to be restarted in order to clear it. When the buffer becomes full, the PLC can send telegrams but it cannot receive any. When looking through the faults on VISU there is an MFS communication error reported at 22:09. At the same time, mfsBoxedOutboundService begins to throw errors regarding its messages for C15_RCB_IN, they cannot be sent and are being rejected. 
As the PLC buffer was full, the PLC was unable to process any telegrams that had been sent by MFS, meaning MFS cannot connect/send anything. MFS will then create a new thread (rather than close the old one) to resend the telegram. This would mean that the threads accumulate overtime and messagingService exceeds its max.
The PLC buffers can become full when there is a surge in messages (caused by a mechanical issue(s)) or a communication issue. As PLC’s do not store any history or log files, it is not possible for SSI to determine what caused the buffers to become full. Further investigation is required by engineering/Ops/TCS to determine root cause. 
",,,,2/24/2021,February,2021,,Closed,,,2020,10,October,9/16/2025,1808,>60
01/10/2020,01/10/2020,87828569,,,,,Group Technology Services,,,,ADHOC,Intermittent issues while using Outlook,"Microsoft
(22147742 & EX223208 / MO223247)",IS-Messaging,,Outlook,RC identified,52070,"Around 06:30, MIM identified an issue while accessing the Outlook. 
Impact: Users onsite and offshore experiencing intermittent issues with accessing and sending/receiving mails Outlook via web, desktop and mobile.
Current Update: 
 
• Services continue to remain stable. The incident has been set to resolved
Previous Update:
 
• Further analysis from Microsoft has determined that the issue is being caused by a recent configuration update to components that route user requests. They have reverted the update and telemetry shows that the service is now recovering.
• Keeping under monitoring.
","Next Actions:
Received PIR report from MS.",01/10/2020,01/10/2020,,,,,,,,3rd party issue,," Microsoft released an update to their front-end components to optimize client requests that had a specific parameters included in their headers in the application pools that were leveraged by Outlook on the web, REST and mail delivery protocols. This update was working as expected for several weeks, however, it contained an underlying code bug specific to date/time parameters, which became exposed on October 1, 2020. The bug only impacted the code path where the parameters in the local cache failed and the front-end components that serviced the request were forced to look up Active Directory (AD) to determine the mailbox location. Outlook on the web, REST and mail delivery requests that were going through this problematic code path immediately started failing. As a side effect of this bug, several front-end components were also randomly left in an unresponsive state, resulting in extended residual impact.",,,,,January,1900,,Closed,,,2020,10,October,9/16/2025,1811,>60
19/09/2020,17/09/2020,87814361,,,,,Platform & Store Ops,,,,SI,Issue with COVID track & trace on Stores Honeywell,Customer,Sharepoint Dev,,Covid Track & Trace,RC identified,51963,"Background:
At 11:46, MIM was made aware of multiple stores reporting that they were encountering the error “The maximum number of people has already responded to the form“ while attempting to submit forms by scanning the track & trace QR on the honeywells.
Impact:
Stores Colleagues were unable to track the customers visiting the stores using Track & trace application between 11:15 to 12:31 
Recovery Actions: 
The SharePoint Dev team were engaged to investigate and they found that the track and trace form had reached its maximum limit of 50K. The team then cleared the old data from the relevant SharePoint repository after which stores confirmed complete service restoration. An IVR was placed on the Service Desk lines between 12:05 and 12:52.  
","Next actions:
Confirmed in Catch up call on 28/10 to close this as actions are completed
Update 21/10: Waiting for confirmation from Rosie
• Can the NHS Test and Trace app be used instead of our Track and Trace application? - Completed - (NHS Test & Trace along with Customer developed Track & Trace) are currently in play for the customers visiting our stores.
o Duane to speak to Charlotte and advise on the next steps
• Service Wrap and Support Model - Completed
o The Track and Trace app will be continued to be supported by the Sharepoint Dev team and the workaround of clearing the customer data on a daily basis at 22:00 is being carried out to avoid a re-occurrence of the issue.  Escalation matrix for the Sharepoint Dev team has been shared with Service Desk and MIM teams.
o Currently the Track and Trace application is supported by the SharePoint Dev Team, application support not handed over to Sharepoint support as yet.
o Duane to work with Suresh (Corp SDM) and agree on the support model for this application.
• Service Introduction and CAB process - Completed
o We also agreed that any new applications being deployed due to this Covid Situation will go through the CAB & Service Introduction process, however, in the instances where this cannot be done due to time limit / short notice etc, you will be updating Rosie (Retail BSP) on your catch up calls, so the information can be passed on to the Operations Team.
o Track and Trace application has not gone through the Service Introduction process.
o CAB process not being followed for any changes / deployments.
o Rosie (BSP) to have fortnightly catch up calls with Duane and provide Operations team on any significant updates / changes happening in this space due to the current Covid Situation.
",19/09/2020,19/09/2020,,,,,,,,Capacity constraint ,,"The COVID track and trace share-point list used to log and maintain customer information had reached its limit of 50K due to a sudden increase in tracking form submissions from stores.
• The sudden increase in the tracking form submissions has been attributed to a new Covid law that came into effect from Friday, 18/09.
• According to this new legal requirement all pubs, cafes and restaurants, as well as other hospitality businesses and close contact venues are required to log the details of customers, visitors and staff.  
• Businesses will be required to keep this information for 21 days and provide details to the NHS Test and Trace scheme when asked to do so. Customers not providing the details will be refused entry.

",,,,28/10/2020,28/10/2020,#VALUE!,,Closed,,,2020,9,September,9/16/2025,1823,>60
14/09/2020,17/09/2020,87806898,,,,,C&H & Intl Supply Chain,,,,MI,Issue in WCS messaging flow at Castle Donington ,SSI,C&H,,CD WCS,RC identified,51851,"
At 00:53 on 14/09, MIM was notified of an issue with the WCS message flow at Castle Donington.
Impact:  
14th Sep:
Potential Capability Loss during the outage times from 00:00 to 06:00
Failed customer orders was 4,043 orders.
 
16th  Sep:
Potential Capability Loss during the outage times from 03:05 to 14:10
Proposition was pushed out 24 hours on 16/09.
Failed customer orders was 1,023 orders.
In addition, a small number of WCS reports were impacted affecting stock and availability reports.  However, the site could still operate without these as the majority of reports ran on WMS.

Recovery Actions: An alert indicated message pileup between WMS and WCS. SSI was engaged who had identified hardware failure with the primary DWH1 database server resulting in the message pileup. A decision was made to failover the services to the secondary server DWH2.
 
At 01:38, the DC operations were stopped. Escalations were made to engage SSI 2nd line support. SSI 2nd line support confirmed that there was an issue while establishing a connection between WCS services and secondary server. A configuration change in the connection was performed followed by WCS service restart to fix the issue.
 
The incident team identified a large number of pending events which did not process as expected. Restart of WCS and DWH services were performed to clear the pending events. The DC was advised to start their operations. A script was run by CD IT team to update the configuration change on the desktops before users log into WCS.
 
From 08:30 all the desktops were working from DWH2. The team remained on a MIM bridge rebooting the server DWH1. It successfully returned to the log-in screen. The team then carried out analysis on the server without connecting it to the network. Support teams could find no further logs that could guide us to the cause of the server crash.
 
SSI devised a plan for recovering DWH1 server which involved rebuilding the server DHW1 on the morning of 15/09 using the backup of the server DWH2 which is scheduled to run overnight.
 
Overnight (14/09): On further investigation, it was found that there is a difference of seven years between DWH1’s firmware and the latest version. Upgrading the firmware in DWH1 and jumping it forward seven years is an unknown risk. The bios upgrade planned was therefore postponed. 
 
15/09: SSI started rebuilding the server DWH1 from 08:45 with the aim to get the service packs including the firmware ready to install after the rebuild.  HP recommended we upgrade to the latest version which is 2.75  however most organisations have a policy to apply one version previous to the latest.  A decision was therefore made to upgrade the firmware to version 2.73. The rebuild process was complete by 18:00. The firmware upgrade to  version 2.73 followed by a BIOS upgrade to version (v20180521) got completed by 22:59 successfully.  
 
16/09: Around 4:00 16/09, MIM were notified that WCS has gone down again at 3:05. In an attempt to fix the issue, services were stopped and tried to re-start the service but encountered an error ‘Destination is full’.  SSI 2nd line identified that disk space got filled up on DWH2 server. Disk space was cleared and the services were restarted to fix the issue. WCS services are back online at 5 AM. The failback activity started at 06:08. By 08:45, all WCS services were up on DWH1 however it was too slow to log in to WCS and support were unable to log into DWH1.  
 
Further investigations confirmed that when DHW2 was failed over to DWH1 this morning, the Data-Guard facility had added a default route in the routing table of DWH1 which was directly conflicting with the normal production route, preventing support teams from logging on to WCS and DWH after the failover.  Once this was removed at 14:10, normal connectivity got re-established to both WCS and DWH1. Castle Donington Operations are now running on DWH1. Hourly hyper-care is in place.  A decision will be made on enabling Golden Gate replication at a later point in time after monitoring the services for 24 hrs.
 
17/09: No further issues were been reported/observed so far. Services continue to remain stable on DWH1.
","Next actions:
5/3: Colin has confirmed over call to close this since all the pending actions are long term which will be managed by team themseleves
Update 15/12:
Operational
*Review alerting around hardware errors on DWH servers – James B / SSI Support- Closed -
5/3: Colin confirmed to close PR as team will mange it themselves since it is a long term action
15/12: Plan to be reviewed by Colin
14/10: SSI support to work with TCS Messaging support to set up email alerting on the ILO event logs
ILO alerting cannot be put in place, as there is no mail server in place to get email alerts.  Check with Messaging Operations support if any of our mail servers can be utilized for alerting.  However, SSI have a script is in place to ping both DWH1 and DWH2 every minute, email alerts will be received by RMIT Support incase of any issues with the servers.  WCS alerting also in place.

*Validate the SOP for failover from DWH1 to DWH2 and failback; also automate if possible – James B / SSI Support-Completed-SOP review has been completed.  Automated failover not recommended for a number of reasons (Momentary network blips will initiate a failover scenario which could cause more issues;  cost implications around Data Guard replication.  Manual failover steps documented in SOP.

*Create an SOP to run the scripts on client machines to update the config changes to establish connectivity to WCS – Colin H-Completed-SOP has been updated, need confirmation that all the analysts are comfortable with the process and have tested on one or two workstations
06/10 - Analysts in different shifts have been trained to implement the script on workstations

*Review and implement system monitoring and alerting along with BIOS and Firmware upgrade on DWH2 – Katy M-In progress- 
15/12: Will be included in CIP
13/10 - Cost involved to enable monitoring on DWH2 - Will be co-ordinated as part of CIP. Colin to raise a risk and share Risk ID
System monitoring being explored by the SSI Systems Monitoring team.
06/10 - In Progress, no ETA as yet

*Chase Oracle to see why Data-Guard sets default route when configuring the primary server – Katy M - Completed- Oracle advised It might be nothing to do with Data Guard but to check on the OS Layer (default route might have been introduced by the start up script during OS reboot).  SSI SysDBA to validate the OS scripts/Network Config file on both DWH1 and DWH2 and perform testing in the lower environment - No ETA
*Re-evaluate all the current open/accepted risks  Martin / Colin-Completed 

*Enable Golden Gate Replication between DWH1 and CD MI Reporting Server - Katy M / James B / SSI Support - Completed- 
13/10 - Golden Gate Replication enabled on 08/10, no further issues
06/10 - Data Replication service was started but the data is not being replicated from DWH1 to MI Reporting server.  A Sev-2 ticket raised with Oracle on 29th Sep and all the logs are being sent across.

Tactical
*Update the local config file on the WCS App server to use IP Address instead of Service names – James B / SSI Support - Completed-SOP has been updated to include the service name / machine name along with the IP Address.  Failing over the services from DWH1 to DWH2 will not have any problems in future


Collaboration
*Agree the involvement of technology operations for Business Continuity text – Michael / Colin-In progress-  CIP
1/12: Ongoing element of the Castel Donington Continuous Improvement Programme (CIP) -Have agreed with site that Castle Donington IT On Call Representative will validate any BC text before being sent out.
Colin / Martin working with the site incident controllers.  Site incident controllers need to lialise with Colin / CD IT Support passing on the information to BC team
*Establish leadership connects between SSI and TCS teams - Michael / Ramesh-Completed-
13/10:Escalation process discussed, Katy to document and share the escalation matrix
Peer to Peer mapping complete. Escalation document has been shared to Martin and Richard. Will be shared with Ramesh / Michael – 13/10/2020
Escalation process discussed, Katy to document and share the escalation matrix 

Strategical

*Understand and remove the dependency between WCS and DWH – Michael / Martin / Katy-In progress-
15/12: Taken as CIP
13/10: Solution proposed, will be taken as a project - Long Term Action with No ETA at the moment
Discussions currently ongoing, awaiting further updates. 

All the below action items will be covered under the CIP (Continuous Improvement Plan) process - TBC
*BIOS and Firmware upgrade on DWH2- James B / SSI Support
*Review and validate other servers in the estate that would need BIOS and firmware upgrade – Colin H & Katy M
*Review the support ownership and responsibilities around Hardware, OS & Application – Michael / Ramesh
*Review the support wrap and reassess the SSI infra support capability  – Katy / Colin / Michael
*Review and agree patching process on DWH and other servers as applicable – Colin / Katy
*Plan and execute an appropriate Disaster Recovery test for WCS and WMS - Michael / Martin / Colin",14/09/2020,21/09/2020,,,,,,,,Infrastructure issue / Hardware failure,,"
Issue 1 - Why did the primary DWH database server (DWH1) stop working?
 The primary DWH server reported a critical BIOS Hardware Health Failure and stopped responding.  HP advised that the crash of DWH1 may have been caused by the fact that the firmware on DWH1 was from 2013.  HP also advised that they will not be able to find the actual cause as the firmware from 2013 would not have that capability to capture the required error logs.

Wednesday, 16/09
Issue 2 - Why did the disk space fill up on secondary database server (DWH2)?
 WCS was unavailable from 03:05 on 16/09 as the disk space had filled up on DWH2. Further investigations revealed that the redo logs from the initial failover and sync had not been removed, these were outside of the normal clean up services and when coupled with the production data lead to the HDD becoming full.

Issue 3 - Why did we have issues during the failback from DWH2 to DWH1?
 The further delay to operations after the failback to DWH1 from DWH2 was caused by a default route being added to DWH1 by the Data-Guard application which directly conflicted with the production route. A ticket has been raised with Oracle to see why Data-Guard sets default route when configuring the primary server.


",,,"589, 681, 1496, 1497",,January,1900,Dimple,Closed,,,2020,9,September,9/16/2025,1828,>60
11/09/2020,18/09/2020,87802197,,,,,Foods Commercial Trading,,,,MI,Intermittent latency in accessing Foods JDA open access user interface ,TCS,Foods,,JDA,RC identified,51849,"
At 15:11 on 11/09, support notified MIM about multiple stores experiencing slowness while accessing the Foods JDA open access application.
Impact: 
Stores and business users faced latency issue while loading the planograms and floor plan functionalities in Foods JDA user interface since 11/09.
Recovery actions :
 Initial investigations revealed an issue with the I/O throughput which was impacting the response time to load the data. Support increased the number of storage disks allocated to JDA in an effort to decrease the storage disk utilization. However, there was no improvement.  Support allocated two new storage ports dedicated to the Foods JDA application and stores confirmed that the performance had improved but with intermittent slowness.
 
On Tuesday 15/09 at 14:00, support advised that 17 stores were experiencing slowness since 11:00.  Further investigations revealed that an increase in latency was observed due an inefficient execution plan. Support then pinned the a more efficient execution plan after which response time improved.  In order to fix the performance issue, support added 2 dedicated ports to the JDA application. On 16/09, at around 10:25, the issue re-occurred. Support then performed some fine tuning on a storage and VMware level and the performance did improve but was still not back to the BAU timescales. Support then deployed an Oracle recommended baseline planning profile and the latency issue was resolved.
 
The affected users were able to perform operations in JDA Open Access at a normal response rate and user comms were rolled out to this effect. The incident was monitored for stability until close of business 17/09 and no issues were observed.
","Next steps:
Update: 2/10
Joey confirmed to close the PR

Update 29/9
Confirmation received from Gunjan that performace is good.
Update 29/9
• Memory Uplift on the Foods JDA DB server - Completed - Boopathi / Oracle DB Support-The average memory utilization is under threshold limit ( 60 - 65 %).
We got approval from capacity team to uplift 10 gb of memory in all the 3 nodes-29th Sep morning 3 AM to 5 AM - CRQ000000128977
ERF creation in progress, once it's done, will get the outage and uplift the memory.
Will keep you posted.
hlxp00or003  - Primary instance
hlxp00or004  - Secondary In instance
hlxp08or003  - DR Database instance

• Implement fine tuning on the Storage and VMware layer on the secondary ESXi host - Completed - Dharma / Sailesh-The systemic adjustments and fine tuning done at the storage and VMware layer to improve the I/O throughput and database performance were done on the primary ESXi host (hosting the active database server), the same settings should be applied to the secondary ESXi host - ETA 19/09 (involves no application downtime)
",11/09/2020,17/09/2020,,,,,,,,Customer Tech change,120113," A recent change in the infrastructure platform used by Foods JDA Open Access, resulted in the requirement for some systemic adjustments to allow it to handle increased usage of the application during peak periods such as phase change.",,,,,January,1900,,Closed,,,2020,9,September,9/16/2025,1831,>60
09/09/2020,22/09/2020,87811919,,,,,Customer Engagement ,,,,MI,Customer bank loyalty points being incorrectly allocated to customers,TCS,Retail,,LPM (Loyalty Point Management / LPA (Loyalty Point Administration),RC identified,51873,"Background:
It was found that 4 promotional codes, each intended to be only used a single time to allocate 300 or 500 points to a customer’s balance was not working as expected.  Instead, each customer, who was using one of the 4 promotion codes was receiving multiple allocations of 300/500 points as opposed to the intended single allocation

Impact:
Customers impacted would have received more points than intended. At the final confirmation 498,000 customers were found to be affected.  At present there is no actual allocation of funds to any of these customers, rather, only points having been erroneously allocated. Therefore, there is no financial impact at present, however potentially there could be legal and PR implications.

Recovery Actions:  
Thursday 17/09
The initial investigation suggested at this point, that in total we had approximately 135k customers affected with more than 5000 points (i.e. 50 pounds or above)
 
The original suggestion of promotions with 300 points being impacted in the same way as the 500-point offers was found not to be correct. It was confirmed that the miscalculation was attributed to the lack of a correct promotion calculation logic with the UPC’s, hence the system assumed each UPC was to be assigned the 500 points logic.  The team continued to work on a correct recalculation of points, using the expected correct logic on the non-prod environment.
 
Friday 18/09
The re-calculation job was started and run for some time until a SQL error appeared.  However, while the job was running the teams could see that the correct re-calculation was taking place for all the accounts they were checking in real time.  This now meant that any future transactions would not be impacted and that support could concentrate on correcting the historical data.  The SQL error was largely attributed to a mis-match between Production and Non-Prod environments so for the rest of the day the teams worked towards fast-tracking a full refresh on non-prod to align as close as possible to Production.
 
Saturday 19/09
The previous plan of re-calculation was confirmed not to be viable during the investigations last night; so, an alternative solution was put together by the teams.  The alternative workaround being developed was through a job that would adjust customer points (this usually processes a very small amount of ad-hoc need for Customer bank customers)
The implementation plan involved testing on the lower environment to confirm feasibility, then Identify the correct bonus points total (either 0 or 500, depending on the account – old or new).  Support to then confirm whether batching a high number of customers in a single job would be feasible and benchmark how many could be processed at once and how long it would take.  Support would then perform a small test on a few colleague accounts on Production.
 
By the end of the day, it was found that the fix with the bonus points adjustment was confirmed successful on non-prod and teams worked on proceeding with the Production implementation.  The 300,000 highest affected customers were processed over night with the remainder on 20/09. The split is intended to ensure job run times are not exceeding expected processing times and to avoid causing issues downstream.  The final total number of impacted customers has been 489K.  The business was happy with the approach and plan which was proposed.

Sunday 20/09
Points correction for the remaining impacted customers (340k) was processed overnight. This included all the promotions for 500 points and a few for lower value vouchers. The points correction process was validated by performing a recon with Postilion which was successful. One issue outstands with refunds - if anyone had made a purchase transaction over the period since 9th Sep and has carried out a refund this week or beyond, then they would have points deducted (incorrectly).  In order to correct this, support has devised a script which will be run on a daily basis.

Monday 21/09
All activities which were planned for yesterday are completed and validated.  Just over £22 million had been erroneously allocated to 517k customers accounts and all incorrectly applied points have now been reverted

The overall value for this quarter’s voucher generation (QRM) is now at £10 million which is in line with the expected value.  The deduction of incorrect points after a refund will not immediately apply to customer balances (receipts, online view, etc.) as they usually run with a 48h delay.  Therefore to avoid these customers having a negative experience, a daily reconciliation job will be run to eradicate the incorrect negative point allocation.  Additionally a script is in place to manually correct these transactions daily to ensure QRM is correct.

Tuesday 22/09
All outstanding customer loyalty points corrections were completed overnight (refunds, correct 500 point allocation, etc.).  Validations have confirmed that we have all updates reaching downstream systems (Postilion) which ensures all customers have a correct view of their receipts.  Once Customer bank has processed the feeds on their end customers will see the correct points balance on their digital accounts.  This issue is now being set to resolved however root cause investigations will continue.
","Next actions:
Latest update:9/11
* Vouchers can be re-used in POS & SCOT Tills multiple times to gain bonus loyalty points-John Dennett- Completed- Valli - 
Update 9/11:   
07-Oct>> We discussed with John Dennett on this, and confirmed that there are no restrictions to vouchers being used multiple times. However, discussed for the options and viable solution to introduce manual intervention when Vouchers being used at SCOT Tills being suggested.
16/10/20 : From POS, It would just be a promo message triggered by the scan of the bonus points voucher. The UPCs on the voucher are not specific to a customer , all customers get the same voucher code. Hence the promo message should be configured against that UPC voucher code.
Short term solution to be identified

* Review job design and alerting for LPM (Product and Service Based UPC) job failures - LPM Support - Completed- 21/10/20 :  Fix for 'Service UPC mapping dropped for Promotion' issue is verified in CATE region and it has been implemented in production on Monday (19/10/2020)

*Review alerting from LPA system for Service Based UPCs - Email alerts to be converted as incidents and introduce job failure for Promotion flow-LPA Support-Completed-21/10/20 :  Alert has been configured in prodcution if there is service UPC change happen in LPA.

*Alert to check service UPC dropping in LPM - daily - LPM Support - Karthik /Ijaz  -Completed - 
18/10/2020 - CR presented in CAB and deployed in production sucessfully

*Increase the number of digits in the Business Weekly report to help validate the loyalty points- LPA Support-Completed-
05-Nov>> Susan reviewed and confirmed report looks fine.
 Update 9/11: 04-Nov-Susan is busy with reconciliations. Susan is available tomorrow to discuss reg. the weekly report changes.
CR was rasied and implemented by 22/10/2020. Validated weekly report and all looks fine. Shared the details with Susan and agree on contents. Susan was out of office. Need to discuss with Susan once she is available.

*Increase the number of digits in the CICS display for Customer Bank users to validate the loyalty points-LPA Product Team-Completed-
Moved to Jira backlog - BLP-9 - will take time to complete, no ETA.
12/10/20 : Checked with Dev team. Analysis going on for this fix.

*QRM date to be notified to LPA support at least 2 weeks prior to the expected statement run date-Dhiren / Promotion Team- Completed- 
Update 9/11: 30-Oct>>High earners query job testing completed. We have to discuss on the report& Needs to be implemented in Prod.
To be co-ordinated by LPA support-7/10/20 : Initaited the mail to dhiren reg. 2 weeks prior notification for QRM run. Dhiren confirmed in mail with 3 tasks.

*Fix process gaps to extend the end date of promotions in LPM-LPM Support / Promotions Team-Completed-Any changes requested by the Business, support to review and follow only the Bulk upload process

*Latest Loyalty Points file to be shared from LPA to Postilion (Previous day file was being sent to Postilion from LPA)-LPA Support-Completed-Code change made in the LPA system to send the latest loyalty points file from LPA to Postilion

*Duplication of Product Data in DataStage:
Foods business user incorrectly changed the main UPC flag from Customer EAN category (8-digit) to Supplier EAN category (13 digit) in SAP Master Data.  This feed was then sent to Enterprise Services and further to DataStage.  However, as per the DataStage logic, only the first 8 digits of the UPC was considered which caused duplicates in the feed sent to LPM- SAP Master Data - As part of the SAP October release, a fix will be deployed on Friday, 09/10 to restrict the business users from changing the main UPC flag from Customer EAN category to Supplier EAN category.  Once this change is live, only Customer EAN category (8-digit UPCs) will be sent from SAP to the downstream systems- Completed - Fix has been deployed on 09th Oct as part of CRQ000000128285.

* Identify the scale of accounts using single use vouches more than one - LPA Support / Promotion Team - Completed - 
Update 9/11: 3/11/20 : Discussions are going on regarding the weekly job.
28-Oct>> Report of last 2 QRMs have been arrived and data has been shared to Peter Green.
Need list of Promotions with Single Use - Nov-20 QRM (Promotion Name, Code, Service Based UPC)

* Understand the scale of the issue and fix the usage of voucher to one time for single use Vouchers - LPA Product Team - Completed - 
Moved to JIRA backlog, BLP-11, no ETA.
Analyze the report from Action item above  and decide accordingly.

* Include Ops & Loyalty product in planning of QRM - LPA / Loyalty Product - Completed - 
5-Nov>> 19th Nov - Dry run of reports, 25th Nov - Correction Processing, 26th Nov Cut-Off for QRM.
Laura Childs - to set meeting for the Feb-21 QRM and share it.

* Business & IT Controls - Requirements and Priority - LPA Support & Product Team and Peter Green / Anabel - Completed - 18-Nov>> Had further discussion on this and requirements have been gathered for items. Further action will be done based on the actions agreed.

*Create SOP for the workaround if the servcie based UPC received as 'Y' from LPA . This include the activity post SEV 1 alert received at LPA end - LPA Support - Completed - 
Update 9/11: 3/11/20 : We are analysing the work around steps to perform manualy in DB instead of going with adhoc job run
30/10/20 : Tried in lower environment by submitting the jobs PLY34, PLY254, PLY06 with the errored file(without UPC). All the jobs are completed & UPC's also dropped. But the PLY06 is doing some reporting related steps also. So, as per our upderstanding adhoc run will not work.
29/10/20 : Checking the PLY06 job, which is inserting the records to promotion table. Trying the recovery scenario in lower environment.

* Correct the automated mail alert code in JCl to fetch the correct UPC value in the mail - LPA Support - Completed - 
Update 9/11: 4/11/20 : Inprogress with raising CR & Approval process.
3/11/20 : Checking with network team regarding the sample mail trigger from non prod. They have shared the implementation plan to enable the mail trigger communication in LPA SIT.
 29/10/20 : File format issue is now solved. But from dev region the mail is not triggering. Messaging team is suggesting some steps to get the mail from dev server. Analysing on the same.
",22/09/2020,23/09/2020,,,,,,,,Data Issue,," A combination of process gap in extending the loyalty promotions and product reference data corruption in LPM, impacted 31 live promotions in LPA.
 
- Support identified that the product reference data processing in LPM (Loyalty Point Management) failed due to duplicate product data sent from DataStage (This was a first time failure and this scenario was never observed previously)
- Due to the duplicate product data received from DataStage, a decision was made to skip the current day product reference feed and process the previous day feed from LPM.
- However, due to a design flaw in the successor job, incomplete product reference data was processed, which deleted the mapping between the Promotions and the Service Based UPCs.
- The corrupted data was then sent to the downstream LPA (Loyalty Point Administration) system which then led to incorrect allocation of loyalty points to the customers.",,,,,January,1900,,Closed,,,2020,9,September,9/16/2025,1833,>60
09/09/2020,10/09/2020,87802152,,,,,C&H & Intl Supply Chain,,,,MI,Slowness in order flow into WMS for Castle Donington and Ollerton,TCS,C&H,,CD WMS,RC identified,51699,"At around 13:30 on 09/09, support were notified of slowness in the order flow into WMS for Castle Donington and Ollerton DC.

Impact: 
• The orders due for delivery to customers on 10/09 were processed with a delay in WMS, due to the latency in processing.
• Customer order proposition for next day delivery was moved from 20:00 to 19:00 on 09/09.
• Ollerton transfer orders to Donington were on hold impacting DC productivity until 01:21 10/09.
• 1400 orders were mis-promised.

Recovery Action:  
The relevant support teams were engaged including Blue Yonder (formally known an JDA). Initial investigations revealed that due to the high volume of orders processed on 09/09, the existing database execution plan was not found to be suitable. Hence, a better execution plan was pinned to the system followed by the restart of the order daemons.  There was initial improvement however, this was short lived.
 
The order daemon was restarted several times yesterday evening to push orders through to the system. NDD (nominated for delivery tomorrow) orders were cleared systemically as of 20:45, 09/09 and Donington continued their picking and packing operations since.
 
As per recommendations from Blue Yonder, support re-indexed two smaller database tables.  Also, better execution plans were applied to the long running SQL queries followed by creation of the new SQL queries, but these actions did not improve the performance.
 
The new inbound data from Sterling was put on hold and at the same time, all new orders in the backlog were processed by 23:00 on 09/09.
 
Support identified 12 missing orders to be processed in WMS after performing a reconciliation between Sterling and WMS. A decision was taken to process the Interface Order line records first and then proceed with processing the interface orders.
 
Overnight 09/09:
As part of further troubleshooting, support disabled the order eligibility script for the auto-bagger solution and observed a considerable improvement in the processing rate. Interface order lines and Interface order header records were processed successfully. The inbound flow of data from Sterling was then released, which progressed as expected. The 12 missing orders were re-triggered from Sterling and got processed successfully. Ollerton hanging orders were released and interfaced successfully.
 
Thursday 10/09:
The services remained stable and the incident was set to resolved.  It is yet to be agreed on how and when the order eligibility script will be re-introduced.
","Next steps:
Update 29/10:
OP confirmed over chat the below:
Business has agreed that there is no need of the eligibility script at all and hence confirmed to close PR as there are no other action pending on this.
Update 5/10:
Working with Product Owner to understand if and when the order eligibility scripts for Auto Bagger solution will need to be re-enabled

* When the order eligibility script for auto-bagger solution will be re-enabled.",09/09/2020,10/09/2020,,,,,,,,Customer Tech change,, The increased volume of sales caused the degraded performance of the order eligibility scripts which is part of the auto bagger solution.  Further investigations are to be carried out.,,,,29/10/2020,29/10/2020,#VALUE!,,Closed,,,2020,9,September,9/16/2025,1833,>60
03/09/2020,07/09/2020,87795253,,,,,C&H Commercial Trading,,,,SI,Delay in C&H feeds processing to tills,TCS,C&H Masterdata,,POS,RC identified,51774," At 00:44 on 04/09, support identified an increase in the volume of C&H data received at POS Beanstore end, which was also taking longer than usual to process. Normally this gets processed by 19:00 on a regular day.

Impact: 

·        Till rebuilds was kept on hold throughout the day on 04/09.
·        C&H new/changed product updates sent in the previous night’s feed, were not applied on the tills for 04/09 and 05/09, however these products were confirmed to be not in stock.

Recovery Actions:
Friday, 04/09:

The incident team identified that the C&H feed was over-running due to the staff discount static file being corrupt resulting in the over writing of staff discount across all the departments.

Support then worked on identifying the price changes in the C&H feed that should have been applied to the tills. Manual corrections were performed before the stores opened for trade to avoid any impact to the tills. NCR and stores service desk were advised not to rebuild any tills or apply any cache throughout the day to avoid any trading without staff discount.  

Further investigations revealed the following regarding the cause: 

There were two files which got triggered at 07:30, 03/09 – country age restriction and staff discount.  On the day of the incident, the staff discount file took a bit longer to process and the country age restriction file automatically triggered in parallel.

As the above files were triggered at the same time, the staff discount file content got replaced with the country age restriction data. Datastage then started processing without the staff discounts.

To prevent two files being triggered at the same time, support changed the trigger time (to ensure both files are treated separately). Support took the extract of all the triggers and added a condition to ensure the files are treated in sequence.

The incident team devised a detailed action plan to retrigger the entire feed for C&H as of 04/09 which progressed overnight. 

Saturday, 05/09:
Support observed slowness in the processing of the C&H feeds of 04/09. Owing to the slowness issue, a revised action plan was made to hold the full cache refresh and proceed with processing the BAU feeds - C&H, Corporate, Foods and Promotion of 05/09 and 06/09.

Sunday, 06/09:
C&H feed of 04/09 got processed successfully at 05:10, 06/09. The BAU feeds for Corporate, C&H, Foods and Promotions of 05/09 got processed successfully by 06:15, 06/09.The BAU processing for 06/09 got completed in the evening.

Monday, 07/09:
The full cache building activity to all store tills had started at 23:49, 06/09 and got completed successfully. More than 97% of the tills received the new cache with the updates by 06:30. The incident has been set to resolved.","Next steps:
Last update: 24/2

*Change the job schedules for all the C&H Master Data jobs to ensure all the data extract jobs do not run at the same time (07:30 AM)-Gopinath / C&H Master Data-Completed-A five minute time delay has been introuduced between each C&H Master Data extract trigger starting 07:30 AM

*Additional validation in Azure Logic Apps to run the C&H Master Data extract jobs in a sequential manner-Gopinath / C&H Master Data-Completed-All C&H extracts which are scheduled to run around same timings implemented with ‘in-condition’ logic to make it run in sequence.

*Validations checks in DataStage-Venkat / DataStage - Completed  - 
Update:7/1: completed the testing, planned to deploy it on 11th Jan morning.
Update 14/12: We had a several challenges in Non-Prod test all changes – We are started testing again after re-fresh code and data into Non-prod.As we have agreed with POS support to continually do the workaround until Jan.. We will deploy this change into PROD next year in Jan second week. 
Update 23/11: Today got updated mail from POS team and Seeva has been completed one round of sample runs for all POS interfaces. He is asked POS testing team to start with actual testing from 24th Nov 2020 and tentative production deployment plan for POS changes are on 8th Dec 2020.
Update 9/11: Devops team has been completed code changes on the top of other changes and these changes need to be tested till POS. POS testing team should start testing & come up with timelines. 
Update 14/10: We are completed DS code changes & deployed into PRODUCTION on 13th Oct 2020 (Task 3). Yesterday's run also completed without any issues. As per our initial discussion we need to implement this fix for 2 interfaces & fix implemented for 1 interface (C&H interface), planned to monitor 1 or 2 weeks and pickup food interface as well.
Note:- DS job will fail if any file didn't come with proper metadata or structure.
Update 29/9
The package is created with the required changes and planned to deploy the change within 9th October. Currently we are performing final round of testing and the walkthrough is planned with ERS team/Architects post testing. Will keep you posted on the actual Deployment date once it is finalised.  
A basic file structure validation check can be put in place in DataStage.If this validation fails then the DataStage job is expected to fail and will not process the file/data to the downstream systems.  Plan is to discuss the approach with Architects/DevOps teams and then implement the validation checks for one interface to see if this introduces additional delays to the overall batch processing. 

*Implement alerting in POS for both C&H and Foods delta creation-Valli / POS Team-Completed-Update: 15/10 - Implementation of the control-m jobs for delta creation alerts has been completed in production on 13th Oct 2020 for both C&H and Foods feeds. 
Enable alerting if there are delays in C&H and Foods delta creation.
ETA for completion 12/10

*Validation checks in POS - Batch file size check before DDS processing-Amy / Andy / POS Product Team- Closed -Introduce validations in POS to check the batch file size before DDS Processing.  If the batch file size is high/suspicious, the XML file should not be passed to DDS for processing 
24/02 - Backlog (RPOS 661) created for this item and will be managed by the POS/Retail Product Team 
15/1: Added in backlog. team to schedule meeting 

*Enable DDS instrumentation-Amy / Andy / POS Product Team-Closed-
24/02 - Backlog (RPOS -662) created for this item and will be managed by the POS/Retail Product Team
15/1: Added in backlog. team to schedule meeting 
",04/09/2020,04/09/2020,,,,,,,,Code/Product bug,," The 'Staff Discount' was replaced with 'Country Age Restriction' data as both these extracts were triggered at the same time.  The corrupted 'Staff Discount' file was further processed to the downstream systems resulting in over writing of staff discount across all the departments.
Multiple data extracts from CHMD are scheduled to run at 07:30 AM and all these data extracts use a global variable called during the interface run.
On that day, before memory(Java Garbage Collection) is cleared off for ""Staff Discount"" extract, ""Age restriction"" extract data was saved resulting in sending age restriction extract data with staff discount file",,,,2/24/2021,February,2021,,Closed,,,2020,9,September,9/16/2025,1839,>60
31/08/2020,03/09/2020,87791056,,,,,Customer Channels,,,,MI,"
Sparks Hub unavailable",Tibco,.COM,,Sparks,RC unknown,51759,"Customers were unable to access Sparks Hub on Customer.COM and receiving 503 Error Page

Impact: 
·        Poor customer experience
·        Customers will not be able to access any Sparks account information

Recovery Action: 
 The holding page was deployed on Sparks page. The Incident team observed a volume spike around 17:00 which was believed due to non-sparks email campaigning. As a workaround, support doubled the memory and CPU and brought the services back online. A decision has been made to throttle 30% traffic on website to monitor the performance and the sparks page across desktop and mobile were throttled back to 100% by 20:38. The business team  re-commenced their non-Sparks email campaign at 21:30 in batches of 500K every 30 mins. No performance issues were observed overnight. Sparks will be monitored on hyper-care throughout the day.
","Next Steps

*Validate the Network Bandwith Usage with Tibco ISP (Century Link)-Vijay / Tibco-Completed-  29/10 - Tibco informed that there were no issue with it and it was all fine. Tibco has raised a case with their ISP for investigation on 03/09.  Awaiting further updates.
* Implement POS config change (retries) and till roll out in production-Dhiren-Completed-
Update 10/12: Hysterix change  got completed on 9/12 to limit traffic chn the calls hits moer than 4secs response time
Update 4/12: POS Till roll out that has the change of retry value set to 0 is completed in Production.
Update 5/11 - Till roll from Nov 16 to Dec 2
Update 23/10- Till roll out will start either on Nov16th or on Nov 23rd based on the confirmation on Patch version. Will get this confirmation by Oct 30/Nov 2.
From the initial roll out start date, it will be deployed in all stores as per the usual roll out window
Update 11/09 - Approvals received from Retail Business.  Change process will be followed for Prod roll out (ETA for completion - 5th Oct).
*Sparks performance test to be carried out with 250+ TPS (transactions per second)-Dhiren / Loyalty Services Testing team-Completed-
11/12- Dhiren informed to close this as New relic is live now.
New relic implementation in PROD now for 3 instances to capture detailed errors/logs and will increase this for upto 6 instances and eventually for 9 to gather more info on DB errors.
29/10 - Have hit around 250TPS is the last test but are now seeing soem BD issues that team is fixing. Weekly PT results review call being set up.
Loyalty Testing team to carry out another performance test with 250 TPS (transactions per second) for Sparks Profile 

*Error handling and retry mechanism on Ingenix (Kubernetes)-Sandeep/Dhiren-Completed-Performance tests were carried out and support confirmed no retires are being made from nginx.
08/09 - Data set up completed in PT environment and testing on Monday (07/09) did not give the desired result.  Another round of testing will be carried out tomorrow (09/09)
11/09 - 2 tests performed but could not ascertain if ngnix retries are happening.  Data set up being worked up and then another test will take place on Monday (14/09)
*Review calls made by Activate Offer API-Harshit / Dhiren-Completed-Production Deployment completed on Monday (14/09)
2 calls being made to Tibco by the Activate Offer API.  50% of the calls from this API can be reduced.  
08/09 - Performance Tests are being carried out, will be deployed as a hot fix in case if we hit issues tomorrow else will be deployed on Thursday (10/09)
11/09 - PT completed, production deployment planned for Monday (14/09)
*Compare latency (response times) between Loyalty Services and Tibco during the incident window on 31/08-Dhiren / David (Tibco)-Competed-Latency response times for Get My Offers API has been compared from Tibco and Loyalty Services.  The latency response times for Get My Offers API is not 5 seconds on Tibco side.
Dhiren to provide the latency graphs from Loyalty Services for Tibco to generate the similar graphs from App Dynamics and compare the latency response times at both ends.11/09 – Awaiting further updates from Tibco
*Review the performance test profile-Harshit / Dhiren-Completed-The initial performance test profile was at 200 Transactions Per Second (150 TPS for web and 50 TPS for POS).  However, the current traffic in production is at 250 TPS for web.  Loyalty Services will build a test profile based on the current production volumes observed in Aug 2020 and further testing to be carried out.  Test preparations complete, team working on the scripts for test profile after which testing will be carried out.
11/09 - PT completed with 220 TPS (transactions per second) for Sparks Profile, test results are as expected (the response times from Tibco are within SLAs).  Team is now working on 250+ TPS
*Reduce POS Retries to minimize the load on Tibco DB - Dhiren / POS team - Completed
*Review the traffic / volumes from both Tibco and Loyalty Services-Harshit / Tibco-Completed
*Latency in response time from Tibco API-Raj / Tibco-Completed
*Pro-Active Monitoring from Tibco-David / Tibco-Completed
*Tibco Stored Procedure improvements-Vijay / Tibco-Completed-Tibco has identified a table (Profile Target) with huge volume of data (311M records).  
08/09 - 300M records from the Profile Target table have been archived which would improve the query response times.  Tests to be carried out in the performance environment to ascertain if this action has given us the desired benefit.
11/09 - PT completed and no issues observed


Dhiren to initiate discussion with Retail PO and further obtain approvals from Retail Business to implement the config changes in production.",02/09/2020,,,,,,,,,,," The incident on 31/08 looks like a repeat of what happened on 19/08.  Latency was observed in the response times from Tibco to the Loyalty Services API calls at 16:40 and POS retries kicked in at 16:49 due to the delay in the increased response times from Tibco.  Tibco has not been able to pin point the cause of the latency, however, they have reviewed the database stats and confirmed nothing abnormal from the database perspective.  Tibco will further work with their technical and R&D teams and come up with a mitigation plan / short term fix.  ",,,,15/12/2020,15/12/2020,#VALUE!,,Closed,,,2020,8,August,9/16/2025,1842,>60
30/08/2020,30/08/2020,87789830,,,,,Customer Channels,,,,MI,"
Intermittent performance issue in Sparks Hub",AWS,.COM,,Sparks,RC identified,51718,"At 12:11 30/08, MIM was made aware of multiple stores reporting an intermittent slowness issue and timeouts with the Sparks hub at store tills and website checkout page.

Impact:  Poor customer experience while using Sparks offer during check out at website and store tills until 15:06 on 30/08.

Recovery Action : Request timeout errors were observed in Sparks loyalty services while the services were trying to reach the TIBCO infrastructure. TIBCO was engaged to investigate the connectivity, however no errors were observed in the application and IIS layers. The incident team believed that the issue was caused by network connectivity timeouts on the AWS (Amazon web services) network between Customer and TIBCO.

 

A Sev1 case was raised with AWS for investigation. Meanwhile, vendor AWS advised a wider network connectivity issue within their external network. No timeout errors were observed since 15:06 and the services continued to remain stable. AWS support confirmed that the external network issue was between 11:03 to 15:35 which prevented the customers from reaching their AWS regions. Support monitored the stability of the services and no further issues were reported.","Closed as per discussion on 5/10:

Update on 11/9:
Dhiren - AWS would provide RC inly for Enterprise accounts and not for Business accounts. Dhiren still chasing AWS for RC.",30/08/2020,,,,,,,,,3rd party issue,, An issue with AWS external network prevented the customers from reaching AWS regions. Awaiting detailed root cause from AWS.,,,,,January,1900,,Closed,,,2020,8,August,9/16/2025,1843,>60
22/08/2020,24/08/2020,87779982,,,,,Group Technology Services,,,,MI,Unable to contact Service Desk via 185999,Anana,Retail,,Service desk line - 185999 ,RC identified,51610,"At 10:04, MIM was made aware that the Service Desk number - 185999 was not reachable since 08:45.

Impact: 
·         Stores and Head Office users were unable to contact the Service Desk between 08:45 and 10:58 via 185999. However, stores were able to reach the Service Desk via escalation lines and via emails during this period.

·         The stores and HO users reaching SD on the alternate numbers provided via their mobile phones after 10:45 as the numbers were not reachable via the ASCOM phones.

Resolution Note: The Stores and Head Office users were unable to reach the Service Desk number 185999 after 08:45. However, the direct dial-in numbers were reachable via mobile phones. User comms were sent out advising users to contact Service Desk via alternate options. The Service Desk started receiving calls on these alternate numbers since 10:58. Meanwhile, support raised Sev-1 tickets with vendors - Microsoft, Mitel and Anana for further investigation. 

Mitel successfully replicated the issue and found that the MiVB (the Mitel service which transfers the call over the Gamma SIP) was not receiving a response back from the Gamma trunk. Mitel analysed the traces and confirmed that the call transfer requests from Mitel were not being responded to by Gamma in a timely manner due to which Mitel was auto-cancelling the calls in their landscape. 

Anana liaised with Gamma for further investigation. Gamma is suspected a routing issue with their carrier (Vodafone) as the cause of the issue. In parallel, Gamma is investigated their network components for any faults. 

Sunday (23/08): At around 08:40 23/08, Gamma informed that the issue was resolved by the 3rd party carrier at approximately 00:20, 23/08. Gamma have monitored traffic for all lines that were affected and confirmed successful calls since 00:20, 23/08. 

A few test calls were made to the 185999 number and all of them connected to the agents successfully. Store users also confirmed ability to make calls via ASCOM devices.","Next steps
Update 5/10
* Exploring other options for SD stability.

Latest update: Call scheduled for 28/9
Need update from Guy/Adam on below:
-Verify with Gamma on what was the exact issue that impacted the call routing to Service Desk (Stores and HO) on the skype solution.
- Pro-active notifications from Gamma incase of an outage at their end / their 3rd party carriers.
- Remove dependency on Vodafone - As Customer owns the number range (02087180000 until 02087189999), can 185999 be transferred from Vodafone over to Gamma?

Update from Guy on 4/9:
*Any pro-active monitoring that can be set up on Anana / Gamma Landscape when the call flow / routing is not working as expected? – There is a dependency on Vodafone as they are the range holder of this number. We cannot build any monitoring for this except for receiving Gamma outage notifications. This wasn’t done with expediency that Anana would prefer.
*Mitel had to do a lot of tracing/analysis to prove it was not an issue at their end after which Gamma confirmed an issue with their 3rd Party Carrier.  
Did Gamma receive any notifications from Vodafone regarding their outage? If yes, was this outage not communicated to Anana? – Example update email from VF was shared in relation to this incident with details like Impact, currrent status, restoration actions.
Restoration Actions:
Vodafone 3rd Line Support Teams identified an issue with parameters configured on the Colt side of the interconnects, in Paris and Frankfurt. 
Colt Support Teams made the required configuration changes to the parameters on the Paris and Frankfurt Interconnects, restoring services.

*If there were no notifications from Vodafone regarding their outage, how can this be managed in future? – We are pursuing currently is whether Gamma own a number ending 185999 so that we can have a better service experience when there are issues.  Alternatively we will suggest changing the number to one owned by Gamma.
*Calls made to the direct skype numbers from mobile phones/Skype were working but Stores could not dial the skype numbers from the desk/fixed phones, why? - We presume Skype will use data networks instead of the PSTN. We remain unclear why mobiles would work, we did not however design this solution. We suggest reviewing the HLD/LLDs of Mitel & the PSTN & Skype as a first step.
*We have seen a similar occurrence on the 19th of June due to an issue with Vodafone network.  Do we have any workaround available within Anana/Gamma in case if we run into these types of issues again? – Due to numerous customers being on the range, it is unlikely that Gamma can request Vodafone to transfer the range holder. The only option would be to change the number to one that is within a Gamma owned range to remove the dependency on the Vodafone porting. Anana are looking at whether Gamma owns a similar number to the current Customer one
TCS:
Pro-active monitoring of Service Desk lines - In Progress - Saran / Messaging Team -  40% of script is complete which now pulls a report, but to convert it into a alerting mechanism would take time as that involves lots of testing. I am also looking into other options apart from this to see that don’t involve any third party tools. 
Alternatively I will drop an email to Chandra from TCS COE team for other options.

Was there an actual fault with the internal Skype Call Queue numbers? - Completed - Gopalan / Messaging Team - Microsoft has confirmed there were no issues with skype calls on Saturday, 22/08.  From the analysis so far, Messaging teams believes there were no issues with the skype call queue numbers as they can see successful calls to other call queues which also use the skype solution.  They cannot see the test calls to the direct skype numbers made by Merlin and Saloni in the logs.  However, once Gopalan joined the MIM bridge around 10:30 AM and advised to try calling the skype numbers with the prefix +44, the calls to skype numbers were successful.

Mitel, Anana
* Any pro-active monitoring that can be set up on the Mitel Landscape when the call routing is not successful / not working as expected? - Andy Humm/Mitel - Mitel are unable to monitor calls once they have left the Customer environment as these calls would have done. Once they leave the Mitel environment we no longer have control. Even if we could see an issue further into the PSTN network, Customer do not have any alternate carriers to route calls through. The SIP trunks connected to the Mitel MBG’s were available hence we routed the calls. Unfortunately the calls failed further out into the PSTN.

* Any pro-active monitoring that can be set up on Anana / Gamma Landscape when the call flow / routing is not working as expected? - Guy/Adam/Anana
* Pro-active monitoring from Service Desk on the Skype Solution? - In Progress  - Options being explored by TCS teams
Mitel had to do a lot of tracing/analysis to prove it was not an issue at their end after which Gamma confirmed an issue with their 3rd Party Carrier.  
*Did Gamma receive any notifications from Vodafone regarding their outage? If yes, was this outage not communicated to Anana? - Guy/Adam/Anana
*If there were no notifications from Vodafone regarding their outage, how can this be managed in future? - Guy/Adam/Anana

* Calls made to the direct skype numbers from mobile phones/Skype were working but Stores could not dial the skype numbers from the desk/fixed phones, why? - Mitel / Anana - Call were possible from mobile and skype phones as they were not routing via the Gamma trunks used by Customer. I could also call the IT Help Desk from my Mitel phone from home which is connected to the Mitel Network and routes via BT. Customer only have the capability to route calls via Gamma (Anana), there is not alternate carrier.

* We have seen a similar occurrence on the 19th of June due to an issue with Vodafone network.  Do we have any workaround available within Anana/Gamma in case if we run into these type of issues again? - Guy/Adam/Anana",22/08/2020,04/09/2020,,,,,,,,3rd party issue,," Calls to 185999 failed due to a central network issue with Gamma's 3rd party carrier.  Anana working with Gamma to understand the detailed root cause. 
Update from Anana/Gamma on RC:
Vodafone 3rd Line Support Teams identified an issue with parameters configured on the Colt side of the interconnects, in Paris and Frankfurt. Colt Support Teams made the required configuration changes to the parameters on the Paris and Frankfurt Interconnects, restoring services.",,,,,January,1900,,Closed,,,2020,8,August,9/16/2025,1851,>60
21/08/2020,21/08/2020,87779020,,,,Foods,Food Supply Chain,,,,MI,Delay in Foods Final Order flow ,TCS,Foods,,Quantum,RC identified,51565,"At 01:57, Support flagged that one of the jobs had failed in the critical overnight food flows. This job is responsible for extracting the foods order plan report. At around 03:40, MIM was made are that another job failed in CXM due to data file discrepancy.

Impact: Foods Final Order SLA of 05:15 was missed. 1888 Purchase Orders and 1812 UPCs were impacted due to the duplicate file issue.

Resolution Note:

The order plan extract job (PFSPOPE320) failed due to an inefficient execution plan in the database. Support implemented a better execution plan to recover the job. The job was re-ran to its successful completion at 03:39. 

At 03:40, CXMDOPW030 job failed due to data file discrepancy in the count of inbound and outbound in CXM caused due to duplicate of the files. On daily basis, there are 84 files for 42 categories (Each category contains 2 files: new order file & Depot split file).  However, today 252 files were sent out from Quantum to CXM as PFSPOPE320 job was triggered multiple times as part of the recovery which caused duplication. 

Further investigations revealed that orders were duplicated for 8 out of 42 order categories. It had already been processed in DataStage and was sent to the suppliers. The remaining file did not reach the downstream systems as support put the subsequent jobs on hold.

Support cleared failed files and triggered the remaining files to the downstream systems which were successfully sent to the suppliers.

Foods Final Orders completed at 06:18. (48.86% of the orders were sent within the SLA at 05:15)

Purchase Order details for the new orders of those 8 duplicate categories were shared with the business. Business has taken corrective action for the 1888 impacted purchase orders.","Next steps:

Update 28/10: Release went in successfully.

Next update 28th Oct - Release date change from 21st oct
*Review all the existing and new tables created as part of DC2 / Ocado project and enable Gather Stats on these tables - Completed - FSOR DevOps-FSORP-15116 – 16/10: Reviewed; no further action required. No threats to the current position of stats set up.
*Validate other data extraction jobs (that would generate & transfer duplicate files on the job re-run) and ensure generic SOPs are in place - Completed– DevOps  FSORP- FSORP-15133 Planned release – Oct 21st (Dot 7 release)
*Validation checks to be put in place before transferring the Order Extract Files from Quantum to the downstream systems - Completedt -FSOR DevOps-FSORP-15133 Planned release – Oct 21st (Dot 7 release)

*Do we have an option to extract the duplicate PO data directly from Quantum? - FSOR DevOps-FSORP-15135 - Completed -  16/10: SOP prepared and handed over. Updated in SOP repository. 
SQL queries are complete. SOP preparation in progress to hand over to ERS.

*Enable Gather Stats on the tables used by the Order Plan Extract job - Completed - FSOR DevOps-FSORP-15121  - Production release on Sept 16th as part of CRQ000000127430.
*SOP  to be created for Order Plan Extract job recovery - Completed - FSOR DevOps
*Duplicate PO data extract requested by the business - It took 4 hours for SAP team to extract this data, explore other options available to get this data in a quicker fashion -SAP Buying-Completed-update on 1/9 - As discussed, we have received the final sheet of tracking numbers to extract the data from SAP only at 13:07 IST. It took 40 mins from SAP to extract the data and consolidate. I had issues in uploading the file in the mail due to the large size (> 32 MB). I have tried uploading the file in mail, teams, compressed format, but wasn't successful.",21/08/2020,21/08/2020,,,,,,,,Process Gap,, The Q job picked up an inefficient execution plan and failed with tablespace errors.  The change in the execution plan is attributed to the increase in data volume processed as part of the 2nd NDC project and duplicate orders were generated during the job recovery.  ,,,,11/09/2020,September,2020,,Closed,,,2020,8,August,9/16/2025,1852,>60
18/08/2020,19/08/2020,87774399,,,,Foods,Food Supply Chain,,,,SI,High Variance in Foods Bradford NDC allocations,Customer,Foods,,Quantum,RC identified,51545,"At 04:00, support flagged a high variance of 146% in the Foods Bradford NDC allocations generated for 18/08, when compared with the backup allocations of 17/08

Impact: Operations at Bradford DC were delayed since the DC was unable to perform picking operations between 06:00 and 10:03 on 18/08.

Recovery Actions: The DC was advised to hold their picking operations since the orders for today were already allocated by 03:34. After getting a concurrence from FCS (Foods Central support) to use the generated allocation, the DC was advised to resume their picking operations by 06:00 as usual. 

Around 08:00, however, Foods business advised that NDC backup allocation be used instead of the actual allocation. Hence , the previously allocated orders were deallocated systemically by 09:11 after which, backup allocations were triggered which got interfaced into DC successfully by 10:03. The DC was advised to resume their operations and un-pick the orders which were previously picked up, in order to avoid over-allocation.

Investigations revealed that, the variance was caused due to the following- an incorrect data fix applied in Quantum and an incorrect fixed order raised in ORCA by business on 17/08. A Quantum outage from 15:30 to 17:30 was secured to perform corrective actions.

Support monitored the overnight Bradford NDC allocation and confirmed no significant variances. DC was advised to proceed with their daily operations as usual.","Business has accidently switch-on that configuration, they already aware of this but we have informed same once again. No further actionis required.",18/08/2020,18/8/2020,,,,,,,,Human error - Business,," Incorrect data fix performed by the Business in Quantum on Monday, 17/08 caused significant variance in the NDC allocations.


 The Reason for the high variance on 18th Aug NDC allocation compared to 17th backup allocation of 18th, while checking we could see that quantum has allocated more trays in the MCD tier due to high display target configuration. 

 

On 17th Aug business accidently switch on a one configuration switch (ENABLE_SCALED_DISPLAY_TARGET) which will enable the products to follow a new calculating for display target while doing the allocation. Due to this, more trays were allocated to meet the display target. ",,,,,January,1900,,Closed,,,2020,8,August,9/16/2025,1855,>60
18/08/2020,18/08/2020,87774488,07:10,15:45,08:35,,Customer Channels,,,,MI,"Users unable to reach Customer.com contact Centres, Colleague Services and Stores Switchboards ",Anana,.COM,,Genesys IWS,RC identified,51543 ," Anana reported an outage at their end affecting multiple customers including Customer. Due to this issue, calls to Customer.com contact centres, colleague services and stores switchboards were affected

Impact: Users were unable to contact Customer.com contact centres, colleague services and stores switchboards via phone between 07:10 and 15:45 on 18/08. However, Customer.com contact centres and Colleague Services were reachable via mail.

Recovery Actions:
As part of their daily checks, Anana identified a major outage at their end in the London area affecting multiple customers. User communications were rolled out advising the users to contact Colleague services via email only. Anana further updated that a fire and subsequent power/UPS outage at Equinix had caused an outage to Gamma and Twilio telecom services since the Equinix data centre houses Twilio and Gamma network hardware.

At 12:00 on 18/08, the incident team was made aware that all calls were routing successfully to stores switchboards contact centres and colleague services. However, a small number of Contact Centre lines including “International” were unavailable. Anana had placed a front-end message on the affected lines. The Colleague Services outbound calls served by Gamma were also down.

Initially Gamma advised that all links had been recovered after the fire at the Equinix data centre. The incident team therefore could not identify a reason for a few lines still being down. However, at 14:25, Gamma have advised that an issue at the Equinix end had caused one out of two gamma links to be out of service.

At 15:45 on 18/08, Anana advised that the services were restored. Contact centre agents were able to make outgoing calls successfully and the emergency messaging was removed. The services were monitored and continued to remain stable. Anana continues to work with their service provider for the RCA.","Latest update: 
24/6: Deployment completed. OTT is not included now in solution dure to the cost restriction. Current solution will be reassessed for OTT in Q2. Currenty the third back up line is MA3 BGP routing ie if HA primary n secondary fail, MA3 SBC will be used ( traffic returning to LD9 stack via the interconnect. Then the PSTN will be used. OTT offers more line, but it needs VPN costing of $1000 a month. Emergency messaging also incorporated in this solution.

22/06: The deployment for testing the resilience to be reattempleted on 23/06 - Jamie Monteero
16/06: (Jamie Mondero) :  The ISTN change has been promoted to staging and UAT has been completed.The deployment was scheduled from 15/06,22:00 to 16/06, 00:00.
but it failed. The recent Jenkins V2 update created a change that was fixed today morning. They will try again tonight.
------------
30/4: Jamie updated the ETA as May 1st week as the builds were underway
31/3: Jamie told that the ETA is 15th April - The actions require additional funding and has been agreed. Waiting for resource to be assigned from Sabio and Anana
15/3: Email sent for update
Last update: 17/2
Next update: From jamie - March 3
*Look at geographical resilience for Twilio services-Chris McGrath-Completed- 
17/2: The solution for this is to add another interconnect to another Twilio Network Exchange. This would incur a charge for the (paired) interconnects of $48K per annum. Hence, this is not a cost effective solution when an internet based OTT connection will just incur the set up cost for the VPN (IPSec Tunnel). 
Therefore we can permanently close this one off.
23/12:The decision has been made to include Twilio resilience into Analysis phase to be included in Q4 deliverables
11/12: Awaiting costing from Twilio.
Option under review with Retail Architects.
Decision expected 15/12
*Can the OTT (Internet Connection) route be re-instated?-Chris McGrath-Completd-
24/6: Deployment completed. OTT is not included now in solution dure to the cost restriction. Current solution will be reassessed for OTT in Q2. 
8/6: The ISTN Change has been promoted to staging and UAT has been completed.
We are hoping to promote this to production by Thursday.
27/5: PSTN intent based routing has been created of the overarching solution and is ready for deployment following testing.
Deploying the code has encountered issue and a Tiller Update has been completed by the Tools Team.
I have requested that the Developer reattempts to complete this, we will test and mitigated the risk.
10/5: The two points above have now been designed differently that Intent Based PSTN routing will act as the resilience for calls, should the primary and secondary links fail.  The developers have created this and is currently in testing and planned to be deployed to production on 17th May.
17/2: VPN OTT solution agreed with Twilio and Anana. Deployment date 3/3. 
11/12: OTT reinstatement in analysing phase.
Cost of change under IA.
Implementation expected w/c 4th Jan
*Gamma Resilience - Services could not be failed over from Equinix LD8 to Equinix MA3 Data Centre-Adam / Anana-Completed- Update 1/10: testing was completed as part of the CR
Both redundant links from LD9 to Gamma were 
down, as they transited through LD8, the datacentre that had the full outage. We should have been able to reroute traffic via our cross connects to MA3, and out into 
the Gamma network from there. When we completed this by advertising the LD9 SBC out from MA3 to restore service, an asymmetric routing issue occurred where the SBC 
believed it was sending traffic to LD8, but was receiving responses from MA3 and was discarding the traffic. Anana are resolving this routing issue with collaboration
from Gamma. This will be fixed by the end of September 2020.
*End to end testing to be carried out once the asymmetric routing is fixed by Anana/scheduling the full failover test cases with Gamma to prove that the routing via MA3 into LD9 works when the link between LD8 and LD9 are not up-Adam / Anana-In Progress - Update 2/11: done virtual tests using the network architecture, but we have not yet agreed a time/date to complete this in production.This is somewhat prohibited by peak!

*Enable routing resiliency from LD8 to MA3 between Twilio and Anana -Chris McGrath-In Progress-
24/6: Deployment completed. OTT is not included now in solution dure to the cost restriction. Current solution will be reassessed for OTT in Q2. 
Currenty the third back up line is MA3 BGP routing ie if HA primary n secondary fail, MA3 SBC will be used ( traffic returning to LD9 stack via the interconnect. Then the PSTN will be used. OTT offers more line, but it needs VPN costing of $1000 a month. Emergency messaging also incorporated in this solution.
27/5: PSTN intent based routing has been created of the overarching solution and is ready for deployment following testing.
Deploying the code has encountered issue and a Tiller Update has been completed by the Tools Team.
I have requested that the Developer reattempts to complete this, we will test and mitigated the risk.
10/5: The two points above have now been designed differently that Intent Based PSTN routing will act as the resilience for calls, should the primary and secondary links fail.  The developers have created this and is currently in testing and planned to be deployed to production on 17th May.
17/2: Active – Passive BGP solution agreed.
Deployment date 3/3.
11/12: Option under review with Retail Architects.Design decision expected end of December.

*Single end point for Genesys and Mitel, Can multiple end points be set up? -Chris McGrath-Completed-
17/2: Multiple endpoints exist
11/12: Under design review until the end of December.Expected implementation by the end of January.
*Explore options if the emergency messaging can be put on by the Contact Center team themselves-Chris McGrath-In Progress- 
24/6: Deployment completed OTT is not included now in solution dure to the cost restriction. Current solution will be reassessed for OTT in Q2. 
Currenty the third back up line is MA3 BGP routing ie if HA primary n secondary fail, MA3 SBC will be used ( traffic returning to LD9 stack via the interconnect. Then the PSTN will be used. OTT offers more line, but it needs VPN costing of $1000 a month. Emergency messaging also incorporated in this solution.
8/6:TheISTN Change has been promoted to staging and UAT has been completed.
We are hoping to promote this to production by Thursday.
10/5: Again, the issue has been funding for developers, however, they have started working on this yesterday and should take 10 days to complete.
17/2: Non coding change under trial.
Testing in progress.
23/12:included in the solution for Emergency Store Messaging Feature, again to be delivered in Q4
11/12: Create self help guide for Steve / team to be able to accomplish this, JIRA request raised.
*Create a prod maintenance test line on the natural language route to test if the calls can get thru to Genesys while the emergency messaging is on-Chris McGrath-In Progress -
17/2: Included in the solution above.
23/12:included in the solution for Emergency Store Messaging Feature, again to be delivered in Q4
 11/12:Testing to be included in the delivery of the emergency messages and treatment as a test and maintenance route.

*Outage comms process to be reviewed and agreed for issues impacting both Colleague Services and Stores-Laura / George-Completed-Comms process agreed with HR, both 
Retail and HR impacts will be called out in the comms going forward
*Twilio Resilience - Validate if both the Twilio routers in the Equinix LD8 data centre were down-Chris McGrath-Completed-Twilio router (Juniper SRX5400) did not lose 
power, because the power outage happened within the Equinix own meet me room, which took down their own devices.
*Twilio Resilience - why did the backup solution to SIP (OTT and PSTN)  fail?-Adam / Anana-Completed-The Over-the-Top fallback option was decommissioned when the 
AudioCodes SBC project was put in, as Twilio SBCs can only be configured with two SIP endpoints, and therefore the internet connectivity legs were decommissioned. 
PSTN was down due to the asymmetric routing issue described in Action 6",18/8/2020,18/8/2020,18/8/2020,22/06/2021,22/06/2021,No,No,No,No,Infrastructure issue / Hardware failure,No, A major power outage caused by a faulty UPS at the Equinix London Data centre had disrupted the telecom services provided by Gamma and Twilio. ,No,NA,NA,24/06/2021,June,2021,Dimple,Closed,,,2020,8,August,9/16/2025,1855,>60
18/08/2020,28/05/2020,87673528,,,,Foods,Food Supply Chain,,,,MI,3 x volume increase observed in Bradford Foods allocation ,TCS,Foods,,Quantum,RC identified,50391,"Around 06:45, MIM was made aware of a threefold increase in Bradford Foods NDC allocations when compared with the allocation volume for 27/05

Impact: 
The excess volume would not be possible for the NDC to pick and the excess stock would not be possible for stores to manage in terms of storage.  The business decided to use the NDC backup allocation and therefore DC resumed picking operations around 11:00 on 28/05 once backup allocation was ready in the system

Recovery action: 
Support was engaged to investigate. A decision was made to use the NDC backup allocation and business asked the depot not to begin their operations. Back-up allocation for Bradford NDC allocation got completed and a total of 5573 orders were generated. The depot commenced their operations by around 11:00, 28/05. Booker supplier was advised to stop the picking the orders for 28/05. Support will “zero” the orders for Booker and hence Booker will pick double the amount on 29/05.Support attributed the RCA to the Q release performed on 27/05 when making a change to the MCD config (Minimum Credible Display). In order to fix the issue, an outage from 17:00-19:30 was secured, the NDC backup allocation was uploaded into Quantum and the corrective code change was performed.
 
Overnight 28/05 - Support ran the targeted FOP for frozen to ensure that frozen orders were updated for 28/05 run and does not need ‘backup’ applied. Support applied a data fix to ensure that the orders for the Booker supplier were zero’d.  The frozen allocation was verified and sent to Bedworth successfully. The Quantum batch was triggered by 21:00 as scheduled. NDC allocation volume generated was validated and confirmed to be within permissible limits. Bradford NDC allocation for 29/05 got interfaced into WMS successfully by 3:28.
","Next actions:
Update 19/4: Called Joey and he confirmed that Business still has not committed to the actions below, so there is no definite ETA and he nce he agreed to close this from our tracker
Update:15/3 : Had meeting  with Joey & Promoth on 15/3. They are following up with Business

Pending actions:
•	Need better diagnostic tools to identify/understand the impact from data perspective - Thomas Hilliard (PO) - Completed- Tools will be built by the team based on the scenarios which is a continous action followed by the team currently. Already number of tools have been delivered.
•	Testing should be carried out on atleast 50% of production data to identify any potential issues . There is a cost involved in the approach, the cost to be funded by the Business - FSOR DevOps / Thomas Hilliard (PO) - In progress  -
 19/4: Called Joey and he confirmed that Business still has not committed to the actions below, so there is no definite ETA and he nce he agreed to close this from our tracker
Working on this to get a definitive statement
•	Agree a percentage on the tolerance level for Bradford NDC Allocations with the Business to setup alerts in Quantum - Joey / Thomas Hilliard (PO) / DevOps - 
 19/4: Called Joey and he confirmed that Business still has not committed to the actions below, so there is no definite ETA and he nce he agreed to close this from our tracker
Conversations around %’s are being kicked off
•	Agree a percentage on the tolerance level for FFO with the Business for each category to setup alerts in Quantum - Joey / Thomas Hilliard (PO) / DevOps - Conversations around %’s are being kicked off

10/06/2020 - Hypercare has been confirmed and will be a part of all future releases (including the midnight check that was missed in error):
- Hypercare to be mandated by the Project team after every Q Release - FSOR DevOps

10/06 - 
Any issues identified should be communicated to MIM in a timely manner - Completed
A Pager Duty alert to be set up in ASO which would trigger an incident if there is a significant difference between the NDC Pre-Allocations and Actual Allocations - Completed
The criteria/threshold for the Pager Duty Alert needs to be discussed and agreed with the FSOR team - Completed
Both of these items are in discussion as part of Q2 PI prep:
o Need better diagnostic tools to identify/understand the impact from data perspective - Thomas Hilliard (PO)
o There is a cost involved in the above approach, the cost to be funded by the Business - FSOR DevOps / Thomas Hilliard (PO)
- Conversations around %’s are being kicked off

--------------------
Tactical Fix/Workaround:
To mitigate the impact to Bradford, the backup allocations from ASO were applied and orders were sent to Bradford WMS.

Permanent Fix: 
• The code introduced for MCD calculation in Quantum was disabled.  
• NDC backup allocations were uploaded into Quantum.  
• Data correction performed for Booker Supplier and Bedworth Frozen Orders.
",28/05/2020,29/05/2020,,,,,,,,Customer Tech change,CR,"?

An issue with the code logic introduced for calculating MCD (Minimum Credible Display) as part of Quantum Release (CRQ115266) on 27th May as requested by the business.",CRQ115266,,,4/19/2021,April,2021,,Closed,,,2020,8,August,9/16/2025,326,>60
15/08/2020,15/08/2020,87772163,,,,,C&H & Intl Supply Chain,,,,MI,Incorrect DPD label printing in Castle Donington DC,TCS,WMS,,Matapack Label Printing for DPD,RC identified,51476,"At 15:42, the Castle Donington logistics team advised of two issues that were encountered with DPD label printing. 
1. Consignments with promised delivery for the weekend were incorrectly labelled to be delivered on Monday, 17th Aug.
2. The orders shipped on 15th Aug had labels printed with an incorrect delivery date of 15th Aug instead of future dates.
Impact: 
• Issue 1: Approximately 60% of the impacted 16.5k orders were recovered and delivered in stores (16/08 Sunday), remainder of the pending volume will be delivered today.
• Issue 2: Deliveries for these orders - approximately 3500 orders were impacted
Recovery actions: The incident team confirmed that the Metapack change (CR# 126756) had caused incorrect labels to be printed on the parcels. In order to restore the services, the change was reverted by 16:00 on 15/08. The labels were then printed correctly. No issues were reported after the reversion and the system continues to remain stable. Support continues to monitor the label printing on hyper-care over the weekend.
","Update: 15/09/2020
Closed as per confirmation from Martin

The only pending action is to raise a change request to metapack.CD have asked the business to be clear on the actual requirement so they can give a clear steer to metapack.
Martin Coy assume it will be an individual service for each hub with its own cut off times.

Update on 8/9:
The DPD trailer cut off times are now being changed from 2.00 AM to 00:01 AM in the Metapack system. This change scheduled for 9/9 10 AM. Teams will be on hypercare.
Need to get confirmation on 8 Sep - Update : 26/8- Colin - DPD will reintroduce clustering to the transport network from 8th September, and this will result in the last trailer leaving Donington at 02:00 (pre covid this time was 03:00).  We have discussed with the Command Centre and the Site Leadership Team on the two weekends between this DPD change and have decided that we will not make any further changes to the cut off time due to the risk of change (and also this weekend being a Bank Holiday in which we have put in specific configuration to deal with that).

Next Steps:

1.Dashboard to highlight orders that have the Ship By Date before the Order Created Date.  Integrate these checks in the WMS hourly health check email - Completed - Dean / WMS Support.
2.Enable service level specific monitoring on the dashboard to check the labels are printing correctly for the specific delivery service being used - Next Day / Saturday / Sunday - Completed - Dean / WMS Support - Jira Ticket Reference - https://jira.platform.mnscorp.net/browse/CAS-293,Customer Planning and Reporting team at Donington is currently creating a report that Donington Operations team will use to highlight when the despatch team have packed any items before the DPD Cutoff time as they sometimes do which produces the wrong labels.
 3.Provide more order examples to Metapack where the DPD label printing picked the Saturday delivery service for orders packed on Friday instead of Next Day delivery - Completed - Dean / WMS Support.
4.Metapack to investigate on the order examples in Action 3 and feedback on the behaviour - Completed - Manuel / Metapack Support - Metapack investigated and confirmed that all the example orders they looked at belong to the same category - Order packed on Friday but produced the next day label due to the change in the trailer cut off times.
 5.Change the DPD trailer Cut Off times on the Metapack System in CATE A region from 22:00 to midnight (00:01) and validate the DPD label printing for Next Day / Saturday / Sunday delivery service - Completed - Pavels / Kishore Radhakrishnanan - Testing completed for Next Day, Saturday and Sunday services and the DPD labels are printing as expected.
6.Any further changes to the trailer cut off times in production, inform the CD Operations team to highlight if the DPD labels are being printed incorrectly &
7.Raise a change request with Metapack to request a new delivery service instead of modifying the trailer cut off times on existing delivery services - Waiting for confirmation on  8th Sep that the last DPD trailer leaves the site at 2 AM - Colin / Martin - 

Lessons Learnt:
• Testing - End to End testing (create order, pre-carrier call, packing) was not carried out for this change in the lower environment and hence the incorrect label printing issue for DPD carrier was not captured during the testing phase.
• Monitoring - No monitoring in place to detect the problematic orders where the Ship By Date is before the Order Created Date.
• CD Operations - No pro-active call outs from the CD Operations team to notify IT on the incorrect DPD label printing.",15/08/2020,15/08/2020,,,,,,,,Customer Tech change,CR,"Root cause :  According to the current theory, it is believed that the change (CR#126576) to adjust the DPD trailer cut off times went wrong due to the gap in understanding the way the Next Day / Saturday / Sunday Delivery service works for DPD carrier to print the Metapack labels.",126576,Process Gap,,,January,1900,,Closed,,,2020,8,August,9/16/2025,1858,>60
10/08/2020,12/08/2020,87765261,,,,,Group Technology Services,,,,SI,GMOR system inaccessible ,TCS,Linux,,GMOR,RC unknown,51447,"Background:?At 14:00 on 10/08, support advised that two out of the ten Stockley GMOR Database nodes were down which had caused the system to be unavailable to users and the batch jobs to not run as expected.
Impact:?The GMOR application was unavailable to users on 10/08 between the below mentioned times: 
• 13:00 – 15:45, due to the Stockley GMOR DB nodes going down 
• 17:20-19:25, during the failback activity of the GMOR DB services from Swindon to Stockley.  
Recovery actions :?The Stockley GMOR DB nodes which were down were taking longer than expected to recover. For timeliness, the incident team failed-over the GMOR DB services to Swindon in a controlled manner and the GMOR application was running out of Swindon by 15:45. After the  failover, health checks revealed that the replication between Stockley and Swindon GMOR had stopped at around 13:00 and execution of the GMOR jobs on Swindon might have resulted in data loss, as the databases were not in sync. 
 
The automatic recovery of the Stockley GMOR DB stack completed by 17:20 on 10/08. The incident team then commenced failback of the GMOR DB to Stockley and resumed the batch from Stockley to avoid any data loss.
 
The Stockley GMOR DB was brought up by 19:25 on 10/08 and health checks were confirmed to be green.?The incident team then performed a table level validation and confirmed no loss in data. ?When the incremental replication from the GMOR Stockley DB to the Swindon DB was initiated, the system over-rode it and triggered a full-replication instead which got completed at 23:53 on 10/08.?The GMOR batch jobs were also released in parallel. 
 
The critical overnight batches were monitored and got completed within their SLAs on 10/08 and 11/08. The GMOR application continued to remain stable since 19:25 on 10/08. 
","Next Steps:
Last update: 2/3
Update from Unix Team on 2/3: 
All the PR RCA action items are completed and best practices will be covered in their patching cycle ( Mar-Sep). 


• IBM - Network parameters settings to be updated on the OS level - Completed - Both Swindon & Stockley.
• Enable Rapid repair - It brings performance improvements - Completed - 
1/3: Change has been completed successfully in Stockley servers on Saturday(27/2).
7/12:  As we didn't face any issues in GMOR recently , it's better to enable the rapid repair after the PEAK. 
We will discuss on the same with Remko once he is back from holiday and update you on the status.
Completed in DR, PROD pending with SAP support for outage
• Blue Chip - Firmware Upgrade -  requires physical presence, will be discussed with support teams and planned - Closed - 2/3: will be covered in their patching cycle ( Mar-Sep). 
• IBM - GPFS upgrade from 4.2.3.16 to 23, however Spectrum Scale 4.2 will run End-of-Service on 30.09.2020 - Closed - 2/3:  will be covered in their patching cycle ( Mar-Sep). 


• Suse Linux - OS Upgrade
• Blue Chip - Hardware Refresh
• SAP - Suspecting hardware issues, awaiting further updates
• The GMOR DB nodes (115 & 111) got expelled out of the cluster due to network packet drops / communication errors between the nodes.-  115 Cable replacement Completed

• The GMOR DB replication from Stockley to Swindon stopped immediately once the DB nodes were out of the cluster.
• Separate cases have been raised with Red Hat, Suse Linux, Blue Chip, IBM and SAP for further investigation.
• During Peak 2019, the GMOR system DB nodes exhibited similar symptoms related to network packet drops, however, the system was stable for almost an year since the cables were replaced between the nodes and the switch.
• It has been also highlighted that the node 115 was not reporting packet drops / communication errors and also running on the new Lenovo Model, hence no cables were replaced as part of the previous incident.  However, as this node is causing problems now, the plan is to proceed with the cable replacement on 115 at around 14:00 today, 12/08 (this is an online activity with no service disruption). 
Whilst the root cause investigations are still ongoing, there are couple of recommendations from the vendors.  These are general recommendations as we are running with a legacy hardware and OS which may or may not be related to our root cause.
 
IBM - Network parameters settings to be updated on the OS level - Completed - Both Swindon & Stockley.
Enable Rapid repair - Completed for SW, PROD Pending with SAP for outage.It brings performance improvements - requires a complete system outage.
Blue Chip - Firmware Upgrade
IBM - GPFS Upgrade, existing version will be out of support end of Sep 2020
Suse Linux - OS Upgrade
Blue Chip - Hardware Refresh
SAP - Suspecting hardware issues, awaiting further updates
 
",12/08/2020,19/08/2020,,,,,,,,RC Unknown,,"
 The GMOR Database nodes were unavailable due to the network packet drops causing communication errors between the nodes. Support is working with the product vendors to understand the cause behind the network packet drops.",,,,3/2/2021,March,2021,,Closed,,,2020,8,August,9/16/2025,1863,>60
31/07/2020,04/08/2020,"87749280, 87752760",,,,,Platform & Store Ops,,,,SI,Intelligent Waste Issues,TCS,Colleague Devices Efficiency,,Intelligent Waste - Reduce Later & Catch Weight,RC identified,51340,"Users reported issues with Reduce Later and Catch Weight while using the reduction function with the Intelligent Waste App.
Impact: App crashes while performing Reductions in Store. Catch Weight Item reductions not processing correctly.

Recovery actions:
 
Reduce Later : When user selected an item in Reduce Later list to carry out a reduction, the app got crashed. Transactions went through but the store user needed to go to the Reprint Waste option to print the yellow sticker. Stores were advised of a workaround. A fix for the issue was deployed to the store estate on 02/08. Stores confirmed on 03/08 morning that the Reduce Later functionality was working as expected.
 
Catch Weight : Certain catch weight products with 12-digit barcodes are showing incorrect original prices in the Intelligent Waste app. This causes the reduction price to be higher than actual price. Stores have been advised of a workaround. As per the recent changes in barcode scanning library, 12-digit barcodes are not treated as catch weight products. Hence, the app will extract UPC from barcode and fetch price from CSSM which will be different than the actual price on the product. From an IT perspective, this is not being treated as a bug as the requirement shared was that catch weight products will have 13-digit bar codes. A fix has been devised that enables the app to handle 12- digit barcodes. This was tested yesterday (04/08) afternoon and was successfully deployed to the store estate overnight. Resolution store comms have been rolled out to this effect.
","Next Steps:
22/2: Closing the PR as fix has been developed but there is no ETA on the app release
Last update 18/2: 
Catch Weight
1. Colleauge Devices foods funded CSSM team need to work with business (Nancy Ledsome) for the permanent fix, which will be picked up in Q4 Sprint (Dec 2020) - Anees Ahmed & Nancy Ledsome 
18/2: Development is complete. With this new change it will be able to distinguish between catchweight 12 digit and 12 digit branded barcodes. It will be clubbed with other changes and will be rolled out in the next app release - No ETAs
29/1: Have created a story for this and added to backlog. Don’t have an exact ETA yet but hoping to pick up in the next sprint or after. 
25/1: A plan to replace the tactical fix with a permanent solution has been identified.  
Based on the discussions we had today it was decided this solution will be implemented from Colleague Operations board. @Ravindran, Rohith will discuss the same with his PO and decide which sprint they can take this for implementation.
5/1: This has not been considered yet due to other priority requirements. 
We have PI Planning till Thursday. Post this we will re-convene to decide on an approach. Will keep you posted.

Recommendations -  

1. If the Reduce Later functionality testing would have carried out during the time of testing, support teams could have identified and would have avoided this incident.
2. If one device was not working during the time of testing, testing team should have used some other device to carry out the testing.
3. Before a release or change, an end to end testing should be performed without any deviations or assumptions.  
",31/07/2020,04/08/2020,,,,,,,,Customer Tech change,CR,"
 Reduce Later : This issue caused by a Firebase analytics integration to trace time taken for doing waste transaction.
 
Catch Weight: As per design, the app expects catch weight barcodes to have 13-digits.",125446,,,2/22/2021,February,2021,,Closed,,,2020,7,July,9/16/2025,1873,>60
30/07/2020,30/07/2020,87750668,,,,,C&H Commercial Trading,,,,MI,New SSI application unavailable ,TCS,Cloud Tools Support,,"SSI, IBT(ordering)",RC identified,51356,"Impact: Users were unable to access New SSI application between 07:00 and 13:00 on 30/07.  IBT ordering functionality was also unavailable as it was unable to connect to New SSI.
Recovery actions: Initial investigations revealed that the New SSI application pods were unavailable after a housekeeping activity was performed on the SSI Nexus Repository.  IBT ordering was also unavailable as it was unable to connect to New SSI. 
Support initiated work to redeploy the SSI application to the pods. The issues encountered during the build steps were resolved in Jenkins after which a new SSI application image was created and deployed to the pods successfully.  The SSI application was available for use from 13:00 on 30/07 and the application health checks were good. Business comms were sent to the users to advise that New SSI was available. The application was monitored overnight and no further issues were observed.
","Next Steps:
SSI application will be migrated to V2 byQ4 and intefaces by Q1 2021 which is known to everyone. 
Once this is done, the dedicated repository for Nexus will be also sorted.

Last update: 22/10

Housekeeping activity in both Nexus and Jenkins to be automated-Completed
- Update 20/10: Nexus automation is not feasible to implement, so we are doing manually when ever it is going out of threashold limit. So we can close this action item.
- SSI Application support to build and test an automated script / batch process to perform housekeeping for both Jenkins and Nexus – We have automated the House Keep activity for Jenkins and its done.But for Nexus, there is no solution to delete the specific images based on the time criteria. For the time being we have to do manually based on the space availability.

Need a dedicated Nexus Repository for Production and Development to store the corresponding images / builds - Cloud Tools Support 
22/10: The Resolution that for nexus space issue was to migrate SSI Nexus to V2 environment. We should be able to start the migration as soon as the application team is ready.
Discussions underway to move Nexus and Jenkins to V2 for high availability and high scalable system (Move to Kubernetes) - ETA 30/09
11/08 - Weekly backup enabled for Nexus and Jenkins systems in V1 prod.
Cloud Tools Support to explore options for having a dedicated Nexus repository for Prod and Dev – In Progress

SSI Application support to check the feasibility of migrating SSI application/Jenkins from Kubernetes V1 to V2 (Jenkins space is not a constraint in V2) - In Progress - Update 20/10: As per the plan application will move by Q4 and Interfaces will move by Q1 2021.
Naga to initiate the discussion with Allen Gahn (HoT for C&H Core)
- ACR (Azure Container Registry) can be used to store/backup images.  This is a backup solution to Nexus

SSI Application support access to Nexus and Jenkins servers - Completed

No Alerting when the application pods could not rebuild after the scale down and scale up activity 
- Alerting to be set up if pod is crashed / not ready / in error state – SSI Application support to work with Cloud Platform team to set up Sev-1 Prometheus Alerting – Completed - Pager Duty Alerting has been enabled.  A Sev-1 alert will be triggered to the SSI Application team in the event of an application or Ignite pod being down for more than 5 minutes

Explore options for manual build for SSI / other cloud-based applications running in V1-Completed
-Update 20/10: As per the architecture guidelines we are not implementing manual build. So we can close this action item. To avoid this situation in future , platform team is building CICD / JENKINS in V2 AKS cluster rather then individual VM.
- SSI Application support to work with Cloud Platform team to carry out testing on the manual build in V1 - In Progress
- Manual build already available in V2

Enabling automated alerts instead of email alerts
- Failures / issues in the SSI Batch process / Application health checks to trigger automated alerts to call out / notify application teams - Completed - We have identified the mandatory exceptions/issues and configured the Pager Duty alerts using the log entries tags. Application health check alerts has been configured before.

",30/07/2020,30/07/2020,,,,,,,,Process Gap,," SSI application pods could not be rebuilt after a planned scale down and scale up activity as the latest Nexus image was unavailable.  As part of the housekeeping activity on the SSI Nexus Repository, a command was executed which deleted the required Nexus images.",,,,08/01/2021,January,2021,,Closed,,,2020,7,July,9/16/2025,1874,>60
09/07/2020,09/07/2020,87723282,,,,,Customer Channels,,,,MI,Sparks unavailable on the Customer website and stores ,TCS,.Com,,Sparks,RC unknown,51048,"Sparks was experiencing a significant latency in stores and Customer website.

Impact: 
• The Sparks hub was unavailable to customers online between 11:43 and 18:48 on Thursday, 9th July.  
• The Sparks card could not be accepted intermittently on Thursday 9th July between 11:43 and 18:48
• Instant Win for Retail was delayed and launched on Friday, 10th July at 20:07.

Recovery action: 
Recovery actions: Support team identified a latency issue with the Loyalty Services database which is hosted on AWS (Amazon Web services). Communications were sent to store colleagues and holding pages were placed for Sparks on the website. The database was then upgraded to increase the capacity. It was found that the Instant Win (IW) functionality was heavily interrogating the database, hence this was switched off as a precautionary measure. The database upgrade got completed at 14:35 on 09/07, however the accompanying Loyalty Services (LS) nodes didn’t come up immediately. The support team investigated further and these nodes were brought up upon liaising with the AWS vendor.
 
Loyalty Service was restored at 16:46, 09/07 with database connectivity established successfully to the nodes. The web traffic was fully restored at 18:48, 09/07. Validations of POS and web traffic were successful. Marketing activity was suspended for the remainder of the day.
 
Overnight Thursday 9th July: The IW functionality was successfully enabled between 20:00 and 22:00 on 09/07 and no issues were observed. A decision was made to re-enable the Sparks registration journey at 22:00 and hyper-care was in place to observe if/how registrations were impacting the underlying infrastructure. No issues were observed during monitoring and Sparks application continued to operate successfully on the web, iOS and Android.
 
Friday 10th July: The incident team convened at 06:30 on 10/07. The database had been stable during the day with Instant Wins (IW) for retail stores turned off.  There had been one Instant Win at 09:07, 10/07 on the website.  Registrations continued throughout the day.  Given the good level of stability observed, the email campaign for Sparks continued as follows; 250k emails were sent at 10am, 250k at 12:30, 250k at 13:30, 170k at 14:30, 500k emails were sent at 15:00 and 16:00, 17:00, 18:00 and 19:00.
 
The teams also worked on two additional issues that were observed and were working on fixing these.
• The front-end pods on the loyalty hub are restarting upon accessing a particular charity page URL.
• FAQ page starts looping when being accessed. The teams are manually clearing the cache every hour, as a workaround.
 
In case it was needed, the team worked on a way of throttling or blocking registrations.  The team has also documented the scenario for the actions required if and when the issue reoccurs.  Also, the process for recovering the service in a timely manner was documented.
 
The teams observed an increase in the number of transactions accessing Sparks that were taking longer than 10 seconds between 15:00 and 16:00, 10/07.  The average number of transactions that took more than 10 seconds was 150 and the count observed between 15:00 and 16:00 was more than 400.  However, the teams remained confident that customer response time throughout the day were consistent and that the issues of yesterday were not repeated.  The average response time for Sparks web traffic had remained stable the whole day through (in conjunction with the 10 second response uptick).
 
Overnight Friday 10th July: In-store Instant Win was enabled at 20:07 and systems remained stable.
 
Saturday 11th July: The majority of the instant wins were redeemed in-store (since they were enabled) and the Sparks loyalty system continued to remain stable.
 
Sunday 12th July: Out of 909 Instant Win (IW) , a total of 882 Instant Win were processed out successfully yesterday, 11/07. The rest of the IW from yesterday has been rolled over for today, making today's targeted Instant Win count as 369. Over 300 IW has already been given away for today out of 369. Database utilisation has been under threshold throughout the day. Teams continue to monitor.

Monday 13th July: All components remained stable through Sunday evening and Monday morning. Instant Win is working as expected, as well as all other Sparks features. A detailed root cause investigation and incident review are put in progress. Full details of the RCA is to follow as part of the problem management process.
","Next actions:
https://confluence.platform.mnscorp.net/display/LNGP/Sparks+Incident+-+PIR++Action+Items
15/2: DR test is inlcuded in Pluto and will be taken up under that
Update 4/2: DR test is included in plan and plan is prepared ahd shared for approval
Update 13/1: Dhiren : This would be included in Pluto and will provide further details shortly

* Explore options to establish connectivity to the DB via Service End Point instead of a private IP- 
Update 4/11: Not prioritized, given other peak items and there are no impacyts as such currently, will inform us on the plan whether to do this after peak

• Review Sparks disaster recovery planning scenarios and failover options – Jose John - 18/08/2020 - In progress- Long Term: Discussion with Hubert - recruitment of a Reliability Engineer will develop a DR framework. - 
15/2 - Will be taken up as part of Pluto project, it has been inlcuded and submitted for approval
Short term:  Failover options have been identified and listed in a runbook.

• Support wrap and ownership (App vs Platform) to be defined for Loyalty Services and other AWS services – Sampath R - Completed- 
4/11: Exponence team has taken up the AWS platform support w.e.f Q3 
Review of all applications currently on AWS .- Update 12/08: Contract negotiations on Exponence in progress . Expected to start from Q3 onwards. Exponence will own and manage the entire infrastructure element of the platform. Latest update: Get Exponence (enterprise monitoring tool for platforms) for AWS.

• Create a standard SOP for the database upgrade activity – Jose John - Completed - SOP page created.

• Additional logging to be enabled on the Sparks Loyalty database to analyse the database performance statistics and capture the relevant error logs – Suresh Gandham / Platform Support – Completed-No short-term plans to make any further changes to logging setup (beyond those done already). Action closed.
 
• Complete end-to-end architecture review of the original Sparks Loyalty design and infrastructure – Jose John - Related tickets: MCLY-1174 (done) & MCLY-1181 (done). Also being worked on as part of Pluto- Completed

• Update on Root Cause investigations – Harshit / Dhiren -  The volume of registrations on the launch date exceeded the forecasts and testing, The mobile journey incorrectly under load, substantially increased the number of attempted registrations, compounding the impact making the slow down unrecoverable. The production volumes on the day of the launch and even higher were replicated in the PT environment but the behaviour was not similar to what happened in production hence the root cause remains inconclusive. The  RDS Database logs were not turned on in production , it did not help in understanding anything wrong that could have gone with the database during the time of the sparks incident - Completed

•  Review other DB & App instances hosted on AWS to verify if the Private IP Addresses are hard coded - Jose - Completed - There are 25 applications on AWS and these have being checked manually and confirmed by platform team that there is no hardcoding of private IP Address.

",09/07/2020,16/07/2020,,,,,,,,RC Unknown,CR,"  The incident team is fully aware that the two features, Instant Win and Sparks Registration utilized the database excessively through ‘Writes’ on the Loyalty Service database causing the latency. This could be due to both genuine customer load and erroneous traffic. There is a third line of enquiry examining the till journey which features Instant Win Code which could have caused issues. Further investigations into the root cause continue as a matter of priority.",CM4625,,,2/15/2021,February,2021,,Closed,,,2020,7,July,9/16/2025,1895,>60
30/06/2020,01/07/2020,87711447,,,,,Group Technology Services,,,,MI,Some BT stores experiencing intermittent connectivity issues,BT,BT,,BT Stores,RC identified,50871,"At 12:01, MIM was made aware that multiple stores were hard down.

Impact: 
BT-hosted stores were intermittently experiencing latency issues impacting Honeywell usage and tills, therefore were trading offline.  Initially 165 stores were impacted however within the first ten minutes these stores started recovering and the last store recovered at 12:49. After this there were two further outages where a much lower number of stores were impacted.

Recovery action: 
On initial investigation, it was clear that the stores impacted were all BT hosted stores and hence BT were engaged to investigate.  BT’s engineering team investigated alongside Vodafone and advised that they could not see any obvious faults on their network.  The network team then advised that at the times of the outages, they were seeing latency on the link between Vodafone and BT.  This then explained why only the BT hosted stores were impacted.
Vodafone advised that the peer (link) they provide for internet traffic towards to BT, is a 40Gb link.  At the exact times of the issue (3 times as in the impact statement above) the utilisation was seen to max out at 40Gb.  This is not a specific Customer link.  It is shared with all Vodafone customers out to BT. 
Vodafone Core team advised that traffic from one of Vodafone's current customers had fully utilised the Vodafone shared link towards BT during the outage times.  We advised Vodafone that Customer's priority is to safeguard our stores trading and we would like Vodafone to either prepare for a reroute of Customer traffic or an expansion of the link until they find out the cause of the huge spike in traffic.
 
Overnight 30/06:
BT were engaged and they confirmed that that traffic levels on BT transit links hit its threshold which might have impacted the service at the same time that Customer experienced the issue. BT teams believe a failed planned works performed during the early hours of 30/06 may have caused the issue. One link was shut down as part of the activity and the traffic from the link got re-directed via the Vodafone link which caused the 40Gb link to get maxed out. BT confirmed that the link that was shut down has now been re-enabled and that traffic is flowing normally since 15:00.
","Next Steps:
Denial / Delay from BT Service Desk raising an incident when an incident is reported to them – Completed
- Any delays / denial in the BT SD raising incident tickets, NOC support need to request to speak to a Senior Analyst or the Service Manager.  Escalations need to be made as per the contacts provided in the BT Service Handbook.
BT Impact Assessment – Completed
- Historic Utilization reports on the core BT link could not be extracted and have been set up for future.
Pro-Active Alerting from BT – Completed
- Proactive monitoring is not available for the BT Net service. The product itself is a non-managed product, therefore the facility to proactively monitor is not available.
BT Service Manager to join our daily operations meeting – Completed
- BT Service Managers to join our Operations Meeting the next day to provide any updates on issues caused by BT. 
No representation of BT technical team/engineer on the major incident bridge - Completed
- Acknowledge by Nick.  BT team now understand how Customer MIM works and have assured to bring in technical representation on the incident meetings for major / escalated incidents.
Process to raise an incident with BT and BT escalation matrix – Completed
- The process to raise incidents with the BT Service Desk and the BT escalation matrix is in place and covered as part of the Service Handbook shared by BT.  NOC team aware.
Technical PIR - Completed
- BT Technical PIR details - On 1/7/2020 at 02:30 during the PEW new filters were deployed on the core network, the number of prefixes previously advertised to each CDN vendor ceased to be provided, forcing all IPv4 traffic served from the local RCN’s [Regional Content/cache nodes] towards the RSN and peering platforms.  At 07:21, the increase in traffic on the RSN was identified via Customer Experience team report into the ISP NOC.  Essentially a change was made on the network which affected the section of the network which is reserved for BT Net customers only and caused the traffic to flood it, in turn bringing it down.
Myself and Nick have had a call with the technical teams, and as we had already previously advised, this fault should never have affected Customer and we can only apologise for this.
Preventive Measure in place to prevent re-occurrence – Completed
- The team have confirmed further resilience checks have been put in place for any future changes.",01/07/2020,21/07/2020,,,,,,,,3rd party change,CR, is attributed to the planned engineering works (config changes) carried out on the BT core network which re-routed the traffic incorrectly into BT Net carriers in error.  This impacted Customer Stores on the BT Network.  ,3rd Party Change – Change was performed on the BT Core Network which should not have impacted the BT Net services.,,,27/07/2020,July,2020,,Closed,,,2020,6,June,9/16/2025,1904,>60
29/06/2020,29/06/2020,87710102,,,,, Finance,,,,SI,Closure of P&L delayed ,TCS,Big Data Support,,Big Data - EIH,RC identified,50860,"The finance team reported that one of their P&L (Profit and Loss) jobs was running for 30 mins and would not complete.  Big Data support  confirmed that they had observed application job failures due to platform errors.

Impact: 
Finance teams were unable to perform their month-end P&L reporting until the issue is resolved.

Recovery action: 
Support teams were engaged and they restarted the file system of the EIH application server.  Finance then resubmitted the failed jobs, however, the jobs failed again with corrupted data.  Finance teams fixed the data at their end and resubmitted the jobs which were re-run to successful completion.  

The average number of job connections handled by the application is around 40.  However, at the time of the issue there were around 100 connections which caused all the jobs in the production EIH cluster to fail.  In order to mitigate the impact and balance the number of connections, a secondary Hive Server was added and Big Data (Hadoop Service) application services were restarted as workaround.  It has also been confirmed that the issue was not due to an increase in the number of finance jobs running for the Finance month end activities. ","Actions/Next Steps:

There was a spike in write operations to file system.
As we are getting data added daily to the EIH cluster and we notice the performance issue when filesystem(storage) reaches the 85% we are regularly doing the housekeeping to keep it under the threshold and also we are moving the data to azure which ongoing process which will help us to keep things in control. 
We have also set up the alerting whenever the filesystem reaches the threshold.

As jobs were running slow the scheduled jobs got triggered after the particular interval and we saw the spike in the number of connections getting piled up and due to which hiveserver2 was not available and then jobs starting failing.

• Analyse and investigate the reason for spike in the number of job connections in the EIH Production Cluster - In Progress - Rupesh / Big Data Platform Support
• Analyse the job connection trend for the last 4 weeks - In Progress - Rupesh / Big Data Platform Support
• The EIH Production Cluster can handle a maximum of 80 job connections (with 2 Hive Servers).  Check the feasibility to set up alerts if the number of job connections exceeds more than 70 for a specific time period - In Progress - Rupesh / Big Data Platform Support
• Check for any specific alerts / issues prior to the issue which might have contributed or triggered the issue - In Progress - Rupesh / Big Data Support",29/06/2020,03/07/2020,,,,,,,,Capacity constraint ,,"?
The initial root cause is believed to be due to an increase in the number of job connections that the application could not handle.  However, the reason for spike in the number of job connections is yet to be investigated by the Big Data Platform Support.
",,,,,January,1900,,Closed,,,2020,6,June,9/16/2025,1905,>60
23/06/2020,23/06/2020,87703257,,,,,C&H & Intl Supply Chain,,,,MI,Performance issues in WMS at Castle Donington and Ollerton ,TCS,IS - Oracle,,CD WMS,RC identified,50828,"Background:At 19:40, MIM was made aware of performance issues in the WMS application at Castle Donington and Ollerton DC. Support also confirmed that they could see slowness in processing of messages from WCS to WMS since 18:15 which resulted in message pile-ups.

Impact: 
• Operational Impact – Pack lost 38k capability, SCS lost 10k capability; Inbound lost 22.5k capability, 21K capability loss in SYW
• Customer.COM Impact – 1150 Orders failed.
• Retail Impact – 3863 Retail box singles failed to meet trailer departures.


Recovery action: 
Support teams identified that the boxed queue messages that were coming from WCS into WMS were processing slower than excepted, which was causing the pile-ups. Support performed the run-stats on the shared tables for CD and Ollerton, followed by daemon restarts but this did not fix the issue. ?A decision was made to restart the application and database servers however, the issue continued to persist. A case was raised with JDA vendor for further triage. Upon further investigations, WMS Support narrowed down the issue to the wait time of the Beauty Bag orders. CD locked the incoming messages for beauty bags into WMS in order to avoid further pile-ups.
 
In the meantime, the JDA vendor identified that one of the processing queries was using an inefficient execution plan from the package logging that had been enabled. Support created a new execution plan & pinned, after which all piled-up messages got cleared by 05:47. CD and Ollerton then confirmed that the pack benches were working as expected.
","The root cause is attributed to SQL query which was found to be inefficient while processing Beauty Bag SKUs.  Further investigations into why this impacted only the Beauty Bag SKUs was primarily due to the volume of orders (allocation and picks) associated with each of the Beauty Bag SKUs along with the change in process of Beauty Bag picking done via automation rather than manually.
The problematic Select Query did not appear on the AWR report - In Progress
- Support working with Oracle Vendor to get the previous run time details of the query

Alerting to check if there is a change in the execution plan for the problematic Select Query - Completed
- Email alert has been setup for this SQL as an hourly cronjob to check for plan change.

Alert to be set up on WMS for messages taking longer time to process for Beauty Bag related SKUs - Completed
- Slow auto message alerting (email alert) has been set up on Jira (https://jira.platform.mnscorp.net/browse/CAS-233)

Extract and share the volume trend of the Beauty Bag SKUs processed and analyse the journey of these SKUs -Completed
- Beauty Bag SKU transactions are only retained for 30 days, the Beauty Bag SKUs have been extracted and analysis in progress. 
",23/06/2020,09/07/2020,,,,,,,,Infrastructure issue / Hardware failure,,"?

An Oracle database query was found to be inefficient while processing Beauty Bag SKUs. This is currently being investigated by both - Oracle and JDA vendors.",,,,,January,1900,,Closed,,,2020,6,June,9/16/2025,1911,>60
10/06/2020,10/06/2020,87688661,,,,,Group Technology Services,,,,MI,Azure Express route connectivity issues at Stockley ,Vodafone,Vodafone,,"Sterling, Intelligent Waste",RC identified,50543,"Background:At 13:15 on 10/06, MIM was made aware that Ollerton DC was unable to access Sterling and that WMS had not received any E-comm orders since 13:00. NOC team confirmed that the Azure Express route at Stockley was down..

Impact: 
• Intelligent waste app was unavailable to users from 13:55 to 15:35.
• New orders were not coming in to Donington from 13:00 to 13:50.
• Donington (Metapack) packing capability was lost between 13:50 to 14:32.
 
Impacts from approximately 12:55 to 14:32:
• Sterling was unavailable to Ollerton to process returns
• All dot com fulfilment flows from suppliers which covered ED, Bradford and external suppliers were unavailable
• No orders were taken for ClearPay during this window.

Recovery action: 
After initial investigations, the network teams carried out a failover of the Azure Express Route from Stockley to Swindon by 13:55 and support teams confirmed positive message flows in various applications. There were however a few network routes that were still looking for the Stockley route, this issue was resolved by the network team from 14:32. The intelligent waste issue was resolved by approximately 15:37 after it was found that that virtual server hosting the application went into an unresponsive state.
 
On 11/06, Vodafone confirmed that in readiness for a move from the Vodafone hosted express route to the new Equinix hosted Express Route, changes were done in the backend to set up the route priorities. Unfortunately, the route priority changes were carried out on the same routing plan as our current production Stockley Express route which caused the incident. The work being done on the Equinix hosted route was separate to the activity that Vodafone initially stated had caused the issue.

The applications continue to operate out of the Swindon Express route without any issues. The failover to Stockley will be planned at a time favourable for all stakeholders.
","20/07/2020
1. No Alerts in place to notify the Intelligent waste Application POD crashes - We have created story for pod status monitoring alerts and will be picked up in early Q2. - COmpleted 

2. Alerts related to Cloud Nexus issues / failures should be modified to Sev1, so these issues can be picked up and actioned / fixed immediately by the respective support teams - Will raise any future issues in Nexus as high priority ones and follow up with plat team. - Completed & thisis agreed with colleague devices support

3. No alerts in place if there are no Waste transactions from Stores - Time based alerts (during store trading hours) to be set up to monitor the Waste transactions in stores - Planned in early Q2 Completed

4. create/maintain a separate configuration set up for the new Equinix Express Route which was completed by Vodafone successfully.  Hence, going forward any config changes on the new Equinix Express route will not impact the existing Express Route to Azure. - Completed

To be followed up with Honeyell support(Rohit Ravindran) End of Q2",10/06/2020,10/06/2020,,,,,,,,Human error - 3rd party ,,"?

Human Error - routing changes were inadvertently carried out by Vodafone on the Azure Express route whilst working on the new Equinix Express Route project.

Final Root cause from Vodafone:
Vodafone reconfigured a BGP route-map relating to Equinix Express route and added  BGP AS-path prepending to Uxbridge CE routers.
Investigations by Vodafone have identified that the above configuration affected existing cloud connect as the ""route-map"" used for Equinix Express route to create AS-PATH prepending was also been used by existing cloud connect service. 
Therefore introducing AS-PATH prepending into Uxbridge made the 10.0.0.0/8 route less favourable than Swindon. This caused traffic for 10.0.0.0/8 to go to Swindon, which would then be black holed.
Vodafone has now rectified the ""sharing of route-maps"" by creating individual ones for each service (Equinix Express route/Cloud connect)
",,,,7/27/2020,July,2020,,Closed,,,2020,6,June,9/16/2025,1924,>60
09/06/2020,09/06/2020,87686106,,,,,C&H & Intl Supply Chain,,,,SI,Castle Donington - Multiple pending events on WCS,SSI,C&H,,WCS ,RC identified,50476,"At 23:40 on 09/06, MIM was made aware that the pending event issue has reoccurred impacting the boxed Service.

Impact: 
Operational:
• 08/06/2020 - Pack – 15k singles across all pack lines, SCS – 9.2k singles;  
• 09/06/2020 - Pack 29.5k singles across all pack lines, SCS - 24.5k singles;
• 10/06/2020 - Pack 6k singles across pack lines, SCS – 3k singles, Inbound 25.6k singles capability loss.

Customer.COM: 128 orders missed promised delivery date

Recovery action: 
In summary, this issue was due to a change in process with the systemic “put away” of the Customer Beauty Bag.  This led to an increase in the beauty bag load entering into the box and carton storage system (BCS) at Donington. This increased the calculation and processing time which caused the backlog on the messaging queue – commonly known as pending events.  This in turn caused performance degradation.

To resolve the issue, IT teams stopped “putting away” the Beauty Bag systemically in the storage areas.  If an item is not systemically put away then it cannot be made available for sale, however the Beauty Bag was to be sold only from 11/06 so stopping the “put away” did not cause any impact.

Overnight 10/06 (Wednesday):  Teams started to slowly “put away” again but with a small load per unit. Due to the smaller load, the processing time returned to normal parameters.
 
Teams then monitored the receipt of the remainder of the Beauty Bags overnight on Wednesday 10/06 into Thursday 11/06 morning and no further issues were experienced. 
","Closed with Colin Approval, No Further actions",09/06/2020,09/06/2020,,,,,,,,Process gap,,"?
Change in process with how the Beauty Bag is systemically “put away” to make available for sale.",,,,24/06/2020,June,2020,,Closed,,,2020,6,June,9/16/2025,1925,>60
28/05/2020,10/01/2020,87504296,04:20,9:30,05:10,,C&H & Intl Supply Chain,,,,MI,Power issue impacting Stoke DC (C&H) Operations ,City FM,C&H,,WMS,RC identified,49117,"Around 4:50, MIM was made aware that Stoke DC was experiencing network issues. 
Impact:
Stoke DC (C&H) was unable to perform any operations such as Clustering, Ordering.? Internet and telephone services were unavailable during the outage. 

Recovery Actions: 
Once the issue was reported, initially it was thought to be a network issue and hence network (NOC) support was engaged.? Further investigations revealed that the issue was with the power being fed through two UPS’s (uninterrupted power supply).? Support teams therefore engaged a Ricoh Engineer (who support the UPS) to visit the site and their response target was 4 hours.? ?In the meantime, NOC team and the site electrical engineer tried to bypass the UPS from the power supply.? A decision was also made to engage Dale Power, manufacturers of the UPS as these UPS’s were still under warranty.? By 09:30 the Dale power engineer arrived at site and successfully bypassed the UPS’s and switched the power back to the mains supply. ?UPS’s are designed to only maintain power for a very short period of time, like during a blip. 
? 
Investigations by the Dale Power engineer found that during the night a possible massive power spike had rendered both UPS’s at Stoke inoperable. The UPS’s do have power surge handling capabilities but not if it is a massive power surge. Parts have now been ordered for both UPS’s and are expected next week. ?We have been reassured that Stoke have fully a functioning generators that will supply power in the event of a power cut/failure. ?Based on this, the incident is being resolved but will maintain close contact with Stoke and more specifically City-Holdings to ensure that UPS’s are repaired in a timely manner. 
","22/6: Naga confirmed to close the PR that since network team has also confirmed that they have all the documents in place will agree and close this by concluding that network team will deal with this issue themselves and do not need to depend on anyone in the DC for initial triaging/assistance.
16/6: Awaiting for Naga's response on the expectation from business on the below 
Is site SPOC ready to support us in an event of outage and at what level ?
Can Site SPOC be trusted with access to Network Cabinets or Comms rooms ?
Does Site SPOC need in person awareness training or just few documents with a remote session ?
------------------
13/4: Checked with Rajesh, he told he will liasie with Naga n Mustafa and come back soon
23/3: Had a meeting with Network team to track the progress on actions. In the call Mustafa confirmed that labelling is completed for all  DC cabinets raised few concerns on training like business need to confirm on it, cost including travel, access ( key as well as access cards), SPOCs who can be given training.
So , Rajesh to pick with Naga to address these concerns raised on training like funding and resource planning. 

8/3: Email sent
1/2: Meeting was scheduled and below actions were agreed  -ETA Feb end
1.	Switch cabinets to be labelled for all four DC's along with the location details. Provide adequate training to the site users to perform the basic kit checks (what lights are flashing on the switch etc) - Kamal, Paul & Rajesh to work with Naga
2.	Site Facilities/point of contact details to be shared with Kamal/Network team across different DCs to diagnose and troubleshoot the issues effectively - Naga
27/1: Meeting scheduled with Naga and Netwrok team to decide on how to proceed.
Update 19/10: Dropped reminder to Naga on the updates
----------
Supportability of UPS and Electrical equipment to be documented
-Update 20/10: [Naga] City is responsible for Floor mounted UPS at Donnington, Bradford, Sheffield ( Furniture ) and Stoke. Contract has been put in place for regular maintenance of these UPS.
All other DCs have rack mounted UPS which is supported by the TCS EUC team through Rico.
- City FM is responsible to support the UPS
- Naga working with C&H Logistics, Customer Property and City FM to ensure we have a proper contract in place to cover the UPS across all the DCs.

Power issue / UPS issue at the site - Who will engage City FM? IT Service Desk / the DC themselves?
-Update 20/10: [Naga] DC will engage City FM. They will call up the Service Desk for our visibility

Training for Electricians to troubleshoot UPS down scenarios across all the DCs
-Update 20/10: [Naga] City FM will engage the UPS service provider for the Floor Mounted UPS in the 4 DCs. In the other DCs, our EUC and Network team will have to show the DC team how to handle the UPS. I have sent an email to Kamal and Promode to start off the process in Welham, but have not received any reply.
City FM engineer was available at the DC but did not know who to handle the UPS / bypass the UPS – Naga to discuss with City FM

Alerting on UPS equipment 
Any alerting possible when the UPS goes down, so these can be pro-actively picked up and addressed 
-Update 20/10: There is no alerting on UPS
-Naga to check with City FM

DC should be instructed to report issues much more in detail
-Update 20/10: [Naga] The DCs have the process in place now.
For Eg: No power issues at the site, however, all the IT systems, phones are down etc.  
Can the Local IT check the comms cabinet and confirm if there is a loss of power?",10/01/2020,12/01/2020,,,,,,,,Design issue,," 
Major power surge at the DC causing both UPS’s to fail. ",,,,6/22/2021,June,2021,Dimple,Closed,,,2020,5,May,9/16/2025,1937,>60
25/05/2020,01/06/2020,87670519,,,,,Customer Channels,,,,MI,Contact centre agents experiencing voice drop outs on customer calls ,Capita,.COM,,Genesys IWS,RC identified,50501,"On Monday 25th May, contact centre agents reported that they were taking calls from customers where agents and customers were unable to hear each other..

Impact: 
• Poor customer experience
• Driving customers to webchat and email
• Only agents in SA were confirmed as affected, all UK based Capita agents were taking voice calls
• Chester and agents on Customer assets were unaffected

Recovery action: 
Anana were engaged but were unable to replicate the issue. They then identified a potential issue with one of their NetScalers. The NetScalers were rebooted but the issue persisted.
Contact centres agents continued to have issues on 26/05. It was confirmed that all the UK based agents and those on Customer assets were unaffected. Anana replicated the issue and confirmed it was only happening over 4G connections. All Capita Agents in South Africa (SA) are using 4G Dongles currently as they are working from home due to the COVID-19 lockdown.
 
It was confirmed that the underlying issue was a Citrix bug, however, it is unknown what triggered this on 25/05 as everything was working fine on 24/05. There were no changes made on Anana side.
 
On 27/05, Anana were able to make intermittent successful calls via 4G. In parallel, Anana set up few agents to receive calls directly to their mobiles. They continued to work with the option to route all calls to agents' mobiles but this was time consuming. Also, this solution would have incurred huge call costs and hence it was decided not to go ahead with this.
 
On 27/05 night while working with Citrix team, some network protocol changes were made which fixed the underlying issue and since then we have not seen the reoccurrence of the issue.
 
However, a lot of users did face different issues on 28/05 and 29/05 but all these were different underlying reasons e.g. Headset issues, Volume control etc. and had been there from day one of the solution which was implemented in a week to get the contact centres operating in the lockdown. This was further complicated by diverse nature of devices being used by the users like desktops, laptops, WYSE terminals and different headsets.
 
Since the equipment belongs to Capita, their IT support agreed to be the first line for these issues going in to the weekend and only raise with Anana if there was a common system issue impacting multiple users.
","Confirmation received from Jaye on 21/10 regarding RC and to close PR:

 This was due to local issues impacting the home working solution. A number of local IT processes were put in place to triage issues with users and service has been stable since. It can be closed .
Update: 17th Sep 2020
Jaye will provide the RC statement

Update: 14th July:  Capita were looking into whether they could enforce and rollout client settings on their estate to resolve. 
Network latency has been the biggest driving factor of issues investigated as this was causing a fall-back to TCP rather than UDP traffic. UDP was already supported and exposed. The workaround that Anana implemented to try and improve end user experience was to try and ensure UDP is negotiated, but again this depends on the client’s network.


Next Update :6th July
--------------
Final Root Cause:
Awaiting final root cause from Anana
----------------
Next Actions:
Set up alerting / business metrics to monitor the voice drop issues resulting in short duration calls – Anana

Investigate open / residual underlying citrix issues and provide a full root cause - Anana

Create a documented process for DR capability.  A robust monitoring and process to be put in place including creating User Guides, FAQs, Checklist, videos etc – Capita

Configuration and compatibility of the kits and devices (Headsets/Laptops etc) for remote working to be tested end to end before they are handed over to the users – Capita

Major incidents with cross portfolio impact should be driven by the Major Incident Management team – Operations

.COM Service Managers and MIM team working on the individual responsibilities and strategy for email comms and texts.


",01/06/2020,,,,,,,,,Code/Product bug,,"?

It has been confirmed that the issue occurred due to a known underlying citrix bug which was fixed on 27/05. However, the reason behind the occurrence of the issue on 25/05 is still unknown.
-------
Initial 
Root cause is currently unknown.  However, according to the current hypothesis, it is believed to be a combination of device/peripheral issues with the kits supplied to the Capita users in South Africa (addition of new users to the voice solution) in response to the Covid Lockdown along with an underlying citrix bug which was fixed by Anana on Wednesday, 27/05.",,,,,January,1900,,Closed,,,2020,5,May,9/16/2025,1940,>60
22/05/2020,23/05/2020,87667332/87668472,,,,,Platform & Store Ops,,,,SI,BOSS Performance and label Printing issues ,TCS,.COM,,.com BOSS,RC identified,50377,"The BOSS estate has been ramped up by 131 stores causing an increase in the overall traffic to the SPPD app. The ramped up traffic has caused the underlying database input/output (I/O) to hit a ceiling which has a domino effect on the SPPD app. 

Impact: 
·         Operational Overhead to stores for BOSS
·         Mispicks in stores - CFR of around 3% (usually 1%),

Recovery action: 
Support teams joined on a bridge to figure out what improvements can be introduced to the overall performance of the underlying infrastructure to alleviate the performance issue. CPU of the MongoDB was maxed out intermittently due to additional load. Store were advised to use the ""pen and paper"" method which means that the store users will have to note the picks in a piece of paper and head to Workstation to update it in the Web Application instead of Honeywell device. However, the web application was slow as well, throwing occasional errors. However, the store continued with the picks. An infrastructure upgrade on the database was performed overnight on 22/05. VMs for SPPD MongoDB instances were rebuilt to a M5 configuration, quite powerful compared to the previous VM instances. Since then we have not seen Performance issues with BOSS.
 
On the morning of 23/05, some BOSS stores reported that their labels are printing in very small text which is too small to read. All 131 BOSS stores that were added this week were impacted. Instead of printing on the label, stores applied a workaround by printing on an A4 paper and paste it on the parcel using tape. Support teams identified that some configuration are missing in the configuration file for newly added 131 stores.  Support teams then manually corrected the configurations to enable stores to be able to print properly.
","BOSS Performance Issue:
Resolution
Initially as a work around the store users are advised to use sppd web app to pick the orders which made the cancellation rate within the threshold.
As a first step of resolution,the infrastructure build up of each db's instance type has been increased to m3.2X Large .This enhanced the cpu's performance.The Api storeorder/department has been revisited and found even the dependency removal in joint queries of storeorder/department api will not fix the performance issue.Hence ,as the second step of Resolution, the Instance type of three MongoDB's instance has been upgraded to c3.8X Large which made the performance 5 times faster than the previous Instance type.
Printing label issue:
Resolution
The Emergency Deployment has been done on 23-05-2020 to fix the overriding issue in Zebra Property Configuration and added all old 81 stores and new 131 stores in Zebra printer config file.This fixed the parcel label issue.
",23/05/2020,23/05/2020,,,,,,,,Capacity constraint ,,"?

• Boss Performance issue: Capacity limitation where BOSS application was unable to scale up to the increased picks per store and new store addition.
• Printing Issue: Incorrect method of updating BOSS printing configurations in SPPD, causing them to be reverted after 10 minutes of successful updates, due to become effective post a server restart.  Server restart to uplift capacity on 22/05 caused the reverted config to become effective impacting printing in stores.
On 24/05, the team performed a proper and full deployment of SPPD application with the updated property files so that the stores are added to the property file and remain added. This had resolved the printing issue permanently.
",,,,26/05/2020,May,2020,,Closed,,,2020,5,May,9/16/2025,1943,>60
21/05/2020,22/05/2020,87666093,,,,Foods,Food Supply Chain,,,,MI,"Delay in Quantum batch impacting FFO, OTS, Quantum UI and Bradford NDC allocations ",TCS,Foods,,"FFO, Quantum UI,  Bradford Foods, OTS",RC identified,50372,"MIM was made aware that one of the foods critical jobs responsible for forecasts and order plans was over-running by 90 minutes where the average runtime of the job is 4 minutes.

Impact: 
Foods Final Order (FFO) completion 5:15 12:55
Quantum UI availability 7:00 15:05
Bradford Foods orders allocation 6:00 6:37
 OTS Reporting 6:00 12:40




Recovery action: 
The relevant teams were engaged to investigate. Support identified that one of the queries that was running was consuming heavy resources. Support killed the query and re-triggered the job however it did not progress. A decision was made to restart the Quantum database and re-trigger the job however the issue persisted. Support tried multiple recovery options with little progress. Suppliers were advised to use 28-day Order plan for FFO (Food Final Orders).
 
For Bradford NDC allocation, support had applied the ASO Backup allocation and orders interfaced into WMS by 06:37.
 
Support teams were joined by the lead architect at vendor Versata.  Continued investigations revealed that the newly created Ocado stores were pointed to DHNDC (a type of NDC in Quantum) in master data.  Depot graphs are systemic structures in Quantum which point stores to a particular depot. DHNDC is not a depot as it is a warehouse, this is why the Ocado store UPCs got into a loop.
 
The master data which highlights which depots the Ocado stores point to was changed from DHNDC to “null”. This allowed the batch to progress successfully without going into a loop.
 
FFO completed at 12:55. ASO was back to using live allocation since 11:28. OTS report got completed successfully at 12:40. The 28-day order plan was loaded, NDC allocations completed and the UI was up and available by 15:05.
 
Business Master Data team corrected the source for these 11 Ocado stores from Bradford to Enfield Depot as a permanent fix. In order to prevent further occurrence in future, a high priority Pager Duty alert was also set up in SCRD to highlight the incorrect data set up received from SAP.
 
Overnight Thursday(21/05): The Quantum batch got completed at 19:13 and the Bedworth frozen allocations were sent out of ASO at 19:55, 21/05. FFO, Quantum UI Availability, Bradford Foods orders allocation and OTS Reporting got completed at 01:32, 03:37, 02:29 and 04:52 22/05/2020 respectively.
","Update 24/9 - Joey cofnirmed
Happy to close – Thank you for seeing this through! Most appreciated!

Updated on 18/9/2020 
Waiting for confirmation from Joey
*Data validation related to store setup, can this be implemented in Quantum? -FSOR Dev Ops-Completed-Implemented in Dot Six release, this week. This is not related to just store setup. We have created a script to validate differences between all input files to Q.
*Enable correct logging and exception handling in Quantum? -FSOR Dev Ops-Completed-No further actions. Current logging is sufficient. No changes expected here.

*As part of Ocado Project, a lot of stores will be set up in SAP going forward, can the project changes be discussed and approved in CAB?
•  Foods BSP and SDMs to discuss with the Ocado Project team and comment further - Completed

Can we restrict the Business Master data team to make changes in SAP (i.e. Systematically restrict any changes that can cause issues related to the incorrect store set up etc)? - 
With the current observation, it is uncertain to add validation in SAP as the specific field is intended for both NDCs and RDCs. 
We have requested SAP & Foods Architects to share their thoughts in terms of adding validation considering the future scope of Foods supply chain solution. - (SAP Masterdata Team)- Adding the validations in SAP requires a complex change which also affects the current attribute of all Foods DC’s. If the fix requires in SAP, can this request place via SAP Demand route as it would require end-to-end analysis and testing to avoid any risks - Completed

*Is there is a quick way of restoring the Q Prod DB backup to the PT/test environment to perform debugging?<Oracle Support> - Snap restore is not possible in CATE environment.  From our previous experience, the prod RMAN backup restoration from Tape takes more than 24 hours as the DB size is almost 5 TB.However, an option to look at is creating a DB restore point which can be done by the Oracle Support team prior to performing debugging / making any changes in the production data.  If things go wrong at any point during the triage (i.e. making changes in prod as part of debug), the DB can be brought back to the point of restoration within 15 - 30 minutes based on the data change- Completed
--------------
Tactical Fix: 
The source NDC was removed, was made null for the 11 OCADO stores, rebuilt the systemic graphs and resubmitted the job which completed successfully.

Permanent Fix: 
The source system (SAP) and the Business Master Data team have been informed regarding this data issue for OCADO stores and the source for these 11 stores have been correct by the BSC Master Data team to DL (Enfield depot). The systemic graphs are created correctly and the overnight FFO batch completed without any issues.
",22/05/2020,22/05/2020,,,,,,,,Human error - Business,,"?

The Quantum job went into an infinite loop while processing UPCs related to 11 Ocado Stores.  These stores were incorrectly set up by the Business to be served by Bradford NDC rather than RDC (Foods Depot).",,,,,January,1900,,Closed,,,2020,5,May,9/16/2025,1944,>60
19/05/2020,19/05/2020,87663696,,,,,C&H & Intl Supply Chain,,,,SI,Intermittent issues while loading New SSI application ,TCS,C&H,,SSI,RC unknown,50363,"Around 08:21 AM, MIM was made aware that business users have reported an intermittent issue while loading the New SSI application.

Impact: 
Users were facing a performance issue while loading New SSI application during the time of the issue.




Recovery action: 
New SSI support confirmed that they could see some ""401"" errors while the application tried connecting to the IBT product services.  Support restarted the C&H product service pods (serving both IBT and New SSI application) followed by a restart of the new SSI application pods  which connect to the IBT product services. However, the issue persisted.  Upon further investigation, Support restarted the application PODs after which no errors were encountered.  Users confirmed that they were able to load  the application successfully from 10:20.
After confirmation from the product owner, support performed a clean restart of the application services by restarting the application pods and ignite pods (a temporary workspace in new SSI).
","as part of this RCA Support had a plan implement the in traceability between SSI and IBT  in either of below . 
 
a. Action taken ( IBT : enabled logs for stack trace, IBT enabled newrelic in Product Service) Completed
b. SSI need to add handshake ID to track request drop ( This required changes in Product Service) - Planned in Q2

Dropped Mail on the Deployment status
----------------
Since the changes involving in existing development approach and we have a JIRA (SSIR-9426) ticket to implement the changes from SSI end.
By implementing this changes we can able to identify If any calls dropped off between SSI and IBT.

Hopefully the changes will be deployed to prod mid of June.

---------------
a. Action taken ( IBT : enabled logs for stack trace, IBT enabled newrelic in Product Service)
b. SSI need to add handshake ID to track request drop( This required changes in Product Service)
",19/05/2020,26/05/2020,,,,,,,,RC Unknown,,"?

Support suspects that the issue was caused due to intermittent connection drops between SSI and IBT.  Detailed logging has been enabled on both SSI and IBT applications for further investigation.",,,,,January,1900,,Closed,,,2020,5,May,9/16/2025,1946,>60
15/05/2020,15/05/2020,87660318,,,,,C&H & Intl Supply Chain,,,,SI,Issue with WMS database serving Castle Donington and Ollerton ,TCS,C&H,,WMS,RC identified,50409,"Support received an alert that the WMS database instance which serves Castle Donington and Ollerton DC had gone down at 11:20

Impact: • Picking and packing  operations in Castle Donington were impacted until 12:15
• Ecom had lost 11K of capability
• No missed promises due to this issue.



Recovery action:Support brought up the problematic database instance, however application performance was degraded. A clean restart of database and application was agreed. Support restarted the data base and application successfully . The health checks were performed and then the WMS application was brought up after which DC operations were resumed. The application continues to remain stable since 12:15.","15/06 - Oracle confirmed this was due to a known bug in the existing DB version.  No fix available as the DB version is out of support. 
--------------
Next Steps:
Database team working along with Oracle vendor on the fix",15/05/2020,15/05/2020,,,,,,,,Code/Product bug,,"?

Issue was due to memory leak caused by a known bug in the Oracle Database.  A Sev-1 case has been raised with Oracle for further investigation.",,,,15/06/2020,June,2020,,Closed,,,2020,5,May,9/16/2025,1950,>60
05/05/2020,05/05/2020,87650043,,,,,Customer Channels,,,,MI,Issues with order placement for Ireland standard orders ,TCS,.COM,,Sterling,RC identified,50251,"It was identified that since 20:00, 04/05 the reserve inventory calls for Ireland standard orders were failing due to an out of stock response. 

Impact: • No standard Ireland orders were placed between 20:00 04/05 to 10:00 on 05/05
• For the same period between Sunday evening to Monday morning, 1130 orders were placed for STD Ireland orders
• Conversion dropped to under 1% between this period, previously this was trending between 4-5%  
• This has had no impact to order placement for Ireland next day delivery orders


Recovery action: Support identified that the reserve inventory calls were checking both UK & Ireland levels of services and since there was a mismatch – 12 days for UK and 4 for Ireland, the calls were failing. To restore services, the UK & Ireland levels of services were aligned to 10 days. Following these changes, the standard Ireland orders were being placed successfully since 10:00. Messaging on the website was updated to reflect the new proposition for Ireland. Investigations have attributed the cause of the issue to a defect in Sterling.","Next Steps:
Informed the Development team and they working to fix the functionality bug. 
Next Update: 31st May",05/05/2020,05/05/2020,,,,,,,,Customer Tech change,,"?

Sterling code defect - a dependency where the level of services calendar being used incorrectly for Order Reservation during the Checkout process for Ireland orders",,,,6/4/2020,June,2020,,Closed,,,2020,5,May,9/16/2025,1960,>60
23/04/2020,24/04/2020,87638660,,,,,Group Technology Services,,,,SI,Multiple BP sites hard down ,Vodafone,Vodafone,,BP Stores,RC identified,50096,"At 20:10 on 23/04, MIM was made aware that multiple BP stores were hard down. PCM advised that they had received alerts between 19:17 and 19:33 on 23/04 indicating that the multiple BP stores were down.

Impact: By 19:33 ,23/04 - 19 BP stores were impacted.  At 03:00 24/04, a load balancing job redistributes the hosting of the stores and after this job ran, further 73 stores were impacted.  These stores would not have been able to use any Customer equipment including workstations and Honeywells.  This would have prevented the stores from doing any CSSM stock activities and printing price tickets during the time of the issue.

Recovery action: MIM and network team liaised with vendors Hughes Networks and the BP service desk. Hughes and BP informed that they could not see any issues at their end and that they were not carrying out any maintenance that could have caused this issue. Network Team continued their investigations and confirmed that some of the BP stores serviced through the Swindon tunnels were unreachable. Therefore, the network team failed over the tunnels of the affected stores from Swindon to Stockley Park and all the affected stores were up from 07:09. After monitoring, support confirmed that the network connectivity remained stable. Network support and Vodafone have confirmed that the issue had occurred due to a routing issue at VF end. The BP stores will continue to be serviced through the Stockley tunnel till the routing issue at the Vodafone end is fixed. Vodafone is working on fixing the routing issue against a Sev-1 ticket.
 
To prevent further impact, support team stopped the job that sends shipment notification information to Global-E. Dev team performed a config fix by reverting a property in the Sterling admin console. This fixed the issue for ongoing orders however, shipment notifications that were held in Demandware needed to be corrected.
Support corrected and replayed all order updates (including shipment notification) to Demandware. Support replayed the messages in smaller batches followed by bigger batches validated by Demandware support. However, the messages in bigger batches did not reach Demandware. Demandware then triggered the held shipment notifications to Global-E.  Most of the messages had incorrect order line values and this caused additional cancellations raising the overall cancellations to 2540 orders. Support team had reconvened and confirmed that no further issues were reported.
","Update: 4/9/2020:

Smokeping has been installed successfully.Started to capturing latency.

Update on 26/08/2020 -  NOC installed application and it is running fine. As the /var capacity is low, unable to upload configuration. So, we are checking with build team for uplifting.
Next Steps:
15/06 -
Outstanding Actions:
 • NOC support to move the store tunnels back to Swindon - Kamal / NOC team-Completed
o A decision has been made to enable the VPN tunnel in Swindon at midnight tonight (this will be done via a change request).  The BP Store VPN tunnels will re-negotiate during the auto load balancing at 2 AM and the traffic will be re-balanced between Stockley and Swindon.  - Completed
• Check if automated alerting can be put in place for Memory Allocation issues on the routers / switches - Joe / Vodafone
o• Being investigated between our Hosting Engineers and Cisco.  Our Hosting Engineers have raised another incident ADV09811377 to add sensors to fetch details from the Nexus VDC’s.  As this event isn’t currently being monitored on our Nexus’s then we need to add the sensors to see if this provides a value to monitor to confirm if this event can be monitored moving forward. -Completed


• Move one of the Jump Server to SW Data Centre to check the connectivity from both SP/SW Data centres during any major issues - Kamal / NOC team
o There is already an existing VM (Jump Server) in SW Data Center - NOC team trying to get access to this VM - ETA Friday, 15th May - Completed
o Create a snapshot of the Smoke Ping Server at Swindon to monitor the ping latency from Swindon DC - ETA 26th Jun
--------------------
A TAC case was raised with Cisco on the back of the memory allocation errors.  Cisco advised that the Switch memory (8 GB) is fully utilized and recommend to upgrade the switch memory.  The switch needs a reboot after the memory upgrade is completed, however, in terms of outage, teams have confirmed that we will only see a blip as the services will fail over to secondary switch.
• Confirm with Cisco on the recommended increase in memory i.e. 32 GB or 64 GB - Joe / Vodafone
o Cisco has advised to increase the memory depending on the number of routes advertised.  Currently there are 15k routes being advertised which equates to 6 MB, so a decision has been made to increase the memory from 8 MB to 32 MB.
o The memory upgrade will take place tonight starting 23:00 on the secondary switch in Swindon Data Center and the switch will be rebooted.  There will be no outage as all the traffic is passing through the primary switch in Swindon.  Joe/VF will create a change request for this activity.

• Check the current memory utilization on the Stockley Nexus Switches as they also have 8 GB memory allocated - Joe / Vodafone
o There is a difference in set up between Stockley and Swindon Data Centers.
• Check with Cisco on why Stockley Nexus 7K Switch did not report memory errors when the new DIA VRF was configured in Stockley Park - Joe / Vodafone
o The memory utilization on the Stockley Nexus Switches is also at 6 MB.  Based on the current set up, there is no risk.  However, this will also need to be upgraded to 32 MB at a later point in time (probably will be done as part of project migrations)
• Check if automated alerting can be put in place for Memory Allocation issues on the routers / switches - Joe / Vodafone
o This is being looked into by the Vodafone monitoring team
• Move one of the Jump Server to SW Data Centre to check the connectivity from both SP/SW Data centres during any major issues - Kamal / NOC team
o Kamal working with his team to create a new VM (Jump Server) in SW Data Center - ETA is Friday, 01st May
• NOC support to move the store tunnels back to Swindon - TBC
o Decision will be made once the memory upgrade on both the primary and secondary switches in Swindon is completed - ETA is Thursday, 30th April

 
Background:
 
Change Description: Configure new DIA VRF on Swindon Nexus Edge switches in Swindon Data Center
Business Justification: Installation of new 10G DIA circuit as part of project Kona
• The above change was implemented by VF on the 24th March 2019.  However, this solution is not live and does not support any customer traffic.
• On Thursday, 24th April, Joe noticed that the BGP Peers on the new DIA VRF in Swindon were not active and requested his team to check the same (By design, the BGP Peers should be automatically active)
• VF Core Team activated the BGP Peers on the DIA VRF on 24/04 which led to memory exhaustion on the Vodafone Nexus 7K Switch in Swindon - Memory allocation errors were reported on the switch from 17:12
• NOC started receiving hard down alerts for BP stores starting from 17:18 on 24/04 and multiple alerts received at 19:17
",24/04/2020,24/04/2020,,,,,,,,Design issue,,"?

• Memory allocation errors were observed on the Vodafone Nexus 7k Switch in Swindon after the BGP Peers were activated on the 10G DIA VRF, due to these errors, the Vodafone Nexus 7K switch in Swindon could not handle the traffic and impacted the Swindon Dirty VRF1 causing issues with BP stores loosing network connectivity.  
• NOC Support failed over all the store tunnels from Swindon to Stockley to restore the services at 07:00 on Friday (24/04)
• As a temporary workaround, Vodafone have shut down the BGP peers on the DIA VRF at around 16:00 on Friday (24/04)



Memory allocation issues on the Vodafone primary Nexus 7k switch in Swindon which affected the Swindon VRF1 routes (BP Store IP Subnets).  A case has been raised with CISCO for further investigation",,,,,January,1900,,Closed,,,2020,4,April,9/16/2025,1972,>60
22/04/2020,23/04/2020,87637735,,,,,Customer Channels,,,,MI,International flagship orders cancelled due to incorrect stock levels sent to Global E ,TCS,.COM,,.COM - Sterling,RC identified,50090,"Approximately 1200 international orders were cancelled after despatch and customers refunded due to incorrect shipped quantity being sent in the despatch message.

Impact: 
• Poor customer experience and loss of revenue.
• Increase in calls to the Customer contact centre.
• 2540 orders impacted overall.


Recovery action: Global-E reported to the Demandware team about a sudden increase in orders getting cancelled due to order lines being sent as 0 instead of a valid number.  1436 orders were cancelled in Global E on 22nd April 2020.
After analysis, it was found that for a valid ASN coming from Donington, Sterling sent order line quantity of 0 to Demandware. Demandware sent this incorrect value to Global-E initiating Auto-Refund for orders in Global-E.
 
To prevent further impact, support team stopped the job that sends shipment notification information to Global-E. Dev team performed a config fix by reverting a property in the Sterling admin console. This fixed the issue for ongoing orders however, shipment notifications that were held in Demandware needed to be corrected.
Support corrected and replayed all order updates (including shipment notification) to Demandware. Support replayed the messages in smaller batches followed by bigger batches validated by Demandware support. However, the messages in bigger batches did not reach Demandware. Demandware then triggered the held shipment notifications to Global-E.  Most of the messages had incorrect order line values and this caused additional cancellations raising the overall cancellations to 2540 orders. Support team had reconvened and confirmed that no further issues were reported.
", Sterling has updated the property file and reverted the change back to decimal which fixed the issue. No further action required,23/04/2020,23/04/2020,,,,,,,,Customer Tech change,,"?
Issue occurred due to a code/configuration defect introduced in Sterling Release 2020.16 (CM4231) to support Stock In Transit changes.

RCA in Detail:
 The Advance shipment notification from sterling to International Demandware had the stock quantity ‘0’ instead of the actual stock for the order. There was a change as part of sterling release which has converted the stock quantity field from decimal into integer , this has caused this issue. ",CM4231,,,27/04/2020,April,2020,,Closed,,,2020,4,April,9/16/2025,1973,>60
15/04/2020,15/04/2020,87629648,,,,,C&H Commercial Trading,,,,SI,PLM - Performance degradation in the Line sheets feature ,PTC,PTC,,PLM,RC identified,50118,"At 13:12, MIM was made aware of multiple users facing latency while accessing the Linesheets feature in PLM. The Linesheets feature is used by users to view multiple products for a particular season together.

Impact: Some users experienced latency issues while accessing the `Linesheets’ feature in PLM until 18:40.

Recovery action: Investigations from PTC attributed the latency to indexes being missing from multiple tables in the PLM database. PTC support recommended that the indexes be created whilst the application was down. A 30-min outage was taken to perform the index creation activity. After the indexes were created, users confirmed service restoration.","No further updates
----------
RCA: 
• On 14th Apr Users in T15 Department reported that they could see few colorways in linesheet but couldnot see in details tab
• To rectify this issue PTC performed to recreate view in the linesheet by following an article which is created by R&D team
• Which in turn led to a PLM performance issue
• On investigating what went wrong to cause the performance issue they found Index creation steps were missed in the article which caused the Performance issue.
• So they recreated the missing indexes and brought the performance stable
• PTC or R&D team was not aware about the missing steps until this performance issue was raised.

Preventive Action:
They will either remove this article or update it to include ""index creation""
",15/04/2020,15/04/2020,,,,,,,,Process Gap,,"?
PTC support missed to create indexes during the recovery of another issue related to the Linesheet feature reported on 14/04. The steps to create indexes were missing in the Recovery doc (SOP).",,,,20/04/2020,April,2020,,Closed,,,2020,4,April,9/16/2025,1980,>60
06/04/2020,04/05/2020,87618121,,,,,Corporate,,,,SI,Employees with incorrect basic rate/hours ,SD Worx,SD Worx,,HRe (HR Payroll),RC identified,50012,"Customer Colleague Services reported that there were a number of employees with incorrect basic hourly rate for date effective 01/04/2020. There was also a combination of incorrect basic hours that were both too high and too low.

Impact: Pay impact to 800+ employees (HO and stores monthly paid)

Recovery action: • 39 advance payment employees and 5 overpaid employees were corrected
• Remaining employees impact was of smaller value
• SD Worx are waiting on confirmation from Customer on the reversal of the incorrect basic hours.
• Based on above confirmation SD Worx will determine if it is required to reverse the incorrect basic hours in the system and to monitor the employees who’s hours were adjusted to ensure May payroll will be accurate.
","Received Final RCA
-----------
Pending / Open action:
? We are currently waiting for Customer to advise on the way forward for the 800+ overpaid employees - Depending on response from Customer, SD Worx will then determine if below actions are required
o Reverse the incorrect basic hours from the system
o Monitor the employees for whom adjustments were done, and ensure the May payroll is accurate",04/05/2020,21/05/2020,,,,,,,,Design issue,,"?
Category 1: The basic hours impact for previous period was due to pay review activity. Whenever we have the pay review there are some data which are wrongly generated by the system for previous period. We have investigated the issue several times however we didn’t find anything in the code and the issue is not reproducible either. It may be related to the mass of records being processed during pay review. We already have BAU check in place which is run after every pay review activity. This time it wasn’t run as the pay review data was already reversed. We will make sure going forward the check is run after any reversal of pay review activity as well.
Category 2: The basic hours impact for the current period is related to a known issue and we already have an open problem ticket for it (PBI000000048543).
Employees received a rate change as part of pay review activity and accordingly the current rate was closed on the system. When reversal of the new rates weas done, the change in rate was to revert to the original rate for employees. Unfortunately, since the original rate was already closed on the system, no recalculation was triggered and this resulted into employees not having their actual rate being calculated.
Due to the nature of the issue, there is no proactive workaround for it and was being dealt on a case by case basic when incident was raised. The volume of incidents received for this scenario has been low as well. Due to the pay review activity and the reversal activity, we had many rate changes interfaced and hence had many impacted records. The script used to retrieve the impacted employees for the issue will now be run monthly to capture and correct impacted employees before the Live run.",,,,21/05/2020,May,2020,,Closed,,,2020,4,April,9/16/2025,1989,>60
04/04/2020,04/04/2020,87618521,,,,,C&H & Intl Supply Chain,,,,SI,WCS Automation services unavailable at Castle Donington ,SSI,C&H,,WCS ,RC unknown,49891,"At around 01:10, CD operations reported  an issue in stock movement. Investigations revealed a spike in pending events on WCS applications

Impact: WCS Automation services were unavailable at Castle Donington DC from 01:10 to 04:40 resulting in Capability loss. No CFR impact

Recovery action: Support investigated and confirmed that the spike in the number of pending events continued to increase despite the efforts to clear them. SSI vendor was engaged and they restarted several WCS application services after which the number of pending events began to reduce at 04:40. Automation and packing operations wsa resumed at DC. Pending events count got reduced to normal by  05:00.","5/3: Colin confirmed to close the PR as this Oracle clusterware patching is along term action.

1/12: Received Risk ID from Colin for Oracle Clusterware patch - 1040. To be Tested in Q4 (Jan- Mar 2021)

19th May - 
• Awaiting connection drop stats from Network team, compare the connection drops with the Packet Re-assembly failures observed on the Database
• OCW (Oracle Clusterware Patch) cannot be deployed on a short term and as the application testing cannot be done due to unavailability of test resources - James B (SSI) to have a separate discussion with Colin and Martin
• Network Recommendation of 'Adding extra uplinks to the Core' - Derek McGrath (Network Support) to share the details with Colin and take it forward
--------------

12/05  - 
• Monitor the Packet Re-assembly failures / drops this week and compare the stats - SSI and Network Support
• OCW patch has been applied in the PT environment, no issues observed from database perspective.  Testing to be carried out from Application perspective - James B / SSI Support
• Check if the 3rd Network Recommendation of 'Adding extra uplinks to the Core' can be implemented with the help of CD IT Support - Colin
----------------------

11/05 - 
Plans to implement the recommendations:
• Increase buffer access for these interfaces by adding a second queue set to be done on Monday 6AM by Network team - Completed on 04/05
• Kernel Upgrade Change will be implemented on Wednesday Morning 6 AM by SSI Team - kept on hold as requested by COlin
• Evenly load-share traffic across both uplinks to the Core will be implemented on next Monday by Network team (11th May, time 6AM) - Completed

--------------
22/04 - Oracle vendor found no issue in network layer and recommended Oracle cluster ware (OCW) patch
SSI support to share a plan of testing and to share the results
• OCW patch has caused an issue earlier in Sainsbury
• DB Backups - need to include a script to send emails when backups are finished

-------------
20/04 -  
1. Upload the screen output
2. Share the OS Watcher output in the archive folder (LSW netstat logs) for the past 48 hours
3. Test and apply the Oracle Cluster Ware Patch to both the database instances (home)

--------------------
16/04 - Action plans
1. Oracle vendor to provide instructions to collect additional logs to review the load/behaviour on the DB tonight
2. SSI Support to collect the logs and upload to Oracle vendor for further investigation

------------------
15/04 - Interconnect latency on the WCS DB and the DB instance evicted out of the cluster - currently being investigated by Oracle Vendor - Oracle have found a bug (21036841), that may have caused both the site becoming unresponsive and instance eviction. This is based on the evidence of seeing high cluster wait times and/or node eviction with high packet reassembles failure.

There are two possible workarounds for this:
- Enable jumbo frames (we have already discussed this on a previous occasion and it was decided not to go ahead with it)
- Increase the Kernel threshold for lower/higher to 15M/16M respectively. This can be done whilst system is running and doesn’t require a restart. Risk is low.

Check if there are any errors related to SAN Storage - No errors recorded
Analyse the  DB2 Disk I/O stats
WCS Application failover mechanism to connect to the active database without a restart
Recovery steps to be documented in case of re-occurrence for quick recovery
Process related to communication during the incident
Check with Oracle further on what basis/logs they have recommended the workaround
Work with WCS Infrastructure team to understand more on the issue, Share the infrastructure topology chart along with the logs with the Network team so end to end tracing can be done to rule out any N/W issues
Request Oracle vendor to join our RC meeting
Enable the cron job to capture the packet failure stats
",04/04/2020,,,,,,,,,RC Unknown,,"?
Initial issue was an interconnect latency on the WCS DB which caused DB2 to be evicted from the cluster and an application services restart was required. Further investigation on RCA still under way.",,,1040,3/5/2021,March,2021,,Closed,,,2020,4,April,9/16/2025,1991,>60
02/04/2020,02/04/2020,87616279,,,,,Group Technology Services,,,,SI,Multiple stores from Ireland unable to reach the Service Desk line ,Anana,Retail,,Service desk line - 185999 ,RC unknown,49889,"At around 15:00 on 01/04, 3 stores from Ireland reported issues with calling the 02087185999 service desk number. When any number from Ireland attempted to call the service desk numbers, they were presented with the IVR to choose an option. When the user chose an option, they heard a message stating “Transferring” followed by an error message stating “we have encountered a technical issue, please call back later” and the call automatically ended.

Impact: Multiple stores from Ireland were unable to reach the service desk lines

Recovery action: Mitel and Anana were engaged. Investigations revealed that the international number format that Gamma was receiving from Mitel was not acceptable and hence the call was being rejected.  Mitel and Gamma advised that no changes had been made on the route from Irish stores to the 02087185999 routes.
The first fix was to add a (+) to any store number greater than 11 digits e.g. +(353)123456789.  Test calls were made to Athlone store and the incident team believed that the issue was resolved. Further test calls revealed that any numbers with 8 digits after the 353 were still failing.  In Ireland, some store numbers have 8 digits after the 353 and some have 9 digits. Mitel then changed the rule to say “add +” to any number greater than ten digits. Stores were then asked to ring the service desk and tests were successful.
",,02/04/2020,09/04/2020,,,,,,,,RC Unknown,,"?
RCA is remains unknown due the the required logs were not found after the many investigations from Anana, Mitel & Gamma",,,,27/04/2020,April,2020,,Closed,,,2020,4,April,9/16/2025,1993,>60
31/03/2020,31/03/2020,87614222,,,,,Platform & Store Ops,,,,SI,Food Deliveries not showing in CSSM ,TCS,Retail,,CSSM,RC identified,49872,"Around 07:03, MIM was made aware that six stores were not able to receive food deliveries in CSSM.

Impact: Stores were unable to book in deliveries for the duration of the incident. No impact was incurred to the stock positions since store counting was blocked in light of the COVID situation.

Recovery action: Users confirmed that they had received the deliveries from depots, but were unable to book them in CSSM. Investigations revealed that the delivery queue reader service was in stopped status in one of the CSSM servers. Support restarted the delivery queue reader service to resolve the issue. Stores then confirmed that they could successfully receive deliveries in CSSM.","Did we  received alert for Qreader service when it is stopped state? - Yes 

• Lesson learnt – end to end recovery process and hyper care post recovery of any major issue. 
• Providing the complete KT to team on understanding the functionality & act on any major issue with out any delay.

---------
Detailed RCA:
• CSSM delivery queue reader service was stopped in MSHSRMSWEBP0206 and thus all the delivery messages was which was sent via this MQ server got  stuck up. All the delivery messages has been processed only  post restart of this service. Not all the stores was affected, since the service in MSHSRMSWEBP0207 server was up and running and hence the delivery messages which was stuck up in 0206 server was only affected. 
• This CSSM delivery queue reader service was stopped in WEBP0206 server as part of the recovery process for the CSSM delivery issue SEV 1 incident on 30th Mar night and was not started post completion of the recovery. 
• On the alert part – we have received alert for this queue pile to our team and we had some conflicts in the checking this alert with SOP we had , hence we missed to process the messages on time. 
",31/03/2020,02/04/2020,,,,,,,,Human Error,,"?
Human Error - The service responsible to process the deliveries into CSSM on one of the web servers was stopped for the recovery of another issue. However, the service was not restarted after recovery.",,,,07/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,1995,>60
31/03/2020,31/03/2020,87614998,,,,,Platform & Store Ops,,,,SI,Store users unable to make external calls ,Anana,Retail,,Stores - External calls,RC identified,49867,"At around 17:50, MIM were made aware that more than 10 stores were unable to make external calls from the store phones..

Impact: Store users were unable to make external calls.

Recovery action: Anana was engaged to investigate and they confirmed that all outbound calls from Customer stores to external numbers were impacted since the call credit threshold set by Anana for Customer had got breached. The threshold had breached three times on Tuesday, 31/03. On the first two occurrences, Anana increased the threshold to permit the call traffic. However, on the third instance which occurred at 17:50, Anana sought an approval from Customer to ensure that the call usage was not fraudulent. At 18:24, Anana increased the call credit threshold to 10 times the normal threshold. Stores were then able to make external calls and confirmed service restoration. Anana is sharing the usage stats on a daily basis which will aid us deciding when the normal call credit threshold can be reinstated.","A decision was made to add two further people, Laura Whittle & Rosemary Flynn, both Customer colleagues, to the email alerting list. We have, at the request of Customer, now set the current spend limit to £2,000 per day and £14,000 per week until further notice.
Anana have continued to monitor the spend on the service, providing Customer colleagues with daily total spend figures.",31/03/2020,07/04/2020,,,,,,,,Capacity constraint ,,"?
Calls made by store managers to manage the welfare of staff during COVID had caused the surge in the call volume. ",,,,07/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,1995,>60
30/03/2020,30/03/2020,87613060,,,,Foods,Food Supply Chain,,,,MI,"Quantum job failure impacting FFO, OTS and Quantum UI ",TCS,Foods,,"FFO, Quantum UI,  Bradford Foods, OTS",RC identified,49964,"At 00:42, MIM was made aware that one of the jobs PFSPACT280 which basically decides the inventory Model for the production location in Quantum batch had failed.

Impact: The SLA of the below critical flows have been impacted : 
Critical Flows SLA ETA Actual Completion
Foods Final Order (FFO) completion 5:15 6:20 5:30
Quantum UI availability 7:00 7:10 In progress
Bradford Foods orders allocation 6:00 7:00 5:43
 OTS Reporting 6:00 6:30 6:21





Recovery action: Support identified that the job had failed while processing the sales profile data which compares the data initially with 01:00 AM and later with 02:00 AM (mainly for 24/7 stores) to determine the effective sales profile. Today, whilst it compared with  01:00 AM for 29th March 2020, it failed as 01:00 AM was not existing in the jar file for 29th March 2020 due to clock change. Incident team had provided a temporary hot fix and re-ran the job to its successful completion. The Foods Final orders and Bradford orders completed at 05:30 and 05:43 respectively. The OTs reports were made available to the user by 06:21. Support is currently monitoring the Quantum UI batch which is expected to complete by 07:10
","Update: 29/10
The fix for FSORP-13454 went in on yesterday deployment as part of CRQ000000129178. PR is also closed, no further actions pending.

Next update 28th Oct - Release date change from 21st oct

Update 10/9:
Waiting for update from Promoth/Lakshmana
Next Update - Oct'2020 - fix for FSORP-13454 is prioritized for Q1 sprint4 backlog. Prod release version is yet to be finalized and will update once its scoped in the coming prod release.  

30June2020
--------------
06/04 - The dev work for the fix is prioritized to pick up in Q1 sprint and prod release is yet to be finalized. Also for your note, the fix would be mandated to deliver in prod well ahead of next march clock change and no underlying impact as of now due to this.
----------------
The permanent fix for this is identified and will be prioritised and delivered in a release before next year's March clock change. Details in FSORP-13454.
------------
Detailed Ananlysis:
We have analysed further and have identified the problem with the previous fix.

The fix provided earlier was to check if the CRC/processing date is a clock change date and if that date doesn't have the time 01:00 AM, it falls into a loop to check if sales profile's first hour is after 02:00 AM.
But the code to check and compare first hour with 01:00 AM still exists. 

Hence, if there is a data condition where first hour of the sales profile is before 02:00 AM, say 00:00 or 01:00 the code will fall into the third if loop and thereby error out.

Data condition is the only cause of this failure and the hard coded 01:00 AM in java is the culprit.
The sales profile are being cleansed by the data science team/users this year and we have been receiving complete sales profile for 24/7 stores. This is the closest possible reason for not hitting this issue in the previous years.

Please don't get the above misinterpreted - sales profile being cleansed is NOT the cause of the issue. We would have hit this failure anyway in future, good that we arrested it now.

Sample sales profile of a 24/7 store. The one we received on 20th starts at 11:00 AM but for the same store for 29th March its from 0000 i.e. from the store opening time (a cleaner sales profile).
",30/03/2020,30/03/2020,,,,,,,,Design issue,,"?
Root cause is attributed to an issue with the code logic (hardcoded timestamp) to check the sales profile for 24x7 stores. The issue was not observed previously as the store trading hourly profile did not start prior to 02:00 AM.",,,,29/10/2020,29/10/2020,#VALUE!,,Closed,,,2020,3,March,9/16/2025,1996,>60
30/03/2020,30/03/2020,87612780,,,,,Group Technology Services,,,,SI,Intermittent connectivity issues exhibited by Kubernetes applications ,"Microsoft
(120033023001095)",Cloud Platform,,SSI,RC unknown,49860,"At 08:30, MIM were made aware that New SSI users were encountering intermittent errors while accessing the “What-if” feature of the New SSI application. Investigations revealed that multiple Kubernetes applications were encountering intermittent issues while establishing a connection with databases hosted on the PaaS solution.

Impact: • Intermittent issues while accessing the “What-if” feature in New SSI between 07:00 and 08:18.
• No significant impact incurred by the remaining Kubernetes applications.

Recovery action: New SSI support performed a restart of the application pods however, the issue continued to persist. Further investigations led the incident team to believe that an underlying network issue was causing intermittent connectivity issues between the Kubernetes applications and the databases hosted on the PaaS solution. NOC team investigated and observed no abnormalities. The connectivity issues disappeared by 08:18 without any intervention. A Sev-1 case : 120033023001095 was raised with Microsoft to investigate the cause for the aforementioned issue.","14/04 - Update from Desh: We have taken the task in this sprint and executing in lower environment. Once tested we will put in production.
------------
Awaiting update from SSI team on the recomendation implementation
----------------
Update from Microsoft support team: PFB discussed points/suggestion,
• Microsoft (MS) and Network support team didn't see any network related issues after analysing the logs.
• Capacity issue will impact only the Virtual Machines which we trying to spin up newly. This doesn't impact any VMs in ""Running"" status.
• MS support team requested SSI team to implement SQL query recommendation and feedback further
",30/03/2020,06/04/2020,,,,,,,,RC Unknown,,"?
Microsoft checked the logs and confirmed no underlying network issues.  SQL query recommendations suggested by Microsoft have been implemented by SSI support and have not seen a re-occurrence of the issue.",,,,20/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,1996,>60
29/03/2020,29/03/2020,87611967,,,,,Data,,,,MI,Power BI application unavailable ,"Microsoft
(120032926000010)",EDW,,Power BI,RC identified,49962,"Around 03:30, MIM identified that the Power BI application was not working and users received an error, ""Something went wrong, please try again later."" while logging in.

Impact: • Power BI application was unavailable from 03:30 to 10:10. Users were unable to access the analytical Power BI reports during the time window.
• Majority of the Power BI dashboards were inaccessible. However, a workaround was available to access few selected dashboards (FFO, AnR, Trailer, Bradford) through the spotfire links.




Recovery action: Support suspected a central issue at Microsoft and a sev1 case 120032926000010 was raised with Microsoft. Microsoft confirmed that services are unavailable due to a migration activity performed for one tenant that catered Power BI services to Customer.
At 10:10, users confirmed service restoration. Support validated and confirmed service availability. Microsoft further confirmed successful completion of the migration activity
","Alert has been set up for the power Bi related updates in O365
----------------
For single tenant migrations, it will be a notification on office 365 portal.

PowerBI Devops + ERS team need to keep an eye out for this.

--------------
Team is picking up with the MS Account manager to understand the agreement they have in place for such migrations

Expecting Detailed RCA from MS",29/03/2020,29/03/2020,,,,,,,,3rd party change,,"?
Power BI service was unavailable due to a planned maintenance activity performed by Microsoft which was not communicated to Customer.",120032926000010,,,07/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,1997,>60
26/03/2020,27/03/2020,87610081,,,,,Group Technology Services,,,,MI,Vodafone connectivity issue ,Vodafone,Vodafone,,"Stores, ASO, SCRD",RC identified,49960,"At 15:51, MIM were made aware that card payments were failing in multiple stores. Investigations revealed that a Vodafone connectivity issue between 15:25 and 15:50 had caused these failures.

Impact: • Connectivity between GIST and Customer was impacted.
• Store connectivity to on-prem applications was impacted.
• Card payments were failing in multiple stores during the below timings:
o 1st Occurrence - 15:25 to 15:50 , 26/03
o 2nd Occurrence – 19:35 to 19:46, 26/03
o 3rd Occurrence – 01:30 to 01:33, 27/03



Recovery action: A sev-1 case was raised with Vodafone to understand the cause for the connectivity issues. Investigations by the Vodafone engineers revealed that a core network issue at MPLS (Multi-Protocol Label Switching) had caused the connectivity failures. In order to fix the issue, Vodafone secured an emergency outage for upgrading the JUNOS operating system on the routing engines. This upgrade activity got completed at 4:00 AM and card payments in stores were stable since.
","The Vendor (Juniper) have confirmed root-caused the reason for the RPD crash after looking in all the core files.
The route cause is a race condition when during BGP route withdraws/updates with multipath/damping enabled , BGP ends up on not finding the correct gateway for the route.
The recommended action by the vendor under PR 1472671 was to upgrade the Routing Engine’s from 16G to 64G + and update the JUNOs from code from 17.4R1-S6.1 to 19.2R1-S4.3.
This was performed under the Emergency Outage on the 27th March between 01:00 – 01:35 and service has remained stable since.",27/03/2020,03/04/2020,,,,,,,,3rd party issue,,"?
Intermittent flaps on a Vodafone hosted PE router impacted the store network connectivity. Awaiting detailed root cause from Vodafone.",,,,15/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,2000,>60
26/03/2020,26/03/2020,87609260,,,,Foods,Food Supply Chain,,,,MI,Delay in Bradford Foods Orders ,TCS,C&H,,Bradford DC allocation,RC identified,49953,"The Bradford orders were successfully processed into WMS by 2:55. At 5:30 however, Bradford Foods DC reported that they were unable to allocate orders.

Impact: 4502 orders got allocated with a delay of 50 minutes.


Recovery action:At 05:30, Bradford Foods DC reported that they were unable to allocate the overnight orders. Support confirmed that the orders had processed into WMS by 02:55. Support identified that order grouping was not performed which had caused the orders to remain unallocated. As a workaround, support performed the order grouping for 4502 orders and confirmed that orders were successfully allocated by 6:50.
 
","Next Action: SOP's has been prepared and shared it with the team

Detailed RCA: As part of today`s POC to deliver for additional 50 stores we need to do a configuration in Bradford WMS.
Usually the request will be to replace an existing store with the new store for which we have got an SOP. This time users asked not to replace any existing stores and asked for new wave plan(Order Grouping).
Since the count was huge and time was less as they need the same within a day, we have configured these new stores from backend. One of the consignment prefix field should have max of 12 characters(WAVE131---11) instead updated with 13 characters(WAVE131----11) which has blocked the entire order grouping and in turn the allocation of today`s orders.
 Once the issue was identified, the same was corrected and Order grouping was performed again.",26/03/2020,26/03/2020,,,,,,,,Human error - Business,,"
A human error occurred whilst configuring 50 additional stores in Bradford WMS (as requested by the Business).  This resulted in the Order Grouping functionality being blocked and hence delayed Order Allocation.
?
",,,,30/03/2020,March,2020,,Closed,,,2020,3,March,9/16/2025,2000,>60
18/03/2020,20/03/2020,87600831,,,,,Customer Channels,,,,MI,"Contact centre agents experiencing slowness in IWS, webchats not being offered ",Anana,.COM,,Genesys IWS,RC identified,49815,"At around 12:00 PM on 18/03, agents across all contact centres were experiencing latency in IWS. Webchats were not being offered and agents were unable to handle emails. Voice calls were coming in but agents had difficulties in accepting calls via IWS.

Impact: Poor customer experience, impact to contact centre operations, customer emails sent by .Com were paused while the incident was ongoing. The email issue however got fixed in the evening of 18/03.


Recovery action:Anana advised that they had a SAN storage failure affecting their UCS database. At 12:50 Anana confirmed that they had moved the UCS database onto their backup SAN. They could observe that service was restored but were still seeing a few intermittent issues from some agents. Users had identified an issue with resetting passwords which later was resolved. Anana carried out a database restore to the production SAN overnight.
 
19/03 
The incident team observed a few isolated issues with emails coming through blank but these issues were resolved. Over the course of the day the .Com business sent 11 million customer emails and no degradation was observed.
 
20/03 
The database was successfully moved to the new SAN overnight and the incident team experienced no service issues on Friday, 20/03.

","Update from Guy on 25/9:
This action has still to be completed as we are awaiting access to the data centres.  This work is not deemed essential maintenance and is therefore delayed due to the Covi-19 pandemic- Completed- Alex Threlfall (infrastructure consultant at Anana) has confirmed the decommissioning of the SAN arrays has now completed. Action can now be closed
Update: 17th Sep 2020
Jaye confirmed via Peak PM Analysis - MI / TTBAO - .Com Contact Center Incidents call that outstanding actions on this issue is completed
-------------
08/06 - Dropped Mail to Guy/Team for latest updates
--------------
Decommission of the Legacy SAN arrays. 
(Target date considering possible government restrictions on movement due to COVID-19)
ETA - 31st May 
",20/03/2020,24/03/2020,,,,,,,,Infrastructure issue / Hardware failure,,"?
“The root cause of the Incident was the failure of the single disk which threw an exception error that the storage controllers could not recover from. The unavailability of the SAN management interface hampered the engineer’s ability to manage the SAN following the disk failure which ultimately made it impossible to prevent the catastrophic failure.”

UCS (Universal Contact Server) database was unavailable due to a SAN storage failure at Anana.  Awaiting detailed root cause on the reason for SAN Storage failure.


",,,,,January,1900,,Closed,,,2020,3,March,9/16/2025,2008,>60
10/03/2020,10/03/2020,87590271,,,,,Group Technology Services,,,,SI,Delay in EDW Critical Flows due to multiple over-running jobs,TCS,IS - Database,," SWAT/FAQT, GM4",RC unknown,49756,"At 00:43, MIM was made aware that one of the jobs in overnight EDW flows was over-running. The runtime of the job normally averages 58 minutes, however in this case it ran for over 3 hours. 

Impact: • The EDW SWAT/FAQT reports were available with latest data by 07:30. The SLA is 07:00
• The EDW GM4 reports were available to the users by 09:21. The SLA is 08:00


Recovery action: At 00:43, MIM was made aware that one of the jobs in overnight EDW flows was over-running. The runtime of the job normally averages 58 minutes, however in this case it ran for over 3 hours. 
The over-run had a cascading delay on all the EDW critical flows. The availability timings of the impacted reports are shown above in the Impact section.

",01/04 - RCA identified,10/03/2020,01/04/2020,,,,,,,,Human error - Customer Tech,,"?
DB2 client tracing was turned ON on the Data Stage nodes on 6th March on the back of IIB DB2 Upgrade Root Cause investigations. However, this was not turned off on the DataStage Nodes hlxp0ds006 and hlxp0ds007 and the jobs running on these nodes were exhibiting slow performance. The traces were turned off and its back to BAU


",,,,07/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,2016,>60
08/03/2020,09/03/2020,87586543,,,,,Corporate,,,,MI,Peoplesoft CRM application unavailable ,TCS,IS - Wintel,,PeopleSoft CRM,RC unknown,49679,"At around 08:59, MIM was made aware that all the Peoplesoft CRM servers (7 servers in total) are down and have lost the C- drive after the Windows patching activity on the servers which was done between 04:20 to 06:30 on 08/03.

Impact:
The Peoplesoft CRM application is unavailable since 08:00. There will be a delay in the automated ticket creation process against the user related emails coming to the HRSS mailboxes(36 mailboxes in total).




Recovery action:
The Peoplesoft CRM application is unavailable from 08:00 after the windows patching activity performed overnight under the CRQ#119156. Support observed that the ‘C’ drive was lost on all the 7 affected servers (Application, DB, batch, web). As per recovery plan, support completed OS installation and C drive restoration from 8th March backup on one of the 7 servers. Support informed the servers were crashing when being rebooted after the server restore. C-Drive and system state restoration failed again when tried to restore from 07th March backup. Support was able to complete C-drive and system state restoration from backup taken on 06th March.
 
Upon further investigation support identified a problematic file in the batch server. These files were removed and the restoration was re-triggered . However the issue continued to persist. Support observed that the TSM fix version from which we were trying to restore was a bug version 7.1.6.3. Support upgraded the TSM fix version to 7.1.6.5. However the restoration has failed again after the upgrade. Support has escalated the IBM case to Sev1 and awaiting update.
 
Support further identified that Buffer parameter on the TSM client on the server was value was 32Kb, which was the default value. As part of troubleshooting, the parameter value was changed to 1023Kb, after which system state restoration completed on one of the seven servers. The server was restarted and added back to the domain successfully. Filesystem validation performed on the server was Green.
Support proceeded with, system rebuild, C-Drive and System state restoration for the remaining 6 servers and got completed successfully. Servers were restarted and added back to the domain. Filesystem checks were also completed on all the servers.
Support identified that the Oracle services are up on the database server. However, database services did not come up due to drive name mismatch on the DB server and the same was fixed by remapping the drives. Support manually started the database and health checks were completed successfully.
Support teams have brought up one of the two application servers and web servers. Support has identified that the webserver is unable to establish connectivity with the app server and are suspecting issues with the Oracle client on the application server. Support teams are in process of restarting the application servers
","Update from Ravi on the RCA Investigations:
• Non-existence of Sep-19 bundle can’t be a cause we believe.  If we miss 1 cycle,  successive bundle will be deployed on next month as its cumulative in nature.  Previous bundle won’t be deployed again
• We suspect that legacy vulnerability patches and its dependencies to be a potential cause for the server crash on FSI Swindon, Infra few Exch. servers & PeopleSoft production servers
• Since we don’t have the vendor support for Win 2008 / 2008 R2, our theory above couldn’t be validated with Microsoft
• Team tried to replicate the issue on cloned copy of rebuilt PeopleSoft machine but they couldn’t and we didn’t face the server crash issue.  Though we rebuilt and restored from backup, we can’t ascertain fully that rebuilt machine and crashed machine are like-to-like.  Rebuilt is one reason, 6th backup restored was another reason.
• Associate who performed the patching is confident that same procedure was followed on PeopleSoft prod similar to other Window 2008 / 2008 R2 servers

-------------
Why were OS health checks missed after the patching activity?
Meeting scheduled tomorrow with the teams to review the steps in the patching activity - Elizabeth - Elizabeth' Request Wintel Team for an update
Why did the PeopleSoft CRM servers did not come up after the Windows Security Patching?
 - Root cause currently unknown, no vendor support as 2008 and 2008 R2 servers are out of support.  Support trying to replicate the issue in the lower environment - Dharma / Wintel Team - Next Update MID of this Week

Did we encounter a similar issue with any other servers after the Windows Security Patching? 
- Yes, a similar issue was encountered last month (Feb 2020) after the Windows Security Patches were applied on one of the secondary FSI servers (Windows 2008).  
- A new server was built and the backup taken prior to the patching activity was restored to recover the situation, no business impact as this was a secondary server.

Why was the backup restoration from 8th March not successful? 
- Check if the backup taken on 8th March is corrupted and also validate if they are of the same size as the backups taken on 6th and 7th March - Sailesh / TSM Support - Team have checked for all the 3 days and there is no change in the backup size

Why did the backup restoration from 6th March & 7th March have problems? 
- What is the default buffer size setting? and check if the buffer size setting needs to be changed on any other servers? - Sailesh / TSM Support  - The default Buffer Size is 32 KB and the maximum is 1023 KB. This issue occurs when the metadata size of the object being restored is greater than 32 KB and it can be different from file to file.
Hence, there is no need to change the settings in other clients, the configuration file can be edified as and when required. 
 
IBM has confirmed that this is a BUG in the current TSM server version and increasing the Buffer size in client is a workaround to mitigate the TCPIP issues during the System state restore

Why did the DB server loose connectivity / trust relationship with Active Directory? 
- Currently Unknown, however, a script is now in place which will notify the Wintel team if the computer account is disabled

Why was the DNS entry missing on the DB server? 
- Currently Unknown, no evidence in the DNS logs.  Static IP has been assigned to the DB server which can be deleted only manually and not via any automated script.

Disable the port 1521 on the Windows firewall of the DB server - Wintel team to perform the activity under a planned change - Wintel team will check with People soft support by this week to raise a change 
",09/03/2020,13/03/2020,,,,,,,,Customer Tech change,CR,"?
RCA Unknown
 legacy vulnerability patches and its dependencies to be a potential cause for the server crash

",119156,,,30/03/2020,March,2020,,Closed,,,2020,3,March,9/16/2025,2018,>60
08/03/2020,08/03/2020,87586240,,,,,Group Technology Services,,,,SI,Stores users unable to accept deliveries from GIST ,TCS,IS-Middleware,,CSSM,RC identified,49759,"At around 08:04, support reported that  systematic information for I1006 was unavailable in CSSM since 05:00 AM, after the completion of the Wintel patching activity (CRQ118955) on two CSSM application servers.

Impact:
• Store colleagues were unable to accept deliveries from GIST
• GIST received 139 calls between 07:45 and 08:30 and are citing this as the cause

Recovery action:At around 08:04, support reported that  systematic information for I1006 was unavailable in CSSM since 05:00 AM, after completion of the Wintel patching activity (CRQ118955) on two CSSM application servers. Investigations revealed that the message gateways on both CSSM servers were in ‘retrying’ state. In order to fix the issue, support restarted the message gateways and confirmed that the message pileups got cleared by 08:37. Store colleagues validated the services and confirmed complete service restoration.
","IODM Plan 
- Include steps in the IODM Plan to validate the channels on the IIB Gateway and CSSM MQ Server respectively - Completed
Alert Suppression
- Provide clear instructions to PCM on suppressing alerts (specify the application component and/or the server name on which alerts should be suppressed) - Completed
Application Health Checks
- Prioritize key application health checks in the SOP / Checklist after the patching activity to ensure the key application functionalities are checked and any issues identified can be immediately addressed - Bharath / CSSM Support - Completed

MIM Engagement 
- MIM should be engaged immediately after the outage window if things are not working as expected - Support teams and SDMs - Completed


-----------
11/03 - IBM is still analysing the logs and and awaiting further updates.
-----------
09/03 - Update from Middleware Admin : A case #TS003452388 has been raised with IBM. Admin team have shared all required logs and details. Currently, we are awaiting response from IBM.",08/03/2020,09/03/2020,,,,,,,,Process Gap,CR,"?
The CSSM cluster sender channels on the IIB Gateway went into re-trying state and did not recover automatically after the CSSM MQ server was brought up on completion of Window Patching Activity.  The CSSM Cluster channels on the IIB Gateway were restarted to restore the services.

There was a separate issue with Channel Initiator process that went down on the IIB Gateway, support identified and recovered all the message pile ups in the transmission queues for few consumers except CSSM, as the Queue Manager on the CSSM server was brought down as part of patching activity.  However, once the QM on the CSSM server was brought up after the patching activity, the CSSM Cluster channels on the IIB Gateway did recover automatically but went into re-trying state. 
.


",118955,,,17/03/2020,March,2020,,Closed,,,2020,3,March,9/16/2025,2018,>60
06/03/2020,06/03/2020,87584904,,,,,Customer Channels,,,,MI,Delay in .com sale launch due to nightly job failure ,TCS,.COM,,WCS ,RC identified,49675,"One of the WCS nightly jobs failed after running for 3 hours. It was re-run and completed successfully but the delay had a knock-on effect to the remaining WCS jobs and the successor Bloomreach jobs

Impact:
• Delay in sale launch for .com
• Sale promotion emails were held and sent once processing was complete
• Android and IOS launch had to be held to align with desktop




Recovery action:
The WCS nightly jobs completed successfully at 08:30.The Bloomreach jobs which populate the prices on Product Listing Pages (PLP) and Search Results Pages (SRP) started at 08:37. After the job completion, it was indexed by Bloomreach at 10:26. The cache for the Bloomreach API was cleared approximately at 10:40. The  PLP & SRP cache clear was triggered at 11:08 and got completed by 11:24 after which, the sale prices were available across the website.
The teams validated and confirmed that the correct sale content was reflecting on all of the relevant pages.  Sales emails were sent out once the validations were completed by 11:45.
",10/03 - Update from .COM : Alert set up was done based on number of promotions modified during the day and then checking in management center regarding whether these are site wide or specific category level promotions.,06/03/2020,06/03/2020,,,,,,,,Process Gap,,"?
The increase in run-time of the nightly job can be attributed to a modification of a set of promotions. This caused an enormous increase in the volume of data which needed to be processed by the nightly jobs.


",,,,06/03/2020,March,2020,,Closed,,,2020,3,March,9/16/2025,2020,>60
03/03/2020,03/03/2020,87579865,,,,,Group Technology Services,,,,MI,Message failures observed after the IIB middleware database upgrade ,TCS,IS - Database,,"ORCA, WMS, CSSM, ASO, GIST, IRP, PLM",RC unknown,49726,"At 08:30, MIM was made aware that messages were failing in the middleware layer after the IIB Middleware DB upgrade (CRQ 115167) was performed.

Impact:
Please note that the impact below includes the planned outage from 05:00. The incident impact can be assumed to start from 08:30. The impacted services below were restored by 14:15.
 Dotcom
• .COM Orders (both UK and Intl) were not sent to WMS since 05:00
• .Com flower and hamper orders were not sent to suppliers since 05:00
• B2B order invoices were not flowing through – also impacting finance.
• New inventory and product updates did not get reflected on the website
• .COM food orders did not flow in to ORCA since 05:00
• POS order retrieval was not available for .COM orders since 05:00
• Global-E were not receiving despatch notifications from WMS
• Digital Receipts were not sent out to customers
Retail
• Store delivery into CSSM was impacted for both C&H and Foods – systemically. The stock was however physically reaching the stores.
• Store counting was centrally blocked for stores till about 14:15. The counting for 63 stores (Franchise and UK Main) was unblocked by 17:53.
• Retail Dashboard in day model was not available
• Top-up APP wasn’t updated with the real time sales therefore stores were unable to use the APP
Foods:
• ASO to Q flow was impacted since 05:00 and depots were in backup from approximately 09:30 – 15:53. The depots were already in backup from 05:00 to 08:30 as part of the planned change.
• Bradford to GIST despatches did not flow through to GIST for the duration of the incident.
• MOS (Movement of Stock)/VAMOS did not receive any messages since 05:00
• GIST did not receive any MOS ASNs from suppliers for the duration of the incident.
• CXM did not receive any articles, contract and PO messages from SAP
• Any new IFOS international orders did not interface into JDA despatcher from Bradford 
C&H
• Orders to ediTRACK and ASN’s from ediTRACK were not flowing since 05:00
• SAP Orders were not being sent to TMS since 05:00
• All stock movements to and from C&H DC's were impacted since 05:00
• SSI purchases orders created since 05:00 were not available in SAP
• No interfaces were running between PLM, Range Planner and SAP – therefore any update done on PLM was not be visible in Range Planner and SAP
• Any DC or store transport plan updates on TMS were not be sent to WMS
International:
• IBT changes were not reflected in IRP
• PLM interface messages (Hourly: I0568 - Adopted Colour Way, I0849 - Planned cost price, 15 mins: I0599 - Phases) were not processed to C&H Enterprise layer (Cloud) and in turn products & updates were not be available in IBT.




Recovery action:
Messages were failing in the middleware layer after the IIB Middleware DB upgrade from v10.5 to 11.1 was performed against CR # 115167. The incident team decided to revert the change to restore services. In the interest of expediting the resolution, the incident team reverted the change only on the primary IIB DB after securing an emergency outage from the BSPs. The stand-by IIB DB is still running on version 11.1 and will be downgraded at a later point in time. The middleware services were brought up by 12:50, after successful reversion and the messages started flowing through successfully. The impacted services were back to BAU by 15:53.","High Availability DR solution being explored for IIB DB2. Currently in the Operations backlog.

No risk at the minute in not having the database upgraded. However we go into extended support at the end of April. Cost for this is being worked out, if we don’t pay for extended support the risk is we can’t get bug fixes for new issues or any new patches. We can still raise PMRs and will be entitled to pre-existing patches. 
-----------
Update from Sandipan - Including this for now, in our operational backlog. Plan to be worked upon
--------
26/03 - Update from deepak - As discussed with Rajasekar, as per the current Integration operating model, engagement and ownership (including funding) from portfolios is required for integration delivery (including such operational initiatives). 
 
In this case, it would seem to make sense for the Operations leadership team (e.g. Rajesh) to drive this; discussing the initiative (including priority) with the I&ES Portfolio TSL (Komathi, cc’d here) or HOTS (Andy Neilson) and agree the approach and budgetary considerations, to take it forward. 

------------
Next Steps:
@Purohit, Harish Kumar / @Venkatesan, Thirucam -have to share the latest on the discussion with Glen and James Blackburn on the design changes / alternate approach to proceed with the DB2 upgrades.

-------------
We tried to replicate the issue on our end by building a new server in production with OS version 7.3 and then upgrading the DB from 10.5 to 11.1.  However, after the DB upgrade we could not replicate the the issue at our end as the IIB message processing was successful and no failures were observed related to Date / Timestamp datatype.

With respect to the DataStage,  job failures were observed after the DB upgrade, however, IBM checked the traces and confirmed that the errors captured in the traces are not related to the Date / Timestamp datatype issue and that this might be a different issue altogether.  In parallel, DataStage/Admin team identified a workaround (binding activity) after which the DS jobs have been re-run successfully.

IBM does not have any further inputs / recommendation as we were unable to replicate the issue.  




IBM to confirm the RCA after have the Brainstrom Sessions after we provide the requested Logs

6/03  - Meeting with IBM and Rocket Software to understand more on their findings.
IBM Will be having the internal brain storming session on Monday 
From Customer end we are building a new server to replicate the issue with upgrade on our end
will be able to supply the logs to IBM only after we do the upgrade and testing on our side which is scheduled today

05/03 – IBM advised that their vendor (Rocket Software) is able to reproduce the problem and requested us to replicate the issue in the lower environment.
The mapping to ODBC 3.0 for Timestamp data type seem to be missing in Db2 CLI driver.  A workaround would be to set Environment attribute SQL_ATTR_ODBC_VERSION to SQL_OV_ODBC3 in the application SQL.
Support confirmed that the issue cannot be replicated like to like in the non-prod environment as the non-prod database servers are now running with the OS version RHEL 7.7, the OS version can be downgraded to 7.5 and not 7.3 as no back up is currently available.  Cloning the production servers online is not possible without server downtime.
 
Meeting scheduled tomorrow, 06/03 at 12 PM with IBM and Rocket Software to understand more on their findings.


03/03 - DB2 and IIB Sev-1 PMRs raised with IBM.  A Sev-1 case raised with RedHat as well.  Support working with IBM to investigate the root cause and all the relevant logs have been uploaded to the PMR for their analysis.",03/03/2020,10/03/2020,,,,,,,,Customer Tech change,,"?
Unable to replicate the issue so RCA is currently unknown



",,,,20/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,2023,>60
01/03/2020,02/03/2020,87577470,,,,,Group Technology Services,,,,MI,Delay to the overnight EDW critical flows ,TCS,IS - Database,,"SWAT, POS",RC identified,49648,"At 15:09, MIM was informed that the job EDWANRH180 in Weekly GM4 reporting was overrunning.

Impact:
• The 7:00 AM SLA for SWAT reports was missed (see below)
• The SLA for EDW POS Reporting SLA was missed (see below)
• Delay in completion of EDW Day -1 and EDW Cube Reports
• EDW DR and PROD databases were not in sync from 06:30 to 14:31




Recovery action:
support verified and confirmed that EDW jobs with upstage command were not progressing and were in stuck state. Sev-1 case was raised with IBM for further investigation. Support along with IBM identified issues in the GPFS layer on the standby DB node huxp007. Problematic node was rebooted and the GPFS issue was cleared after which the two problematic jobs completed successfully. 
 
Further issues were reported with EDW jobs and Golden Reports as support observed that they were stuck. Support could not execute some commands on the DB. Support identified issues with partition 17 on the database.  As advised by IBM, support stopped and started the DB services after which the DB came up and the health checks were Green. 
 
The jobs were released in a controlled manner. Support confirmed that all 84 Golden reports were refreshed successfully at 04:08 (SLA: 06:00) after validation. Support restarted all the application servers.
 
BI SWAT flow in Control M completed successfully at 08:17 AM and the report refresh completed at 09:03. EDW Support confirmed that the delay in MQT processing was due to the execution of the jobs outside the scheduled window and due to high Monday morning load. 

EDW POS and Day -1 reporting flows completed at 11:40 and 13:21 respectively.  EDW Cube ETL completed at 13:05 and the report refresh completed at 14:01. Support confirmed that the EDW DR and PROD databases are in sync since 14:31.
","As per IBM case, some (FODC hung) logs should be collected during the hung in future to make the job easy for them to identify the RCA. So team is aware of what are all the logs to be collected if it re-occurs ( hopefully not for upstage) in future. 
So why the EDW jobs with upstage commands are dependent on the standby node Huxp007 will remains unknow for now.
----------
18/03 - Update from DB2

Explore alerting mechanism for DB partitions going into hung state - DB2 can perform health checks for all nodes but DB partitions that were hung could not be identified with any script/query. Will check with IBM for any possiblility in Alerting.

Include DB partition health checks as part of the standard DB health checks in the SOPs - Yet to Add in SOP, ETA 19/03

Investigate why the EDW jobs with upstage commands are dependent on the standby node Huxp007 - In Progress 

IBM to confirm the root cause for DB partition issue - In Progress 





IBM suggested No interim fix is prefered instead full stack yet to be planned

Waiting for IBM to confirm if the firmware upgrade to be done as a interm fix 
------------
09/03 - Update from UNIX : While performing the log analysis, multiple communication errors were reported in HBA(caused by attached devices, switch or SCSI-to-FC convertor) and they were not able to find any issues in GPFS waiters.This issue is same as in PDOA-DR node huxc808, where HBA port was returning EEH error. Suspecting a problem with underlying the FCA/HBA card driver where it cannot handle the error as expected.The primary issue that caused the hang is the AIX APAR IV89988 which was fixed by upgrading to the next PDOA fixpack level 1(PDOA V1.1 FP2 from PDOA V1.1 FP1).With this upgrade,AIX levels would go to AIX 7.1 TL5 SP1 (7100-05-01-1731) +IJ03033 and the FCA driver would no longer be exposed to the error handling problem. Upgrading to a higher PDOA Fixpack will upgrade AIX to a level that includes this fix and this will prevent the hang from occurring in the future. 

Yet to receive the final update from IBM Hardware support. 

Next Actions Pending: 
         Review with IBM and get the compatibility for PDOA V1.1 FP1 Interim fix 1 upgrade.
         Estimate the duration based on the previous stack upgrade
        Draft detailed plan for Interim fix in DR
        Fix Upgrade and monitor the system
        Fix Upgrade in PROD                                                                                                                                                                                                                                                                                                                                                                                             06/03 - Update from Unix Team : IBM is currently investigating the issue from h/w perspective. 
we had a call with IBM AVP/SME's on implementing Interim fix in PDOA, a detailed plan will be published once we hear back from H/W support team.

--------------------
Health Checks
- Focus was completely on the database until DB2 support spotted OS related errors after which Unix team was engaged
- End to end health checks to be requested by MIM during a major incident to identify any underlying infra issues with File System, Storage, etc
- Standard OS health checks were performed however, GPFS File system checks were not included
- Include GPFS file system checks as part of the standard OS health checks in the SOPs - Completed
- Include DB partition health checks as part of the standard DB health checks in the SOPs - In Progress

FCA errors are reported as Sev-3 alerts
- Amend the AMS document to report FCA errors as Sev-2 alerts - Completed

Alerting
- No alerting for File System Check Sum logs
- No alerting for DB partition going into hung state
- Explore alerting on the FS check sum jobs on all the nodes - Email Notification Setup
- Explore alerting mechanism for DB partitions going into hung state - In Progress

IBM Case
- Ensure the tickets being raised with IBM are under the correct category i.e. PDOA and not under Hardware
- IBM to confirm if the node HUXP007 went into hung state due to FCA errors causing GPFS file system to be inaccessible - In Progress
- IBM to confirm the root cause for DB partition issue - In Progress
- Can the firmware version be upgraded on the Fiber Channel Adapter to fix the FCA errors - In Progress

Dependency on the PDOA standby node
 - Investigate why the EDW jobs with upstage commands are dependent on the standby node Huxp007 - In Progress
",02/03/2020,09/03/2020,,,,,,,,Code/Product bug,,"?
PDOA standby node HUXP007 went into hung state preventing EDW jobs with upstage commands to connect to the database.
IBM to confirm if the node went into a hung state due to FCA errors causing the GPFS file system to be inaccessible and further resulting in database partition errors.

----------------
The GPFS file system was not accessible on the PDOA standby node which prevented the EDW jobs using upstage commands from connecting to the database. IBM to confirm if FCA (Fibre Channel Adapter) errors on the standby node caused the GPFS file system to be inaccessible and resulted in the database partition errors.



",,,1076,08/04/2020,April,2020,,Closed,,,2020,3,March,9/16/2025,2025,>60
29/02/2020,29/02/2020,87575899,,,,,Group Technology Services,,,,SI,GMOR system unavailable ,TCS,IS - Unix,,GMOR,RC identified,49630,"At 03:15 on 29/02, support observed a slowness in the GMOR system. At 09:50, support reported that GMOR application was unavailable.

Impact:
GMOR application was unavailable between 09:50 and 13:59 on 29/02.



Recovery action:
On 26/02, support were alerted that one out of the 9 active GMOR DB nodes was unreachable at around 19:10 and the services failed over to the standby DB node. The problematic node was rebooted and brought back up by 21:45.
 
Overnight 26/02: At around midnight, support observed slowness in the processing of the GMOR jobs. Health checks were performed and no issues could be identified. A Sev-1 case was raised with the vendors BlueChip, IBM, SAP and the relevant logs were shared for analysis. The overnight GMOR batch got completed by 06:22 and the BO Reports were available by 09:08. SSI morning batch along with MP Recon Report completed at 08:45 and 09:53 respectively.
 
Overnight 28/02: At around 01:05, the GMOR application was exhibiting slowness which caused multiple jobs to overrun. The GMOR batches got completed successfully with minor delays. At 09:50, support reported that GMOR system was unavailable. Investigations revealed that 3 NSDs (Network shared disks) were in ‘recovering’ status causing database unavailability. The disk recovery got completed successfully at 10:45. Support performed the IBM-recommended cable replacement for one of the SAP HANA database nodes to fix the issue and the GMOR application was available by 13:59.
","Update 5/10:
Physical checks will be carried out in Swindon Data Center after Covid situation
---------------
21/03 - Cables replaced between 4 servers and the IBM Network Switch at Stockley Park.  The other two servers are of a different model and does not require cable replacement

14/03 - Cables replaced between 4 servers and the IBM Network Switch at Stockley Park

Next Action:
Check on all the other cables btwn HANA DB switch and the network switch for Swindon & Stockley

Activicty has been scheduled this weekend - yet to check with Unix",29/02/2020,29/02/2020,,,,,,,,Infrastructure issue / Hardware failure,,"?
A faulty cable between one of the DB nodes and the network switch caused connectivity issues within the GPFS nodes. The faulty cable was replaced to restore services.
",,,1472,,January,1900,,Closed,,,2020,2,February,9/16/2025,2026,>60
20/02/2020,21/02/2020,87562249,,,,,Customer Channels,,,,MI,Customer.com issues with 'Access Denied' and spinner on log in page ,Akamai,.COM,,Customer.Com,RC identified,49601,"On 20/02, two issues were reported on the website in the morning.
• Some customers were getting an ‘Access Denied’ error message. This error appeared intermittently and was noticed after 05:00.
• Problems with sign-in on the website. When clicking on sign-in button, the page was not getting loaded. This was reported at 07:38.


Impact:
Poor customer experience and loss of Sales
• Approx. 40% customers impacted with the Sign-in issue
• Approx. 1-2% customer impacted with the Access Denied issue


Recovery action:
• Sign-in issue
Alerting indicated that there were problems with the sign-in page. This was also reported by some internal users. This corresponded to a dip in sales. The issue resolved itself without intervention at 08:18. This issue started after a base cache clear was performed at 7:32am (normal activity). A bug in the GNAV script sometimes prevents it from loading cleanly after a cache clear and this impacted the sign-in page. The reason this automatically got resolved was that GNAV cache has an expiry time of 45 minutes, so the cache refresh resolved the issue. Until this bug is permanently fixed by the development team, support have put a few mitigating steps around cache clear activity. 
• Access Denied
As it was related to Bot Manager, after engaging Akamai a workaround was put in place which restored services at 10 AM. Once this was performed, the ‘Access Denied’ errors dropped off. A permanent fix was put in place on Thursday night by making the necessary configuration changes on Customer side. However there is more work which is required and is being tracked/managed outside this incident.
","Update 21/9- Confirmation from Elaine
No, I don’t think that we need to raise a peak risk for this. We have had no further Production incidents resulting from this. Also, it was originally caused because an incorrectly rendered page was cached so it was partly a timing issue

Update - 15/09/2020: The Jira ticket has been CNP-4220 prioritized by PO to be picked up and Ops teams will reach out to PO’s as well to get this prioritized. Challenge with this scenario is this cannot be replicated but team will perform some POC

22/06: we are working with Ops to understand the priority and accordingly will push this to picked in Q2.
---------------
Team will keep posted on an ETA of when this fix will be implemented in production.

this tracked under MYA-2580, we have requested the PO to prioritize this. We can check and confirm when this will be planned for
 
This JIRA reference is MYA-2580. It is a common issue for all the FEAR pages

--------------
Following up with Cyber Security to ensure a proper Service Wrap exists in place for Bot Manager
Dev team is working to fix the bug in the log in page which is not rendering properly - Dropped mail to saurabh for an update from Dev team",21/02/2020,26/02/2020,,,,,,,,Customer Tech change,,"?
Sign In issue - Following F&F launch on 20th February, the team were performing cache clears as part of their BAU validations on content being available on website. One of this cache clears (Base cache) resulted in a surge in traffic hitting database directly. This resulted in several transient DB blockers. Unfortunately at that moment, login page template (FEAR) was not rendering properly and got cached in the Dynacache layer. This is a bug because caching framework did not check if the page rendered is erroneous before caching it in Dynacache. We have raised this with Dev team to fix it asap. 

Access Denied – Akamai made a change on Wednesday evening which we now understand required a corresponding configuration change on Customer side but was never communicated.  The specific legacy configuration causing an issue was removed at our end to restore the services.
.

",,,,,January,1900,,Closed,,,2020,2,February,9/16/2025,2035,>60
19/02/2020,20/02/2020,87559569,,,,,Group Technology Services,,,,MI,Outlets Peterborough DC unable to access JDA Dispatcher application ,TCS,IS - Database,,WMS,RC identified,49480,"At around 05:30, MIM was made aware that the JDA dispatcher application at the Outlet Peterborough DC was inaccessible since it was unable to connect to the database.

Impact:
19/02 - The Outlets Peterborough Outlet DC was unable to access JDA Dispatcher application from 06:00 – 14:25.
• 120 DC users were unable to perform operational activities and were asked to leave for the day.
• The DC could not receive 3 trailers and were unable to complete 16 store loads.
20/02 
• No significant impact was incurred by the DC operations.



Recovery action:Support team investigated and identified that the JDA dispatcher application was unable to connect to the database after the Oracle database patching activity (CRQ117916) was performed on 18/02. A decision was taken to revert the database patches however, support encountered errors during the patch reversion. A Sev-1 case was then raised with the vendor – Oracle for triage. Oracle vendor provided scripts to facilitate the reversion however, the issue continued to persist even after running the scripts. Support then created an action plan to restore the DB backup on the Stockley DB server. The DB backup restoration started at 12:02. In parallel, support worked with Oracle to successfully recover the JDA Dispatcher database on the Swindon DB server by 12:45. The DB restore on the Stockley DB server completed successfully at 13:27 however, registry errors were observed in the Stockley DB while performing health checks. The incident team then brought up the JDA Dispatcher DB on the Swindon server and JDA dispatcher was accessible since 14:25.
 
Overnight Wednesday (19/02) - Oracle support recovered the Stockley DB after closure of the DC operations at 22:00. Database replication from Swindon to Stockley was enabled and databases were in sync by Thursday, 20/02 03:20.
 
Thursday (20/02) - At around 07:33, DC users reported that JDA dispatcher application was inaccessible again. Investigations revealed that the JDA dispatcher application was trying to connect to the standby database in Stockley instead of the primary database which caused application unavailability. Support manually shut down the standby database at Stockley to restore the services at 08:40.
","Failback is completed in Apr patching schedule sucessfully
-------------
failback will be scheduled in next Unix OS  (peterborogh) patching if there is no any urgency - Apr 2020

failback the outlet DB from swindon to Stockley

02/25 – We have not received a concrete RCA or an update on the questions that have been put forth to Oracle. Account level escalations are underway.

02/24 – Investigations from Oracle have revealed the following - after the Linux DB servers were restarted post patch deployment, the shared memory that is allocated to the DB by the operating system did not get cleared automatically which resulted in the ORA600 errors and issues with the patch reversion. Oracle have confirmed that this is a rare issue however, the cause for it to occur on both the Stockley and the Swindon DB servers is yet to be determined. DB Support continues to follow-up with Oracle for updates on the RC against the ongoing Sev-1 ticket - SR 3-22394029691.

",20/02/2020,24/02/2020,,,,,,,,Customer Tech change,CR,"? The root cause is attributed to the corruption of Shared Memory File System on the DB servers (both in Stockley and Swindon) which is aligned with the findings from Oracle Vendor - ORA 600 errors were due to some files missing in the Shared Memory File System (/dev/shm) on the DB servers.

Once the DB and MQ patching was completed, support identified that one of the NFS File systems on the DB servers was in a stale state and did not come up automatically after the MQ Cluster services were restarted post MQ patching activity.

In order to bring up the NFS File System on the DB servers, support issued a command to “mount all the file-systems” on the DB servers (both Stockley and Swindon) which corrupted the Shared Memory File System as the DB was already up and running.  The ideal solution would have been to re-mount only the stale NFS File System and not all the File Systems in the DB servers to avoid the Share Memory File system corruption.

",CRQ117916,,,20/04/2020,April,2020,,Closed,,,2020,2,February,9/16/2025,2036,>60
11/02/2020,11/02/2020,87548504,,,,,Customer Channels,,,,MI,Main customer line (0333 0148000) into the contact centres unavailable ,Anana,.COM,,Genesys IWS,RC identified,49566,"Customers calling the main customer service line into the contact centres received an automated message stating"" the number is no longer in use "" and the calls were disconnected abruptly.

Impact:
Noother lines or channels were impacted, so, in the main, Customers used other channels/numbers
to initiate communication with the contact centre.


Recovery action:Ananasupport identified that the calls were being routed straight to Genesys instead of Twilio.  Further investigations revealed that the call plan that controlled the call routing in Gamma was
missing so the call ‘default routed’.  The call plan on the Gamma Network was rebuilt thus restoring the
route into Twilio for these calls.
To prevent re-occurrence for the remainder of the migration:
·         The(hidden) call-flows are all being documented and checked prior to migration. 
·         Fullpost implementation regression testing is going to be undertaken against all customer facing lines.
","All the actions are closed
------------
02/24 - dropped mail to steve for an update
------------
Next Action: Document the end-to-end call flow from NGN to Contact Centre. ETA 21/02
--------------
The issue was caused as a result of a call plan associated to SIP Trunk Call Manager (STCM) not being copied across as part of the product port. STCM is used to apply business continuity in the event of a total loss of service and can also apply special routing to redirect calls if needed. Due to the low volume of calls identified in the Genesys reporting for the Geographical Number (GN), it was deemed that the routing for this number did not warrant full end-to-end auditing. Had this been performed it would have revealed the existence of the call plan routing the call to Twilio and more thorough testing following the port would have been performed. Moving forward we are addressing our processes and using data from Gamma to understand if our assumptions on call volumes are correct. This will also reveal if there are other special routing scenarios we need to consider prior to porting. Documentation of the end-to-end call flow from NGN to Contact Centre is also being produced.",11/02/2020,14/02/2020,,,,,,,,Customer Tech change,,"? Acall plan controlling the call routing in Gamma was missing during a change implemented by
Anana to migrate the Customer Contact Centre number ranges to support the new Secure Payments solution.
",,,,18/02/2020,February,2020,,Closed,,,2020,2,February,9/16/2025,2044,>60
08/02/2020,09/02/2020,87544312,,,,,Customer Channels,,,,MI,Blank pages on search for Product Listing Pages (PLP) & Search Response Pages (SRP) ,TCS,.COM,,Bloomreach,RC identified,49543,"Through monitoring of the site, it was identified that a majority of Product Listing Pages (PLP) and Search Response Pages (SRP) were returning zero results or blank responses for search results from 10:50.
Impact:
• Customers were unable to search for any products in Customer.com and IE. 
• Store and Contact Center colleagues were unable to search for any products via Assist and Customer.com+. 
• Reduction in the number of orders created via Customer.com, Assist application and via Contact Center using Customer.com+ resulting in loss of revenue. 
• Approximate loss of 1.3k orders and 40k revenue estimated based on the comparison of orders via Assist application on the previous day (Friday, 07/02)
. 

Recovery Actions: 

Relevant teams joined a bridge and a Sev 1 incident raised with Bloomreach (Search Provider) to investigate the issue. Bloomreach confirmed a global issue on their end impacting all customers. Holding page was applied on the website (both UK and IE) as of 11:40 am to protect the customer experience. Bloomreach ran the full feed and indexing after which, the issue was resolved for Customer from their end. Support teams performed cache clear and completed basic validations. Then, the holding page was throttled down to permit customers to access the website at 12:36 – 20% UK traffic and 50% Ireland traffic. 
 
UK and IE traffic throttled to 100% at 12:43 pm. Order volumes since then came back to normal levels. All the PLP and SRP pages are returning results without any issues. Monitoring in place for the rest of the weekend until we get further updates from Bloomreach on the final resolution on the incident from their end.
 
09/02 - No further issues were reported overnight and the search functionality remained stable.
","27/03 - All the actions were closed
---------------
24/03 -  Team is in the process (current sprint in next 7 days) of setting up pager duty alerts for BR if it is down.
---------------
17/03 - Requested the support team for an update, they have dropped a mail to the Bloomreach for the status
-------------
Adding automated alerting (24/7) and Bloomreach support paging for this new
type of alert - Requested Bloomreach to provide an update

-----------------
In preparation for an upcoming feature release, Bloomreach executed a change to our customer
configurations for a new configuration flag setting to enable two new capabilities to be released
in Q1: Content Search and Content Suggest. A new attribute and associated value was added
into customer configurations for this upcoming release. This change was executed on a per
customer basis in an automated way using a script.
The script contained a defect which resulted in erroneous values being populated in some
customer configurations. The system recognized the erroneous value, and as a result,
defaulted to our “new customer” default state for some customers. This change erased any
custom configurations for impacted customers. As a result, some customer search calls
rendered partial to null results.",09/02/2020,14/02/2020,,,,,,,,Customer Tech change,,"  Bloomreach executed a change to our customer
configurations for a new configuration flag setting to enable two new capabilities to be released
in Q1: Content Search and Content Suggest. A new attribute and associated value was added
into customer configurations for this upcoming release. This change was executed on a per
customer basis in an automated way using a script.
The script contained a defect which resulted in erroneous values being populated in some
customer configurations. The system recognized the erroneous value, and as a result,
defaulted to our “new customer” default state for some customers. This change erased any
custom configurations for impacted customers. As a result, some customer search calls
rendered partial to null results.
",,,,27/03/2020,March,2020,,Closed,,,2020,2,February,9/16/2025,2047,>60
06/02/2020,08/02/2020,87542163,,,,,Group Technology Services,,,,SI,DataStage primary node unresponsive ,TCS,IS Linux,,"GMOR BO , FAQT, RD",RC unknown,49545,"Around 17:15, Linux support advised that the DataStage Primary Node has become unresponsive
Impact:

• Customer.com sales for 06/02 are understated in GMOR BO (Business Objects) Reports.
• BWS despatches from Bradford were understated in the Customer.com Performance Daily Report.
• Received 307K of Foods and 101K of C&H (POS) as late sales. This data was missing in FAQT and Retail Dashboard reports on 07/02.
• Orders from Sterling to Hampers (FlowerPlus) between 16:40 and 18:00 went sent by 18:48, 10 orders missed the 18:00 cut off.
Recovery Actions: Support advised that the DataStage Primary node was in a hung state, the server was rebooted to resolve the issue.  All the failed DataStage and EDW jobs were recovered in a controlled manner.  However, DataStage wave jobs were failing.  The DB2 switch file was recreated and the DataStage Queue Manager was restarted to resolve the issue with the DataStage wave jobs.

Support had identified that one of the batch id was not processed in SAGG causing sales figure dip in Retail dashboard & FAQT. The missing sales data was loaded into EDW system and the missing sales for 06/02 were updated in FAQT and Retail Dashboard on 07/02.

On 07/02, C&H along with Foods Business highlighted low sales figures for Customer.com in the GMOR BO reports and Customer.com Performance Daily Report for 06/02. Further investigations revealed this was due to the order transactions that were not processed from Sterling into Datahub.  The missing order transactions were re-processed from Sterling into Datahub which then processed into the downstream systems. Support confirmed Customer.com sales for 06/02 were reflecting in the affected reports as expected on the morning of 08/02.

","No issues were observed for a week in the Post-deployment monitoring.
----------------------
02/13: Redhat couldn't provide any conclusive root cause for NFS and MQ resources getting timed out. The following changes have been deployed in Prod against CR # 118436.
 
•         Volume Group and NFS resource time-out value has been increased from 20s to 60s
•         The HAmon (HA Monitor) has been enabled to capture granular logging if any resource gets timed out in the next occurrence.
 


----------------
11/02: The teams had a call with RedHat and the below action items were agreed upon.

• DS Admin team to share the DataStage job logs with IBM to determine if any issues with IBM DataStage could have caused this issue.
• Linux Support to enable the performance co-pilot parameter to ensure that the required diagnostic logs are captured should the issue re-occur
• Linux Support to increase the NFS time-out value from 20 seconds to 60 seconds
--------------------
10/02: Linux Support in parallel are also checking with Middleware / DS Admin to identify the load of messages that were being processed which might have caused the two file systems to become unresponsive.
---------------------
06/02: A Sev-1 case was raised with Red Hat for further investigation.  Red Hat investigated and could not find any anomalies and hence advised to enable additional logging on the server in case of re-occurrence.
",08/02/2020,,,,,,,,,RC Unknown,,?Redhat couldn't provide any conclusive root cause for NFS and MQ resources getting timed out.,,,853,05/03/2020,March,2020,,Closed,,,2020,2,February,9/16/2025,2049,>60
04/02/2020,07/02/2020,87537027&?87540221,,,,,Corporate,,,,SI,Incorrect Payment for Weekly paid employees ,SD Worx,SD Worx,,Payroll,RC identified,49547,"Colleague Services raised an incident with SD Worx?at around 10:00?highlighting an issue whereby weekly paid colleagues (due to be paid on Friday) had an extra week once the trial payroll run was completed.

Impact:
Late/split payment for some leavers 

Recovery action: :?SD Worx was engaged and they confirmed that?a calendar issue had caused the duplication 
in payroll for the weekly paid employees.? The payroll for the impacted employees was rectified by running a 
script.? Sample checks were performed following the correction that was loaded onto the system and confirmed 
that data was appearing correctly 
 
Wednesday(05/02) -?Following the trial payroll run, SD Worx support teams sample checked some employees 
whose payments were?rectified and were confirmed to be appearing correctly on the HRe system 
(Hours Worked Screen).?
The live payroll run was processed and completed successfully, the corrections on the payslips are?visible and 
payments have been sent to BACS?at 14:49. 
 
A further incident (INC87540221)?was raised on Wednesday as it seemed that there were a number 
of leavers who were due to be paid on 07/02 but were missing from the weekly payroll. A plan was put together 
to validate the impacted employees and to calculate the correct payment. They would then be paid using the 
emergency payment process. 
 
Friday(07/02/2020) –?Final validations were completed and the emergency payments invoked. 
","The testing was inconclusive so they are putting it down as manual error. They have added 2 additional checks in the flow to ensure this is captured if it reoccurs.
-------------
Next Action:
To perform testing on a non-prod environment to understand whether the calendar date population resulted from a system issue or human error - ETA 02/28
------------
When the Resolver team investigated the issue, they found that there was a date issue with the payroll calendar in CPS1 where the start and end period was incorrectly populated for the weekly payroll.
This is a task that is carried out by the Payroll Ops team and is an auto generated process which populates itself based on the first period.
Further investigation/testing is in progress in order drill down whether this is a human error or an issue with the system.",07/02/2020,14/02/2020,,,,,,,,Human error - Customer Tech,,"? When the Resolver team investigated the issue, they found that there was a date issue with the payroll calendar in CPS1 where the start and end period was incorrectly populated for the weekly payroll.
This is a task that is carried out by the Payroll Ops team and is an auto generated process which populates itself based on the first period.",,,,05/03/2020,March,2020,,Closed,,,2020,2,February,9/16/2025,2051,>60
04/02/2020,04/02/2020,87537467,,,,,Customer Channels,,,,SI,Contact centre agents unable to log into voice channels ,Anana,.COM,,Genesys IWS,RC identified,49526,"Agents across all M& contact centres were unable to log in to the Genesys voice channel in the morning.
Impact:
• Agents in Chester, Preston Brook and South Africa were unable to take voice interactions from customers.
• Poor customer experience.
• Email and chat were unaffected.

. 

Recovery Actions: 

Anana in their daily checks activities identified that the Voice media channel was unavailable (‘greyed out’) therefore meaning agents would not be able to go ‘Ready’ on the that media channel. Inbound calls received the correct In Queue experience.
Some ‘role’ based configuration data was missing for the agents which controlled whether this channel should be visible to them.  This configuration was restored.
Any affected agents were requested to log-out & log back in to make the channel visible again.

","Update from Guy on 25/9:
The date for this upgrade is yet to be scheduled as resources have been prioritized onto other Customer activity in the last month. Is the upgrade yet to be scheduled or is it completed? - Completed-The Genesys Configuration Server upgrade is yet to be scheduled. This is unlikely to be performed before peak and ideally needs to be considered as part of a more strategic initiative to upgrade the platform as the work involved is significant.  The suggestion is this problem ticket is closed and the conversation carried on with the account manager (David Kime)
Update: 17th Sep 2020

Jaye confirmed via Peak PM Analysis - MI / TTBAO - .Com Contact Center Incidents call that outstanding actions on this issue is completed
------------
4 Aug - The date for this upgrade is yet to be scheduled as resources have been prioritised onto other Customer activity in the last month. Next update -  7th September 2020

22 Jun-The date for this upgrade is to be confirmed as resources have been prioritised onto other Customer activity in the last month. Next update 27th July 2020.

09/Mar - It will take weeks time to provide an update on the requested query

Upgrade on the Genesys component is not  firmly scheduled as yet but as soon as we have something in place Anana will let us know
----------------
ANANA provided the detailed RCA

The root cause of the issue was a bug in a Genesys Component which resulted in the clearing of the memory resident ‘role’ based configuration data for the agents. This data should not have persisted in memory alone and should have also been written to the Config DB. An upgrade of the Genesys component will be scheduled to prevent this from re-occurring.",04/02/2020,05/02/2020,,,,,,,,Code/Product bug,," The root cause of the issue was a bug in a Genesys Component which resulted in the clearing of the memory resident ‘role’ based configuration data for the agents. This data should not have persisted in memory alone and should have also been written to the Config DB. An upgrade of the Genesys component will be scheduled to prevent this from re-occurring.
",,,,,January,1900,,Closed,,,2020,2,February,9/16/2025,2051,>60
03/02/2020,03/02/2020,87535572,,,,,Corporate,,,,MI,Multiple stores network are down due to Vodafone Central network issue ,Vodafone,Network,,Stores Network,RC identified,49528,"At around 04:11, MIM was made aware of multiple alerts being received stating multiple stores are hard down.
Impact:
89 stores are hard down and OLA code has been shared with stores to facilitate offline trade. 

Recovery Actions: 

Vodafone technical team and SIM was engaged. Investigations revealed that their Birmingham site network is down impacting multiple Vodafone customer. Vodafone has engaged their third party to resolve the issue. Vodafone suspects that there is a possible hardware issue and continues to investigate further. IVR is in place and Store are provided with OLA code to trade offline. ","All the actions were completed
------------
02/24 - Dropped mail to Steve for the update
-----------
Next Action 20th Feb

Investigation into local HSRP not activated during service failure. - Open

SIM/MIM Improvements - Open

Vendor to support core IP to understand if we can improve the CPU and Alerting - OPen
----------------
VF shared the Draft RCA
------------
Raised a PR and requested VF to share the RCA",03/02/2020,13/02/2020,,,,,,,,Capacity constraint ,," 
Two Nokia PE routers experienced memory exhaustion. Initial symptoms started in the West Midlands (Birmingham) and then were observed in the South (Chart Street, London). The reason that the issues in the South were identified later is that it took longer for the router’s memory in Chart Street to exhaust. Memory exhaustion occurred on this occasion because of three primary contributing factors:
? Two incorrectly configured routes for a Vodafone customer were in place on the PE routers. These lay dormant due to the nature of their configuration.
? The customer in question started to advertise a prefix, which activated static routes, in turn, activating the Border Gateway Protool (BGP) across them. This caused conflict resulting in both the static route and subsequent BGP protocol creating alarms across the PE routers.
? The flooding of these alarms and routing updates exhausted the PE router memory. After a number of hours, this caused critical router processes to fail.",,,,27/02/2020,February,2020,,Closed,,,2020,2,February,9/16/2025,2052,>60
02/02/2020,03/02/2020,87535866,,,,,Group Technology Services,,,,SI,EDW DR Database out of sync impacting report availability  ,TCS,IS - Database,,"Spotfire, Power BI",RC identified,49417,"At around 18:00 02/02, support received an alert stating that EDW DR database went out of sync since 15:45 02/02. 
Impact:
Spotfire Analytical Reports, Power BI and user owned reports pointing to DR database will not fetch the latest EDW data. However, a few critical reports have been pointed towards EDW Prod database to mitigate the impact.  Until the DR environment is fully in synch with PROD, we cannot fully use the DR environment for reporting. 

Recovery Actions: 

Support identified that the ‘apply’ process which is responsible to sync the EDW PROD and EDW DR was in hung state. A Sev1 case was raised with IBM (TS003304581 ). Support killed the apply process at OS level and started the process in debug mode to gather additional logging as advised by IBM. However the issue continued to persist. 

Support team have collected the logs and were shared with IBM DB2 Engineers for analysis. While restarting the apply process one of the data node in DR is hung. Hence support was unable to restart the DB2 instance as suggested by IBM . Further investigations underway.  All the EDW critical flows are progressing as expected and No threat to the SLAs

The problematic node 808 was isolated and successfully restarted.  Replication was restarted and EDW Prod and DR are in sync as of 17:25.
","closing the PR with the risk ID 1076 as there is no confirmed ETA on the full stack upgrade
----------
IBM suggested No interim fix is prefered instead full stack yet to be planned

27/02 - IBM has recommended a full stack upgrade be performed for fixing this issue. Considering that it is being tracked against Risk ID # 1076, 

25/02 - IBM reviewed the logs and confirmed that no errors were reported. FCA looked healthy post reboot.  Discussions are underway with IBM on deciding stack upgrade approach, which will include (OS, Firmware, DB components).

02/13 : IBM investigated the logs and suggested the below actions.
 
• FCA firmware to be updated to address the EEH errors reported in adapters. H/W support team to carry out wrap test and isolate any physical failures in HBA card. 
• AIX OS to be updated to address the reported APAR (IV89988)
 
Support is working with IBM to plan the implementation of the aforementioned.
 


--------------
02/11: The IBM hardware team is analyzing the logs to determine if any hardware related failures could have caused DR to go out of sync with Prod
------------------
IBM to run a Wrap test",03/02/2020,13/02/2020,,,,,,,,Code/Product bug,," 
Fiber Channel Adapter (HBA) port fcs13 returning a EEH error.",,,1076,20/04/2020,April,2020,,Closed,,,2020,2,February,9/16/2025,2053,>60
30/01/2020,03/02/2020,87531872,,,,,Platform & Store Ops,,,,MI,Store colleagues reported tills rebooting while scanning products from T53 and T05 department ,PCMS (Flooid),Retail,,POS - Tills,RC identified,49504,"At around 12:00, MIM was made aware multiple stores reporting that the tills are rebooting while scanning few products from T53 and T05 departments.
Impact:
Store Colleagues were not able to sell or refund the products belonging to 10 strokes in T53 and T05 departments.  Comms were sent to stores advising to sell products using the department price work around. 

Recovery Actions: 

Support identified that the issue occurred due to incorrect deltas (data) updated in 10 strokes related to T53 and T05 Department
 
As a workaround, the data for the problematic strokes was fetched to create right delta to process in tills. The workaround process was tested in non-prod environment before pushing it to tills and it was successful.
 
Once workaround was completed, stores confirmed that the tills are scanning the products without any issue and performance have been stable from 13:57. 
 
Support teams also created an action plan to ensure that the issue does not re-occur. POS Support completed the Full Cache Refresh to the tills overnight and support might perform the same tonight as well(31/10).  Monitoring in place.
 
Friday(31/01) - No further issues of till reboots have been reported from stores.  The 3rd Party vendor Flooid (PCMS) have provided a patch fix which they believe will resolve the issue and this is currently being tested in the lower environment.  Support is on hypercare and will send a full cache refresh to the tills in case of any Level 8 changes identified over the weekend.
 
No further issues of till reboots were reported overnight.  Support continues to perform hypercare activities. 
","The DDS Level 8 patch fix from Flooid has been deployed into Production on 11th Feb. There are 468 Level 8 changes today, support on hypercare. 
Support Validated and no abnormalities

----------------
We tested few UPC’s part of the level 8 changes in T02 dept in one of the Tills which has received the delta , no issues observed and the products scanned successfully. No Till crash happened.  Have also verified the reboot stats after 6:30 PM , and no abnormal reboots observed. 

We will monitor for any issues. 

---------------
• The fix patch was provided by Flooid on Friday (31/01) around 16:00 to address the above issue.
• The patch was deployed in the Dev environment for testing around 18:00 (31/01).
• The testing of the patch fix (both functional and performance testing) is expected to complete by EoD Wednesday (05/02).
• Once the testing is successful, plan to deploy the fix on Thursday (06/02) - The deployment includes reverting the fix patch that was deployed on 27/01 to address the Level 8 changes.
• Anupam Singh (RTE) to engage Meena Meghani to liaise with Flooid to understand what changes have been made to the code as part of the fix, so we can include additional test cases if required.
",03/02/2020,03/02/2020,,,,,,,,Customer Tech change,," 
A bug in the Beanstore POS Centre Release (CRQ115452) causing till reboots while scanning products with any Level 8 changes along with stroke description changes.",CRQ115452,,,17/02/2020,February,2020,,Closed,,,2020,1,January,9/16/2025,2056,>60
30/01/2020,03/02/2020,87532359,,,,,C&H Commercial Trading,,,,MI,Performance issue on SSI application ,TCS,C&H,,SSI,RC identified,49413,"At 09:00, support along with the business users reported performance issues in the New SSI application.
Impact:
• Business users were unable to load data scope in New SSI and hence unable to use the application for their BAU activities.
• New SSI and IBT applications were unavailable from 13:00 until 13:20 during the database restart.

Recovery Actions: 
The New SSI application pods were restarted which did not resolve the issue.  Cloud Platform Support was engaged and could not identify anything abnormal.  Further investigations revealed that the CPU on the C&H common RP replication database was at 100%.  The database is shared between both New SSI and IBT applications, however, no issues/slowness observed on the IBT application.  An outage was agreed with the business and the shared database was restarted at 13:00.
 
The shared DB restart was completed at 13:00 which did not fix the issue with New SSI application.  Health checks were performed on the IBT application and confirmed to be working fine.  Support identified slowness with one specific query fetching the data from the database which was causing all the other connections/threads to pile up.  A configuration change  was applied to optimize the query execution to resolve the issue.  The New SSI application pods were restarted to flush out the stale connections after which SSI support tested and confirmed the application along with data scopes can be loaded as normal.
 
At 16:30, there were further reports of intermittent slowness. There are number of caching pods (Ignite) and normal SSI applications pods. Investigations revealed that, during nights application pods are scaled down to a minimal number to save the resources and at 6:00 AM they are scaled back up.  Every night the caches Ignite pods are cleared, but they were not scaled down. Support suspect this might be because the Ignite pods are not releasing resources even after being cleared. Hence, support have decided to scale down the Ignite pods at night and then scale up again in the morning to ensure we have fresh pods overall in the SSI application ready for use.  At 20:10,support have performed this action and will monitor for the next couple of days to find the most efficient way of scaling up and down these pods. Application performance has been stable since these actions at 20:10, 30/01.  The incident team has decided to monitor this application for the next few days and keep the incident open until the performance remains stable.

Friday (31/01) – Users reported further slowness in SSI.
 
Saturday(01/02) - SSI support made a code change in production after testing in lower environments to make UI retry exit gracefully rather than wait for a long time for a response. This in turn will reduce the application load during peak usage. Support teams continue to monitor the application performance.
","Further investigations ongoing to fine tune the application performance (not related to this incident) being managed by the project team.

Update from RTE Team:
1. The SQL server was unable to find the appropriate type of query explain plan 
2. The query in question is now configured to force sql server to always use correct type of explain plan
3. Subsequent to this application showed intermittent issues with huge load – team put up affix to optimize data load UI retry to enable more requests to be handled simultaneously by application orchestration layer - GRID
",03/02/2020,04/02/2020,,,,,,,,Design issue,," 
Issue with a specific query executing on the DB resulting in high CPU and causing all the other user connections/threads to pile up. The explain plan of the query was changed to optimize the query execution.

In order to mitigate the application slowness/performance issues, a fix was put in place to optimize the data load UI retry to enable more requests to be handled simultaneously by application orchestration layer (GRID).
",,,,05/02/2020,February,2020,,Closed,,,2020,1,January,9/16/2025,2056,>60
30/01/2020,30/01/2020,87532537,,,,,Group Technology Services,,,,SI,Users reporting Customer World web page is appearing blank ,"Microsoft
(SP202628 / OD202633)",Sharepoint Support,,Customer World Home Page and Sharepoint,RC identified,49415,"At around 14:23, MIM was made aware that users across different locations received blank page while trying to access Customer World web page.
Impact:
Users were able to intermittently access the Customer World services in the web browsers and might have had to use some alternate routes. 

Recovery Actions: 

Microsoft advised that they identified a certificate issue within their third-party content delivery network (CDN) provider that caused impact to the service. Microsoft’s actions to re-route the impacted traffic was successful in mitigating impact and they confirmed that the issue is no longer observed with multiple users.","Next Actions:
All the below actions will be completed by Feb 2020

We're working with the third-party CDN to move traffic back onto their infrastructure, no impact is expected during that change

We're reviewing our internal anomaly detection systems and looking for ways to improve detection time. Additionally, we are adding monitoring probes to alert for this kind of issue.

We’re reviewing our code for automated recovery options to reduce impact in similar scenarios.",30/01/2020,06/02/2020,,,,,,,,Certificate issue,," 
A configuration error by our third-party CDN provider resulted in impact to SharePoint Online, OneDrive for Business and Project Online Classic UI page rendering.",SP202628 and OD202633,,,11/02/2020,February,2020,,Closed,,,2020,1,January,9/16/2025,2056,>60
24/01/2020,29/01/2020,87522787,,,,,Group Technology Services,,,,MI,Intermittent performance issues with multiple cloud based applications ,TCS,Cloud Platform,,"ASO, IBT, SSI, ORCA, SCRD, Intelligent Waste, MPG, PWM, VOD",RC identified,49088,"At 06:28, MIM were advised that multiple GRs were stuck in ASO and were not being processed. GIST also reported the issue for Cumberland and Thatcham at the same time.
Impact:
·Foods:
• Intermittent performance issues were observed with ASO,SCRD and ORCA.
• All depots were in backup allocations from 21.00 to 23:23 on 24/01 and from 05:45 to 09:07 on 25/01.
• Foods Bradford NDC allocation missed 06:00 AM SLA (25/01).
• On 27/01 All depots except Thatcham were in backup allocations from 19:42. All depots except Bristol and Crewe went back to live allocations from 22:41.
 
International:
• Intermittent performance issues were observed with IBT application.

C&H:
• Intermittent performance issues were observed with new SSI application.
 
Retail:
25/01
• Intelligent Waste was unavailable between 19.30 and 19.40 as the Kubernetes cluster was not responsive.
27/01
• MPG, PWM(Pay With Me) and VoD was unavailable from 16:30 to 17:30.
 
Multi-Channel:
23/01 15:00 (issue started at 13:13 but very intermittent, starting impacting customers at 15:00)
27/01 09:45 - 11:05
• Alerting indicated an increase in errors and slowness in server response times. 
• Colleagues reported  intermittent slowness during search leading to  ‘sorry something went wrong’ pages .



Recovery Actions: 
Friday 24th January
On 24/01,  when the issue occurred, support teams mitigated the impact on the foods depots by managing the number of pods to process ASO data and manually processed all the stuck GR’s. To minimise the impact, the ASO application was switched to backup systemically
 
There were reports from other cloud applications such as ASO, SCRD , ORCA, IBT, New SSI about intermittent slowness as well. Microsoft was already engaged and they suggested a mitigation plan as they were experiencing network latency across Northern Europe on their Virtual Machine Scale Set (VMSS) and Load Balancers.  
 
The majority of our cloud applications are hosted on Kubernetes Engine which runs on Microsoft’s VMSS. As Microsoft suggested, we disabled the node auto scaling and had up scaled the number of nodes available. All the depots were then switched to live allocations.

– Further investigations from Microsoft revealed that Kubernetes nodes's  Kernel version was not in sync and they suggested to upgrade to the latest kernel version. Cloud Platform support completed the testing in the development region and deployed into production. 

Saturday 25th January
Around 02:45 (25/01), support advised that Foods Bradford NDC allocations were stuck and progressing very slowly within ASO. After redeploying the ALCO PODs multiple times, the messages were still not progressing. Further investigations revealed that one of the ALCO POD was running in the older version. Later, several other PODs were also running with the old version and therefore support moved these PODs to the new nodes which had the new Kernel version.

Support completed the backup allocation process and all Bradford orders were interfaced into WMS at 07:32. All impacted depots were switched back to live from 09:07 and no further issues were reported. All the application PODS (ASO, SCRD, IBT , New SSI, Location services & Price services in ES) were moved to the new kernel version from 10:02 and the ASO application remained stable since. 

No further issues were reported for any other applications during monitoring. However, support teams prepared a DR plan for ASO and SCRD applications to mitigate the impact during further occurrence of the issue.
 
Support monitored the ASO application overnight and confirmed stability of the application. No further issues were reported. The Foods Bradford NDC orders were interfaced successfully at 03:25 26/01 – Sunday.
 
Monday 27th January 
Users of multiple applications as mentioned in the impact above, reported issues at 09:47 and 13:11. Support teams could continually see that the Inginx ingress nodes were restarting.  To try and reduce the chance of failures at 13:58 we increased the number of ingress nodes from 5 to 10 and the occurrence of impact to users did reduce however the nodes were still restarting.  As suggested by Microsoft, the support team upgraded the Nginx engine version. However, issues were reported for MPG/VOD and PWM and therefore the Nginx engine upgrade was reverted. 

In order to further mitigate impact, support team made configuration changes to the Kubernetes PODs responsible for Enterprise Services(ES) Location services and Pricing services so that the PODs will not have dedicated nodes and could be moved to different nodes where resources are available. However, issue persisted and depots went to backup.  Support further created a new node and moved the Enterprise services PODs for Location Services and Pricing Services to restore services.

The cloud support team identified two Kubernetes nodes that were marked not be scheduled for use with any applications.  They were marked as such due to underlying issues that needed to be investigated however the team explained that they could still cause issues as traffic would still route through the nodes.  A decision was therefore taken to delete these nodes at approximately 20:30.  After deleting the nodes and removing the problematic components cloud platform support team have not observed any further Inginx node restarts.  The ASO inbound queue was released and all the 200 GRs in the ASO inbound queue got processed out of ASO by 22:10. GIST was informed to advise depots to go back to live allocation and all depots except Bristol and Crewe went back to live allocation by 22:41.

Tuesday 28th January
No performance issues reported from any cloud based applications. Applications remains stable.  

Wednesday 29th January 
No performance issues reported from any cloud based applications
",,29/01/2020,29/01/2020,,,,,,,,Code/Product bug,," 
The two problematic Kubernetes nodes which were believed to be impacting the performance of the whole cluster were deleted after which the random restarts of the Nginx ingress pods stopped and the applications remained stable.  The core DNS pods on the Kubernetes cluster were also scaled up from 1 to 3 to improve the communication between the different application pods.",,,,30/01/2020,January,2020,,Closed,,,2020,1,January,9/16/2025,2062,>60
23/01/2020,23/01/2020,87521097,,,,,Platform & Store Ops,,,,MI,Stores reporting tills not accepting AMEX cards  ,Amex,Retail,,POS - Tills,RC identified,49182,"At around 07:50, MIM was made aware of an issue reported by multiple stores that AMEX cards were not being accepted on all the tills at the stores..
Impact:
Inconvenience to the customers who wanted to pay via AMEX cards and they had to use an alternative method of payment. Other credit cards were working fine.

Recovery Actions: 
Support teams confirmed that the issue was not at the Customer end and hence the third party vendors (TNS & AMEX) were engaged. Further investigations revealed that there was a gateway issue between TNS and AMEX.
The gateway was isolated by TNS and Amex from use. Since 11:54, approximately 1000 successful AMEX transactions in all Customer stores were observed and remained stable since then.
","All the action & recommendations has been closed
---------
Requested Amex for the next action updates
-------
13/03 - Shared the final RCA report from AMEX
-------------
03/03 - Updated from Nikki (Amex): Awaiting some final input from our American Express Technologies team in the US re action taken and clear recommendations for Marks and Spencer going forward.- Next update this Week
-------------
• A change was made by Amex to their authorization method on their Global Authorization System.  Due to this change extra bytes were being sent on the authorization message from Amex which caused payment failures.  The change was reverted by Amex to restore the services.
• This was supposed to be an internal change within Amex, however, it went live on their production system.
• TNS also made changes at the same time as Amex - This change was a BAU change, not related to Amex (was just a coincidence).  Due to this confusion, it took longer than expected to resolve the issue.
• Amex confirmed this did not impact merchants / customers globally, however, only small number of merchants were impacted.  This might be due to some parameters hard-coded in their specific environment.
• Amex to share a full RCA document by Tuesday (3rd March) and also provide a handbook which outlines the process to raise tickets with their 24x7 Service Desk directly along with escalation contacts for a quicker turnaround.
• Amex to share the transaction traces and work with our RAS Payments testing team to understand any settings that are hard-coded and identify the difference between the till payments which were rejected and the .com payments that were successful during the outage.

------------
02/12 - RCA meeting scheduled with Amex Support on 27/02
----------
02/11 - Amex will share the details by this wednesday
----------
01/30 - TNS confirmed the RCA will be driven by AMEX as they didn’t find any issue from their end
------------
01/28 - TNS responded stating they were working on this RCA
-----------
Dropped a Note to Rosie to get the detailed RCA from TNS & AMEX",23/01/2020,28/02/2020,,,,,,,,Customer Tech change,," 
A change was made on the American Express Authorisation System, which had the effect of an optional data field (sub field 19 in the auxiliary data, SCA exemption indicator) being incorrectly identified as a mandatory data field.  Marks & Spencer do not support the optional field and therefore do not send the additional data.  This resulted in In App Apple Pay transactions being declined.  Once the error was identified, it was immediately remediated and full acceptance of In App Apple Pay transactions was restored.",,,,30/03/2020,March,2020,,Closed,,,2020,1,January,9/16/2025,2063,>60
23/01/2020,23/01/2020,87521035,,,,,Platform & Store Ops,,,,MI,Tills rebooting while scanning some C&H products ,TCS,Retail,,POS,RC identified,49176,"At 18:55, around 20 stores called into the desk to report issues with the tills randomly crashing and rebooting automatically.
Impact:
Store tills were getting crashed and rebooted while scanning C&H products related to Lingerie departments (T33 and T81)

Recovery Actions: 
At 18:55 22/01, around 20 stores reported that the tills were randomly getting crashed and rebooted automatically. Upon further investigation, Support identified that a few tills across multiple stores were rebooted on scanning some C&H products. The sample UPCs provided by the stores did reveal that all these UPCs were part of the C&H Minor Restructure activity which was carried out on 21/01/2020, for the Lingerie departments (Level 8 UPC changes for T33 and T81). As part of recovery activity, support carried out a complete cache refresh in production and this was applied on to all tills across the Customer estate which fixed the issue.  Stores confirmed the issue resolution.
","01/27 - The DDS Fix patch has been applied in production at 14:00 and post implementation checks are successful.  No further issues reported.
 01/23 -  A DDS patch fix has been provided by PCMS vendor which is currently being tested and will be implemented in production on Monday (27/01).",23/01/2020,24/01/2020,24/01/2020,27/01/2020,27/01/2020,,,,,Customer Tech change,CR," 
A bug in the Beanstore POS Central Release (CRQ115452) causing till reboots while scanning products with any Level 8 UPC changes.",CRQ115452,,,30/01/2020,January,2020,,Closed,,,2020,1,January,9/16/2025,2063,>60
20/01/2020,21/01/2020,87515995,,,,,C&H Commercial Trading,,,,MI,Incorrect sales figures in SSI across all C&H departments ,TCS,EDW,,SSI,RC identified,49164,"The business users reported incorrect (very low) C&H sales figures for the last week in SSI (both old and new).
Impact:
The business users were unable to perform sales reporting and planning due to incorrect C&H sales figures in SSI.

Recovery Actions: 
The issue occurred due to an EDW job failure on Sunday, 19/01 in the SSI Weekly batch. The sales job that failed processed partial data from GMOR and sent to the downstream systems - Range Planner, ESSI and New SSI.  An outage was agreed with the business for Range Planner and the SSI applications from 17:00 (20/01) after which the weekly sales data was reprocessed from GMOR into Range Planner.
Overnight Monday - The weekly sales data from Range Planner was successfully loaded into New SSI.  The SSI weekly batch was re-ran to load the data from RP into Old SSI.  The Business validated the sale figures and confirmed complete service restoration.
","01/30 -Time out parameter of the EDW job has been increased from 90 to 300 seconds in production last night. No further issues observed on Sunday (02/02).

Rest of the remediation steps from DS end, pertaining to the partial data load, will be investigated further once the above change is tested and deployed in prod which will address the original failure issue.

----------------
Meanwhile SAP team have confirmed the this was not an issue with the availability of SAP BW work processes, so SAP team investigating on actual root cause of this issue
-------------------
01/24 - Update from Mizra - • Timeout parameter increase:
IBM has comeback with the remedial steps (via case TS003260347) and currently team are trying to implement them in lower env. 

Rest of the remediation steps from DS end, pertaining to the partial data load, will be investigated further once the above change is tested and deployed in prod which will address the original failure issue.
The relevant SOP (PFA) has been circulated which team will follow in case of similar failure on Sunday (26/01). 

Next update: 30/01
 

Next update: 30/01

--------------------
RC of original DS failure:
As per existing DS design, job will wait for 90 seconds to receive the response from BW while starting a new process-chain. The response time on 5th Jan & 19th Jan exceeded 90 seconds (109 seconds & 102 seconds respectively) due to unavailability of the BW system processes, and eventually the DS job had failed at the first place. However, at the backend the BW process-chain had successfully been submitted and was in progress. 
 
Mitigation:
Increase the timeout parameter from 90 seconds to 300 seconds in DS. This looks to be an environmental variable at server level and will touch base with DS admin /Abdul on the this in order to get this increased. (ETA:24/01)
 
RC of partial data load post restart:
There were total 140 packets of data on 19/01 (~7m records) to be processed. When the job was restarted @ ~ 02:04, 69 packets already processed at the backend through 1st process chain without any receiver at DS end as the BW? DS connection was disconnected at 01:41 at the time of original failure. The DS job @2:04 started accumulating the data from packet#70 onwards and finished processing packet#140 (~3.5m of records)
 
Mitigation: 
1.     When the DS job is failing, force the process chain off in BW – Need to do feasibility check (ETA: 24/01)
2.     If the point#1 not feasible, then force abort the DS job if there is an active process chain already running in BW 
3.     We found an error ('Message on the 2nd process chain failure') during DS execution but it came as a warning which didn’t force the job off  - will check the feasibility of changing the 'Warning' to 'Fatal' in DS so that it force the job to fail instead of silently processing partial data
 
Precautionary measures:  
We would try to automate the record count check between BW OHD and DS output file in order to eliminate any need of human intervention. 
",21/01/2020,24/01/2020,24/01/2020,27/01/2020,27/01/2020,,,,,Design issue,," 
The EDW sales job failed with a time out error due to no response from the SAP BW system within the defined 90 second time limit.  Once the job was re-ran, partial data from the SAP BW process chain was captured and sent to SSI.

SAP Basis team confirmed that this is a very rare occurrence where they BW system might commit the data after sometime (not necessarily within the 90 second limit).  This is confirmed to be a normal behaviour within SAP BW systems as the backend SAP BW process chain was running and completed successfully

RC of partial data load post restart:
There were total 140 packets of data on 19/01 (~7m records) to be processed. When the job was restarted @ ~ 02:04, 69 packets already processed at the backend through 1st process chain without any receiver at DS end as the BW? DS connection was disconnected at 01:41 at the time of original failure. The DS job @2:04 started accumulating the data from packet#70 onwards and finished processing packet#140 (~3.5m of records

  ",,,,04/02/2020,February,2020,,Closed,,,2020,1,January,9/16/2025,2066,>60
17/01/2020,17/01/2020,87513124,,,,,Group Technology Services,,,,SI,Remote refrigeration monitoring unavailable for 240 stores ,Vodafone,Network,,Fridge Monitoring,RC identified,49153,"At around 02:27, City FM reported that they were unable to monitor the refrigeration systems across 240 stores since 22:30 on 16/01.
Impact:
City FM were unable to monitor the refrigeration systems across 240 stores from 22:30 (16/01) until 06:30 (17/01).

Recovery Actions: 
Network support validated and confirmed that the routes were not flowing from City FM to Digital Perimeter after a Vodafone routing change (CRQ116573) that was implemented on 16/01 after business hours. Vodafone identified that one of the subnet IPs was keyed-in incorrectly in the pre-fix list filter during change implementation and this was corrected to resolve the issue at 06:10 (17/01). Network Support along with City FM confirmed complete service restoration.","
The process document for logging issues with Service Desk has been created, agreed and shared with City FM (who monitor the refrigeration systems in our stores).

Post confirmation check was not done, yet to check with Paul. - VF was iterated to ensure all th post confirmation checks were done with all parties
Customer and Laura has to create process doc to reach SD
",17/01/2020,17/01/2020,,,,,,,,Human error - Customer Tech,CR," 
One of the subnet IPs was keyed-in incorrectly during the change implementation - Human Error",116573,,,30/01/2020,January,2020,,Closed,,,2020,1,January,9/16/2025,2069,>60
09/01/2020,09/01/2020,87502983,,,,,C&H & Intl Supply Chain,,,,SI,Three C&H DCs unable to print despatch manifest reports ,TCS,IS - Wintel,,C&H DC Printer,RC identified,49018,"At around 09:30, MIM was made aware that Long Eaton DC was unable to print the despatch manifest reports on their A4 printers.? Label printers were not affected. 
Impact:
Long Eaton,?Hemel and?West Thurrock DCs were unable to print the despatch manifest reports (Stock In Transit Reports) during the outage impacting trailer despatch and store stock delivery. 

Recovery Actions: 
Support identified that one out of the four DC Print Servers was unresponsive due to high memory utilisation and hence all the print queues configured on this server were unavailable. The problematic DC Print server was restarted, however, the restart process was slow and did not progress.? The server was powered off and powered back on again which did not resolve the issue. 
? 
The server was then put into safe-mode and Sophos antivirus was disabled.? The server stayed up for around 15 minutes but the Sophos antivirus attempted to restart rendering the server unresponsive again.? In order to mitigate the impact, support started to configure all the critical print queues for the impacted DCs on a resilient Print Server.? In parallel, the problematic DC Print Server was restarted again and the server was stable from 12 PM and no further issues reported or observed. 
? 
All the critical print queues for the impacted DCs have now been configured on a resilient Print Server.? Should we have an issue again with this DC Print Server, users will be advised to use the Print Queues on the resilient Print Server. 
","
Next Steps:
• To set up monitoring for high Page file utilization on an optimum threshold limit - SCOM Team - Closed
• To perform OS refresh of these print servers from legacy Win 2008 ( Win 2008 & Win 2008 R2 Extended support end date – 14th Jan 2020, this means end of regular security updates and no support from Microsoft) - SP&I Governance Team - This is a ongoing activity whereas based on the priority the update will be done to the app teams, incase if any team dosent provide approval for the update then a risk will be raised and owned by the respective team - Windows refresh will be taken care as part of SP&I projects.
• Based on the memory utilisation trend post Windows refresh, frame a plan to reboot during next Peak trading period - Dharma & Team - 01/20 - worked with SCOM and have setup event ID monitoring for low available page file in the server.. Have to test it when such a scenario occurs. But its random and can happen anytime.. - Since we don’t have simulator to induce high paging, we will wait for the paging situation to occur to confirm that alerting works as expected in any of the Windows machine.
Detailed Root cause: 
MSHSRMNSUKP0831 went unresponsive due to high memory (Physical and Virtual - paging) usage since no patching & reboot carried out during peak trading period 
(Last Reboot date: 9th Oct 2019).   Windows 2008 servers’ patch has some issues with Sophos in general which could have slowed down the server boot potentially. 


",09/01/2020,12/01/2020,,,,,,,,Code/Product bug,," 
This was due to high memory utilisation on the affected Print Server. Further investigations on how this issue can be mitigated in future is being carried out. ",,,1405,22/01/2020,January,2020,,Closed,,,2020,1,January,9/16/2025,2077,>60
08/01/2020,08/01/2020,87501182,,,,,Platform & Store Ops,,,,SI,Stores unable to carry out RTW (Return To Warehouse),TCS,IDC,,RTW,RC identified,49105,"Stores reported that they were unable to carry out RTW (return to warehouse). Users were getting an error message "" Return Route information is not available"". 
Impact:
Stock in stores that was destined to be returned to the warehouse could not be scanned during the outage and hence delaying the process. 

Recovery Actions: 
This issue occurred during the OS AIX patching on the SAGG production servers. The impact to “RTW” should have been called out but unfortunately this was not communicated. ",This return route impact should be included in the impact statement and CSSM Team should be informed about this from next outage,08/01/2020,08/01/2020,,,,,,,,Customer Tech change,CR," 
This was caused due to a planned 'OS AIX patching’ on the SAGG production servers due to which Stores were unable to perform their RTWs after the Peak trading period.",115502,,,08/01/2020,January,2020,,Closed,,,2020,1,January,9/16/2025,2078,>60
